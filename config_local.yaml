project: "nova"

files:
  pot_dir: "./pot"
  po_dir: "./po"

  origin_po: "./data/{project}/{lang}/LC_MESSAGES/{project}.po"
  target_pot: "./pot/{project}.pot"
  
  ai_po: "./po/{model}/{lang}/{project}.po"
  merged_po: "./merged/{model}/{lang}/LC_MESSAGES/{project}.po"  

# -----------------------------
# translate.py
# -----------------------------
model: "llama3.2:3b"

languages:
  - "ko_KR"

# You can tune these arguments for performance / partial translation:
#   --mode  : Choose your LLM mode[`ollama` (default), `gpt`, `claude`, `gemini`]
#   --workers   : number of parallel threads (default: 1)
#   --start/end : entry index range to translate (default: 0 ~ all)
#   --batch-size: entries per LLM call (default: 5)
llm:
  mode: "ollama"
  workers: 1
  start: 0
  end: -1
  batch_size: 5

# -----------------------------
# Glossary
# -----------------------------
glossary:
  dir: "./glossary"
  url: "https://opendev.org/openstack/i18n/raw/commit/129b9de7be12740615d532591792b31566d0972f/glossary/locale/{lang}/LC_MESSAGES/glossary.po"
  po_file: "glossary.po"
  json_file: "glossary.json"

# -----------------------------
# Example
# -----------------------------
examples:
  example_dir: "./po-example"
  example_url: "https://opendev.org/openstack/nova/raw/branch/master/nova/locale/{lang}/LC_MESSAGES/nova.po"
  example_file: "nova-nova-locale.po"
  fixed_example_json: "fixed_examples.json"