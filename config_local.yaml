project: "nova"

files:
  pot_dir: "./pot"
  po_dir: "./po"

  origin_po: "./data/target/{project}/en_AU/LC_MESSAGES/{project}.po" # 기준이되는 po
  origin_trans_po: "./data/target/{project}/{languages}/LC_MESSAGES/{project}.po" # 번역이 일부 된 po
  ai_target_pot: "./pot/{project}.pot" # 넘겨주는 것
  ai_po: "./po/{model}/{languages}/{project}.po" # AI 번역된 po
  merged_po: "./data/result/{model}/{languages}/LC_MESSAGES/{project}.po"  

# -----------------------------
# translate.py
# -----------------------------
model: "llama3.2:3b"

languages:
  - "ko_KR"

# You can tune these arguments for performance / partial translation:
#   --mode  : Choose your LLM mode[`ollama` (default), `gpt`, `claude`, `gemini`]
#   --workers   : number of parallel threads (default: 1)
#   --start/end : entry index range to translate (default: 0 ~ all)
#   --batch-size: entries per LLM call (default: 5)
llm:
  mode: "ollama"
  workers: 1
  start: 0
  end: -1
  batch_size: 5

# -----------------------------
# Glossary
# -----------------------------
glossary:
  dir: "./glossary"
  url: "https://opendev.org/openstack/i18n/raw/commit/129b9de7be12740615d532591792b31566d0972f/glossary/locale/{lang}/LC_MESSAGES/glossary.po"
  po_file: "glossary.po"
  json_file: "glossary.json"

# -----------------------------
# Example
# -----------------------------
examples:
  example_dir: "./po-example"
  example_url: "https://opendev.org/openstack/nova/raw/branch/master/nova/locale/{lang}/LC_MESSAGES/nova.po"
  example_file: "nova-nova-locale.po"
  fixed_example_json: "fixed_examples.json"