#
msgid ""
msgstr ""
"Project-Id-Version: OpenStack Nova Release Notes\n"
"POT-Creation-Date: 2025-02-01 00:00+0000\n"
"PO-Revision-Date: 2025-10-09 22:00+0900\n"
"Last-Translator: Automatically generated\n"
"Language-Team: Korean\n"
"Language: ko_KR\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../<reno.sphinxext stable/2024.1>:80 stable/2024.2>:80 stable/2025.1>:59
#: stable/2025.2>:415
msgid ""
"\"When updating volumeId, this API is typically meant to only be used as "
"part of a larger orchestrated volume migration operation initiated in the "
"block storage service via the os-retype or os-migrate_volume volume actions."
" Direct usage of this API to update volumeId is not recommended and may "
"result in needing to hard reboot the server to update details within the "
"guest such as block storage serial IDs. Furthermore, updating volumeId via "
"this API is only implemented by certain compute drivers.\""
msgstr ""
"\"volumeId를 업데이트하는 경우, 이 API는 일반적으로 블록 스토리지 서비스를 통해 os-retype 또는 os-"
"migrate_volume 볼륨 액션을 통해 시작된 대규모 조직화된 볼륨 이식 작업의 일부로만 사용되어야 합니다. volumeId를 직접"
" 업데이트하는 것은 추천되지 않으며, guest에서 블록 스토리지 시리얼 ID와 같은 세부 사항을 업데이트하는 데 hard reboot을"
" 필요로 할 수 있습니다. 또한, volumeId를 이 API를 통해 업데이트하는 것은 특정 컴퓨터 드라이버에 의해만 구현됩니다.\""

#: ../../<reno.sphinxext origin/stable/ocata>:1077
msgid "**Filtering**"
msgstr "집합"

#: ../../<reno.sphinxext stable/ussuri>:688
msgid "**New Defaults(Admin, Member and Reader)**"
msgstr ""
"**New Defaults(Admin, Member and Reader)**\n"
"\n"
"* New Defaults(Admin, Member and Reader) → **New Defaults(Admin, Member 및 Reader)**"

#: ../../<reno.sphinxext origin/stable/ocata>:1168
msgid "**Other**"
msgstr "**집합**"

#: ../../<reno.sphinxext stable/ussuri>:694
msgid "**Policies granularity**"
msgstr "집합 크기"

#: ../../<reno.sphinxext stable/stein>:662
msgid "**Ports**"
msgstr "**포트**"

#: ../../<reno.sphinxext stable/ussuri>:677
msgid "**Scope**"
msgstr "**집합**"

#: ../../<reno.sphinxext origin/stable/ocata>:1132
msgid "**Sorting**"
msgstr "집합"

#: ../../<reno.sphinxext stable/stein>:656
msgid "**Volumes**"
msgstr "**집합**"

#: ../../<reno.sphinxext stable/rocky>:692
msgid "/etc/nova/placement-policy.yaml"
msgstr "/etc/nova/집합정책정책.yaml"

#: ../../<reno.sphinxext stable/rocky>:691
msgid "/etc/nova/policy.yaml"
msgstr "/etc/nova/policy.yaml"

#: ../../<reno.sphinxext stable/rocky>:698
msgid "/etc/placement/policy.yaml"
msgstr "/etc/placement/policy.yaml"

#: ../../<reno.sphinxext origin/stable/ocata>:506
msgid "15.0.0"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:425
msgid "15.0.1"
msgstr "15.0.1"

#: ../../<reno.sphinxext origin/stable/ocata>:398
msgid "15.0.2"
msgstr "15.0.2"

#: ../../<reno.sphinxext origin/stable/ocata>:368
msgid "15.0.5"
msgstr "15.0.5"

#: ../../<reno.sphinxext origin/stable/ocata>:349
msgid "15.0.7"
msgstr "15.0.7"

#: ../../<reno.sphinxext origin/stable/ocata>:325
msgid "15.0.8"
msgstr "15.0.8"

#: ../../<reno.sphinxext origin/stable/ocata>:240
msgid "15.1.0"
msgstr "15.1.0"

#: ../../<reno.sphinxext origin/stable/ocata>:142
msgid "15.1.1"
msgstr "15.1.1"

#: ../../<reno.sphinxext origin/stable/ocata>:120
msgid "15.1.3"
msgstr "15.1.3"

#: ../../<reno.sphinxext origin/stable/ocata>:89
msgid "15.1.4"
msgstr "15.1.4"

#: ../../<reno.sphinxext origin/stable/ocata>:53
msgid "15.1.5"
msgstr "15.1.5"

#: ../../<reno.sphinxext origin/stable/ocata>:5
msgid "15.1.5-28"
msgstr "15.1.5-28"

#: ../../<reno.sphinxext stable/pike>:575
msgid "16.0.0"
msgstr "16.0.0"

#: ../../<reno.sphinxext stable/pike>:521
msgid "16.0.1"
msgstr "16.0.1"

#: ../../<reno.sphinxext stable/pike>:488
msgid "16.0.2"
msgstr "16.0.2"

#: ../../<reno.sphinxext stable/pike>:444
msgid "16.0.3"
msgstr "16.0.3"

#: ../../<reno.sphinxext stable/pike>:367
msgid "16.0.4"
msgstr "16.0.4"

#: ../../<reno.sphinxext stable/pike>:289
msgid "16.1.0"
msgstr "16.1.0"

#: ../../<reno.sphinxext stable/pike>:266
msgid "16.1.1"
msgstr "16.1.1"

#: ../../<reno.sphinxext stable/pike>:211
msgid "16.1.2"
msgstr "16.1.2"

#: ../../<reno.sphinxext stable/pike>:116
msgid "16.1.5"
msgstr "16.1.5"

#: ../../<reno.sphinxext stable/pike>:97
msgid "16.1.7"
msgstr "16.1.7"

#: ../../<reno.sphinxext stable/pike>:61
msgid "16.1.8"
msgstr "16.1.8"

#: ../../<reno.sphinxext stable/pike>:5
msgid "16.1.8-57"
msgstr "16.1.8-57"

#: ../../<reno.sphinxext stable/queens>:576
msgid "17.0.0"
msgstr "17.0.0"

#: ../../<reno.sphinxext stable/queens>:177
msgid "17.0.10"
msgstr "17.0.10"

#: ../../<reno.sphinxext stable/queens>:125
msgid "17.0.11"
msgstr "17.0.11"

#: ../../<reno.sphinxext stable/queens>:75
msgid "17.0.12"
msgstr "17.0.12"

#: ../../<reno.sphinxext stable/queens>:59
msgid "17.0.13"
msgstr "17.0.13"

#: ../../<reno.sphinxext stable/queens>:5
msgid "17.0.13-73"
msgstr "17.0.13-73"

#: ../../<reno.sphinxext stable/queens>:533
msgid "17.0.2"
msgstr "17.0.2"

#: ../../<reno.sphinxext stable/queens>:501
msgid "17.0.3"
msgstr "17.0.3"

#: ../../<reno.sphinxext stable/queens>:481
msgid "17.0.4"
msgstr "17.0.4"

#: ../../<reno.sphinxext stable/queens>:403
msgid "17.0.5"
msgstr "17.0.5"

#: ../../<reno.sphinxext stable/queens>:258
msgid "17.0.6"
msgstr "17.0.6"

#: ../../<reno.sphinxext stable/queens>:243
msgid "17.0.8"
msgstr "17.0.8"

#: ../../<reno.sphinxext stable/queens>:224
msgid "17.0.9"
msgstr "17.0.9"

#: ../../<reno.sphinxext stable/rocky>:494
msgid "18.0.0"
msgstr "18.0.0"

#: ../../<reno.sphinxext stable/rocky>:479
msgid "18.0.1"
msgstr "18.0.1"

#: ../../<reno.sphinxext stable/rocky>:455
msgid "18.0.3"
msgstr "18.0.3"

#: ../../<reno.sphinxext stable/rocky>:369
msgid "18.1.0"
msgstr "18.1.0"

#: ../../<reno.sphinxext stable/rocky>:269
msgid "18.2.0"
msgstr "18.2.0"

#: ../../<reno.sphinxext stable/rocky>:243
msgid "18.2.1"
msgstr "18.2.1"

#: ../../<reno.sphinxext stable/rocky>:167
msgid "18.2.2"
msgstr "18.2.2"

#: ../../<reno.sphinxext stable/rocky>:149
msgid "18.2.3"
msgstr "18.2.3"

#: ../../<reno.sphinxext stable/rocky>:110
msgid "18.3.0"
msgstr "18.3.0"

#: ../../<reno.sphinxext stable/rocky>:5
msgid "18.3.0-55"
msgstr "18.3.0-55"

#: ../../<reno.sphinxext stable/stein>:396
msgid "19.0.0"
msgstr "19.0.0"

#: ../../<reno.sphinxext stable/stein>:370
msgid "19.0.1"
msgstr "19.0.1"

#: ../../<reno.sphinxext stable/stein>:313
msgid "19.0.2"
msgstr "19.0.2"

#: ../../<reno.sphinxext stable/stein>:226
msgid "19.0.3"
msgstr "19.0.3"

#: ../../<reno.sphinxext stable/stein>:157
msgid "19.1.0"
msgstr "19.1.0"

#: ../../<reno.sphinxext stable/stein>:125
msgid "19.2.0"
msgstr "19.2.0"

#: ../../<reno.sphinxext stable/stein>:86
msgid "19.3.0"
msgstr "19.3.0"

#: ../../<reno.sphinxext stable/stein>:56
msgid "19.3.2"
msgstr "19.3.2"

#: ../../<reno.sphinxext stable/stein>:5
msgid "19.3.2-19"
msgstr "19.3.2-19 → 19.3.2-19"

#: ../../<reno.sphinxext stable/train>:448
msgid "20.0.0"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:383
msgid "20.1.0"
msgstr "20.1.0"

#: ../../<reno.sphinxext stable/train>:346
msgid "20.1.1"
msgstr "20.1.1"

#: ../../<reno.sphinxext stable/train>:295
msgid "20.2.0"
msgstr "20.2.0"

#: ../../<reno.sphinxext stable/train>:274
msgid "20.3.0"
msgstr "20.3.0"

#: ../../<reno.sphinxext stable/train>:256
msgid "20.4.0"
msgstr "20.4.0"

#: ../../<reno.sphinxext stable/train>:207
msgid "20.4.1"
msgstr "20.4.1"

#: ../../<reno.sphinxext stable/train>:146
msgid "20.5.0"
msgstr "20.5.0"

#: ../../<reno.sphinxext stable/train>:126
msgid "20.6.1"
msgstr "20.6.1"

#: ../../<reno.sphinxext stable/train>:5
msgid "20.6.1-41"
msgstr "20.6.1-41 → 20.6.1-41"

#: ../../<reno.sphinxext unmaintained/2023.1>:5
msgid "2023.1-eom-15"
msgstr "2023.1월 15일"

#: ../../<reno.sphinxext stable/2023.2>:5
msgid "2023.2-eol"
msgstr "2023.2-eol"

#: ../../<reno.sphinxext stable/pike>:733
msgid "204 NoContent on success"
msgstr "204 NoContent on success"

#: ../../<reno.sphinxext stable/ussuri>:324
msgid "21.0.0"
msgstr "21.0.0"

#: ../../<reno.sphinxext stable/ussuri>:275
msgid "21.1.0"
msgstr "21.1.0"

#: ../../<reno.sphinxext stable/ussuri>:218
msgid "21.1.1"
msgstr "21.1.1"

#: ../../<reno.sphinxext stable/ussuri>:154
msgid "21.1.2"
msgstr "21.1.2"

#: ../../<reno.sphinxext stable/ussuri>:114
msgid "21.2.0"
msgstr "21.2.0"

#: ../../<reno.sphinxext stable/ussuri>:92
msgid "21.2.2"
msgstr "21.2.2"

#: ../../<reno.sphinxext stable/ussuri>:51
msgid "21.2.3"
msgstr "21.2.3"

#: ../../<reno.sphinxext stable/ussuri>:5
msgid "21.2.4-19"
msgstr "21.2.4-19 → 21.2.4-19"

#: ../../<reno.sphinxext unmaintained/victoria>:310
msgid "22.0.0"
msgstr "22.0.0"

#: ../../<reno.sphinxext unmaintained/victoria>:277
msgid "22.0.1"
msgstr "22.0.1"

#: ../../<reno.sphinxext unmaintained/victoria>:242
msgid "22.1.0"
msgstr "22.1.0"

#: ../../<reno.sphinxext unmaintained/victoria>:208
msgid "22.2.1"
msgstr "22.2.1"

#: ../../<reno.sphinxext unmaintained/victoria>:186
msgid "22.2.2"
msgstr "22.2.2"

#: ../../<reno.sphinxext unmaintained/victoria>:145
msgid "22.3.0"
msgstr "22.3.0"

#: ../../<reno.sphinxext unmaintained/victoria>:116
msgid "22.4.0"
msgstr "22.4.0"

#: ../../<reno.sphinxext unmaintained/victoria>:5
msgid "22.4.0-34"
msgstr "22.4.0-34"

#: ../../<reno.sphinxext unmaintained/wallaby>:375
msgid "23.0.0"
msgstr "23.0.0"

#: ../../<reno.sphinxext unmaintained/wallaby>:305
msgid "23.0.2"
msgstr "23.0.2"

#: ../../<reno.sphinxext unmaintained/wallaby>:246
msgid "23.1.0"
msgstr "23.1.0"

#: ../../<reno.sphinxext unmaintained/wallaby>:189
msgid "23.2.0"
msgstr "23.2.0"

#: ../../<reno.sphinxext unmaintained/wallaby>:157
msgid "23.2.1"
msgstr "23.2.1"

#: ../../<reno.sphinxext unmaintained/wallaby>:103
msgid "23.2.2"
msgstr "23.2.2"

#: ../../<reno.sphinxext unmaintained/wallaby>:5
msgid "23.2.2-38"
msgstr "23.2.2-38"

#: ../../<reno.sphinxext unmaintained/xena>:281
msgid "24.0.0"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/xena>:201
msgid "24.1.0"
msgstr "24.1.0"

#: ../../<reno.sphinxext unmaintained/xena>:149
msgid "24.1.1"
msgstr "24.1.1"

#: ../../<reno.sphinxext unmaintained/xena>:53
msgid "24.2.0"
msgstr "24.2.0"

#: ../../<reno.sphinxext unmaintained/xena>:5
msgid "24.2.1-23"
msgstr "24.2.1-23 → 24.2.1-23"

#: ../../<reno.sphinxext unmaintained/yoga>:254
msgid "25.0.0"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/yoga>:226
msgid "25.0.1"
msgstr "25.0.1"

#: ../../<reno.sphinxext unmaintained/yoga>:121
msgid "25.1.0"
msgstr "25.1.0"

#: ../../<reno.sphinxext unmaintained/yoga>:91
msgid "25.1.1"
msgstr "25.1.1"

#: ../../<reno.sphinxext unmaintained/yoga>:68
msgid "25.2.0"
msgstr "25.2.0"

#: ../../<reno.sphinxext unmaintained/yoga>:51
msgid "25.2.1"
msgstr "25.2.1"

#: ../../<reno.sphinxext unmaintained/yoga>:24
msgid "25.3.0"
msgstr "25.3.0"

#: ../../<reno.sphinxext unmaintained/yoga>:5
msgid "25.3.0-26"
msgstr "25.3.0-26"

#: ../../<reno.sphinxext unmaintained/zed>:177
msgid "26.0.0"
msgstr "26.0.0"

#: ../../<reno.sphinxext unmaintained/zed>:125
msgid "26.1.0"
msgstr "26.1.0"

#: ../../<reno.sphinxext unmaintained/zed>:108
msgid "26.1.1"
msgstr "26.1.1"

#: ../../<reno.sphinxext unmaintained/zed>:85
msgid "26.2.0"
msgstr "26.2.0"

#: ../../<reno.sphinxext unmaintained/zed>:47
msgid "26.2.1"
msgstr "26.2.1"

#: ../../<reno.sphinxext unmaintained/zed>:28
msgid "26.3.0"
msgstr "26.3.0"

#: ../../<reno.sphinxext unmaintained/zed>:5
msgid "26.3.0-27"
msgstr "26.3.0-27"

#: ../../<reno.sphinxext unmaintained/2023.1>:259
msgid "27.0.0"
msgstr "27.0.0"

#: ../../<reno.sphinxext unmaintained/2023.1>:236
msgid "27.1.0"
msgstr "27.1.0"

#: ../../<reno.sphinxext unmaintained/2023.1>:172
msgid "27.2.0"
msgstr "27.2.0"

#: ../../<reno.sphinxext unmaintained/2023.1>:129
msgid "27.3.0"
msgstr "27.3.0"

#: ../../<reno.sphinxext unmaintained/2023.1>:114
msgid "27.4.0"
msgstr "27.4.0"

#: ../../<reno.sphinxext unmaintained/2023.1>:74
msgid "27.5.1"
msgstr "27.5.1"

#: ../../<reno.sphinxext stable/2023.2>:168
msgid "28.0.0"
msgstr "28.0.0"

#: ../../<reno.sphinxext stable/2023.2>:131
msgid "28.0.1"
msgstr "28.0.1"

#: ../../<reno.sphinxext stable/2023.2>:95
msgid "28.1.0"
msgstr "28.1.0"

#: ../../<reno.sphinxext stable/2023.2>:80
msgid "28.2.0"
msgstr "28.2.0"

#: ../../<reno.sphinxext stable/2024.1>:285
msgid "29.0.1"
msgstr "29.0.1"

#: ../../<reno.sphinxext stable/2024.1>:262
msgid "29.0.2"
msgstr "29.0.2"

#: ../../<reno.sphinxext stable/2024.1>:173
msgid "29.2.1"
msgstr "29.2.1"

#: ../../<reno.sphinxext stable/2024.1>:135
msgid "29.2.2"
msgstr "29.2.2"

#: ../../<reno.sphinxext stable/2024.1>:67
msgid "29.3.0"
msgstr "29.3.0"

#: ../../<reno.sphinxext stable/2024.1>:5
msgid "29.3.0-9"
msgstr "29.3.0-9"

#: ../../<reno.sphinxext stable/2024.2>:211
msgid "30.0.0"
msgstr "집합"

#: ../../<reno.sphinxext stable/2024.2>:67
msgid "30.1.0"
msgstr "30.1.0"

#: ../../<reno.sphinxext stable/2024.2>:5
msgid "30.1.0-12"
msgstr "30.1.0-12"

#: ../../<reno.sphinxext stable/2025.1>:163
msgid "31.0.0"
msgstr "31.0.0"

#: ../../<reno.sphinxext stable/2025.1>:111
msgid "31.0.1"
msgstr "31.0.1"

#: ../../<reno.sphinxext stable/2025.1>:46
msgid "31.1.0"
msgstr "31.1.0"

#: ../../<reno.sphinxext stable/2025.1>:5
msgid "31.1.0-14"
msgstr "31.1.0-14"

#: ../../<reno.sphinxext stable/2025.2>:5
msgid "32.0.0"
msgstr "32.0.0"

#: ../../<reno.sphinxext branch>:5 current
msgid "32.0.0-14"
msgstr "32.0.0-14"

#: ../../<reno.sphinxext stable/train>:604
msgid "400 for unknown param for query param and for request body."
msgstr ""
"400 for unknown param for query param and for request body.\n"
"\n"
"* unknown param : 집합\n"
"* param : 매개변수\n"
"* query param : URL 매개변수\n"
"* request body : 요청 바디"

#: ../../<reno.sphinxext stable/pike>:734
msgid "404 NotFound for missing resource provider"
msgstr ""
"404 NotFound for missing resource provider → 404 NotFound for missing "
"resource provider"

#: ../../<reno.sphinxext stable/pike>:735
msgid "405 MethodNotAllowed if a microversion is specified that is before"
msgstr "405 MethodNotAllowed (if a microversion is specified that is before )"

#: ../../<reno.sphinxext stable/pike>:737
msgid "409 Conflict if inventory in use or if some other request concurrently"
msgstr "집합 conflict if inventory in use or if some other request concurrently"

#: ../../<reno.sphinxext unmaintained/2023.1>:482
msgid ""
"A Nova workaround option of ``enable_qemu_monitor_announce_self`` was added "
"to fix `bug 1815989`_ which when enabled would interact with the QEMU "
"monitor and force a VM to announce itself."
msgstr ""
"Nova의 workaround 옵션으로 `enable_qemu_monitor_announce_self`를 추가하여 `bug "
"1815989`_을修正했습니다. 이 옵션을 활성화하면 QEMU 모니터와 상호 작용하여 VM이 자신을 발표합니다."

#: ../../<reno.sphinxext stable/rocky>:135 stable/stein>:212
#: stable/train>:1435
msgid ""
"A ``--dry-run`` option has been added to the ``nova-manage placement "
"heal_allocations`` CLI which allows running the command to get output "
"without committing any changes to placement."
msgstr ""
"``--dry-run`` 옵션은 ``nova-manage placement heal_allocations`` CLI에 추가되어, 배치에 "
"변경 사항을.commit하지 않고, 명령을 실행하여 출력을 얻는 것을 허용합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:684
msgid ""
"A ``--force`` flag is provided to skip the above checks but caution should "
"be taken as this could easily lead to the underlying ABI of the instance "
"changing when moving between machine types."
msgstr ""
"``--force`` 플래그가 제공되며 위의 확인을 skipping할 수 있지만 주의가 필요하다. 이는 인스턴스 간의 기계 타입을 이동할"
" 때 하위 ABI가 쉽게 바뀌는 것을 방지하기 위해 사용할 수 있다."

#: ../../<reno.sphinxext unmaintained/xena>:364
msgid ""
"A ``--sleep`` option has been added to the ``nova-manage db "
"archive_deleted_rows`` CLI. When this command is run with the ``--until-"
"complete`` option, the process will archive rows in batches in a tight loop,"
" which can cause problems in busy environments where the aggressive "
"archiving interferes with other requests trying to write to the database. "
"The ``--sleep`` option can be used to specify a time to sleep between "
"batches of rows while archiving with ``--until-complete``, allowing the "
"process to be throttled."
msgstr ""
"``--sleep`` 옵션은 nova-manage db archive_deleted_rows CLI에 추가되었다. 이 명령은 "
"``--until-complete`` 옵션과 함께 실행되면, 프로세스는 tight loop에서 행을 배치로 아카이브할 수 있으며, "
"busy 환경에서 아카이브가 다른 데이터베이스에 쓰기 시도하는 다른 요청과 충돌할 수 있다. ``--sleep`` 옵션은 "
"``--until-complete`` 옵션과 함께 아카이브할 때 행의 배치 사이에 시간을 잠시 지속할 수 있게 해주어, 프로세스를 제한할"
" 수 있다."

#: ../../<reno.sphinxext unmaintained/xena>:375
msgid ""
"A ``--task-log`` option has been added to the ``nova-manage db "
"archive_deleted_rows`` CLI. When ``--task-log`` is specified, ``task_log`` "
"table records will be archived while archiving the database. The ``--task-"
"log`` option works in conjunction with ``--before`` if operators desire "
"archiving only records that are older than ``<date>``. The ``updated_at`` "
"field is used by ``--task-log --before <date>`` to determine the age of a "
"``task_log`` record for archival."
msgstr ""
"``--task-log`` 옵션은 nova-manage db archive_deleted_rows CLI에 추가되었다. ``--task-"
"log`` 옵션을 지정하면 데이터베이스를 아카이브할 때 ``task_log`` 테이블 레코드를 아카이브한다. ``--task-log`` "
"옵션은 ``--before`` 옵션과 함께 사용되어 ``<date>``보다 오래된 레코드만 아카이브하고자 하는 경우에 사용된다. "
"``--task-log --before <date>`` 옵션은 ``task_log`` 레코드의อาย기를 결정하기 위해 "
"``updated_at`` 필드를 사용한다."

#: ../../<reno.sphinxext unmaintained/wallaby>:486
msgid ""
"A ``[compute]image_type_exclusion_list`` configuration option was added to "
"remove supported image types from being advertised by a compute node as "
"supported. This is to be used in conjunction with "
"``[scheduler]query_placement_for_image_type_support`` to prevent instances "
"from booting on a compute node with a given image type, even if the "
"underlying hypervisor supports it."
msgstr ""
"``[compute]image_type_exclusion_list`` 설정 옵션은 지원되는 이미지 타입을 제외하는 것을 위해 컴퓨터 "
"노드가 지원되는 이미지 타입을 광고하는 것을 제거합니다. 이 설정은 "
"``[scheduler]query_placement_for_image_type_support``와 함께 사용하여 컴퓨터 노드에 특정 "
"이미지 타입을 사용하여 인스턴스를 부팅하는 것을 방지하기 위해 사용됩니다."

#: ../../<reno.sphinxext stable/pike>:1063
msgid ""
"A ``default_floating_pool`` configuration option has been added in the "
"``[neutron]`` group. The existing ``default_floating_pool`` option in the "
"``[DEFAULT]`` group is retained and should be used by nova-network users. "
"Neutron users meanwhile should migrate to the new option."
msgstr ""
"``default_floating_pool`` 설정 옵션은 ``[neutron]`` 그룹에 추가되었으며, ``[DEFAULT]`` 그룹의"
" ``default_floating_pool`` 옵션은 유지되어 nova-network 사용자에 의해 사용되어야 합니다. "
"meanwhile neutron 사용자들은 새로운 옵션으로 이주해야 합니다."

#: ../../<reno.sphinxext stable/rocky>:526
msgid ""
"A ``nova-manage db purge`` command to `purge archived shadow table data`_ is"
" now available. A new ``--purge`` option is also available for the ``nova-"
"manage db archive_deleted_rows`` command."
msgstr ""
"``nova-manage db purge`` 명령을 `purge archived shadow table data`_로 사용할 수 "
"있습니다. 새로운 ``--purge`` 옵션도 `nova-manage db archive_deleted_rows` 명령에 추가되었습니다."

#: ../../<reno.sphinxext stable/rocky>:541
msgid ""
"A ``nova-manage placement heal_allocations`` command is now available to "
"allow users of the CachingScheduler to get the placement service populated "
"for their eventual migration to the FilterScheduler. The CachingScheduler is"
" deprecated and could be removed as early as Stein."
msgstr ""
"``nova-manage placement heal_allocations`` 명령은 캐싱 스케줄러 사용자들이 필터 스케줄러로 "
"eventual migration 할 때까지 배치 서비스가 populate되도록 허용하는 것을 허용합니다. 캐싱 스케줄러는 Stein에서"
" 최소화 될 수 있으며, Stein에서 최소화 될 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1038
msgid ""
"A ``nova-manage placement sync_aggregates`` command has been added which can"
" be used to mirror nova host aggregates to resource provider aggregates in "
"the placement service. This is a useful tool if you are using aggregates in "
"placement to optimize scheduling:"
msgstr ""
"``nova-manage placement sync_aggregates`` 명령은 nova 호스트 집합을 리소스 제공자 집합에 반영하는 "
"placement 서비스에 사용할 수 있는 명령이 추가되었다. 이는 scheduling을 최적화하기 위해 aggregates를 "
"placement에서 사용하는 경우에 유용한 도구이다."

#: ../../<reno.sphinxext stable/queens>:252 stable/rocky>:488
#: stable/stein>:1366
msgid ""
"A change has been introduced in the libvirt driver to correctly handle IPv6 "
"addresses for live migration."
msgstr ""
"IPv6 주소에 대한 live migration을 correctly 처리하기 위해 libvirt 드라이버에 변경이 도입되었습니다."

#: ../../<reno.sphinxext stable/train>:1165
msgid ""
"A check for the use of the ``nova-consoleauth`` service, added to the "
"``nova-status upgrade check`` CLI in Rocky, is now removed."
msgstr ""
"nova-consoleauth 서비스를 사용하는 확인을 nova-status upgrade check CLI에서 추가한 "
"로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status upgrade check CLI에서 "
"추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status upgrade check "
"CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status upgrade "
"check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status "
"upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-"
"status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 "
"nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 "
"확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스"
" 사용 확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth"
" 서비스 사용 확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-"
"consoleauth 서비스 사용 확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now "
"nova-consoleauth 서비스 사용 확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 "
"now nova-consoleauth 서비스 사용 확인을 nova-status upgrade check CLI에서 추가한 "
"로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status upgrade check CLI에서 "
"추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status upgrade check "
"CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status upgrade "
"check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-status "
"upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 nova-"
"status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 확인을 "
"nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스 사용 "
"확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth 서비스"
" 사용 확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-consoleauth"
" 서비스 사용 확인을 nova-status upgrade check CLI에서 추가한 로키(Rocky)에서 now nova-"
"consoleauth 서비스"

#: ../../<reno.sphinxext unmaintained/yoga>:703
msgid ""
"A config option ``[workarounds]unified_limits_count_pcpu_as_vcpu`` is "
"available for operators who require the legacy quota usage behavior where "
"VCPU = VCPU + PCPU. Note that if ``PCPU`` is specified in the flavor "
"explicitly, it will be expected to have its own unified limit registered and"
" PCPU usage will *not* be merged into VCPU usage."
msgstr ""
"``[workarounds]unified_limits_count_pcpu_as_vcpu`` 속성은 legacy quota 사용 "
"behavior를 사용하고자 하는 운영자에게 사용할 수 있는 옵션입니다. VCPU = VCPU + PCPU를 사용하는 legacy "
"behavior를 사용하고자 하는 경우 이 속성이 사용됩니다. 그러나 ``PCPU``가 특정하게 명시된 경우, PCPU 사용은 VCPU "
"사용에 합쳐지지 않으며, PCPU 사용을 VCPU 사용과 분리하여 riêng한 제한이 등록되어야 합니다."

#: ../../<reno.sphinxext stable/2023.2>:226
msgid ""
"A couple of other improvements target reducing the number of bugs we have, "
"one `checking at reboot if stale volume attachments still reside "
"<https://specs.openstack.org/openstack/nova-"
"specs/specs/2023.2/approved/cleanup-dangling-volume-attachments.html>`_ and "
"another one ensuring a `strict linkage between a compute, a service and the "
"instances it runs <https://specs.openstack.org/openstack/nova-"
"specs/specs/2023.2/approved/compute-object-ids.html>`_."
msgstr ""
"다른 몇 가지 개선 사항이 버그의 수를 줄이기 위해 목표로 설정되었습니다. 하나는 재부팅 시 오래된 볼륨 액세서리를 여전히 존재하는지 "
"확인하는 것이고, 다른 하나는 컴퓨터, 서비스, 그리고 그것이 실행되는 인스턴스의 강한 연결을 보장하는 것입니다. "
"<https://specs.openstack.org/openstack/nova-"
"specs/specs/2023.2/approved/cleanup-dangling-volume-attachments.html> "
"<https://specs.openstack.org/openstack/nova-"
"specs/specs/2023.2/approved/compute-object-ids.html>"

#: ../../<reno.sphinxext stable/2024.1>:365
msgid ""
"A couple of other improvements target reducing the number of bugs we have: "
"one automatically detecting the maximum number of instances with memory "
"encryption which can run concurrently, another one allows specifying a "
"specific IP address or hostname for incoming move operations (by setting "
"``[libvirt]/migration_inbound_addr``) and yet another one that improves "
"stability of block device management using libvirt device aliases."
msgstr ""
"다른 몇 가지 개선 사항이 버그의 수를 줄이는 것을 목표로 한다. 하나는 메모리 암호화가 가능한 인스턴스의 최대 수를 tự động "
"감지하고, 다른 하나는 incoming move 연산을 위해 특정 IP 주소 또는 호스트 이름을 지정할 수 있게 한다. "
"(``[libvirt]/migration_inbound_addr``를 설정하여) 마지막으로, libvirt 장치 별명이 사용되는 블록 "
"장치 관리의 안정성을 개선한다."

#: ../../<reno.sphinxext stable/2024.2>:255
msgid ""
"A couple of other improvements target reducing the number of bugs we have: "
"one is changing how the Ironic driver sends metadata to the Ironic API, and "
"another one created a new ``nova.wsgi`` WSGI module that allows different "
"WSGI servers to set their WSGI application the same way (using module "
"instead of a binary)."
msgstr ""
"다른 몇 가지 개선 사항이 버그의 수를 줄이는 것을 목표로 한다. 하나는 Ironic 드라이버가 Ironic API에 메타데이터를 보냈을"
" 때 방법을 변경하는 것이고, 다른 하나는 nova.wsgi WSGI 모듈을 새로 만들어서 WSGI 서버가 WSGI 애플리케이션을 동일한"
" 방식으로 설정할 수 있도록 하는 것이다. WSGI 애플리케이션을 모듈로 설정하는 대신 비니터리 형태로 설정하는 것이다."

#: ../../<reno.sphinxext stable/pike>:661
msgid "A few examples of versioned notifications that use InstancePayload:"
msgstr "few examples of versioned notifications that use InstancePayload: 집합"

#: ../../<reno.sphinxext unmaintained/xena>:487
msgid ""
"A few of the APIs return code was not consistent for the operations/ "
"features not implemented or supported. It was returned as 403, 400, or 409 "
"(for Operation Not Supported For SEV , Operation Not Supported For VTPM "
"cases). Now we have made it consistent and return 400 always when any "
"operations/features are not implemented or supported."
msgstr ""
"API 중 일부의 반환 코드가 비 implement 또는 지원되지 않은 연산/기능에 대한 경우에 대한 일관성이 없었습니다. 403, "
"400 또는 409 (Operation Not Supported For SEV, Operation Not Supported For "
"VTPM의 경우)로 반환되었습니다. 현재는 일관성이 있게 만들었고, 비 implement 또는 지원되지 않은 연산/기능이ใด든지 400를"
" always 반환합니다."

#: ../../<reno.sphinxext stable/2025.2>:167
msgid ""
"A few of the Nova APIs are meant only for use by other Openstack services. "
"Those APIs are not supposed to be used by any non-service users (even "
"admins) because they can make deployment or resources in unwanted state. To "
"restrict the usage of those APIs by users, Nova now defaults those APIs to a"
" policy rule of the ``service`` role. This will make sure they are allowed "
"to be used by the OpenStack services only."
msgstr ""
"nova API 중 일부는 다른 openstack 서비스를 위한만 사용되어야 하는데, 그 API는 비 서비스 사용자(또한 관리자도 "
"포함)에게 사용할 수 없도록 설계되어 있기 때문이다. 비대상적인 상태로 배포 또는 자원들을 만들 수 있기 때문이다. 이러한 API의 "
"사용을 제한하기 위해, nova는 현재 비대상적인 사용자에게는 API를 사용할 수 있는 권한이 없도록 \"service\" 역할의 정책 "
"규칙으로 기본적으로 API를 설정하고 있다. 이로 인해 openstack 서비스만이 API를 사용할 수 있게된다."

#: ../../<reno.sphinxext stable/train>:373 stable/ussuri>:1246
msgid ""
"A fix for serious `bug 1862205`_ is provided which addresses both the "
"performance aspect of schema migration 399, as well as the potential fallout"
" for cases where this migration silently fails and leaves large numbers of "
"instances hidden from view from the API."
msgstr ""
"`bug 1862205`_에 대한 심각한 고정점이 제공되며, 이 고정점은 스키마 이민화 399의 성능 측면을 address하고, 이민화가"
" 무시되어 API의 시각에서 많은 인스턴스를 감추는 경우의潜在 fallout를 address한다."

#: ../../<reno.sphinxext stable/queens>:1670
msgid ""
"A fix is made for `bug 1482040`_ where a request to rebuild a volume-backed "
"server with a new image which is different than what is in the root volume "
"will now fail with a `400 Bad Request` response. The compute API would "
"previously return a `202 Accepted` response but the backend compute service "
"does not replace the image in the root disk so the API behavior was always "
"wrong and is now explicit about the failure."
msgstr ""
"`bug 1482040`_에 대한fix가 `bug 1482040`_에서 요청을 재건하는 볼륨-backed 서버에 새로운 이미지로 바꾸는 "
"것을 요청할 때 400 Bad Request 응답을 받게되었습니다. previously compute API는 202 "
"Accepted응답을 반환했지만 백엔드 컴퓨터 서비스는 루트 디스크에 이미지의 교체를 수행하지 않아 API 행동이 항상 오류였고 now는"
" 실패를 명확히합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:909
msgid ""
"A list of valid vif models is extended for Virtuozzo hypervisor "
"(virt_type=parallels) with VIRTIO, RTL8139 and E1000 models."
msgstr ""
"virt_type=parallels의 Virtuozzo 하이퍼바이저에 대해 유효한 VIF 모델 목록이 확장되어 VIRTIO, "
"RTL8139 및 E1000 모델이 포함됩니다."

#: ../../<reno.sphinxext stable/train>:831
msgid ""
"A mandatory scheduling pre-filter has been added which will exclude disabled"
" compute nodes where the related ``nova-compute`` service status is mirrored"
" with a ``COMPUTE_STATUS_DISABLED`` trait on the compute node resource "
"provider(s) for that service in Placement. See the `admin scheduler "
"configuration docs`__ for details."
msgstr ""
"다음은 원문입니다.\n"
"\n"
"`nova-compute` 서비스의 상태가 `COMPUTE_STATUS_DISABLED` 트레이트에 매핑되는 컴퓨터 노드 리소스 프로바이더(s)에서만 활성화된 컴퓨터 노드에서 제외되도록 강제적으로 스케줄링 전 필터가 추가되었습니다.  자세한 내용은 `admin scheduler configuration docs`__에 참조하십시오."

#: ../../<reno.sphinxext stable/pike>:851
msgid ""
"A new 1.10 API microversion is added to the Placement REST API. This "
"microversion adds support for the GET /allocation_candidates resource "
"endpoint. This endpoint returns information about possible allocation "
"requests that callers can make which meet a set of resource constraints "
"supplied as query string parameters. Also returned is some inventory and "
"capacity information for the resource providers involved in the allocation "
"candidates."
msgstr ""
"새로운 1.10 API 마이크로 버전이 배치 REST API에 추가된다. 이 마이크로 버전은 GET "
"/allocation_candidates 리소스 엔드포인트에 지원을 추가한다. 이 엔드포인트는 호출자가 제공하는 리소스 제한 조건에 "
"따라서 할당 요청을 할 수 있는 가능성에 대한 정보를 반환한다. 또한 할당 후보자에 참여한 리소스 제공자에 대한 일부 인벤톤과 용량 "
"정보도 반환된다."

#: ../../<reno.sphinxext stable/queens>:926
msgid ""
"A new 1.11 API microversion is added to the Placement REST API.  This adds "
"the ``resource_providers/{rp_uuid}/allocations`` link to the  ``links`` "
"section of the response from ``GET /resource_providers``."
msgstr ""
"새로운 1.11 API 마이크로 버전이 배치 REST API에 추가되었습니다.  이에 따라 "
"``resource_providers/{rp_uuid}/allocations`` 링크가 ``GET "
"/resource_providers``의 응답에서 ``links`` 섹션의 ``links`` 부분에 추가됩니다."

#: ../../<reno.sphinxext stable/rocky>:969
msgid ""
"A new 1.24 placement API microversion adds the ability to specify multiple "
"`member_of` query parameters for the `GET /resource_providers` and `GET "
"allocation_candidates` endpoints. When multiple `member_of` query parameters"
" are received, the placement service will return resource providers that "
"match all of the requested aggregate memberships. The `member_of=in:<agg "
"uuids>` format is still supported and continues to indicate an IN() "
"operation for aggregate membership. Some examples for using the new "
"functionality: Get all providers that are associated with BOTH agg1 and "
"agg2: ?member_of=agg1&member_of=agg2 Get all providers that are associated "
"with agg1 OR agg2: ?member_of=in:agg1,agg2 Get all providers that are "
"associated with agg1 and ANY OF (agg2, agg3): "
"?member_of=agg1&member_of=in:agg2,agg3 Get all providers that are associated"
" with ANY OF (agg1, agg2) AND are also associated with ANY OF (agg3, agg4): "
"?member_of=in:agg1,agg2&member_of=in:agg3,agg4"
msgstr ""
"다음은 1.24 배치 API 마이크로 버전이 추가한 기능을 설명하는 텍스트입니다.\n"
"\n"
"1.24 배치 API 마이크로 버전이 새로운 1.24 배치 API 마이크로 버전을 추가합니다. 이 마이크로 버전은 `member_of` 쿼리 파라미터를 여러 개로 spécifying 할 수 있습니다. `GET /resource_providers` 및 `GET allocation_candidates` 엔드포인트에서 `member_of` 쿼리 파라미터를 여러 개로 spécifying 할 수 있습니다. 여러 `member_of` 쿼리 파라미터가 수신되면 배치 서비스는 모든 요청된 집합 membership를 일치시키는 리소스 제공자가 리소스 제공자를 리턴합니다. `member_of=in:<agg uuids>` 형식은 여전히 지원되고, 집합 membership에 대한 IN() 연산을 나타내는 연산을 나타냅니다. 새로운 기능을 사용하는 예를 들어:\n"
"\n"
"모두 agg1과 agg2와 관련된 제공자가 리소스 제공자를 리턴합니다: ?member_of=agg1&member_of=agg2\n"
"모두 agg1과 agg2와 관련된 제공자가 리소스 제공자를 리턴합니다: ?member_of=in:agg1,agg2\n"
"모두 agg1과 ANY OF (agg2, agg3)와 관련된 제공자가 리소스 제공자를 리턴합니다: ?member_of=agg1&member_of=in:agg2,agg3\n"
"모두 agg1과 ANY OF (agg2, agg3)와 관련된 제공자가 리소스 제공자를 리턴합니다. 또한 agg1과 agg2와 ANY OF (agg3, agg4)와 관련된 제공자가 리소스 제공자를 리턴합니다: ?member_of=in:agg1,agg2&member_of=in:agg3,agg4"

#: ../../<reno.sphinxext origin/stable/ocata>:861
msgid ""
"A new 2.41 microversion was added to the Compute API. Users specifying this "
"microversion will now see the 'uuid' attribute of aggregates when calling "
"the `os-aggregates` REST API endpoint."
msgstr ""
"새로운 2.41 마이크로 버전이 컴퓨터 API에 추가되었다. 사용자가 이 마이크로 버전을 지정하면 now os-aggregates "
"REST API endpoint를 호출할 때 uuid 속성의 집합을 볼 수 있다."

#: ../../<reno.sphinxext stable/pike>:748
msgid ""
"A new 2.47 microversion was added to the Compute API.  Users specifying this"
" microversion or later will see the \"flavor\" information displayed as a "
"dict when displaying server details via the `servers` REST API endpoint. If "
"the user is prevented by policy from indexing extra-specs, then the "
"\"extra_specs\" field will not be included in the flavor information."
msgstr ""
"다음은 Compute API에 2.47 microversion이 추가되었다.  이 microversion 이상을 사용하는 사용자는 "
"`servers` REST API endpoint를 통해 서버รายละเอียด을 표시할 때 \"flavor\" 정보가 dict 형태로 "
"표시되게된다.  사용자가 정책에 의해 인덱싱에 제한이 있으면, \"extra_specs\" 필드는 \"flavor\" 정보에 포함되지 "
"않는다."

#: ../../<reno.sphinxext stable/2025.1>:197
msgid ""
"A new ImagePropertiesWeigher is able to spread or pack instances using the "
"same image properties."
msgstr ""
"새로운 ImagePropertiesWeigher는 동일한 이미지 속성으로 인하여 spread 또는 pack할 수 있는 인스턴스를 "
"spread 또는 pack할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:528
msgid ""
"A new PCI NUMA affinity policy is available. The "
"``hw:pci_numa_affinity_policy`` flavor extra spec and "
"``hw_pci_numa_affinity_policy`` image metadata property now accept a "
"``socket`` policy value. This value indicates that the PCI device must be "
"affined to the same host socket as at least one of the guest NUMA nodes. For"
" more information, see the `PCI Passthrough`__ guide."
msgstr ""
"PCI NUMA 집합 정책이 새로운 버전이 उपलब합니다. `hw:pci_numa_affinity_policy` 플레버 추가 속성과 "
"`hw_pci_numa_affinity_policy` 이미지 메타데이터 속성은 `socket` 정책 값을 수용합니다. 이 값은 게스트 "
"NUMA 노드 중 ít nhất 하나와 일치하는 호스트 소켓에 PCI 장치가 집합 정책을 적용해야 하는ことを 나타냅니다. 더 많은 정보를"
" 얻으려면 `PCI Passthrough`__ 가이드를 참조하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:814
msgid ""
"A new Placement API microversion 1.3 is added with support for filtering the"
" list of resource providers to include only those resource providers which "
"are members of any of the aggregates listed by uuid in the `member_of` query"
" parameter. The parameter is used when making a `GET /resource_providers` "
"request. The value of the parameter uses the `in:` syntax to provide a list "
"of aggregate uuids as follows::"
msgstr ""
"새로운 배치 API 마이크로 버전 1.3은 리소스 제공자 목록을 필터링하는 지원을 추가합니다. 이 필터링은 aggregates에 "
"uuid을 사용하여 listing된 aggregates의 멤버로만 구성된 리소스 제공자만 포함합니다. 이 필터링은 `GET "
"/resource_providers` 요청을 할 때 사용됩니다. 필터링을 사용하는 parameter의 value는 `in:` 문법을 "
"사용하여 aggregates의 uuid 목록을 제공합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:828
msgid ""
"A new Placement API microversion 1.4 is added. Users may now query the "
"Placement REST API for resource providers that have the ability to meet a "
"set of requested resource amounts. The `GET /resource_providers` API call "
"can have a \"resources\" query string parameter supplied that indicates the "
"requested amounts of various resources that a provider must have the "
"capacity to serve. The \"resources\" query string parameter takes the form:"
msgstr ""
"새로운 배치 API 마이크로 버전 1.4이 추가되었다. 사용자는 now resource providers가 요청한 리소스 양을 đáp "
"ứng할 수 있는 리소스 제공자에 대한 Placement REST API를 querying할 수 있다. `GET "
"/resource_providers` API 호출은 \"resources\" query string parameter가 제공되는 "
"\"리소스\" query parameter가 포함될 수 있다. 이 \"resources\" query parameter는 다양한 리소스를"
" 제공할 수 있는 capacity가 있는 제공자에 대한 요청된 리소스 양을 나타낸다. \"resources\" query "
"parameter의 형식은 다음과 같다."

#: ../../<reno.sphinxext stable/2025.1>:232
msgid ""
"A new `ImagePropertiesWeigher` has been added. It will compare the number of"
" image properties of the image being booted for each of the host with how "
"many existing instances use them. By default this weigher is enabled but "
"with a value of 0.0 for `[filter_scheduler]/image_props_weight_multiplier` "
"which won't modify the existing scheduling behavior. If you want to pack "
"instances having the same image properties on the same hosts, modify "
"`image_props_weight_multiplier` to a positive value. If you want to spread "
"instances with the same properties around all hosts, then please modify "
"`image_props_weight_multiplier` to a negative value. Another configuration "
"option `[filter_scheduler]/image_props_weight_setting` allows you to define "
"fine-grained weights for each of the properties you actually would like to "
"weigh (eg. `os_distro`). Please refer to the documentation for more details "
"about how to use this configuration option."
msgstr ""
"`ImagePropertiesWeigher`는 이미지가 부트되는 각 호스트에서 이미지가 사용하는 속성의 수를 비교하여 사용하는 속성의 "
"수를 사용하는 인스턴스의 수를 비교합니다. 기본적으로 이 가중치가 활성화되지만 "
"`[filter_scheduler]/image_props_weight_multiplier`의 Wert는 0.0으로 설정되어 있는ため, "
"기존의 스케줄링 behavior를 변경하지 않습니다. 이미지가 same image properties를 same 호스트에 pack하고 "
"싶다면, `image_props_weight_multiplier`를 양수로 설정하세요. 이미지가 same properties를 same "
"호스트에 spread하고 싶다면, `image_props_weight_multiplier`를 음수로 설정하세요. 또 다른 구성 옵션 "
"`[filter_scheduler]/image_props_weight_setting`는 실제로 가중치로 사용하고 싶은 속성들을 정의할 수"
" 있습니다. 예를 들어 `os_distro`와 같은 속성을 가중치로 사용하고 싶다면, 이 구성 옵션을 사용하세요. 더 많은 세부 사항에 "
"대한 사용 방법에 대한 정보는 문서를 참조하세요."

#: ../../<reno.sphinxext unmaintained/yoga>:376
msgid ""
"A new ``[cinder]/debug`` configurable has been introduced to enable DEBUG "
"logging for both the ``python-cinderclient`` and ``os-brick`` libraries "
"independently to the rest of Nova."
msgstr ""
"``[cinder]/debug`` configurable가 도입되어 Nova의 나머지 부분과 함께 ``python-"
"cinderclient`` 및 ``os-brick`` 라이브러리에서 DEBUG 로그를 활성화할 수 있는 새로운 "
"``[cinder]/debug`` configurable가 도입되었다."

#: ../../<reno.sphinxext stable/train>:1404
msgid ""
"A new ``[libvirt]/rbd_connect_timeout`` configuration option has been "
"introduced to limit the time spent waiting when connecting to a RBD cluster "
"via the RADOS API. This timeout currently defaults to 5 seconds."
msgstr ""
"``[libvirt]/rbd_connect_timeout`` 설정 옵션은 RBD 클러스터에 대한 RADOS API를 통해 연결하는 동안 "
"대기 시간을 제한하기 위해 mới에 도입된 새로운 설정 옵션입니다. 현재 이.timeout은 5초로 기본적으로 설정되어 있습니다."

#: ../../<reno.sphinxext stable/rocky>:404
msgid ""
"A new ``[workarounds]/report_ironic_standard_resource_class_inventory`` "
"configuration option has been added."
msgstr ""
"``[workarounds]/report_ironic_standard_resource_class_inventory`` 설정 옵션은 새로운"
" 것으로 추가되었습니다."

#: ../../<reno.sphinxext stable/stein>:106 stable/train>:283
#: stable/ussuri>:305 unmaintained/victoria>:782
msgid ""
"A new ``[workarounds]/reserve_disk_resource_for_image_cache`` config option "
"was added to fix the `bug 1878024`_ where the images in the compute image "
"cache overallocate the local disk. If this new config is set then the "
"libvirt driver will reserve DISK_GB resources in placement based on the "
"actual disk usage of the image cache."
msgstr ""
"``[workarounds]/reserve_disk_resource_for_image_cache`` 설정 옵션을 추가하여 `bug "
"1878024`_의 문제를 해결할 수 있습니다. 이 새로운 설정이 설정되면 libvirt 드라이버는 실제 디스크 사용량에 따라 배치에 "
"DISK_GB 리소스를 예약합니다."

#: ../../<reno.sphinxext stable/pike>:497 stable/queens>:1210
msgid ""
"A new ``keystone`` config section is added so that you can set session link "
"attributes for communicating with keystone. This allows the use of custom "
"certificates to secure the link between Nova and Keystone."
msgstr ""
"``keystone`` 구성 섹션에 새로운 ``keystone`` 구성 부분이 추가되어 keystone와 communicate하는 데 "
"session link 속성들을 설정할 수 있습니다. 이로 인해 Nova와 Keystone 사이의 연결을 보안하기 위해 사용할 수 있는 "
"custom certificate가 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:2084
msgid ""
"A new ``nova-manage placement heal_allocations`` CLI has been added to help "
"migrate users from the deprecated CachingScheduler. Starting in 16.0.0 "
"(Pike), the nova-compute service no longer reports instance allocations to "
"the Placement service because the FilterScheduler does that as part of "
"scheduling. However, the CachingScheduler does not create the allocations in"
" the Placement service, so any instances created using the CachingScheduler "
"after Ocata will not have allocations in Placement. The new CLI allows "
"operators using the CachingScheduler to find all instances in all cells "
"which do not have allocations in Placement and create those allocations. The"
" CLI will skip any instances that are undergoing a task state transition, so"
" ideally this would be run when the API is down but it can be run, if "
"necessary, while the API is up. For more details on CLI usage, see the man "
"page entry:"
msgstr ""
"``nova-manage placement heal_allocations`` CLI가 deprecated CachingScheduler를"
" 사용하는 사용자들을 도와 migration을 도와주는 새로운 CLI가 추가되었다. 16.0.0 (Pike)부터 nova-compute "
"서비스는 instance 할당을 Placement 서비스에รายงาน하지 않게 되는데, 이는 FilterScheduler가 "
"scheduling의 일부로 할당을 Placement 서비스에 reports한다. 그러나 CachingScheduler는 "
"Placement 서비스에 할당을 생성하지 않기 때문에, Ocata 이후에 CachingScheduler를 사용한 모든 인스턴스는 "
"Placement 서비스에 할당이 없을 것이다. 새로운 CLI는 CachingScheduler를 사용하는 운영자들이 Placement "
"서비스에 할당이 없는 모든 인스턴스를 찾고, 그 할당을 생성할 수 있게 한다. CLI는 task state transition을進行하는 "
"인스턴스를 skips한다. 따라서, ideally는 API가 down일 때 CLI를 실행해야 하지만, 만약 필요하다면 API가 up일 "
"때도 CLI를 실행할 수 있다. CLI 사용에 대한 더 많은 정보는 man page entry에서 확인할 수 있다."

#: ../../<reno.sphinxext stable/ussuri>:1356
msgid ""
"A new ``nova-manage`` command, ``placement audit``, has been added. This can"
" be used to identify and optionally remove compute allocations in placement "
"that are no longer referenced by existing instances or migrations. These "
"orphaned allocations typically occur due to race conditions during instance "
"migration or removal and will result in capacity issues if not addressed. "
"For more details on CLI usage, see the man page entry: "
"https://docs.openstack.org/nova/latest/cli/nova-manage.html#placement"
msgstr ""
"nova-manage 명령은 새로운 \"placement audit\" 옵션을 추가했습니다. 이 옵션은 현재 인스턴스 또는 이민이 "
"참조하지 않는 placement에서 더 이상 사용되지 않는 컴퓨터 할당을 식별하고 선택적으로 제거할 수 있습니다. 이러한 비참조 할당은 "
"일반적으로 인스턴스 이민 또는 제거 시에 경쟁 조건으로 발생하며, 제대로 addressing되지 않으면 capacity 문제가 발생할 수"
" 있습니다. CLI 사용에 대한 더 많은 정보는 다음 man page 항목을 참조하십시오: "
"https://docs.openstack.org/nova/latest/cli/nova-manage.html#placement"

#: ../../<reno.sphinxext origin/stable/ocata>:771
msgid ""
"A new ``nova-status upgrade check`` CLI is provided for checking the "
"readiness of a deployment when preparing to upgrade to the latest release. "
"The tool is written to handle both fresh installs and upgrades from an "
"earlier release, for example upgrading from the 14.0.3 Newton release. There"
" can be multiple checks performed with varying degrees of success. More "
"details on the command and how to interpret results are in the `nova-status "
"man page`_."
msgstr ""
"``nova-status upgrade check`` CLI는 नव아 스타tatus의 최신 릴리스로 업그레이드를 준비하는 동안 "
"배포의พร้อม성을 확인하는 데 사용할 수 있는 새로운 ``nova-status upgrade check`` CLI가 제공됩니다. 이 "
"도구는 새로운 설치와 이전 릴리스에서 업그레이드를 수행하는両方를 처리할 수 있습니다. 예를 들어, 14.0.3 뉴턴 릴리스에서 "
"업그레이드를 수행할 수 있습니다. 여러 확인이 수행될 수 있으며 성공도 다르게 나타날 수 있습니다. 명령과 결과를解释하는 방법에 대한 더"
" 많은 정보는 `nova-status man page`_에 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:694
msgid ""
"A new ``nova-status`` check has been introduced to help operators identify "
"if any instances within their environment have ``hw_machine_type`` unset "
"before they attempt to change the ``[libvirt]hw_machine_type`` configurable."
msgstr ""
"nova-status 확인은 환경 내의 인스턴스에 대해 hw_machine_type이 설정되지 않은지 확인하는 데 도움을 주는 새로운 "
"\"nova-status\" 확인이 도입되었다."

#: ../../<reno.sphinxext stable/rocky>:623
msgid ""
"A new ``traceback`` field has been added to each versioned instance "
"notification. In an error notification this field contains the full "
"traceback string of the exception which caused the error notification. See "
"the `notification dev ref`_ for the sample file of ``instance.create.error``"
" as an example."
msgstr ""
"``traceback`` 필드는 각 버전화된 인스턴스 알림에 추가된 새로운 필드입니다. 오류 알림에서 이 필드는 오류 알림이 발생한 "
"예외의 전체 트레이스백 문자열을 포함합니다. 예를 들어, `notification dev ref`_의 "
"`instance.create.error` 예제 파일을 참조하십시오."

#: ../../<reno.sphinxext stable/2023.2>:243
msgid ""
"A new `num_instances_weigher` weigher has been added. This weigher will "
"compare the number of instances between each node and order the list of "
"filtered results by its number, By default, this weigher is enabled but with"
" a default of 0.0 which doesn't change the current behavior. In order to use"
" it, please change the value of "
"``[filter_scheduler]/num_instances_weight_multiplier`` config option where a"
" positive value will favor the host with the higher number of instances (ie."
" packing strategy) vs. a negative value that will spread instances between "
"hosts. As a side note, this weigher will count *all* of the existing "
"instances on the host, even the stopped or shelved ones."
msgstr ""
"`num_instances_weigher` 가중치가 새로운 가중치가 추가되었다. 이 가중치는 각 노드 간의 인스턴스 수를 비교하여 결과의"
" 필터된 목록을 순서대로 정렬한다. 기본적으로 이 가중치는 활성화되어 있지만 0.0으로 기본값이 설정되어 있지만 현재 행동을 변경하지 "
"않는다. 사용하기 위해서는 `[filter_scheduler]/num_instances_weight_multiplier` 구성 옵션의 "
"값이 변경되어야 한다. 양수 값이면 호스트에 높은 인스턴스 수를 favors 하며, 음수 값이면 호스트 간 인스턴스를 spread 한다."
" 이 가중치의 한 side note는 현재 활성화된 인스턴스도 포함하여 모든 인스턴스를 카운트한다. 예를 들어, 중단된 또는 보관된 "
"인스턴스도 포함된다."

#: ../../<reno.sphinxext stable/2023.2>:239
msgid ""
"A new `os_compute_api:os-migrate-server:migrate:host` policy is created, "
"being by default only an admin-only policy. This will help operators to have"
" different policies between cold-migrate without providing a host or not."
msgstr ""
"`os_compute_api:os-migrate-server:migrate:host` 정책을 새로 생성합니다. 이 정책은 기본적으로 "
"관리자만의 정책으로만 존재합니다. 이 정책은冷-migrate를 제공하지 않거나 호스트를 제공하지 않는 경우에만 다른 정책을 제공할 수 "
"있는 것을 도와줍니다."

#: ../../<reno.sphinxext origin/stable/ocata>:847
msgid ""
"A new administrator-only resource endpoint was added to the OpenStack "
"Placement REST API for managing custom resource classes. Custom resource "
"classes are specific to a deployment and represent types of quantitative "
"resources that are not interoperable between OpenStack clouds. See the "
"`Placement REST API Version History`_ documentation for usage details."
msgstr ""
"OpenStack Placement REST API의 관리용 관리자 전용 리소스 엔드포인트가 새로운 것으로 추가되었다. 관리자 전용 "
"리소스 엔드포인트는 OpenStack 클라우드 간에 interoperable하지 않은 양적 리소스类型을 나타내는 특정 배포에 대한 "
"custom resource class를 관리한다. OpenStack Placement REST API Version History_ "
"documentation을 참조하여 사용 방법에 대한รายละเอียด을 확인할 수 있다."

#: ../../<reno.sphinxext stable/queens>:424 stable/rocky>:1516
msgid ""
"A new check is added to ``nova-status upgrade check`` which will scan all "
"cells looking for ``nova-osapi_compute`` service versions which are from "
"before Ocata and which may cause issues with how the compute API finds "
"instances. This will result in a warning if:"
msgstr ""
"``nova-status upgrade check``에 새로운 확인이 추가되며, 이 확인은 모든 세포를 대상으로 ``nova-"
"osapi_compute`` 서비스 버전을 찾는 것을 목표로 한다. 이 서비스 버전은 오카타 이전의 버전이므로 컴퓨터 API가 인스턴스를"
" 찾는 방법에 문제가 발생할 수 있다. 이 경우 경고가 발생할 것이다."

#: ../../<reno.sphinxext stable/rocky>:1593
msgid ""
"A new check is added to the ``nova-status upgrade check`` CLI to make sure "
"request spec online migrations have been run per-cell. Missing request spec "
"compatibility code is planned to be removed in the Stein release."
msgstr ""
"nova-status upgrade check CLI에 새로운 확인을 추가하여 online migration의 request spec이 "
"세포당 each에 실행되도록 확인합니다. missing request spec compatibility code는 Stein 릴리스에서 "
"제거될 예정입니다."

#: ../../<reno.sphinxext stable/queens>:437 stable/rocky>:1587
msgid ""
"A new check is added to the ``nova-status upgrade check`` CLI which can "
"assist with determining if ironic instances have had their embedded flavor "
"migrated to use the corresponding ironic node custom resource class."
msgstr ""
"nova-status upgrade check의 CLI에 새로운 확인을 추가하여, 이ironic 인스턴스의 내부 플레버가 "
"corresponding ironic node custom resource class를 사용하기 위해 이ironic 인스턴스의 플레버가 "
"이ironic 노드의 custom resource class에 mig리ated했는지 여부를 결정하는 데 도움을 줄 수 있습니다."

#: ../../<reno.sphinxext stable/2023.2>:301
msgid ""
"A new command ``nova-manage limits migrate_to_unified_limits`` has been "
"added to make migration from the ``nova.quota.DbQuotaDriver`` to the "
"``nova.quota.UnifiedLimitsDriver`` easier. This will enable operators to "
"have their existing quota limits copied from the Nova database to Keystone "
"automatically."
msgstr ""
"nova-manage limits migrate_to_unified_limits를 추가하여 "
"nova.quota.DbQuotaDriver에서 nova.quota.UnifiedLimitsDriver로의 이식이 더 쉽게 가능합니다. "
"이로써 노바 데이터베이스에서 현재 사용중인_quota 제한을 tự động으로 keystone에 복사할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:45 stable/rocky>:66 stable/stein>:143
#: stable/train>:332 stable/ussuri>:1254
msgid ""
"A new config option ``[neutron]http_retries`` is added which defaults to 3. "
"It controls how many times to retry a Neutron API call in response to a HTTP"
" connection failure. An example scenario where it will help is when a "
"deployment is using HAProxy and connections get closed after idle time. If "
"an incoming request tries to re-use a connection that is simultaneously "
"being torn down, a HTTP connection failure will occur and previously Nova "
"would fail the entire request. With retries, Nova can be more resilient in "
"this scenario and continue the request if a retry succeeds. Refer to "
"https://launchpad.net/bugs/1866937 for more details."
msgstr ""
"``[neutron]http_retries`` 옵션은 기본적으로 3으로 설정되어 있으며, Neutron API 호출에 대한 HTTP 연결"
" 실패를 대비하여 retry한 횟수를 제어합니다. 예를 들어, HAProxy를 사용하는 배포가 idle 시간이 지남에 따라 연결이 닫히는"
" 경우, incoming 요청이 동시에 파괴되는 연결을 재사용하려고 시도하면 HTTP 연결 실패가 발생하고, 이전에는 Nova가 전부 "
"요청을 실패했습니다. retry를 사용하면 Nova는 이 상황에서 더 강력하게 resilient할 수 있으며, retry가 성공하면 "
"요청을 계속할 수 있습니다. 더 많은 정보는 https://launchpad.net/bugs/1866937에 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:633
msgid ""
"A new configuration option ``[libvirt]/live_migration_timeout_action`` is "
"added. This new option will have choices ``abort`` (default) or "
"``force_complete``. This option will determine what actions will be taken "
"against a VM after ``live_migration_completion_timeout`` expires. Currently "
"nova just aborts the live migrate operation after completion timeout "
"expires. By default, we keep the same behavior of aborting after completion "
"timeout. ``force_complete`` will either pause the VM or trigger post-copy "
"depending on if post copy is enabled and available."
msgstr ""
"``[libvirt]/live_migration_timeout_action``의 새로운 구성 옵션이 추가되었습니다. 이 새로운 옵션은 "
"기본적으로 ``abort`` (기본값) 또는 ``force_complete``를 선택할 수 있습니다. 이 옵션은 "
"``live_migration_completion_timeout``이 만료되면 VM에 대한 액션을 결정합니다. 현재 nova는 만료 "
"시간이 지남에 따라 live migrate 연동을 중단합니다. 기본적으로는 만료 시간이 지남에 따라 live migrate 연동을 "
"중단하는 behavior를 유지합니다. ``force_complete``는 post copy가 활성화되어 있는지 확인하고, 활성화되어 "
"있으면 post copy를 트리거하거나, post copy가 활성화되어 있지 않으면 VM을 일시정지시합니다."

#: ../../<reno.sphinxext stable/pike>:885
msgid ""
"A new configuration option ``[quota]/recheck_quota`` has been added to "
"recheck quota after resource creation to prevent allowing quota to be "
"exceeded as a result of racing requests. It defaults to True, which makes it"
" impossible for a user to exceed their quota. However, it will be possible "
"for a REST API user to be rejected with an over quota 403 error response in "
"the event of a collision close to reaching their quota limit, even if the "
"user has enough quota available when they made the request. Operators may "
"want to set the option to False to avoid additional load on the system if "
"allowing quota to be exceeded because of racing requests is considered "
"acceptable."
msgstr ""
"``[quota]/recheck_quota`` 옵션은 리소스 생성 후 쿼타를 재 확인하기 위해 추가된 새로운 구성 옵션입니다. 이 옵션은"
" True로 기본적으로 설정되어, 사용자가 쿼타를 초과할 수 없게 됩니다. 그러나, REST API 사용자가 쿼타 한계에 가깝게 충돌이 "
"발생할 때, 사용자가 충분한 쿼타가 있는지 확인하지 않아도 쿼타 초과 403 오류를 받을 수 있습니다. 사용자가 쿼타를 초과하는 것을 "
"허용하는 이유가 충돌이 발생할 때만 허용하는지 여부에 따라, 시스템에 추가적인 부하가 발생할 수 있기 때문에, 운영자는 이 옵션을 "
"False로 설정하여 충돌이 발생할 때만 쿼타를 초과하는 것을 허용하는지 여부를 결정할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:898
msgid ""
"A new configuration option ``reserved_host_cpus`` has been added for compute"
" services. It helps operators to provide how many physical CPUs they would "
"like to reserve for the hypervisor separately from what the instances use."
msgstr ""
"``reserved_host_cpus``라는 새로운 구성 옵션은 컴퓨터 서비스에 추가되었다. 이 옵션은 하이퍼바이저와 인스턴스의 물리 "
"CPU 수를 separately 제공하는 것을 도와준다."

#: ../../<reno.sphinxext stable/rocky>:686
msgid ""
"A new configuration option is introduced, ``[placement]/policy_file``, which"
" is used to configure the location of the placement policy file. By default,"
" the ``placement-policy.yaml`` file may live alongside the nova policy file,"
" e.g.:"
msgstr ""
"``[placement]/policy_file``는 배치 정책 파일의 위치를 구성하는 옵션을 도입합니다. 기본적으로, "
"``placement-policy.yaml`` 파일은 nova 정책 파일과 함께 살 수 있습니다. 예를 들어:"

#: ../../<reno.sphinxext unmaintained/xena>:401
msgid ""
"A new configuration option is now available for supporting PCI devices that "
"use the `VFIO-mdev`_ kernel framework and are stateless. Instead of using "
"the ``VGPU`` resource class for both the inventory and the related "
"allocations, the operator could ask to use another custom resource class for"
" a specific mdev type by using the dynamic ``mdev_class``."
msgstr ""
"PCI 장치가 `VFIO-mdev`_ 커널 프레임워크와 상태less인 경우에 새로운 구성 옵션이 now available입니다. 대신에 "
"`VGPU` 리소스 클래스를 사용하여 both inventory와 관련된 할당에 사용하는 대신, 운영자는 특정 mdev 타입에 대해 다른"
" custom 리소스 클래스를 사용할 수 있습니다. `mdev_class`를 사용하여 dynamic으로."

#: ../../<reno.sphinxext unmaintained/victoria>:418
msgid ""
"A new configuration option, ``[DEFAULT]/max_concurrent_snapshots``, has been"
" added. This allow operator to configure maximum concurrent snapshots on a "
"compute host and prevent resource overuse related to snapshot."
msgstr ""
"``[DEFAULT]/max_concurrent_snapshots``는 새로운 구성 옵션으로 추가되었다. 이 옵션은 컴퓨터 호스트에서 "
"최대 동시 스냅샷을 구성할 수 있는지 여부를 설정하고 스냅샷과 관련된 자원 오버 사용을 방지한다."

#: ../../<reno.sphinxext stable/2024.2>:291
msgid ""
"A new configuration option, ``[api] response_validation``, has been added. "
"This allows operators to configure the behavior of ``nova-api`` when a "
"response fails schema validation."
msgstr ""
"``[api] response_validation`` 옵션은 새로운 구성 옵션입니다. 이 옵션은 ``nova-api``의 반응이 스키마 "
"유효성 검사에 실패할 때 행동을 구성할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:753
msgid ""
"A new configuration option, ``[compute] cpu_dedicated_set``, has been added."
" This can be used to configure the host CPUs that should be used for "
"``PCPU`` inventory."
msgstr ""
"``[compute] cpu_dedicated_set``는 새로운 구성 옵션으로 추가되었다. 이 옵션은 PCPU 인 inventory에 "
"사용할 호스트 CPU를 구성할 수 있다."

#: ../../<reno.sphinxext stable/queens>:382 stable/rocky>:2064
msgid ""
"A new configuration option, ``[compute]/live_migration_wait_for_vif_plug``, "
"has been added which can be used to configure compute services to wait for "
"network interface plugging to complete on the destination host before "
"starting the guest transfer on the source host during live migration."
msgstr ""
"``[compute]/live_migration_wait_for_vif_plug``라는 새로운 구성 옵션은 원본 호스트에서 라이브 "
"마이그레이션을 시작하기 전에 목적지 호스트의 네트워크 인터페이스 플러그가 완료되기까지 대기하는 compute 서비스를 구성할 수 있는 "
"옵션입니다."

#: ../../<reno.sphinxext stable/stein>:543
msgid ""
"A new configuration option, ``[compute]/max_disk_devices_to_attach``, which "
"defaults to ``-1`` (unlimited), has been added and can be used to configure "
"the maximum number of disk devices allowed to attach to a single server, per"
" compute host. Note that the number of disks supported by a server depends "
"on the bus used. For example, the ``ide`` disk bus is limited to 4 attached "
"devices."
msgstr ""
"``[compute]/max_disk_devices_to_attach`` 옵션은 기본적으로 ``-1`` (무한)으로 설정되어 있으며, 한"
" 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 "
"한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에"
" 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 "
"호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터"
" 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 "
"컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한"
" 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 "
"한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에"
" 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 "
"호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터"
" 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 "
"컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터 호스트에 한 컴퓨터"

#: ../../<reno.sphinxext stable/train>:759
msgid ""
"A new configuration option, ``[workarounds] disable_fallback_pcpu_query``, "
"has been added. When creating or moving pinned instances, the scheduler will"
" attempt to provide a ``PCPU``-based allocation, but can also fallback to a "
"legacy ``VCPU``-based allocation. This fallback behavior is enabled by "
"default to ensure it is possible to upgrade without having to modify compute"
" node configuration but it results in an additional request for allocation "
"candidates from placement. This can have a slight performance impact and is "
"unnecessary on new or upgraded deployments where the compute nodes have been"
" correctly configured to report ``PCPU`` inventory. The ``[workarounds] "
"disable_fallback_pcpu_query`` config option can be used to disable this "
"fallback allocation candidate request, meaning only ``PCPU``-based "
"allocation candidates will be retrieved."
msgstr ""
"``[workarounds] disable_fallback_pcpu_query`` 옵션은 새로운 구성 옵션으로 추가되었다. pinned "
"인스턴스를 생성하거나 이동할 때, 스케줄러는 ``PCPU``-based 할당을 제공하려고 시도하지만, legacy "
"``VCPU``-based 할당으로 fallback할 수 있다. 이 fallback 행동은 기본적으로 활성화되어, 컴퓨터 노드 구성 "
"modifications가 필요하지 않아 업그레이드를 가능하게 하지만, allocation 후보자에 대한 추가적인 요청이 발생한다. 이로"
" 인해 성능에 약간의 영향을 줄 수 있으며, 새로운 또는 업그레이드된 배포에서 컴퓨터 노드가 ``PCPU`` 인 "
"inventory를正确하게 보고 있는 경우, 필요하지 않다. ``[workarounds] "
"disable_fallback_pcpu_query`` 구성 옵션을 사용하여, fallback 할당 후보자 요청을 비활성화할 수 있다. "
"이는, ``PCPU``-based 할당 후보자만을 리trieved 할 수 있게 한다."

#: ../../<reno.sphinxext stable/train>:488
msgid ""
"A new framework supporting hardware-based encryption of guest memory to "
"protect users against attackers or rogue administrators snooping on their "
"workloads when using the libvirt compute driver. Currently only has basic "
"support for `AMD SEV (Secure Encrypted Virtualization) "
"<https://docs.openstack.org/nova/latest/admin/configuration/hypervisor-"
"kvm.html#amd-sev-secure-encrypted-virtualization>`_."
msgstr ""
"`AMD SEV (Secure Encrypted Virtualization) "
"<https://docs.openstack.org/nova/latest/admin/configuration/hypervisor-"
"kvm.html#amd-sev-secure-encrypted-virtualization>`를 지원하는 새로운 프레임워크가 있습니다. 이 "
"프레임워크는 게스트 메모리에 하드웨어 기반의 암호화 기능을 지원하여 사용자들이 공격자나 위장 관리자가 사용자 로드의 정보를 "
"snooping 할 때를 대비하여 보호합니다. 현재는 `libvirt compute driver`를 사용할 때만 기본적인 지원을 "
"제공합니다."

#: ../../<reno.sphinxext stable/2023.2>:337
msgid ""
"A new hypervisor version weigher has been added that will prefer selecting "
"hosts with a newer hypervisor installed. This can help simplify rolling "
"upgrades by preferring the already upgraded hosts when moving workloads "
"around using live or cold migration. To restore the old behavior either "
"remove the weigher from the list of enabled weighers or set "
"``[filter_scheduler] hypervisor_version_weight_multiplier=0``. The default "
"value of the hypervisor_version_weight_multiplier is 1 so only a mild "
"preference is given to new hosts, higher values will make the effect more "
"pronounced and negative values will prefer older hosts."
msgstr ""
"다음은 가상화 하이퍼바이저 버전 가중치 계산기(weigher)가 추가된 새로운 하이퍼바이저 버전 가중치 계산기입니다. 이 가중치 계산기는"
" newer 하이퍼바이저가 설치된 호스트를 선호하여 선택하는 것을 선호합니다. 이로 인해 live 또는冷 마이그레이션을 사용하여 작업을 "
"이동할 때, 이미 업그레이드 된 호스트를 선호하여 업그레이드를 쉽게 할 수 있습니다. older 호스트를 선호하는 효과를 강화하고, 더 "
"큰 효과를 얻으려면 가중치 계산기에서 `hypervisor_version_weight_multiplier`를 설정하여 `0`으로 설정할 "
"수 있습니다. 기본 가중치 계산기 가중치는 `1`이므로, 새로운 호스트에만 가중치를 부여합니다. 더 높은 가중치로 설정하면 효과가 더 "
"강해지고, 더 낮은 가중치로 설정하면 older 호스트를 선호합니다."

#: ../../<reno.sphinxext stable/2023.2>:280
msgid ""
"A new hypervisor version weigher has been added to prefer selecting hosts "
"with newer hypervisors installed. For the libvirt driver, this is the "
"version of libvirt on the compute node not the version of qemu. As with all "
"weighers this is enabled by default and its behavior can be modified using "
"the new ``hypervisor_version_weight_multiplier`` config option in the "
"``filter_scheduler`` section."
msgstr ""
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"다음과 같은 영어 텍스트를 한국어로 번역합니다."

#: ../../<reno.sphinxext stable/ussuri>:536
msgid ""
"A new image metadata prefilter has been added to allow translation of "
"hypervisor-specific device model requests to standard traits. When this "
"feature is enabled, nova is able to utilize placement to select hosts that "
"are capable of emulating the requested devices, avoiding hosts that could "
"not support the request. This feature is currently supported by the libvirt "
"driver and can be enabled by configuring the "
"``[scheduler]/image_metadata_prefilter`` to ``True`` in the controller "
"``nova.conf``."
msgstr ""
"새로운 이미지 메타데이터 프리필터가 추가되어, 하이퍼바이저 특성에 대한 장치 모델 요청을 표준 특성으로 번역할 수 있도록 허용되었습니다."
" 이 기능이 활성화되면, nova는 요청된 장치를 시뮬레이션할 수 있는 호스트를 선택할 수 있으며, 요청을 지원하지 못하는 호스트를 피할"
" 수 있습니다. 현재 libvirt 드라이버가 지원하고, controler `nova.conf`의 "
"`[scheduler]/image_metadata_prefilter`를 `True`로 구성하여 활성화할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:437
msgid ""
"A new image metadata property, ``hw_input_bus``, has been added. This allows"
" you to specify the bus used for input devices - a pointer and keyboard - "
"which are attached to the instance when graphics are enabled on compute "
"nodes using the libvirt virt driver. Two values are currently accepted: "
"``usb`` and ``virtio``. This image metadata property effectively replaced "
"the ``hw_pointer_model`` image metadata property, which is nontheless "
"retained for backwards compatibility purposes."
msgstr ""
"``hw_input_bus``라는 새로운 이미지 메타데이터 속성은 추가되었다. 이 속성은 libvirt virt 드라이버를 사용하여 "
"컴퓨팅 노드에서 그래픽이 활성화된 경우에 인풀 디바이스 - 포인터와 키보드 -를 연결하는 버스를 지정할 수 있다. 현재接受되는 두 가지 "
"값은 ``usb``와 ``virtio``이다. 이 이미지 메타데이터 속성은 ``hw_pointer_model`` 이미지 메타데이터 속성의"
" 대체를 효과적으로 수행했으며, backwards compatibility의 목적으로 여전히 보존된다."

#: ../../<reno.sphinxext stable/2025.1>:250
msgid ""
"A new microversion has been added that allows users to retrieve server's "
"associated scheduler hints in ``GET /servers/{server_id}``, ``GET "
"/servers/detail``, ``PUT /servers/{server_id}`` and ``POST "
"/server/{server_id}/action`` (rebuild) responses."
msgstr ""
"``GET /servers/{server_id}``, ``GET /servers/detail``, ``PUT "
"/servers/{server_id}`` 및 ``POST /server/{server_id}/action`` (rebuild) 응답에서 "
"서버의 연관된 스케줄러 힌트를 얻는 사용자가 새로운 마이크로 버전이 추가되었다."

#: ../../<reno.sphinxext stable/2024.2>:269
msgid ""
"A new module, ``nova.wsgi``, has been added as a place to gather WSGI "
"``application`` objects. This is intended to ease deployment by providing a "
"consistent location for these objects. For example, if using uWSGI then "
"instead of:"
msgstr ""
"nova.wsgi 모듈이 WSGI \"application\" 개체를 모아두는 곳으로 추가되었다. 이는 배포를 आसान하게 하는 데 "
"도움이 되는-consistent location를 제공한다. 예를 들어, uWSGI를 사용할 때는 대신에:"

#: ../../<reno.sphinxext stable/pike>:147 stable/queens>:288
#: stable/rocky>:1558
msgid ""
"A new online data migration has been added to populate missing "
"instance.availability_zone values for instances older than Pike whose "
"availability_zone was not specified during boot time. This can be run during"
" the normal ``nova-manage db online_data_migrations`` routine. This fixes "
"`Bug 1768876`_"
msgstr ""
"새 온라인 데이터 이민이 추가되어, Pike 이상의 인스턴스에 대해 missing instance.availability_zone의值을 "
"채워주고, 부트 시간에 availability_zone가 명시되지 않은 인스턴스에 대해 채워준다. 이는-normal `nova-"
"manage db online_data_migrations` 루틴에서 실행할 수 있다. 이는 `Bug 1768876`_를修正한다."

#: ../../<reno.sphinxext stable/rocky>:593
msgid ""
"A new option ``disabled`` has been added to nova-manage cell_v2 create_cell "
"command by which users can create pre-disabled cells. Hence unless such "
"cells are enabled, no VMs will be spawned on the hosts in these cells."
msgstr ""
"nova-manage cell_v2 create_cell 명령에 새로운 옵션 \"disabled\"가 추가되었다. 이 옵션을 사용하면 "
"사용자들은 미启성 셀을 생성할 수 있다. 따라서 이러한 셀에서 활성화되지 않은 셀은 활성화되지 않으면 호스트에서 VM이 생성되지 않는다."

#: ../../<reno.sphinxext stable/ussuri>:1339
msgid ""
"A new pair of ``ssl_ciphers`` and ``ssl_minimum_version`` configuration "
"options have been introduced for use by the ``nova-novncproxy``, ``nova-"
"serialproxy``, and ``nova-spicehtml5proxy`` services.  These new options "
"allow one to configure the allowed TLS ciphers and minimum protocol version "
"to enforce for incoming client connections to the proxy services."
msgstr ""
"``ssl_ciphers``와 ``ssl_minimum_version``의 새로운 집합 및 구성 옵션들이 ``nova-"
"novncproxy``, ``nova-serialproxy``, 및 ``nova-spicehtml5proxy`` 서비스에 사용할 수 있는"
" 것으로 도입되었다.  이러한 새로운 옵션은 incoming client 연결에 대한 프록시 서비스에 TLS cipher를 허용하고 최소"
" 프로토콜 버전을 강제하는 데 사용할 수 있는Allow'd TLS cipher 및 최소 프로토콜 버전을 구성할 수 있다."

#: ../../<reno.sphinxext stable/queens>:781
msgid ""
"A new param ``key_name`` was added to the instance rebuild API (v2.54), then"
" it is able to reset instance key pair. It is worth noting that users within"
" the same project are able to rebuild other user's instances in that project"
" with a new keypair. If set ``key_name`` to None in API body, nova will "
"unset the keypair of instance during rebuild."
msgstr ""
"``key_name`` 파라미터가 인스턴스 재건 API (v2.54)에서 추가되었으며, 이로 인해 인스턴스 키 페어를 재설정할 수 "
"있습니다. 이에 대한 주의할 점은 같은 프로젝트 내의 사용자들은 같은 프로젝트 내의 다른 사용자의 인스턴스를 재건할 수 있으며, 새로운 "
"키 페어를 사용할 수 있습니다. API 바디에 ``key_name``를 None으로 설정하면, nova는 인스턴스의 키 페어를 재건 시 "
"제거합니다."

#: ../../<reno.sphinxext stable/queens>:865
msgid ""
"A new policy rule ``os_compute_api:os-flavor-manage:update`` is added to "
"control access to the ``PUT /flavors/{flavor_id}`` API."
msgstr ""
"``os_compute_api:os-flavor-manage:update`` 정책 규칙을 추가하여 ``PUT "
"/flavors/{flavor_id}`` API에 접근을 제어합니다."

#: ../../<reno.sphinxext stable/stein>:498
msgid ""
"A new policy rule ``os_compute_api:servers:allow_all_filters`` has been "
"added to control whether a user can use all filters when listing servers."
msgstr ""
"``os_compute_api:servers:allow_all_filters`` 정책 규칙이 새로운 정책 규칙을 추가하여 사용자들이 서버"
" 목록을 listing할 때 모든 필터를 사용할 수 있는지 제어하는것이 추가되었다."

#: ../../<reno.sphinxext stable/ussuri>:519
msgid ""
"A new policy rule ``os_compute_api:servers:show:host_status:unknown-only`` "
"has been added to control whether a user can view a server host status of "
"``UNKNOWN`` in the following APIs:"
msgstr ""
"``os_compute_api:servers:show:host_status:unknown-only`` 정책 규칙은 사용자가 "
"``UNKNOWN`` 이라는 호스트 상태를 보는 것을 제어할 수 있는 다음 API에서 추가되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:98 stable/pike>:163
#: stable/queens>:304 stable/rocky>:1928
msgid ""
"A new policy rule, ``os_compute_api:servers:create:zero_disk_flavor``, has "
"been introduced which defaults to ``rule:admin_or_owner`` for backward "
"compatibility, but can be configured to make the compute API enforce that "
"server create requests using a flavor with zero root disk must be volume-"
"backed or fail with a ``403 HTTPForbidden`` error."
msgstr ""
"``os_compute_api:servers:create:zero_disk_flavor`` 정책 규칙이 도입되었으며, 뒤로 호환성에 대한"
" 보장을 위해 기본적으로 ``rule:admin_or_owner``로 설정되지만, 컴퓨터 API가 서버 생성 요청이 0 개의 루트 "
"디스크가 있는_flavor_를 사용해야면 볼륨-백업이 필요하거나,  ``403 HTTPForbidden`` 오류로 실패해야한다."

#: ../../<reno.sphinxext stable/pike>:1398
msgid ""
"A new request_log middleware is created to log REST HTTP requests even if "
"Nova API is not running under eventlet.wsgi. Because this is an api-"
"paste.ini change, you will need to manually update your api-paste.ini with "
"the one from the release to get this functionality. The new request logs "
"will only emit when it is detected that nova-api is not running under "
"eventlet, and will include the microversion of the request in addition to "
"all the previously logged information."
msgstr ""
"nova API가 eventlet.wsgi에서 실행되지 않는 경우에도 REST HTTP 요청을 로그하는 새로운 request_log "
"미들웨어가 생성됩니다. 이는 api-paste.ini의 변경이므로 api-paste.ini를 release의 version과 일치시키는 "
"것을 수동으로 업데이트해야 합니다. 새로운 로그 정보는 nova-api가 eventlet에서 실행되지 않는 경우에만 발행되며, 이전에 "
"로그된 모든 정보와 함께 요청의 마이크로 버전도 포함됩니다."

#: ../../<reno.sphinxext stable/rocky>:549
msgid "A new zVM virt driver is now available."
msgstr "새로운 zVM virt 드라이버가 현재 उपलबуществ합니다."

#: ../../<reno.sphinxext stable/pike>:1577
msgid ""
"A number of `nova-manage` commands have been deprecated. The commands, along"
" with the reasons for their deprecation, are listed below:"
msgstr "`nova-manage` 명령어의 수는 已弃Use되었습니다. 그 명령어와 그弃Use 이유는 아래에 나열되어 있습니다."

#: ../../<reno.sphinxext unmaintained/xena>:348
msgid ""
"A number of commands have been managed to ``nova-manage`` to help update "
"stale volume attachment connection info for a given volume and instance."
msgstr ""
"다음은 glossary에 정의된 단어를 사용하여 번역됩니다.\n"
"\n"
"* \"commands\" → \"명령\"\n"
"* \"managed\" → \"관리\"\n"
"* \"nova-manage\" → \"nova-manage\"\n"
"* \"update\" → \"업데이트\"\n"
"* \"stale\" → \"기존\"\n"
"* \"volume\" → \"볼륨\"\n"
"* \"attachment\" → \"연결\"\n"
"* \"connection\" → \"접속\"\n"
"* \"info\" → \"정보\"\n"
"* \"given\" → \"given\"\n"
"* \"instance\" → \"인스턴스\"\n"
"\n"
"번역된 텍스트는 다음과 같습니다.\n"
"\n"
"nova-manage 명령이 여러 가지를 관리하여 기존 볼륨 연결 접속 정보를 업데이트하여 특정 볼륨과 인스턴스를 위해 도와줍니다."

#: ../../<reno.sphinxext unmaintained/xena>:524
msgid ""
"A number of scheduler-related config options were renamed during the 15.0.0 "
"(Ocata) release. The deprecated aliases have now been removed. These are:"
msgstr ""
"다음은 15.0.0 (Ocata) 릴리스 시 스케줄러 관련 구성 옵션의 이름이 변경되었으며, 비활성화된 별명은 현재 제거되었습니다. \n"
"\n"
"이們은 다음과 같습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:433
msgid ""
"A serial number is extracted from PCI VPD of network devices (if present) "
"and is sent to Neutron in port updates."
msgstr "PCI VPD의 네트워크 장치 (이것이 존재할 경우)에서 시리얼 번호를 추출하고 Neutron에 포트 업데이트에서 보냄."

#: ../../<reno.sphinxext stable/ussuri>:1144
msgid ""
"A server is created without an availability zone specified but with pre-"
"existing volume block device mappings"
msgstr "서버는 가용 구역을 지정하지 않았지만 이전에 volume 블록 디스크 매핑이 존재하는 경우에만 생성됩니다."

#: ../../<reno.sphinxext stable/queens>:273 stable/rocky>:1447
msgid ""
"A simple warning will reference the above if this issue is encountered by "
"Nova however operators of the environment will still need to update Libvirt "
"to a version where this issue has been fixed to resolve the issue."
msgstr ""
"아래에 설명된 내용이 Nova에 의해遇치면 간단한 경고가 참조되지만, 환경의 운영자들은 여전히 이 문제가 해결되기 위해 이 문제가 해결된"
" 버전으로 Libvirt를 업데이트해야 한다."

#: ../../<reno.sphinxext stable/queens>:1360
msgid ""
"A sys-admin privsep daemon has been added and needs to be included in your "
"rootwrap configuration."
msgstr "시스템 관리자의 privsep 데몬이 추가되었으며, 루트wrapping 구성에 포함되어야 합니다."

#: ../../<reno.sphinxext stable/train>:14 stable/ussuri>:60
#: unmaintained/victoria>:154 unmaintained/wallaby>:314 unmaintained/xena>:616
msgid ""
"A vulnerability in the console proxies (novnc, serial, spice) that allowed "
"open redirection has been `patched`_. The novnc, serial, and spice console "
"proxies are implemented as websockify servers and the request handler "
"inherits from the python standard SimpleHTTPRequestHandler. There is a "
"`known issue`_ in the SimpleHTTPRequestHandler which allows open redirects "
"by way of URLs in the following format::"
msgstr ""
"`콘솔 프록시( novnc, serial, spice )의 보안 취약점이 열린 리다이렉션을 허용하는 것이 `patched`_ been. "
"novnc, serial, spice console proxies는 websockify 서버로 구현되어 request handler는 "
"Python 표준 SimpleHTTPRequestHandler의 상속을 받습니다. SimpleHTTPRequestHandler에 대한 `"
" bekannt issue`_은 URL의 다음 형식으로 열린 리다이렉션을 허용합니다::"

#: ../../<reno.sphinxext unmaintained/2023.1>:560 unmaintained/xena>:136
#: unmaintained/yoga>:213 unmaintained/zed>:164
msgid ""
"A workaround has been added to the libvirt driver to catch and pass "
"migrations that were previously failing with the error:"
msgstr "집합에 workaround이 추가되어 이전에 오류로 실패하던 이ми그레이션을 잡고 전달하는 데 도움이 됩니다."

#: ../../<reno.sphinxext stable/rocky>:568
msgid ""
"AArch64 architecture is supported by Nova with libvirt min version 3.6.0. "
"See the Nova `support matrix`_ for more details."
msgstr "AArch64 아키텍처는 노바가 libvirt min 버전 3.6.0을 지원합니다. 노바의 지원 매트릭스_를 참조하십시오."

#: ../../<reno.sphinxext stable/pike>:301 stable/queens>:1200
msgid ""
"AArch64 currently lacks ``host-model`` support because neither libvirt nor "
"QEMU are able to tell what the host CPU model exactly is and there is no CPU"
" description code for ARM(64) at this point."
msgstr ""
"AArch64 현재 호스트 모델 지원을 lacking 하기 때문에 neither libvirt nor QEMU가 호스트 CPU 모델을 "
"정확하게 알 수 없고 ARM(64)에 대한 CPU 설명 코드가 현재 존재하지 않습니다."

#: ../../<reno.sphinxext stable/2024.2>:366
msgid ""
"AMI, AKI, and ARI image formats are now deprecated in Nova. The special- "
"case handling for them will be removed in a future release, and Nova will "
"stop being able to boot from them at that time. The kernel-id and ramdisk-id"
" linkages will continue to be honored from base images and are unaffecfted "
"by this deprecation."
msgstr ""
"AMI, AKI, 및 ARI 이미지 형식은 현재 노바에서 비활성화되었습니다. 그 중에서 특별한 경우를 처리하는 처리는 향후 릴리스에서 "
"제거되며, 노바는 그 때부터 부트할 수 없게 됩니다. 커널-아이디와 RAMDISK-아이디 연결은 기본 이미지에서 계속 존중되고, 이 "
"비활성화에 의해 영향을 받지 않습니다."

#: ../../<reno.sphinxext stable/pike>:1831
msgid ""
"API calls to ``/os-quota-sets`` and flavor access will now attempt to "
"validate the project_id being operated on with Keystone. If the user token "
"has enough permissions to perform ``GET /v3/projects/{project_id}``, and the"
" Keystone project does not exist, a 400 BadRequest will be returned to "
"prevent invalid project data from being put in the Nova database. This fixes"
" an effective silent error where the project_id would be stored even if it "
"was not a valid project_id in the system."
msgstr ""
"API calls to ``/os-quota-sets`` 및 flavor 접근은 현재 Keystone에 의해 운영되는 프로젝트 ID를 "
"유효화하려고 시도합니다. 사용자 토큰이 ``GET /v3/projects/{project_id}``를 수행할 수 있는 권한이 있는지 "
"확인하고, Keystone 프로젝트가 존재하지 않으면 400 BadRequest를 반환하여 Nova 데이터베이스에 비정상 프로젝트 ID가"
" 들어가지 않도록 방지합니다. 이로 인해 프로젝트 ID가 유효하지 않은 프로젝트 ID로도 저장되는 효과적인 무시 오류가 해결됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:989
msgid ""
"API configuration options have been moved to the 'api' group. They should no"
" longer be included in the 'DEFAULT' group. Options affected by this change:"
msgstr ""
"API 구성 옵션은 'api' 그룹으로 이동되었으며, 더 이상 'DEFAULT' 그룹에 포함되지 않아야 합니다. 이 변경에 영향을 받는 "
"옵션:"

#: ../../<reno.sphinxext stable/stein>:722
msgid "API limitations:"
msgstr "API 제한"

#: ../../<reno.sphinxext stable/stein>:717
msgid ""
"API microversion 2.72 adds support for creating servers with neutron ports "
"that has resource request, e.g. neutron ports with `QoS minimum bandwidth "
"rule`_. Deleting servers with such ports have already been handled properly "
"as well as detaching these type of ports."
msgstr ""
"API microversion 2.72은 네트워크 포트를 포함한 서버를 생성하는 데 지원을 제공합니다. 예를 들어, `QoS "
"minimum bandwidth rule`를 포함한 네트워크 포트를 포함한 서버를 생성할 수 있습니다. 이러한 포트가 있는 서버를 "
"삭제하는 것은 이미 적절하게 처리되었으며, 이러한 tipo의 포트를 분리하는 것도 이미 처리되었습니다."

#: ../../<reno.sphinxext stable/train>:538
msgid ""
"API microversion 2.74 adds support for specifying optional ``host`` and/or "
"``hypervisor_hostname`` parameters in the request body of ``POST /servers``."
" These request a specific destination host/node to boot the requested "
"server. These parameters are mutually exclusive with the special "
"``availability_zone`` format of ``zone:host:node``. Unlike "
"``zone:host:node``, the ``host`` and/or ``hypervisor_hostname`` parameters "
"still allow scheduler filters to be run. If the requested host/node is "
"unavailable or otherwise unsuitable, earlier failure will be raised. There "
"will be also a new policy named "
"``compute:servers:create:requested_destination``. By default, it can be "
"specified by administrators only."
msgstr ""
"API microversion 2.74은 `POST /servers`의 요청 바디에 `host` 및/or "
"`hypervisor_hostname` 매개 변수를 지정하는 것을 지원합니다. 이 매개 변수는 특정 목적지 호스트/노드를 부트하는 요청을"
" 요청합니다. 이러한 매개 변수는 `zone:host:node` 형식의 특별한 `availability_zone` 형식과 상호 배치할 수"
" 있습니다. `zone:host:node`와는 달리 `host` 및/or `hypervisor_hostname` 매개 변수는 스케줄러 "
"필터를 실행할 수 있습니다. 요청된 호스트/노드가 unavailable 또는 다른 경우 이전에 실패하는 것을.raise합니다. 또한 "
"새로운 정책이 `compute:servers:create:requested_destination`로 명명되며, 기본적으로 관리자만 지정할"
" 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1023
msgid "API restrictions:"
msgstr "API 제한"

#: ../../<reno.sphinxext stable/rocky>:1551
msgid "Accept this change along with the performance penalty"
msgstr "이 변경 사항을 성능 부하와 함께 수용하십시오."

#: ../../<reno.sphinxext unmaintained/wallaby>:460
msgid "Add Cyborg shelve/unshelve support."
msgstr "Cyborg shelf/unshef 지원을 추가합니다."

#: ../../<reno.sphinxext unmaintained/yoga>:425
msgid ""
"Add VPD capability parsing support when a PCI VPD capability is exposed via "
"node device XML in Libvirt. The XML data from Libvirt is parsed and "
"formatted into PCI device JSON dict that is sent to Nova API and is stored "
"in the extra_info column of a PciDevice."
msgstr ""
"PCI VPD 능력의 capability parsing를 추가하여 Libvirt에서 node device XML을 통해 노출되는 PCI "
"VPD capability을 지원합니다. Libvirt에서 가져온 XML 데이터를 Libvirt에서 파싱하고 formatting하여 "
"PCI device JSON dict로 변환하여 Nova API에 전송하고 PciDevice의 extra_info column에 "
"저장합니다."

#: ../../<reno.sphinxext stable/ussuri>:562
msgid ""
"Add ``--cell`` option to the ``nova-manage placement heal_allocations`` "
"command. This option allows healing instance allocations within a specific "
"cell."
msgstr ""
"``--cell`` 옵션을 ``nova-manage placement heal_allocations`` 명령에 추가합니다. 이 옵션은 "
"특정 세ลล에서 인스턴스 할당을 healing 할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:401
msgid ""
"Add ``--force`` option to the ``nova-manage placement heal_allocations`` "
"command to forcefully heal allocations for a specific instance."
msgstr ""
"``--force`` 옵션을 ``nova-manage placement heal_allocations`` 명령에 추가하여 특정 인스턴스의"
" 할당을 강제적으로 heal합니다."

#: ../../<reno.sphinxext stable/rocky>:587
msgid ""
"Add ``CPUWeigher`` weigher. This can be used to spread (default) or pack "
"workloads on hosts based on their vCPU usage. This can be configured using "
"the ``[filter_scheduler] cpu_weight_multiplier`` configuration option."
msgstr ""
"``CPUWeigher`` 가중치를 추가합니다. 이 가중치를 사용하면 호스트에 따라 vCPU 사용에 따라 작업을 분산하거나 패킹할 수 "
"있습니다. 이 가중치를 설정하는には ``[filter_scheduler] cpu_weight_multiplier`` 설정 옵션을 사용할 "
"수 있습니다."

#: ../../<reno.sphinxext stable/pike>:670
msgid ""
"Add ``PCIWeigher`` weigher. This can be used to ensure non-PCI instances "
"don't occupy resources on hosts with PCI devices. This can be configured "
"using the ``[filter_scheduler] pci_weight_multiplier`` configuration option."
msgstr ""
"``PCIWeigher`` 가중치 가중자를 추가합니다. 이 가중자는 PCI 장치가 있는 호스트에 resource를 점유하지 않는 "
"비-PCI 인스턴스를 보장하는 데 사용할 수 있습니다. 이 가중자는 ``[filter_scheduler] "
"pci_weight_multiplier`` 구성 옵션을 사용하여 구성할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:756
msgid ""
"Add ``policy`` and ``rules`` fields in the request of POST ``/os-server-"
"groups``. The ``policy`` represents the name of policy. The ``rules`` field,"
" which is a dict, can be applied to the policy, which currently only "
"supports ``max_server_per_host`` for ``anti-affinity`` policy."
msgstr ""
"``policy`` 및 ``rules`` 필드를 ``POST`` ``/os-server-groups`` 요청의 필드에 추가합니다. "
"``policy``는 정책 이름을 나타냅니다. ``rules`` 필드는 dict로 구성된 필드이며, 현재 ``anti-affinity``"
" 정책에만 ``max_server_per_host``를 지원하는 ``policy``에 적용할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:719
msgid ""
"Add ``required`` query parameter to the ``GET /allocation_candidates`` API "
"in new placement microversion 1.17. The parameter accepts a list of traits "
"separated by ``,``, which is used to further limit the list of allocation "
"requests to resource providers that have the capacity to fulfill the "
"requested resources AND *collectively* have all of the required traits "
"associated with them. In the same microversion, the provider summary "
"includes the traits associated with each provider."
msgstr ""
"``required`` 파라미터를 `GET /allocation_candidates` API의 새로운 배치 마이크로 버전 1.17에 "
"추가합니다. 이 파라미터는 ``,``로 구분된 특성의 목록을 수용합니다. 이 목록은 자원 제공자가 requested 리소스를 "
"fulfillment 할 수 있는 자원과 함께 모든 required 특성이 them에关联되어 있는 경우에 한정된 allocation 요청"
" 목록을 더 제한합니다. 마이크로 버전 1.17에서 제공자 요약에는 각 제공자와 관련된 특성이 포함됩니다."

#: ../../<reno.sphinxext stable/queens>:829
msgid ""
"Add a ``nova-manage cell_v2 list_hosts`` command for listing hosts in one or"
" all v2 cells."
msgstr ""
"nova-manage cell_v2 list_hosts 명령을 추가하여 v2 세ลล에서 하나 이상의 호스트를 liệt어낼 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1393
msgid ""
"Add a new option of ``image_handler`` in the ``xenapi`` section for "
"configuring the image handler plugin which will be used by XenServer to "
"download or upload images. The value for this option should be a short name "
"representing a supported handler."
msgstr ""
"``image_handler`` 옵션을 ``xenapi`` 섹션에 추가하여 XenServer가 이미지 tải-down 또는 tải-"
"up로드를 위해 사용하는 이미지 핸들러 플러그인을 구성하는 옵션을 추가합니다. 이 옵션의 값은 지원되는 핸들러를 대표하는 짧은 이름이 될"
" 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:759
msgid ""
"Add granularity to the ``os_compute_api:os-flavor-manage`` policy with the "
"addition of distinct actions for create and delete:"
msgstr ""
"``os_compute_api:os-flavor-manage`` 정책에 granularity을 추가하여 create와 delete의 "
"distinct actions을 추가합니다."

#: ../../<reno.sphinxext unmaintained/zed>:299
msgid ""
"Add new ``hw:locked_memory`` extra spec and ``hw_locked_memory`` image "
"property to lock memory on libvirt guest. Locking memory marks the guest "
"memory allocations as unmovable and unswappable. ``hw:locked_memory`` extra "
"spec and ``hw_locked_memory`` image property accept boolean values in string"
" format like 'Yes' or 'false' value. Exception `LockMemoryForbidden` will "
"raise, if you set lock memory value but not set either flavor extra spec "
"``hw:mem_page_size`` or image property ``hw_mem_page_size``, so we can "
"ensure that the scheduler can actually account for this correctly and "
"prevent out of memory events."
msgstr ""
"``hw:locked_memory`` 추가 spec 및 ``hw_locked_memory`` 이미지 속성을 사용하여 libvirt "
"게스트의 메모리를 잠기게 할 수 있습니다. 메모리를 잠기게 할 때는 게스트의 메모리 할당을 무조건적이고 교환할 수 없는 것으로 "
"표시합니다. ``hw:locked_memory`` spec 및 ``hw_locked_memory`` 이미지 속성은 'Yes' 또는 "
"'false'와 같은 문자열 형식의 부울 값으로 수신됩니다. LockMemoryForbidden 예외가 발생할 수 있습니다. lock "
"memory value를 설정할 때는 flavor spec ``hw:mem_page_size`` 또는 image property "
"``hw_mem_page_size``를 설정하지 않으면, scheduler가 실제로 이 값을 계산할 수 있도록 보장하고 메모리 "
"오버lfow 이벤트를 방지할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:729
msgid ""
"Add pagination support and ``changes-since`` filter for os-instance-actions "
"API. Users can now use ``limit`` and ``marker`` to perform paginated query "
"when listing instance actions. Users can also use ``changes-since`` filter "
"to filter the results based on the last time the instance action was "
"updated."
msgstr ""
"``os-instance-actions API``에 pagination support와 ``changes-since`` 필터를 "
"추가합니다. 사용자는 ``limit``과 ``marker``을 사용하여 인스턴스 액션을 listing할 때 paginated 쿼리를 "
"수행할 수 있습니다. 또한, 사용자는 ``changes-since`` 필터를 사용하여 인스턴스 액션의 마지막 업데이트 시점에 기반을 둔 "
"결과를 필터링할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:739
msgid ""
"Add pagination support and ``changes-since`` filter for os-migrations API. "
"Users can now use ``limit`` and ``marker`` to perform paginate query when "
"listing migrations."
msgstr ""
"``os-migrations API 에 pagination support 및 changes-since filter 를 추가합니다. "
"사용자는 now limit 및 marker 를 사용하여 paginate query 를 수행할 수 있습니다. migrations 목록을 "
"listing 할 때. ``limit`` 및 ``marker``를 사용하여 paginate query를 수행할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:653
msgid ""
"Add support for LAN9118 as a valid nic for hw_vif_model property in qemu."
msgstr "LAN9118를 hw_vif_model 속성의 유효 nic로 지원하도록 추가합니다."

#: ../../<reno.sphinxext stable/rocky>:720
msgid ""
"Add support for configuring the ``rx_queue_size`` and ``tx_queue_size`` "
"options in the QEMU virtio-net driver by way of nova.conf. Only supported "
"for vhost/vhostuser interfaces"
msgstr ""
"``rx_queue_size`` 및 ``tx_queue_size`` 옵션을 nova.conf를 통해 구성하는 QEMU virtio-net"
" 드라이버에 추가 지원을 합니다. vhost/vhostuser 인터페이스만 지원됩니다."

#: ../../<reno.sphinxext stable/queens>:808
msgid ""
"Add support for graceful shutdown of VMware instances. The timeout parameter"
" of the power_off method is now considered by the VMware driver. If you "
"specify a timeout greater than 0, the driver calls the appropriate soft "
"shutdown method of the VMware API and only forces a hard shutdown if the "
"soft shutdown did not succeed before the timeout is reached."
msgstr ""
"VMware 인스턴스의 조용한 종료를 지원합니다. VMware 드라이버에서는 now power_off method의 timeout 매개 "
"변수를 고려합니다. timeout이 0보다 더 큰 경우, 드라이버는 VMware API의 적절한 소프트 셰ด다운 메소드를 호출하고, "
"소프트 셰드다운이 성공하지 않으면 종료를 강제할 때만 하드 셰드다운을 강제합니다."

#: ../../<reno.sphinxext stable/rocky>:158 stable/stein>:361
#: stable/train>:1358
msgid ""
"Add support for noVNC >= v1.1.0 for VNC consoles. Prior to this fix, VNC "
"console token validation always failed regardless of actual token validity "
"with noVNC >= v1.1.0. See https://bugs.launchpad.net/nova/+bug/1822676 for "
"more details."
msgstr ""
"noVNC >= v1.1.0를 위한 VNC 콘솔에 대한 지원을 추가합니다. 이전에 이修复이 이루어지기 전에 noVNC >= v1.1.0의"
" 경우, VNC 콘솔 토큰의 유효성에 상관없이 always failed했습니다. 더 많은 정보는 "
"https://bugs.launchpad.net/nova/+bug/1822676 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:737
msgid ""
"Add support for reporting CPU traits to Placement in libvirt driver. For "
"more detail, see https://specs.openstack.org/openstack/nova-"
"specs/specs/rocky/approved/report-cpu-features-as-traits.html and "
"https://docs.openstack.org/nova/latest/user/support-matrix.html."
msgstr ""
"집합에 CPU 특성에 대한 보고를 지원하도록 libvirt 드라이버에 추가합니다. 더 많은รายละเอ시는 "
"https://specs.openstack.org/openstack/nova-"
"specs/specs/rocky/approved/report-cpu-features-as-traits.html 및 "
"https://docs.openstack.org/nova/latest/user/support-matrix.html 에서 확인하실 수 "
"있습니다."

#: ../../<reno.sphinxext unmaintained/xena>:444
msgid ""
"Add support for smartnic via Cyborg device profiles in Neutron ports with "
"vnic type ``accelerator-direct``. When such port is used Cyborg will manage "
"the smartnic and Nova will pass through the smartnic VF to the server. Note "
"that while vnic type ``accelerator-direct-physical`` also exists in Neutron "
"it is not yet supported by Nova and the server create request will fail with"
" such port."
msgstr ""
"Cyborg 장치 프로필을 통해 Neutron 포트에 스마트 NIC 지원을 추가합니다. 스마트 NIC를 사용할 때 Cyborg은 스마트 "
"NIC를 관리하고 Nova는 스마트 NIC VF를 서버로 forwarding합니다. 그러나 vnic type ``accelerator-"
"direct-physical``도 Neutron에서 존재합니다. 그러나 Nova와 서버가 이 포트를 사용할 때는 요청이 실패합니다."

#: ../../<reno.sphinxext unmaintained/xena>:437
msgid ""
"Add support for the ``bochs`` libvirt video model.  This is a legacy-free "
"video model that is best suited for UEFI guests.  In limited cases (e.g. if "
"the guest does not depend on direct VGA hardware access), it can be useable "
"for BIOS guests as well."
msgstr ""
"``bochs`` libvirt 비디오 모델에 지원을 추가합니다.  이 모델은 유니버스 UEFI 게스트에 적합하며, legacy-"
"free입니다.  유니버스 UEFI 게스트가 직접 VGA 하드웨어 접근에 의존하지 않는 경우(예: 게스트가 직접 VGA 하드웨어 접근에 "
"의존하지 않는 경우), BIOS 게스트에도 사용 가능합니다."

#: ../../<reno.sphinxext stable/train>:135 stable/ussuri>:195
#: unmaintained/victoria>:758
msgid ""
"Add support for the ``hw:hide_hypervisor_id`` extra spec. This is an alias "
"for the ``hide_hypervisor_id`` extra spec, which was not compatible with the"
" ``AggregateInstanceExtraSpecsFilter`` scheduler filter. See `bug 1841932 "
"<https://bugs.launchpad.net/nova/+bug/1841932>`_ for more details."
msgstr ""
"``hw:hide_hypervisor_id``를 지원하도록 추가합니다. 이는 ``hide_hypervisor_id`` extra "
"spec의 별명이며, ``AggregateInstanceExtraSpecsFilter`` 스케줄러 필터와 호환되지 wasn't "
"compatible.  `bug 1841932 <https://bugs.launchpad.net/nova/+bug/1841932>`_에 "
"더 많은 정보가 있습니다."

#: ../../<reno.sphinxext stable/rocky>:651
msgid ""
"Add support, in a new placement microversion 1.21, for the ``member_of`` "
"query parameter, representing one or more aggregate UUIDs. When supplied, it"
" will filter the returned allocation candidates to only those "
"resource_providers that are associated with (\"members of\") the specified "
"aggregate(s). This parameter can have a value of either a single aggregate "
"UUID, or a comma-separated list of aggregate UUIDs. When specifying more "
"than one aggregate, a resource provider needs to be associated with at least"
" one of the aggregates in order to be included; it does not have to be "
"associated with all of them. Because of this, the list of UUIDs must be "
"prefixed with ``in:`` to represent the logical ``OR`` of the selection."
msgstr ""
"``member_of`` 쿼리 파라미터를 포함하여, 새로운 배치 microversion 1.21에서 집합 UUID를 나타내는 "
"``member_of`` 쿼리 파라미터를 지원합니다. 이 파라미터가 제공되면, 반환된 할당 후보를 only those "
"resource_providers가 포함되어 있는지 확인하여,指定된 집합(s)와 관련된(``members of``) 리소스 제공자만을 "
"필터링합니다. 이 파라미터는 단일 집합 UUID 또는 집합 UUID를 구분하는 쉼표-separated 목록으로 사용할 수 있습니다. 여러"
" 집합을 지정할 때, 리소스 제공자는 적어도 하나의 집합과 관련되어 있어 포함되어야 합니다. 그러나 모든 집합과 관련되어 있어야 할 "
"필요는 없습니다. 따라서, UUID 목록은 ``in:``로 prefix되어야 logical ``OR``를 나타내는 것을 의미합니다."

#: ../../<reno.sphinxext stable/queens>:708
msgid ""
"Add support, in new placement microversion 1.16, for a ``limit`` query "
"parameter when making a ``GET /allocation_candidates`` request. The "
"parameter accepts an integer value, `N`, which limits the number of "
"candidates returned. A new configuration item "
"``[placement]/randomize_allocation_candidates``, defaulting to `False`, "
"controls how the limited results are chosen. If `True`, a random sampling of"
" the entire result set is taken, otherwise the first N results are returned."
msgstr ""
"``limit`` 파라미터를 포함하여 새로운 배치 microversion 1.16에 대한 지원을 추가합니다. 이 파라미터는 "
"`N`의整수值을 받아서 `GET /allocation_candidates` 요청을 할 때候, 제한된 결과를 반환합니다. 새로운 구성 항목"
" ``[placement]/randomize_allocation_candidates``, 기본적으로 `False`로 설정되어 있는지 "
"확인합니다. `True`면 전체 결과 집합의 무작위 샘플링이 수행되고, 반면에 `False`면 첫 `N` 결과가 반환됩니다."

#: ../../<reno.sphinxext unmaintained/victoria>:457
msgid ""
"Add the ``mixed`` instance CPU allocation policy for instance mixing with "
"both ``PCPU`` and ``VCPU`` resources. This is useful for applications that "
"wish to schedule the CPU intensive workload on the ``PCPU`` and the other "
"workloads on ``VCPU``. The mixed policy avoids the necessity of making all "
"instance CPUs to be pinned CPUs, as a result, reduces the consuption of "
"pinned CPUs and increases the instance density."
msgstr ""
"``mixed`` 인스턴스 CPU 할당 정책을 ``PCPU``와 ``VCPU`` 리소스를 모두 포함하여 인스턴스 混합을 위해 추가합니다."
" 이 정책은 CPU 비중이 높은 작업을 ``PCPU``에, 다른 작업을 ``VCPU``에 스케줄 करन을 원하는 애플리케이션에 "
"유용합니다. mixed 정책은 모든 인스턴스 CPU를 고정 CPU로 고정해야 하는必要성이 없도록 avoids합니다. 이로 인해 고정 "
"CPU의 소비를 줄이고 인스턴스 밀도를 증가시킵니다."

#: ../../<reno.sphinxext unmaintained/victoria>:437
msgid ""
"Add the ability to use ``vmxnet3`` NIC on a host using the QEMU/KVM driver. "
"This allows the migration of an ESXi VM to QEMU/KVM, without any driver "
"changes. ``vmxnet3`` comes with better performance and lower latency "
"comparing to an emulated driver like ``e1000``."
msgstr ""
"``vmxnet3`` 네트워크 인터페이스를 호스트에서 사용할 수 있도록 QEMU/KVM 드라이버를 사용할 수 있도록 추가합니다. 이 "
"기능은 ESXi VM의 이식을 QEMU/KVM에서 이식할 수 있도록 하며, 드라이버 변경이 필요하지 않습니다. ``vmxnet3``는 "
"``e1000``와 같은 э뮬레이션 드라이버와 비교하여 더 좋은 성능과 낮은 지연을 제공합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:471
msgid ""
"Add the extra spec ``hw:cpu_dedicated_mask`` to set the pinned CPUs for the "
"mixed instance. This is a core mask and can be used to include or exclude "
"CPUs. Any core not included or explicitly excluded is treated as a shared "
"CPU."
msgstr ""
"hw:cpu_dedicated_mask를 추가하여 혼합 인스턴스에 고정 CPU를 설정합니다. 이는 코어 마스크의 핵심이며, 포함 또는 "
"제외를 포함하여 CPU를 포함할 수 있습니다. 제외되지 않은 코어 또는 명시적으로 제외된 코어는 공유 CPU로 처리됩니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:472
msgid ""
"Added IP addresses to the metadata in libvirt XML. If an instance has more "
"than one IP address, enumerate those IP addresses. The port attach or detach"
" is performed dynamically after the creation of the instance. Every time "
"there is a change, it is reflected in the contents of the XML."
msgstr ""
"libvirt XML에 metadata에 IP 주소를 추가합니다. instance가 하나 이상의 IP 주소를 가지고 있다면, 이 IP "
"주소를 집합화합니다. 포트를 연결하거나 detached하는 것은 instance가 생성된 후 동적으로 수행됩니다. 변경이 발생할 때마다,"
" XML의 내용이 반영됩니다."

#: ../../<reno.sphinxext stable/train>:627
msgid ""
"Added a new ``locked_reason`` option in microversion 2.73 to the ``POST "
"/servers/{server_id}/action`` request where the action is lock. It enables "
"the user to specify a reason when locking a server. This information will be"
" exposed through the response of the following APIs:"
msgstr ""
"``locked_reason`` 옵션을 마이크로 버전 2.73에서 ``POST /servers/{server_id}/action`` "
"요청의 액션이.lock인 경우 추가했습니다. 이 액션은 서버를 잠기게 할 때 사용됩니다. 사용자는 서버를 잠기게 할 때 이유를 지정할 수"
" 있습니다. 이 정보는 다음 API의 응답에서 노출됩니다."

#: ../../<reno.sphinxext stable/stein>:1082
msgid ""
"Added a new ``unique`` choice to the ``[libvirt]/sysinfo_serial`` "
"configuration which if set will result in the guest serial number being set "
"to ``instance.uuid``. This is now the default value of the "
"``[libvirt]/sysinfo_serial`` config option and is the recommended choice "
"since it ensures the guest serial is the same even if the instance is "
"migrated between hosts."
msgstr ""
"``unique`` 옵션을 ``[libvirt]/sysinfo_serial`` 구성에 추가했습니다. 이 옵션을 설정하면 게스트 시리얼 "
"번호를 ``instance.uuid``로 설정하게 됩니다. 이는 ``[libvirt]/sysinfo_serial`` 구성 옵션의 기본값이"
" 되고, 이 옵션을 사용하면 게스트 시리얼이 호스트를 이동할 때도 동일하게 유지됩니다."

#: ../../<reno.sphinxext stable/queens>:1046
msgid ""
"Added a new boolean configuration option "
"``[filter_scheduler]shuffle_best_same_weighed_hosts`` (default is False)."
msgstr ""
"``[필터 스케줄러] shuffle_best_same_weighed_hosts (기본값은 False) ''를 추가한 새로운 불리hood "
"구성 옵션을 추가했습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:14 unmaintained/wallaby>:166
#: unmaintained/xena>:210 unmaintained/yoga>:324
msgid ""
"Added a new configuration option "
"``[workarounds]/enable_qemu_monitor_announce_self`` that when enabled causes"
" the Libvirt driver to send a announce_self QEMU monitor command post live-"
"migration. Please see `bug 1815989 "
"<https://bugs.launchpad.net/nova/+bug/1815989>`_ for more details. Please "
"note that this causes the domain to be considered tainted by libvirt."
msgstr ""
"``[workarounds]/enable_qemu_monitor_announce_self`` 옵션을 추가하여, 이 옵션을 활성화하면 "
"라이브irt 드라이버가 라이브-migration 시 announce_self QEMU 모니터 명령을 보냄으로써, 새로운 구성 옵션을 "
"추가합니다. 더 많은 정보는 `bug 1815989 "
"<https://bugs.launchpad.net/nova/+bug/1815989>`_에서 확인할 수 있습니다. 그러나, 이 옵션을 "
"활성화하면 라이브irt에 의해 domain이汚染된 것으로 간주됩니다."

#: ../../<reno.sphinxext stable/rocky>:891
msgid ""
"Added a new flavor extra_spec, ``hide_hypervisor_id``, which hides the "
"hypervisor signature for the guest when true ('kvm' won't appear in "
"``lscpu``). This acts exactly like and in parallel to the image property "
"``img_hide_hypervisor_id`` and is useful for running the nvidia drivers in "
"the guest. Currently, this is only supported in the libvirt driver."
msgstr ""
"``extra_spec``의 새로운 flavor ``hide_hypervisor_id``를 추가했습니다. "
"``hide_hypervisor_id``가 True면, 게스트에서 가상화자 서명이-hidden됩니다. ('kvm'는 ``lscpu``에 "
"나타나지 않습니다.). 이것은 ``img_hide_hypervisor_id`` 속성과 동등하고 동시에 동작하며, 게스트에서 nvidia "
"드라이버를 실행하는 데 유용합니다. 현재, 이것은 libvirt 드라이버에서만 지원됩니다."

#: ../../<reno.sphinxext stable/queens>:1110
msgid ""
"Added a number of new configuration options to the ``[vnc]`` group, which "
"together allow for the configuration of authentication used between the "
"*nova-novncproxy* server and the compute node VNC server."
msgstr ""
"``[vnc]`` 그룹에 새로운 구성 옵션을 추가했습니다. 이 구성 옵션은 *nova-novncproxy* 서버와 컴퓨터 노드 VNC "
"서버 사이에 사용되는 인증 구성에 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1261
msgid ""
"Added ability to extend an attached ScaleIO volume when using the libvirt "
"compute driver."
msgstr "ScaleIO 볼륨을 사용할 때 libvirt 컴퓨터 드라이버를 사용할 때 볼륨을 확장할 수 있는 능력을 추가했습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:702
msgid ""
"Added boot order support in the Hyper-V driver. The HyperVDriver can now set"
" the requested boot order for instances that are Generation 2 VMs (the given"
" image has the property \"hw_machine_type=hyperv-gen2\"). For Generation 1 "
"VMs, the spawned VM's boot order is changed only if the given image is an "
"ISO, booting from ISO first."
msgstr ""
"Hyper-V 드라이버에서 부트 오더 지원을 추가했습니다. HyperVDriver는 now Generation 2 VMs (given "
"image의 속성 \"hw_machine_type=hyperv-gen2\"이 있는 인스턴스에)에서 요청된 부트 오더를 설정할 수 "
"있습니다. Generation 1 VM의 경우, given image가 ISO인 경우에만 VM의 부트 오더가 변경되며, ISO에서 "
"부트하는 것을 우선합니다."

#: ../../<reno.sphinxext stable/stein>:697
msgid ""
"Added configuration option ``[api]/local_metadata_per_cell`` to allow users "
"to run Nova metadata API service per cell. Doing this could provide "
"performance improvement and data isolation in a multi-cell deployment. But "
"it has some caveats, see the `Metadata api service in cells v2 layout`_ for "
"more details."
msgstr ""
"``[API]/local_metadata_per_cell`` 옵션을 사용하여 사용자들이 세포별로 노바 메타데이터 API 서비스를 실행할 수 있도록 추가했습니다. 이 방법은 여러 세포 배포에서 성능 개선과 데이터 분리에 도움이 될 수 있습니다. 그러나 이 방법에는 caveats가 있습니다. 더 많은 정보는 `Metadata api service in cells v2 layout`_에서 확인할 수 있습니다.\n"
"\n"
"*   [API]/local_metadata_per_cell\n"
"*   `Metadata api service in cells v2 layout`"

#: ../../<reno.sphinxext origin/stable/ocata>:793
msgid ""
"Added microversion v2.40 which introduces pagination support for usage with "
"the help of new optional parameters 'limit' and 'marker'. If 'limit' isn't "
"provided, it will default to the configurable max limit which is currently "
"1000."
msgstr ""
"microversion v2.40을 추가하여 사용에 도움을 주는 새로운 옵션적인 매개 변수 'limit'와 'marker'를 도입한 "
"pagination 지원을 제공합니다. 'limit'가 제공되지 않으면 현재 1000으로 configurable max limit으로 "
"기본적으로 설정됩니다."

#: ../../<reno.sphinxext stable/pike>:701
msgid ""
"Added microversion v2.48 which standardize VM diagnostics response. It has a"
" set of fields which each hypervisor will try to fill. If a hypervisor "
"driver is unable to provide a specific field then this field will be "
"reported as 'None'."
msgstr ""
"microversion v2.48을 추가하여 VM diagnostics response를 표준화했다. 이에는 각 하이퍼바이저가fill할 "
"field의 집합이 포함되어 있다. 하이퍼바이저 드라이버가 특정 field를 제공할 수 없으면 이 field는 'None'으로 보고된다."

#: ../../<reno.sphinxext stable/2024.1>:381
msgid ""
"Added new flavor extra_specs and image properties to control the physical "
"address bits of vCPUs in Libvirt guests. This option is used to boot a guest"
" with large RAM."
msgstr ""
"새로운 flavor extra_specs 및 image 속성들을 Libvirt 게스트의 vCPUs의 물리적 주소 비트를 제어하는 데 "
"추가했습니다. 이 옵션은 큰 RAM을 사용하는 게스트를 부트하는 데 사용됩니다."

#: ../../<reno.sphinxext stable/queens>:737
msgid "Added pagination support for migrations, there are four changes:"
msgstr "집합을 추가하여 mig레이션에 페이지네이션 지원을 추가했습니다. 이에 따라 다음과 같은 4가지 변경 사항이 있습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:444
msgid ""
"Added params ``[libvirt]/rbd_destroy_volume_retries``, defaulting to 12, and"
" ``[libvirt]/rbd_destroy_volume_retry_interval``, defaulting to 5, that Nova"
" will use when trying to remove a volume from Ceph in a retry loop that "
"combines these parameters together. Thus, maximum elapsing time is by "
"default 60 seconds."
msgstr ""
"``[libvirt]/rbd_destroy_volume_retries`` 및 "
"``[libvirt]/rbd_destroy_volume_retry_interval``를 사용하여 Nova가 Ceph에서 볼륨을 제거하는 "
"재전 loop에 사용하는 매개 변수를 추가했습니다. 이 매개 변수를 결합하여 사용하는 재전 loop의 최대 시간은 기본적으로 "
"60초입니다."

#: ../../<reno.sphinxext stable/2025.2>:56
msgid ""
"Added support for AMD Secure Encrypted Virtualization – Encrypted State "
"(SEV-ES) with libvirt, extending confidential computing capabilities in Nova"
" to protect guest memory and CPU register state."
msgstr ""
"AMD 보안 암호화 가상화 – 암호화 상태 (SEV-ES)를 libvirt와 함께 지원하여 노바에서 게스트 메모리 및 CPU 레지스터 "
"상태를 보호하는 confidential computing 기능을 확장합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:714
msgid ""
"Added support for Hyper-V VMs with UEFI Secure Boot enabled. In order to "
"create such VMs, there are a couple of things to consider:"
msgstr ""
"Hyper-V VMs에 UEFI Secure Boot가 활성화된 것을 지원합니다. 이러한 VM을 생성하기 위해, 몇 가지 것을 고려해야 "
"합니다."

#: ../../<reno.sphinxext stable/pike>:923
msgid ""
"Added support for Keystone middleware feature for interaction of Nova with "
"the Glance API. With this support, if service token is sent along with the "
"user token, then the expiration of user token will be ignored. In order to "
"use this functionality a service user needs to be created first. Add the "
"service user configurations in ``nova.conf`` under ``service_user`` group "
"and set ``send_service_user_token`` flag to ``True``."
msgstr ""
"Keystone 미드웨어 기능을 Nova와 Glance API의 상호작용에 대한 지원이 추가되었다. 이 지원을 통해 사용자 토큰이 함께 "
"전송되는 경우 사용자 토큰의 만료는 무시된다. 이 기능을 사용하려면 먼저 서비스 사용자를 생성해야 한다. 서비스 사용자 설정은 "
"`nova.conf`의 `service_user` 그룹에서 `nova.conf`에 추가되어야 하며 "
"`send_service_user_token` 플래그를 `True`로 설정해야 한다."

#: ../../<reno.sphinxext origin/stable/ocata>:882
msgid ""
"Added support for Keystone middleware feature where if service token is sent"
" along with the user token, then it will ignore the expiration of user "
"token. This helps deal with issues of user tokens expiring during long "
"running operations, such as live-migration where nova tries to access Cinder"
" and Neutron at the end of the operation using the user token that has "
"expired. In order to use this functionality a service user needs to be "
"created. Add service user configurations in ``nova.conf`` under "
"``service_user`` group and set ``send_service_user_token`` flag to ``True``."
" The minimum Keytone API version 3.8 and Keystone middleware version 4.12.0 "
"is required to use this functionality. This only currently works with Nova -"
" Cinder and Nova - Neutron API interactions."
msgstr ""
"Keystone 미드웨어 기능을 추가하여, 서비스 토큰이 사용자 토큰과 함께 전송되면 사용자 토큰의 만료를 무시한다. 이 기능은 사용자 "
"토큰이 장거리 연산에서 만료되는 문제를 해결하는 데 도움이 된다. 예를 들어, live-migration에서 nova는 연산이 끝날 때 "
"사용자 토큰이 만료된 경우 Cinder와 Neutron을 사용하여 접근을 시도한다. 이 기능을 사용하려면 서비스 사용자を作成해야 한다. "
"서비스 사용자 구성은 `nova.conf`의 `service_user` 그룹에서 `nova.conf`에 추가되어야 한다. "
"`send_service_user_token` 플래그를 `True`로 설정해야 한다. 이 기능을 사용하려면 Keytone API 버전 "
"3.8 이상과 Keystone 미드웨어 버전 4.12.0 이상이 필요하다. 이 기능은 현재 Nova - Cinder와 Nova - "
"Neutron API 상호 작용만 작동한다."

#: ../../<reno.sphinxext stable/queens>:997
msgid ""
"Added support for PCI device NUMA affinity policies. These allow you to "
"configure how strict your NUMA affinity should be on a per-device basis or, "
"more specifically, per device alias. These are configured as part of the "
"``[pci]alias`` configuration option(s)."
msgstr ""
"PCI 장치의 NUMA 경계성 정책을 추가했습니다. 이 정책은 각 장치에 대한 NUMA 경계성의 강도를 설정하는 것을 허용합니다. 더 "
"cụ어지면, 각 장치의 별명에 따라 NUMA 경계성의 강도를 설정할 수 있습니다. 이 정책은 ``[pci]alias`` 구성 옵션(의 "
"일부)로 구성됩니다."

#: ../../<reno.sphinxext unmaintained/yoga>:319
msgid ""
"Added support for VMware VStorageObject based volumes in VMware vCenter "
"driver. vSphere version 6.5 is required."
msgstr ""
"VMware vCenter 드라이버에 VMware VStorageObject 기반 볼륨을 추가로 지원합니다. vSphere 버전 6.5가"
" 필요합니다."

#: ../../<reno.sphinxext stable/rocky>:733
msgid "Added support for ``nvmeof`` type volumes to the libvirt driver."
msgstr "``nvmeof`` 유형의 볼륨에 대한 지원을 libvirt 드라이버에 추가했습니다."

#: ../../<reno.sphinxext stable/ussuri>:356
msgid ""
"Added support for `evacuate, live migrate and unshelve servers with minimum "
"bandwidth guarantees`__."
msgstr "`evacuate, live migrate and unshelve` 서버에 최소帯幅 보장을 위해 추가된 지원입니다."

#: ../../<reno.sphinxext stable/rocky>:844
msgid ""
"Added support for forbidden traits to the scheduler. A flavor extra spec is "
"extended to support specifying the forbidden traits. The syntax of extra "
"spec is ``trait:<trait_name>=forbidden``, for example:"
msgstr ""
"forbidden traits를 스케줄러에 추가한 support를 제공한다. flavor extra spec은 forbidden traits를 지정하는 것을 지원한다. forbidden traits의 trait_name은 trait:<trait_name>=forbidden의 문법을 따르며, 예를 들어: \n"
"\n"
"```\n"
"trait:my_trait=forbidden\n"
"```"

#: ../../<reno.sphinxext stable/rocky>:865
msgid ""
"Added support for granular resource and traits requests to the scheduler. A "
"flavor extra spec is extended to support specifying numbered groupings of "
"resources and required/forbidden traits.  A ``resources`` key with a "
"positive integer suffix (e.g. ``resources42:VCPU``) will be logically "
"associated with ``trait`` keys with the same suffix (e.g. "
"``trait42:HW_CPU_X86_AVX``). The resources and required/forbidden traits in "
"that group will be satisfied by the same resource provider on the host "
"selected by the scheduler. When more than one numbered grouping is supplied,"
" the ``group_policy`` extra spec is required to indicate how the groups "
"should interact. With ``group_policy=none``, separate groupings - numbered "
"or unnumbered - may or may not be satisfied by the same provider. With "
"``group_policy=isolate``, numbered groups are guaranteed to be satisfied by "
"*different* providers - though there may still be overlap with the "
"unnumbered group."
msgstr ""
"집합 및 특성 요청의 세부적 자원 및 trait 요청을 스케줄러에 추가했습니다.  flavor extra spec은 자원 및 필요/금지 "
"trait의 numbered 그룹을 지정하는 것을 지원합니다.  자원과 그 그룹에 대한 필요/금지 trait이 동일한 suffix를 갖는"
" ``trait`` 키와 함께 ``resources`` 키가 논리적으로 연결됩니다. (예: ``resources42:VCPU``). "
"그룹의 자원과 필요/금지 trait은 스케줄러에 의해 선택된 호스트에서 동일한 자원 제공자에 의해 충족됩니다.  여러 numbered "
"grouping이 제공되면, ``group_policy`` extra spec이 필요합니다. 그룹이 how they should "
"interact를 나타냅니다.  ``group_policy=none``의 경우, numbered 또는 unnumbered "
"grouping은 동일한 제공자에 의해 충족될 수 있습니다.  ``group_policy=isolate``의 경우, numbered "
"grouping은 다른 제공자에 의해 충족됩니다. (예: unnumbered grouping과는 중복이 가능합니다.)"

#: ../../<reno.sphinxext stable/ussuri>:574
msgid ""
"Added support for instance-level PCI NUMA policies using the "
"``hw:pci_numa_affinity_policy`` flavor extra spec and "
"``hw_pci_numa_affinity_policy`` image metadata property. These apply to both"
" PCI passthrough and SR-IOV devices, unlike host-level PCI NUMA policies "
"configured via the ``alias`` key of the ``[pci] alias`` config option. See "
"the `VM Scoped SR-IOV NUMA Affinity`_ spec for more info."
msgstr ""
"PCI NUMA 정책을 인스턴스 수준으로 추가하여 `hw:pci_numa_affinity_policy` flavor의 추가 속성과 "
"`hw_pci_numa_affinity_policy` 이미지 메타데이터 속성을 사용합니다. 이들은 PCI passthrough와 SR-"
"IOV 장치에 대해 적용되며, `pci` alias config 옵션의 `[pci] alias` 속성의 `alias` 키를 통해 호스트 "
"수준의 PCI NUMA 정책을 구성하는 것과는 달리입니다. 더 많은 정보는 `VM Scoped SR-IOV NUMA Affinity` "
"spec에서 확인할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:478
msgid ""
"Added support for off-path networking backends where devices exposed to the "
"hypervisor host are managed remotely (which is the case, for example, with "
"various SmartNIC DPU devices). ``VNIC_TYPE_REMOTE_MANAGED`` ports can now be"
" added to Nova instances as soon as all compute nodes are upgraded to the "
"new compute service version. In order to use this feature, VF PCI/PCIe "
"devices need to be tagged as ``remote_managed: \"true\"` in the Nova config "
"in the ``passthrough_whitelist`` option."
msgstr ""
"``하위 경로 네트워킹 백엔드에 대한 지원이 추가되었습니다. 하이퍼바이저에 노출된 장치가 호스트에서 원격으로 관리되는 경우 (예를 들어,"
" 다양한 스마트 NIC DPU 장치와 같은 경우) 이러한 장치가 관리됩니다. ``VNIC_TYPE_REMOTE_MANAGED`` 포트는 "
"nova 인스턴스에 ``compute`` 노드가 새로운 컴퓨팅 서비스 버전으로 업그레이드된 후에 추가될 수 있습니다. 이 기능을 "
"사용하려면 ``passthrough_whitelist`` 옵션의 ``remote_managed: \"true\"`로 마크된 VF "
"PCI/PCIe 장치가 nova config에 추가되어야 합니다."

#: ../../<reno.sphinxext unmaintained/yoga>:444
msgid ""
"Added support for ports with minimum guaranteed packet rate QoS policy "
"rules. Support is provided for all server operations including cold "
"migration, resize, interface attach/detach, etc. This feature required "
"adding support for the ``port-resource-request-groups`` neutron API "
"extension, as ports with such a QoS policy will have multiple rules, each "
"requesting resources. For more details see the  `admin guide`_."
msgstr ""
"ports에 최소 보장된 패킷률을 보장하는 QoS 정책 규칙을 추가했습니다. 이 기능은 모든 서버 운영을 포함하여冷 마이그레이션, "
"리사이즈, 인터페이스 연결/분리, etc.에 대한 지원을 제공합니다. 이 기능은 `port-resource-request-groups` "
"네트워크 API 확장에 대한 지원을 추가해야 합니다. 이러한 QoS 정책이 있는 포트는 각기적으로 리소스를 요청하는 규칙이 여러 개가 "
"있기 때문입니다. 더 많은 정보는 `admin guide`에서 확인할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/zed>:239
msgid ""
"Added support for rebuilding a volume-backed instance with a different "
"image. This is achieved by reimaging the boot volume i.e. writing new image "
"on the boot volume at cinder side. Previously rebuilding volume-backed "
"instances with same image was possible but this feature allows rebuilding "
"volume-backed instances with a different image than the existing one in the "
"boot volume. This is supported starting from API microversion 2.93."
msgstr ""
"집합에 대한 지원이 추가되었습니다. 이는 부트 볼륨에 대한 인스턴스를 재건하는 것을 위해 새로운 이미지로 부트 볼륨을 재이미징하는 것을 "
"의미합니다. 즉, cinder 측에서 부트 볼륨에 새로운 이미지를 쓰는 것입니다. 이전에 same 이미지로 부트 볼륨-backed "
"인스턴스를 재건하는 것이 가능했지만, 이 기능은 existing 이미지와 다른 이미지로 부트 볼륨-backed 인스턴스를 재건하는 것을 "
"허용합니다. 이 기능은 API microversion 2.93부터 지원됩니다."

#: ../../<reno.sphinxext stable/queens>:790
msgid ""
"Added support for service create and destroy versioned notifications. The "
"``service.create`` notification will be emitted after the service is created"
" (so the uuid is available) and also send the ``service.delete`` "
"notification after the service is deleted."
msgstr ""
"``service.create``通知는 서비스가 생성되면 발신되고 uuid이 readily available합니다. 또한 서비스가 "
"삭제되면 ``service.delete``通知가 발신됩니다."

#: ../../<reno.sphinxext stable/stein>:535
msgid ""
"Added support for the High Precision Event Timer (HPET) for x86 guests in "
"the libvirt driver when image property ``hypervisor_type=qemu`` is set. The "
"timer can be set by setting a ``hw_time_hpet=True`` image property key/value"
" pair. By default HPET remains turned off. When it is turned on the HPET is "
"activated in libvirt."
msgstr ""
"HPET를 x86 게스트에 추가하여 libvirt 드라이버에서 `hypervisor_type=qemu` 속성이 설정된 경우 High "
"Precision Event Timer (HPET) 지원이 추가됩니다. HPET는 `hw_time_hpet=True` 속성 키/값 쌍으로"
" 설정할 수 있습니다. 기본적으로 HPET는 비활성화되어 있습니다. HPET가 활성화되면 libvirt에서 활성화됩니다."

#: ../../<reno.sphinxext stable/queens>:633
msgid "Added the StorPool libvirt volume attachment driver."
msgstr "StorPool libvirt 볼륨 연결 드라이버를 추가했습니다."

#: ../../<reno.sphinxext stable/stein>:670
msgid ""
"Added the ability to allow users to use ``Aggregate``'s ``metadata`` to "
"override the global config options for weights to achieve more fine-grained "
"control over resource weights."
msgstr ""
"``집합``의 ``metadata``를 사용하여 사용자들이 가중치의 전역 설정 옵션을 오버라이드하여 리소스 가중치를 더 fine-"
"grained으로 제어할 수 있는 능력을 추가했습니다."

#: ../../<reno.sphinxext stable/queens>:976
msgid ""
"Added traits support to the scheduler. A new flavor extra spec is added to "
"support specifying the required traits. The syntax of extra spec is "
"``trait:<trait_name>=required``, for example:"
msgstr ""
"집합 trait를 스케줄러에 추가합니다. 새로운 flavor extra spec이 추가되어 trait를 지정하는 것을 지원합니다. "
"trait:<trait_name>=required의 문법을 사용하여 extra spec을 지정합니다. 예를 들어:"

#: ../../<reno.sphinxext unmaintained/zed>:602
msgid ""
"Added validation for image machine type property. Different APIs which uses "
"machine type for server creation, resize or rebuild will raise "
"InvalidMachineType exception with message \"provided machine type is not "
"supported by host\" and suggest possible/valid machine types in compute "
"logs. For more details see: `bug 1933097`_"
msgstr ""
"이미지 머신 타입 속성을 유효성 검사로 추가했습니다. 서버 생성, 리사이즈 또는 재건을 위해 머신 타입을 사용하는 API는 \"호스트에 "
"의해 지원되지 않는 머신 타입이 제공된 머신 타입\"이라는 메시지와 함께 InvalidMachineType 예외를 raises하고 "
"compute 로그에 possible/valid 머신 타입을 제안합니다. 더 많은 정보는 `bug 1933097`에서 확인할 수 "
"있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:575
msgid ""
"Adding aarch64 to the list of supported architectures for NUMA and hugepage "
"features. This requires libvirt>=1.2.7 for NUMA, libvirt>=1.2.8 for hugepage"
" and qemu v2.1.0 for both."
msgstr ""
"aarch64를 NUMA 및 hugepage 기능을 지원하는 아키텍처 목록에 추가한다. 이에 대해 libvirt>=1.2.7가 NUMA,"
" libvirt>=1.2.8가 hugepage, qemu v2.1.0가 ambos를 지원하는 것을 필요로 한다."

#: ../../<reno.sphinxext stable/2024.1>:40 stable/2024.2>:40 stable/2025.1>:28
#: stable/2025.2>:513
msgid ""
"Adding uptime information to the periodic resource updates sent by nova-"
"compute to the database, eliminating the need for synchronous RPC calls "
"during API requests"
msgstr ""
"nova-compute에서 데이터베이스에 주기적으로 업데이트되는 자원 정보에 uptime 정보를 추가하여, API 요청 시 동기 RPC "
"호출을 제거하는 것을 의미합니다."

#: ../../<reno.sphinxext stable/2025.2>:197
msgid ""
"Additional attributes relating to the image metadata an instance is "
"configured with have been added to the libvirt domain metadata under the "
"``<nova:image>`` element. This allows downstream services that read libvirt "
"domain metadata, such as Ceilometer, to use and expose more information "
"about an instance without needing to perform additional API queries "
"(potentially to multiple services) to get that information."
msgstr ""
"다음은 인스턴스에 대한 이미지 메타데이터와 관련된 추가 속성들이 libvirt 도메인 메타데이터에 ``<nova:image>`` 요소를 "
"사용하여 추가되었다. 이로 인해 libvirt 도메인 메타데이터를 읽는 하위 서비스, 예를 들어 Ceilometer와 같은, "
"libvirt 도메인 메타데이터를 읽을 수 있는 추가 정보를 사용하고 노출할 수 있게 되고, 이 정보를 얻기 위해 추가 API 요청을 "
"수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 "
"여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 "
"필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 "
"서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 "
"없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 "
"대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 "
"되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 "
"추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이"
" 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 "
"API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 "
"정보를 얻기 위해 여러 서비스에 대한 추가 API 요청을 수행할 필요가 없게 되고, 이 정보를 얻기 위해 여러 서비스에 대한 추가 API"
" 요청을 수행할 필요가 없게 되고, 이 정보"

#: ../../<reno.sphinxext stable/pike>:1937
msgid ""
"Additional availability zone check is added to the volume attach flow, which"
" results in an availability zone check when an instance gets unshelved. In "
"case the deployment is not sensitive to availability zones and not using the"
" AvailabilityZoneFilter scheduler filter the current default settings "
"(cross_az_attach=True) are allowing to perform unshelve the same way as "
"before this change without additional configuration."
msgstr ""
"다음은 availability zone에 대한 추가 확인이 볼륨 연결 흐름에 추가되었으며, 인스턴스가 비공개화되면 availability"
" zone 확인이 발생한다. 인스턴스 비공개화 시, 배포가 availability zone에敏感하지 않으며 "
"AvailabilityZoneFilter 스케줄러 필터를 사용하지 않는다면, 현재 기본 설정(cross_az_attach=True)으로 "
"인해 이전 변경 사항과 동일하게 비공개화할 수 있다."

#: ../../<reno.sphinxext stable/2025.2>:188
msgid ""
"Additional information regarding an instance's flavor (the ID and the "
"defined extra specs at the time the instance was launched) has been added to"
" the libvirt domain metadata for instances. This allows downstream clients "
"that queries libvirt domain metadata, such as Ceilometer, to avoid "
"performing additional Nova API queries to get this information."
msgstr ""
"instance의 flavor에 대한 추가 정보 (launch 시에 ID와 정의된 추가 속성)가 libvirt domain "
"metadata에 추가되었다. 이로써, libvirt domain metadata를 querying하는 하위 클라이언트 (예: "
"Ceilometer)가 이 정보를 얻기 위해 추가적인 Nova API 요청을 수행하지 않도록 할 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:960
msgid ""
"Additionally, where virtual machines already exist that were created using "
"earlier versions of Libvirt interactions with these virtual machines via "
"Nova or other utilities (e.g. `virsh`) may result in similar errors."
msgstr ""
"다음과 같은 경우, 이전 버전의 Libvirt와의 상호 작용으로 생성된 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 "
"이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미지가 이미"

#: ../../<reno.sphinxext stable/stein>:79 stable/train>:239 stable/ussuri>:258
#: unmaintained/victoria>:828
msgid ""
"Addressed an issue that prevented instances using multiqueue feature from "
"being created successfully when their vif_type is TAP."
msgstr ""
"TAP vif_type가 TAP인 인스턴스에서 멀티 큐어 기능을 사용하여 성공적으로 인스턴스를 생성할 수 없다는 문제를 해결했습니다."

#: ../../<reno.sphinxext stable/ussuri>:85 unmaintained/victoria>:179
#: unmaintained/wallaby>:285 unmaintained/xena>:701
msgid ""
"Addressed an issue that prevented instances with 1 vcpu using multiqueue "
"feature from being created successfully when their vif_type is TAP."
msgstr ""
"1 vCPU를 사용하는 인스턴스를 successfully 생성할 수 없는 경우를 해결하기 위해 TAP vif_type가 있는 인스턴스에 "
"대해 multiqueue 기능을 사용할 수 없다는 문제를 해결했습니다."

#: ../../<reno.sphinxext stable/queens>:1854
msgid ""
"Adds ``sata`` as a valid disk bus for qemu and kvm hypervisors. Setting the "
"``hw_disk_bus`` custom property on glance images allows for selecting the "
"type of disk bus e.g. VIRTIO/IDE/SCSI. Some Linux (custom) images require "
"use of SATA bus rather than any other that seem to be allowed."
msgstr ""
"``sata``를 qemu 및 kvm 하이퍼바이저에 대한 유효한 디스크 버스로 추가합니다. glance 이미지는 hw_disk_bus "
"custom 속성을 설정하여 디스크 버스의 유형을 선택할 수 있습니다. 예를 들어 VIRTIO/IDE/SCSI. 일부 리눅스 "
"(custom) 이미지는 다른 것보다 SATA 버스를 사용해야 합니다."

#: ../../<reno.sphinxext stable/rocky>:278 stable/stein>:919
msgid ""
"Adds a ``use_cache`` parameter to the virt driver ``get_info`` method. Out "
"of tree drivers should add support for this parameter."
msgstr ""
"``use_cache`` 매개 변수를 virt 드라이버의 ``get_info`` 메소드에 추가합니다. 트리드 outside 드라이버는 이"
" 매개 변수에 대한 지원을 추가해야 합니다."

#: ../../<reno.sphinxext stable/rocky>:779
msgid ""
"Adds a new ``generation`` column to the consumers table. This value is "
"incremented every time allocations are made for a consumer. The new "
"placement microversion 1.28 requires that all ``POST /allocations`` and "
"``PUT /allocations/{consumer_uuid}`` requests now include the "
"``consumer_generation`` parameter to ensure that if two processes are "
"allocating resources for the same consumer, the second one to complete "
"doesn't overwrite the first. If there is a mismatch between the "
"``consumer_generation`` in the request and the current value in the "
"database, the allocation will fail, and a 409 Conflict response will be "
"returned. The calling process must then get the allocations for that "
"consumer by calling ``GET /allocations/{consumer}``. That response will now "
"contain, in addition to the allocations, the current generation value for "
"that consumer. Depending on the use case, the calling process may error; or "
"it may wish to combine or replace the existing allocations with the ones it "
"is trying to post, and re-submit with the current consumer_generation."
msgstr ""
"``generेशन`` стол을 소비자 테이블에 추가합니다. 이 값은 소비자가 할당을 할 때마다 증가합니다. 새로운 배치 마이크로 버전"
" 1.28은 모든 ``POST /allocations`` 및 ``PUT /allocations/{consumer_uuid}`` 요청에서 "
"``consumer_generation`` 매개 변수를 포함해야 합니다. 이로 인해 두 프로세스가 동일한 소비자에 자원을 할당하는 경우,"
" 두 번째 프로세스가 완료되면 첫 번째가 overwritten되지 않도록 보장합니다. ``consumer_generation`` 매개 "
"변수가 요청과 데이터베이스의 현재 값이 일치하지 않으면 할당은 실패하고, 409 Conflict 응답이 반환됩니다. 호출 프로세스는 "
"``GET /allocations/{consumer}``를 호출하여 할당을 가져오세요. 이 응답은 할당 외에도 소비자의 현재 "
"generation 값을 포함합니다. 사용กรณ에 따라 호출 프로세스는 오류를 발생시킬 수 있지만, 할당을 업데이트하거나 대체하고, 현재"
" 소비자_ generation 값을 포함하여 다시 제출할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:643
msgid ""
"Adds interface attach/detach support to baremetal nodes using ironic virt "
"driver. Note that the instance info cache update relies on getting a "
"``network-changed`` event from neutron, or on the periodic task healing the "
"instance info cache, both of which are asynchronous. This means that nova's "
"cached network information (which is what is sent e.g. in the ``GET "
"/servers`` responses) may not be up to date immediately after the attachment"
" or detachment."
msgstr ""
"baremetal 노드에 ironic virt driver를 사용하여 인터페이스.attach/detach를 지원합니다. 인스턴스 정보 "
"캐시를 업데이트하는 것은 neutron에서 ``network-changed`` 이벤트를 얻거나, 주기적인 일정으로 인스턴스 정보 "
"캐시를修復하는 것을 의존합니다. 이 두 가지 모두 비동기적입니다. 따라서 nova의 캐시된 네트워크 정보 (이것은 예를 들어 ``GET "
"/servers`` 응답에서 보낸다. )는.attach/detach를 완료한 후 즉시 업데이트되지 않을 수 있습니다. 집합"

#: ../../<reno.sphinxext origin/stable/ocata>:741
msgid ""
"Adds serial console support to Ironic driver. Nova now supports serial "
"console to Ironic bare metals for Ironic ``socat`` console driver. In order "
"to use this feature, serial console must be configured in Nova and the "
"Ironic ``socat`` console driver must be used and configured in Ironic. "
"Ironic serial console configuration is documented in "
"http://docs.openstack.org/developer/ironic/deploy/console.html."
msgstr ""
"Ironic 드라이버에 시리얼 콘솔 지원을 추가합니다. Nova는 Ironic Bare Metal에 Ironic \"socat\" 콘솔 "
"드라이버를 사용하여 시리얼 콘솔 지원을 지원합니다. 이 기능을 사용하려면 Nova에서 시리얼 콘솔을 구성해야 하며 Ironic에서 "
"Ironic \"socat\" 콘솔 드라이버를 사용하고 구성해야 합니다. Ironic 시리얼 콘솔 구성은 "
"http://docs.openstack.org/developer/ironic/deploy/console.html에 설명되어 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:643
msgid ""
"Adds soft power off support to Ironic virt driver. This feature requires the"
" Ironic service to support API version 1.27 or later. It also requires "
"python-ironicclient >= 1.10.0."
msgstr ""
"아래는 원문에서 제공된 영어 텍스트를 한국어로 번역한 결과입니다.\n"
"\n"
"아래는 원문에서 제공된 영어 텍스트를 한국어로 번역한 결과입니다.\n"
"\n"
"아이스온 (Ironic) virt 드라이버에 소프트 파워 오프 (soft power off) 지원을 추가합니다. 이 기능은 아이온릭 (Ironic) 서비스가 API 버전 1.27 또는 이후를 지원해야 하며, 또한 python-ironicclient >= 1.10.0을 지원해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:639
msgid ""
"Adds soft reboot support to Ironic virt driver. If hardware driver in Ironic"
" doesn't support soft reboot, hard reboot is tried. This feature requires "
"the Ironic service to support API version 1.27 or later. It also requires "
"python-ironicclient >= 1.10.0."
msgstr ""
"아래는 원문에서 정의된 단어에 따라 번역된 텍스트입니다.\n"
"\n"
"아직은 소프트 리부트를 support하는 Ironic virt driver에 추가합니다. Ironic에서 hardware driver가 소프트 리부트를 support하지 않으면, 하드 리부트를 시도합니다. 이 기능은 Ironic 서비스가 API 버전 1.27 이상을 support하는 경우에만 사용할 수 있습니다. 또한, python-ironicclient >= 1.10.0을 support하는 경우에만 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:919
msgid ""
"Adds support for applying tags when creating a server. The tag schema is the"
" same as in the 2.26 microversion."
msgstr "서버를 생성할 때 태그를 적용할 수 있는 지원을 추가합니다. 태그 스키마는 2.26 마이크로 버전과 동일합니다."

#: ../../<reno.sphinxext stable/stein>:509
msgid ""
"Adds support for extending RBD attached volumes using the libvirt network "
"volume driver."
msgstr ""
"RBD에 연결된 볼륨을 확장하는 데 대한 지원을 추가합니다. \n"
"\n"
"libvirt 네트워크 볼륨 드라이버를 사용하여 RBD에 연결된 볼륨을 확장하는 데 대한 지원을 추가합니다."

#: ../../<reno.sphinxext stable/pike>:906
msgid ""
"Adds support to OVS vif type with direct port (SR-IOV). In order to use this"
" OVS acceleration mode, ``openvswitch`` 2.8.0 and 'Linux Kernel' 4.8 are "
"required. This feature allows control of an SR-IOV virtual function (VF) via"
" OpenFlow control plane and gain improved performance of 'Open vSwitch'. "
"Please note that in Pike release we can't differentiate between SR-IOV "
"hardware and OVS offloaded on the same host. This limitation should be "
"resolved when the enable-sriov-nic-features will be completed. Until then "
"operators can use host aggregates to ensure that they can schedule instances"
" on specific hosts based on hardware."
msgstr ""
"OVS vif 유형에 직접 포트 (SR-IOV)를 지원합니다. 이 OVS 가속화 모드를 사용하려면 ``openvswitch`` 2.8.0"
" 및 'Linux Kernel' 4.8가 필요합니다. 이 기능은 OpenFlow 제어 플레인과 함께 SR-IOV 가상 함수 (VF)를 "
"통제하여 'Open vSwitch'의 성능을 개선합니다. Pike 릴리스에서는 SR-IOV 하드웨어와 OVS를 동일한 호스트에서 오프로드"
" 한 것 사이의 차이를 구별할 수 없습니다. 이 제한은 enable-sriov-nic-features를 완료할 때까지 해결해야 합니다. "
"그 meantime, 운영자는 호스트 집합을 사용하여 호스트에 특정 하드웨어에 인스턴스를 스케줄할 수 있도록 보장할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:635
msgid ""
"Adds trigger crash dump support to ironic virt driver. This feature requires"
" the Ironic service to support API version 1.29 or later. It also requires "
"python-ironicclient >= 1.11.0."
msgstr ""
"이러한 기능은 Ironic 서비스가 API 버전 1.29 또는 이후를 지원하는 경우에만 작동합니다. 또한, 이 기능은 python-"
"ironicclient >= 1.11.0를 지원하는 경우에만 작동합니다."

#: ../../<reno.sphinxext stable/rocky>:727
msgid ""
"Adjustable RX queue sizes requires QEMU 2.7.0, and libvirt 2.3.0 (or newer) "
"Adjustable TX queue sizes requires QEMU 2.10.0, and libvirt 3.7.0 (or newer)"
msgstr ""
"조정 가능한 수신Queue 크기는 QEMU 2.7.0, libvirt 2.3.0 (또는 newer) 조정 가능한 수신Queue 크기는 "
"QEMU 2.10.0, libvirt 3.7.0 (또는 newer)"

#: ../../<reno.sphinxext stable/2023.2>:270
msgid ""
"After Nova database cleanup, similarly Cinder database is checked for "
"attachments related to instance. If attachments found in Cinder DB that are "
"not present in Nova DB, they will get deleted from Cinder databse."
msgstr ""
"노바 데이터베이스 청소 후에, 인스턴스와 관련된.attachments을 확인합니다. Cinder 데이터베이스에서 attachments이 "
"Nova 데이터베이스에 존재하지 않는 attachments이 발견되면, Cinder 데이터베이스에서 attachments을 삭제합니다."

#: ../../<reno.sphinxext stable/2024.2>:310
msgid ""
"After a compute host reboots, if you have a GPU that supports SR-IOV, then "
"the virtual functions for the GPU must be enabled again before instances "
"will be able to use their vGPUs. Please see "
"https://docs.openstack.org/nova/latest/admin/virtual-gpu.html for more "
"information."
msgstr ""
"Compute 호스트가 재부팅되면, SR-IOV를 지원하는 GPU가 있는 경우, GPU의 가상 함수를 다시 활성화해야 하며, 인스턴스들은"
" vGPU를 사용할 수 있습니다. 더 많은 정보는 "
"https://docs.openstack.org/nova/latest/admin/virtual-gpu.html 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:464
msgid ""
"After shelve offload the ARQs of the instance will be feered in Cyborg."
msgstr "ARQ의 인스턴스에서 옮겨진 후, Cyborg에서 처리된다."

#: ../../<reno.sphinxext unmaintained/wallaby>:462
msgid "After shelve the ARQs are still kept bound to the instance."
msgstr "ARQ를 보관하면 여전히 인스턴스에 결합된 상태로 유지된다."

#: ../../<reno.sphinxext origin/stable/ocata>:694
msgid ""
"After the compute nodes have been configured, the nova-api, nova-scheduler, "
"and the nova-compute services will have to be configured next [2]."
msgstr ""
"compute 노드가 구성된 후에는 nova-api, nova-scheduler, 및 nova-compute 서비스가 다음으로 구성되어야"
" 합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:647
msgid "Aggregatefilter, AggregateRAMFilter, AggregateDiskFilter"
msgstr "집합 필터, 집합 RAM 필터, 집합 디스크 필터"

#: ../../<reno.sphinxext stable/pike>:1419
msgid ""
"Aliases are provided but these are marked as deprecated and will be removed "
"in the next release of nova."
msgstr "aliases는 제공되지만, 이들은 deprecated로 표시되어 다음 nova 릴리스에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/queens>:1831
msgid "All *nova-compute* services are upgraded"
msgstr "모든 *nova-compute* 서비스가 업그레이드되었습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:510
msgid ""
"All APIs except deprecated APIs were modified to implement ``scope_type`` "
"and use new defaults in 21.0.0 (Ussuri). The remaining APIs have now been "
"updated."
msgstr ""
"모든 API는 21.0.0 (Ussuri)에서 `scope_type`을 구현하고 새로운 디폴트를 사용하기 위해 비대면 API를 "
"수정했습니다. 현재 남은 API는 업데이트되었습니다."

#: ../../<reno.sphinxext stable/2023.2>:357 unmaintained/2023.1>:248
#: unmaintained/victoria>:31 unmaintained/wallaby>:17 unmaintained/xena>:17
#: unmaintained/yoga>:80 unmaintained/zed>:97
msgid ""
"All Nova configuration files must configure the ``[service_user]`` section "
"as described in the `documentation`__."
msgstr ""
"Nova configuration files의 모든 경우에, `[service_user]` 섹션은 `documentation`__에 "
"설명된 방식으로 구성해야 합니다."

#: ../../<reno.sphinxext stable/train>:985
msgid "All compute and conductor services are upgraded to Train code."
msgstr "전체 컴퓨터 및 컨트롤러 서비스는 트레이닝 코드로 업그레이드됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:783
msgid ""
"All deployments will function as a single-cell environment. Multiple v2 "
"cells are technically possible, but should only be used for testing as many "
"other things will not work across cell boundaries yet. For details on cells "
"v2 and the setup required for Nova with cells v2, see the cells "
"documentation. [1]_"
msgstr ""
"모든 배포는 단일 세포 환경으로 작동합니다. 여러 v2 세포는 기술적으로 가능합니다, 그러나 세포 간 경계를 가로지르는 많은 다른 것들이"
" 아직 작동하지 않기 때문에 테스트를 위해만 사용해야 합니다. 세포 v2와 Nova와 함께 세포를 설정하는 데 대한รายละเอียด은 "
"세포 v2와 관련된 세포 문서를 참조하십시오. [1]"

#: ../../<reno.sphinxext origin/stable/ocata>:1038
msgid ""
"All general scheduler configuration options have been added to the "
"``scheduler`` group."
msgstr "모든 일반 스케줄러 구성 옵션은 ``scheduler`` 그룹에 추가되었습니다."

#: ../../<reno.sphinxext unmaintained/zed>:210
msgid ""
"All lifecycle actions are now fully supported for `instances with vDPA "
"ports`__, including vDPA hot-plug live migration, suspend and attach/detach."
msgstr ""
"`인스턴스에 vDPA 포트가 있는 경우` 모든 라이프 사이클 액션은 현재 `인스턴스와 vDPA 포트`에 대한 완전한 지원을 제공합니다. "
"이는 vDPA.hot-plug live migration, suspend 및 attach/detach를 포함합니다."

#: ../../<reno.sphinxext stable/rocky>:1801
msgid ""
"All of the existing consoles have expired. For example, if a deployment has "
"configured a token TTL of one hour, the operator may disable the "
"``[workarounds]/enable_consoleauth`` option, one hour after deploying the "
"new code."
msgstr ""
"existing consoles의 모든 집합이 만료되었습니다. 예를 들어, 배포가 토큰 TTL을 1시간으로 구성하면, 운영자 may "
"disable the ``[workarounds]/enable_consoleauth`` 옵션, 배포 후 new code가 1시간 후에."

#: ../../<reno.sphinxext unmaintained/yoga>:691
msgid ""
"All other resources classes requested via flavors are also now supported as "
"unified limits. Note that nova configuration is ignored, as the default "
"limits come from the limits registered for the Nova endpoint in Keystone."
msgstr ""
"다른 리소스 클래스를 flavor로 요청한 모든 다른 리소스 클래스도 now unified limits로 지원됩니다. nova "
"configuration은 default limits가 Keystone에서 Nova endpoint에 등록된 limits로 coming "
"from이므로 ignored됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1029
msgid ""
"All pci configuration options have been added to the 'pci' group. They "
"should no longer be included in the 'DEFAULT' group. These options are as "
"below:"
msgstr ""
"pci 구성 옵션은 'pci' 그룹에 추가되었으며, 더 이상 'DEFAULT' 그룹에 포함되지 않도록 할 수 있습니다. 이 옵션은 아래와"
" 같습니다."

#: ../../<reno.sphinxext stable/rocky>:700
msgid ""
"All placement policy rules are defined in code so by default no extra "
"configuration is required and the default rules will be used on start of the"
" placement service."
msgstr ""
"모든 배치 정책 규칙은 코드에 정의되어 있으므로 기본적으로 추가적인 구성이 필요하지 않으며 배치 서비스 시작 시 기본 규칙이 사용됩니다."

#: ../../<reno.sphinxext stable/pike>:1305
msgid ""
"All policy rules with the following naming scheme have been removed: "
"``os_compute_api:{extension_alias}:discoverable`` These policy rules were "
"used to hide an enabled extension from the list active API extensions API. "
"Given it is no longer possible to disable any API extensions, it makes no "
"sense to have the option to hide the fact an API extension is active. As "
"such, all these policy rules have been removed."
msgstr ""
"``os_compute_api:{extension_alias}:discoverable``라는 이름의 모든 정책 규칙이 제거되었습니다. "
"이러한 정책 규칙은 활성화된 확장자가 API 활성화된 확장자 목록에서 보이지 않도록 사용되었습니다. 그러나 현재 API 확장자가 "
"비활성화될 수 없기 때문에, 활성화된 API 확장자가จริง으로 활성화된 것인지 보이지 않도록 하는 옵션을 유지하는 의미가 없기 때문에 "
"모든 이러한 정책 규칙이 제거되었습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:695
msgid ""
"All previous quotas other than ``cores``, ``instances`` and ``ram`` are "
"still enforced, but the limit can only be changed globally in Keystone as "
"registered limits. There are no per project or per user overrides possible."
msgstr ""
"전체 이전의 자산은 \"cores\" , \"instances\" 및 \"ram\" 이외의 모든 자산이 여전히 enforce되지만, "
"Keystone에서 등록된 제한만으로 전역으로 제한이 변경될 수 있다. 프로젝트 또는 사용자별로의 오버라이드가 가능하지 않다."

#: ../../<reno.sphinxext stable/ussuri>:670
msgid ""
"All the policies except the deprecated APIs policy have been changed to "
"implement the ``scope_type`` and new defaults. Deprecated APIs policy will "
"be moved to ``scope_type`` and new defaults in the next release."
msgstr ""
"모든 정책은 deprecated APIs 정책 이외에 ``scope_type`` 및 새로운 기본값을 구현하기 위해 변경되었다. "
"deprecated APIs 정책은 다음 릴리스에서 ``scope_type`` 및 새로운 기본값으로 이동된다."

#: ../../<reno.sphinxext unmaintained/zed>:254
msgid ""
"Allow 2 new special characters: '@' and '.' (dot), in addition to the "
"existing constraints of ``[a-z][A-Z][0-9][_- ]``"
msgstr ""
"`@`와 `.`(점)과 함께 새로운 특수 문자를 허용합니다. 기존의 제약 조건 ` [a-z][A-Z][0-9][_- ]`에 추가됩니다."

#: ../../<reno.sphinxext stable/ussuri>:429
msgid ""
"Allow the following filter parameters for ``GET /servers/detail`` and ``GET "
"/servers`` for non-admin in microversion 2.83:"
msgstr ""
"``GET /servers/detail`` 및 ``GET /servers`` 에서 non-admin microversion 2.83 에서"
" 다음 필터 파라미터를 허용합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:104 stable/pike>:169
#: stable/queens>:310 stable/rocky>:1934
msgid ""
"Allowing image-backed servers with a zero root disk flavor can be "
"potentially hazardous if users are allowed to upload their own images, since"
" an instance created with a zero root disk flavor gets its size from the "
"image, which can be unexpectedly large and exhaust local disk on the compute"
" host. See https://bugs.launchpad.net/nova/+bug/1739646 for more details."
msgstr ""
"이미지 기반 서버를 허용하고 0 개의 루트 디스크 플레버를 사용할 수는 потенци적으로 위험할 수 있다. 사용자가 자신의 이미지를 "
"업로드할 수 있기 때문이다. 이미지를 사용하여 인스턴스를 만들면, 루트 디스크의 크기는 이미지가 될 수 있기 때문에 "
"unexpectedly 큰 size가 될 수 있으며, 이로 인해 컴퓨터 호스트의 로컬 디스크가 exhaust 될 수 있다. 더 많은 "
"정보는 https://bugs.launchpad.net/nova/+bug/1739646 에서 확인할 수 있다."

#: ../../<reno.sphinxext stable/train>:399 stable/ussuri>:1195
msgid ""
"Also a virtio-serial controller is created when ``hw_qemu_guest_agent=yes`` "
"option is used, together with iommu driver for it."
msgstr ""
"``hw_qemu_guest_agent=yes`` 옵션을 사용할 때 virtio-serial 컨트롤러가 생성되며, 이에 대한 iommu "
"드라이버가 함께 사용됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:616
msgid ""
"Also, the following Cinder front-end QoS specs are now supported for SMB "
"Cinder backends:"
msgstr "다음은 SMB Cinder 백엔드에 대한 Cinder 프론트 엔드 QoS spécifik이 현재 지원됩니다."

#: ../../<reno.sphinxext unmaintained/zed>:316
msgid ""
"Also, the project reader role is ready to use. Users with reader role can "
"only perform the read-only operations within their project. This role can be"
" used for the audit purposes."
msgstr ""
"Also, 프로젝트 리더 역은 사용할 수 있습니다. 리더 역을 가진 사용자는 프로젝트 내에서만 읽기만 가능한 연산을 수행할 수 있습니다."
" 이 역은 감사 목적으로 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:429
msgid ""
"Alternatives to this workaround would be unsetting ``memory_mb`` and/or "
"``vcpus`` properties from ironic nodes, or using host aggregates to "
"segregate VM from BM compute hosts and restrict flavors to those aggregates,"
" but those alternatives might not be feasible at large scale."
msgstr ""
"이 workaround의 대체 방법은 `memory_mb` 및/or `vcpus` 속성을 ironic 노드에서 비우거나, host "
"aggregates를 사용하여 VM을 BM 컴퓨팅 호스트에서 분리하고 flavors를 그 aggregates로 제한하는 것입니다. 그러나"
" 이러한 대체 방법은 대규모에서 feasible하지 않을 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:61 unmaintained/xena>:168
#: unmaintained/yoga>:594
msgid ""
"Amended the guest resume operation to support mediated devices, as libvirt's"
" minimum required version (v6.0.0) supports the hot-plug/unplug of mediated "
"devices, which was addressed in v4.3.0."
msgstr ""
"게스트 리소스 연산을 수정하여 매개화 장치의 지원을 위해 수정되었습니다. libvirt의 최소 필요 버전 (v6.0.0)은 매개화 "
"장치의.hot 플러그/ unpl러그를 지원합니다. 이 문제는 v4.3.0에서 해결되었습니다."

#: ../../<reno.sphinxext stable/pike>:1132 stable/pike>:1162
msgid ""
"An IPTables-compatible interface is used, e.g. an OVS VIF in hybrid mode, "
"where the VIF is a tap device connected to OVS with a bridge"
msgstr ""
"IPTables-Compatibl한 인터페이스를 사용합니다. 예를 들어, 하이브리드 모드에서 OVS VIF를 사용하는 경우, VIF는 "
"OVS와 브리지에 연결된 TAP 장치입니다."

#: ../../<reno.sphinxext stable/rocky>:141 stable/stein>:218
#: stable/train>:1441
msgid ""
"An ``--instance`` option has been added to the ``nova-manage placement "
"heal_allocations`` CLI which allows running the command on a specific "
"instance given its UUID."
msgstr ""
"``--instance`` 옵션은 nova-manage placement heal_allocations CLI에 추가되어 특정 인스턴스를"
" 사용하여 UUID를 사용하여 명령을 실행할 수 있도록 허용합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:346
msgid ""
"An `emulated Virtual Trusted Platform Module`__ can be exposed to instances "
"running on a ``libvirt`` hypervisor with ``qemu`` or ``kvm`` backends."
msgstr ""
"`emulated Virtual Trusted Platform Module`__는 `libvirt` 하이퍼바이저에서 `qemu` 또는 "
"`kvm` 백엔드와 함께 실행되는 인스턴스에 노출될 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:78 stable/stein>:176 stable/train>:416
#: stable/ussuri>:1266
msgid ""
"An instance can be rebuilt in-place with the original image or a new image. "
"Instance resource usage cannot be altered during a rebuild. Previously Nova "
"would have ignored the NUMA topology of the new image continuing to use the "
"NUMA topology of the existing instance until a move operation was performed."
" As Nova did not explicitly guard against inadvertent changes to resource "
"requests contained in a new image, it was possible to rebuild with an image "
"that would violate this requirement; see `bug #1763766`_ for details. This "
"resulted in an inconsistent state as the instance that was running did not "
"match the instance that was requested. Nova now explicitly checks if a "
"rebuild would alter the requested NUMA topology of an instance and rejects "
"the rebuild if so."
msgstr ""
"인스턴스는 원래 이미지 또는 새로운 이미지와 함께 재건할 수 있습니다. 인스턴스 리소스 사용이 재건에 alteration할 수 없습니다."
" 이전에 노바는 새로운 이미지의 NUMA topology를 계속 사용하고, 기존 인스턴스의 NUMA topology를 사용하는 것을 "
"무시했습니다. 새로운 이미지의 NUMA topology를 사용하는 것을 무시하고, 기존 인스턴스의 NUMA topology를 계속 "
"사용했습니다. 인스턴스가 실행 중이면, 원래 이미지와 일치하지 않는 상태가 발생했습니다. 노바는 새로운 이미지의 리소스 요청에 대한 "
"불분명한 변경을 방지하기 위해, 인스턴스의 NUMA topology가 변경되는지 확인하고, 변경이 발생한다면 재건을 거부합니다."

#: ../../<reno.sphinxext stable/pike>:1024 stable/queens>:1173
msgid ""
"An instance is deployed with a flavor smaller than a node (only possible "
"when exact filters are not used)"
msgstr "인스턴스는 노드보다 작은 플레버와 함께 배포된다. (만약 exact 필터가 사용되지 않는다면만 가능하다)"

#: ../../<reno.sphinxext stable/train>:230 stable/ussuri>:241
#: unmaintained/victoria>:811
msgid ""
"An issue that could result in instances with the ``isolate`` thread policy "
"(``hw:cpu_thread_policy=isolate``) being scheduled to hosts with SMT "
"(HyperThreading) and consuming ``VCPU`` instead of ``PCPU`` has been "
"resolved. See `bug #1889633`__ for more information."
msgstr ""
"``isolate`` 스레드 정책 (``hw:cpu_thread_policy=isolate``)로 인해 스케줄되는 호스트에 SMT (HyperThreading)가 사용되며 ``VCPU`` 대신 ``PCPU``를 소비하는 문제가 해결되었습니다. 더 많은 정보는 `bug #1889633`__에 참조하십시오.\n"
"\n"
"*   ``isolate`` 스레드 정책 (``hw:cpu_thread_policy=isolate``)로 인해 스케줄되는 호스트에 SMT (HyperThreading)가 사용되며 ``VCPU`` 대신 ``PCPU``를 소비하는 문제가 해결되었습니다.\n"
"*   더 많은 정보는 `bug #1889633`__에 참조하십시오."

#: ../../<reno.sphinxext stable/pike>:1433
msgid ""
"An online data migration has been added to populate the ``services.uuid`` "
"column in the nova database for non-deleted services records. Listing or "
"showing services out of the ``os-services`` API will have the same effect."
msgstr ""
"온라인 데이터 마이그레이션이 서비스 `uuid`列를 nova 데이터베이스에 population하기 위해 추가되었습니다. 비 삭제 서비스 "
"레코드의 경우. `os-services` API에서 서비스를 listing 또는 showing 할 경우 동일한 효과가 발생합니다."

#: ../../<reno.sphinxext stable/train>:815
msgid ""
"An option ``--before`` has been added to `nova-manage db "
"archive_deleted_rows` command. This options limits archiving of records to "
"those deleted before the specified date."
msgstr ""
"`nova-manage db archive_deleted_rows` 명령에 `--before` 옵션을 추가했습니다. 이 옵션은 기록의 "
"아카이브를 특정 날짜 이전에 삭제된 기록으로 제한합니다."

#: ../../<reno.sphinxext stable/rocky>:1081
msgid ""
"An optional configuration group ``placement_database`` can be used in "
"nova.conf to configure a separate database for use with the placement API."
msgstr ""
"`placement_database` 그룹은 nova.conf에서 placement API와 함께 사용할 separates "
"database를 구성하기 위해 사용할 수 있는 선택적 구성 그룹입니다."

#: ../../<reno.sphinxext stable/stein>:1125
msgid ""
"And as noted above, as more of the code base evolves to rely on resource "
"allocations being tracked in the placement service (created during "
"scheduling), out-of-tree scheduler driver support may be severely impacted."
msgstr ""
"위에서 언급된 내용과는 달리, 코드 베이스가 자원 할당이 트랙킹되는 위치 서비스(스케줄링 시에 생성된)에 의존하는 것이 더 많은 경우, "
"아웃-오브-트리 스케줄러 드라이버의 지원은 심각하게 영향을 받을 수 있습니다."

#: ../../<reno.sphinxext stable/train>:567
msgid ""
"And following fine control policy  use to keep host only information to "
"admin:"
msgstr "과 fine control policy를 사용하여 admin에게만 호스트의 정보를 유지합니다."

#: ../../<reno.sphinxext stable/pike>:1760
msgid ""
"And the following fields are removed from the same APIs in the same "
"microversion:"
msgstr "다음과 같은 필드는 동일한 마이크로 버전에서 동일한 API에서 제거됩니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:795
msgid ""
"Any remaining users of these workarounds should plan to disable these "
"workarounds as soon as possible. Note that this requires that any instances "
"on compute hosts using the workaround be shutdown ahead of the value of the "
"workaround changing, before being restarted."
msgstr ""
"이러한 workaround의 남은 사용자들은 이 workaround을 가능한 한 빠르게 비활성화해야 합니다. 이에 대한 주의할 점은 이 "
"workaround이 변경되는 시점보다 compute 호스트에 있는 인스턴스를 전환하기 전에 비활성화해야 하는 것입니다."

#: ../../<reno.sphinxext stable/stein>:598
msgid ""
"Any traits provided by the driver will be automatically added during startup"
" or a periodic update of a compute node.  Similarly any traits later "
"retracted by the driver will be automatically removed."
msgstr ""
"집합이 제공되는 특성은 시작 또는 컴퓨터 노드의 주기적인 업데이트시에 따라 tự động 추가되며, 드라이버가 후에 제거한 특성은 "
"likewise tự động 제거됩니다."

#: ../../<reno.sphinxext stable/rocky>:1787
msgid ""
"Are performing a live, rolling upgrade and all compute hosts are not "
"currently running Rocky code"
msgstr ""
"집합은 currently running Rocky code가 아니며, live, rolling upgrade를 수행하고 있습니다."

#: ../../<reno.sphinxext unmaintained/xena>:62 unmaintained/yoga>:160
#: unmaintained/zed>:504
msgid ""
"As a fix for `bug 1942329 "
"<https://bugs.launchpad.net/neutron/+bug/1942329>`_ nova now updates the MAC"
" address of the ``direct-physical`` ports during mova operations to reflect "
"the MAC address of the physical device on the destination host. Those "
"servers that were created before this fix need to be moved or the port needs"
" to be detached and the re-attached to synchronize the MAC address."
msgstr ""
"`bug 1942329 <https://bugs.launchpad.net/neutron/+bug/1942329>`_을 해결하기 위해 "
"nova는 `direct-physical` 포트의 MAC 주소가 `mova` 연산을 수행할 때 물리적 장치의 MAC 주소를 반영하도록 "
"업데이트합니다. 이fix이 적용된 후에 생성된 서버가 필요하다면 이 서버를 이동하거나 포트를 분리하고 다시 연결하여 MAC 주소를 "
"동기화해야 합니다."

#: ../../<reno.sphinxext stable/pike>:1261
msgid ""
"As a reminder, there is also the "
"``[scheduler]/discover_hosts_in_cells_interval`` configuration option which "
"can be used to automatically discover hosts from the nova-scheduler service."
msgstr ""
"``[스케줄러]/discover_hosts_in_cells_interval`` 설정 옵션은 nova-scheduler 서비스에서 tự "
"động적으로 호스트를 발견할 수 있는 것을 reminder로 남겨두고 있습니다."

#: ../../<reno.sphinxext stable/queens>:1581
msgid ""
"As a result, the following option is also deprecated for removal since it is"
" only used when specified with ``[DEFAULT]/monkey_patch_modules``:"
msgstr ""
"다음 옵션은 또한 제거를 위해 비고된 것으로, `[DEFAULT]/monkey_patch_modules`로 지정된 경우만 사용되기 "
"때문이다."

#: ../../<reno.sphinxext stable/2024.1>:345
msgid ""
"As a security mechanism, a new ``[consoleauth]/enforce_session_timeout`` "
"configuration option provides the ability to automatically close a server "
"console session when the token expires. This is disabled by default to "
"preserve the existing behaviour for upgrades."
msgstr ""
"``[consoleauth]/enforce_session_timeout`` 설정 옵션은 토큰이 만료되면 서버 콘솔 세션을 tự động "
"닫는能力를 제공하는 보안 메커니즘으로 사용됩니다. 이 설정은 기본적으로 비활성화되어 업그레이드 시 существ하는 behaviour를 "
"유지하기 위해 사용됩니다."

#: ../../<reno.sphinxext stable/2024.1>:90 stable/2024.2>:90 stable/2025.1>:69
#: stable/2025.2>:425
msgid ""
"As an admin only api, direct usage has always been limited to admins or "
"service like ``watcher``. This longstanding recommendation is now enforced "
"as a security hardening measure and restricted to only cinder. The prior "
"warning alluded to the fact that directly using this api can result in a "
"guest with a de-synced definition of the volume serial. Before this change "
"it was possible for an admin to unknowingly put a VM in an inconsistent "
"state such that a future live migration may fail or succeed and break tenant"
" isolation. This could not happen when the api was called by cinder so Nova "
"has restricted that api exclusively to that use-case. see: "
"https://bugs.launchpad.net/nova/+bug/2112187 for details."
msgstr ""
"admin으로만 API를 사용하는 경우, 항상 admin 또는 서비스 \"watcher\"와 같은 것만을 사용할 수 있었다. 이 장기적인"
" 권고는 현재 보안 강화asures로 강제되고, 이 API를 사용할 수 있는 것은 Cinder만이다. 이전 경고는 이 API를 직접 "
"사용할 경우, 볼륨 시리얼의 정상화에 대한 정상화가失어버리게 된 게스트가 발생할 수 있다는 사실을 언급했다. 이전에 이 API를 호출한 "
"경우, admin이 불 의식적으로 VM을 불일관된 상태에 올려두면, 미래의 live migration이 실패하거나 성공할 수 있고, "
"tenant isolation을 깨트릴 수 있었다. 이 경우, Cinder API를 호출할 때만 이 API를 사용할 수 있었다. 더 많은"
" 정보는 다음 URL을 참조할 수 있다. https://bugs.launchpad.net/nova/+bug/2112187"

#: ../../<reno.sphinxext origin/stable/ocata>:865
msgid ""
"As new hosts are added to Nova, the `nova-manage cell_v2 discover_hosts` "
"command must be run in order to map them into their cell. For deployments "
"with proper automation, this is a trivial extra step in that process. "
"However, for smaller or non-automated deployments, there is a new "
"configuration variable for the scheduler process which will perform this "
"discovery periodically. By setting "
"`scheduler.discover_hosts_in_cells_interval` to a positive value, the "
"scheduler will handle this for you. Note that this process involves listing "
"all hosts in all cells, and is likely to be too heavyweight for large "
"deployments to run all the time."
msgstr ""
"nova-manage cell_v2 discover_hosts 명령을 실행하여 nova에 추가된 호스트를 그들의 세ลล에 "
"mapping하는 것이 필요하다. 적합한 자동화가 있는 배포에서는 이 것이 프로세스에 추가적인 단순한 단계이다. 그러나 비용이 적거나 비"
" 자동화된 배포에서는 스케줄러 프로세스에 새로운 구성 변수가 추가되어 이 발견을 주기적으로 수행한다. "
"`scheduler.discover_hosts_in_cells_interval`를正의 값으로 설정하면 스케줄러가 이를 처리할 것이다. "
"그러나 이 프로세스는 모든 세ลล에 모든 호스트를 liệt어내는 것이 포함되어 있으며, 큰 배포에서 모든 시간에 이를 실행하는 것은 너무"
" 무거울 수 있다."

#: ../../<reno.sphinxext ../source/newton.rst:192 origin/stable/ocata>:953
msgid ""
"As of Libvirt 1.3.3 (`commit`_) and later Libvirt no longer accepts an empty"
" path attribute to the script element of the interface. Notably this "
"includes Libvirt 2.0.0 as provided with RHEL 7.3 and CentOS 7.3-1611. The "
"creation of virtual machines with offending interface definitions on a host "
"with Libvirt 1.3.3 or later will result in an error \"libvirtError: Cannot "
"find '' in path: No such file or directory\"."
msgstr ""
"Libvirt 1.3.3 (`commit`) 이상의 버전에서, Libvirt는 스크립트 요소의 인터페이스에 비어 있는 path 속성을 더"
" 이상 수용하지 않습니다. 특히, RHEL 7.3 및 CentOS 7.3-1611을 제공하는 Libvirt 2.0.0을 포함하여. "
"Libvirt 1.3.3 이상의 호스트에서 오판인 인터페이스 정의를 사용하여 가상 머신을 생성하면, 오류 \"libvirtError: "
"Cannot find '' in path: No such file or directory\"가 발생합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1208
msgid ""
"As of Ocata, the minimum version of VMware vCenter that nova compute will "
"interoperate with will be 5.1.0. Deployments using older versions of vCenter"
" should upgrade. Running with vCenter version less than 5.5.0 is also now "
"deprecated and 5.5.0 will become the minimum version in the 16.0.0 Pike "
"release of Nova."
msgstr ""
"Ocata 이후, nova compute가 VMware vCenter와 interoperability를 위해 최소한의 버전은 "
"5.1.0입니다. 오래된 vCenter 버전을 사용하는 배포는 업그레이드해야 합니다. vCenter 버전이 5.5.0 이하인 경우 현재 "
"deprecated이며, 16.0.0 Pike 릴리스의 Nova에서 5.5.0이 최소 버전이 됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1406
msgid ""
"As of Ocata, the minimum version of Virtuozzo that nova compute will "
"interoperate with will be 7.0.0. Deployments using older versions of "
"Virtuozzo should upgrade."
msgstr ""
"Ocata 이후 Virtuozzo의 최소 버전은 nova compute와 interoperability를 위해 7.0.0이지만, "
"Virtuozzo의 이전 버전을 사용하는 배포는 업그레이드해야 합니다."

#: ../../<reno.sphinxext stable/queens>:1560
msgid ""
"As of the 16.0.0 Pike release, the ``ExactRamFilter``, ``ExactCoreFilter``, "
"and ``ExactDiskFilter`` scheduler filters are all deprecated along with the "
"``[scheduler]/use_baremetal_filters`` and "
"``[scheduler]/baremental_enabled_filters`` options. Deployments should "
"migrate to using resource classes with baremetal flavors as described in the"
" ironic install guide:"
msgstr ""
"``16.0.0 피크 릴리즈 이후``, ``ExactRamFilter``, ``ExactCoreFilter``, 및 "
"``ExactDiskFilter`` 스케줄러 필터는 모두 ``[scheduler]/use_baremetal_filters`` 및 "
"``[scheduler]/baremetal_enabled_filters`` 옵션과 함께 비활성화되었습니다. 배포는 리소스 클래스를 "
"사용하여 Bare Metal 플레어를 사용하는 방법에 대해 설명된 ironic 설치 가이드에 따라 Bare Metal 플레어를 사용해야 "
"합니다."

#: ../../<reno.sphinxext stable/rocky>:1345
msgid ""
"As of the ``2018-08-27`` metadata API version, a boolean ``vf_trusted`` key "
"appears for all network interface ``devices`` in ``meta_data.json``, "
"indicating whether the device is a trusted virtual function or not."
msgstr ""
"``2018-08-27`` metadata API 버전에서, `vf_trusted` 키가 모든 네트워크 인터페이스 `devices`에 "
"대해 `meta_data.json`에 나타나며, 기계적 기능이 신뢰할 수 있는지 여부를 나타낸다."

#: ../../<reno.sphinxext stable/2024.1>:331
msgid ""
"As of the new 2.96 microversion, the ``server show`` and ``server list`` "
"APIs now return a new parameter called ``pinned_availability_zone`` that "
"indicates whether the instance is confined to a specific AZ. This field "
"supplements the existing ``availability_zone`` field which reports the "
"availability zone of the host where the service resides. The two values may "
"be different if the service is shelved or is not pinned to an AZ which can "
"help operators plan maintenance and better understand the workload "
"constraints."
msgstr ""
"``서버 show`` 및 ``서버 list`` API는 새로운 파라미터 ``pinned_availability_zone``를 반환합니다."
" 이 파라미터는 인스턴스가 특정 AZ에 제한되는지 확인하는 것을 나타냅니다. 이 필드는 현재 ``availability_zone`` "
"필드를 보완합니다. 이 필드는 호스트에서 서비스가 resides하는 가용 구역의 가용성에 대한 정보를 제공합니다. 두 값은 서비스가 "
"보관되거나 AZ에 고정되지 않은 경우에만 다르며, 이러한 정보를 사용하여 운영자는 유지 관리를 계획하고 부하 제한을 더 잘 이해할 수 "
"있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:255 unmaintained/xena>:478
msgid ""
"As part of the fix for bug 1910466, code that attempted to optimize VM CPU "
"thread assignment based on the host CPU topology as it was determined to be "
"buggy, undocumented and rejected valid virtual CPU topologies while also "
"producing different behavior when CPU pinning was enabled vs disabled. The "
"optimization may be reintroduced in the future with a more generic "
"implementation that works for both pinned and unpinned VMs."
msgstr ""
"bug 1910466에 대한修复의 일부로서, CPU topologie에 따라 호스트 CPU topology에 기반을 둔 VM CPU "
"스레드 할당을 최적화하는 코드가 bug이기 때문에 비공식적이고 비 undocumented이기 때문에 정상적인 가상 CPU "
"topologie가 제대로 작동하지 않으며 CPU pinning이 활성화된 vs 비활성화된 경우에 다른 행동을 생성하는 것을 "
"시도했습니다. 최적화는 미래에 더 일반적인 구현으로 두드러진ly pinned 및 비드루어 VMs에 작동하는 것을 위해 다시 "
"reintroduced 될 수 있습니다."

#: ../../<reno.sphinxext stable/2025.1>:396
msgid ""
"As we have learned over the years per compute periodic tasks that call other"
" services do not scale well as the number of compute nodes increases. In "
"``ce936ea5f3ae0b4d3b816a7fe42d5f0100b20fca`` the os-server-external-events "
"API was introduced. The server external events API allows external systems "
"such as Neutron to trigger cache refreshes on demand, this was part of the "
"Icehouse release. With the introduction of this API, neutron was modified to"
" send network-changed events on a per-port basis as API actions are "
"performed on neutron ports. When that was introduced the default value of "
"``[compute]heal_instance_info_cache_interval`` was not changed to ensure "
"there was no upgrade impact."
msgstr ""
"``compute``를 반복적으로 수행하는 일은 다른 서비스를 호출할 때 잘 작동하지 않다. 컴퓨터 노드의 수가 증가할수록. "
"\"ce936ea5f3ae0b4d3b816a7fe42d5f0100b20fca\"에서 os-server-external-events "
"API가 도입되었다. os-server-external-events API는 Neutron과 같은 외부 시스템을 통해 캐시를 강제로 "
"refresh할 수 있는 기능을 제공한다. 이는 아이스하우스 릴리즈의 일부였다. 이 API가 도입되면서 Neutron은 네트워크가 변경된"
" 이벤트를 각 포트에 따라 API 액션을 수행할 때 전송되었다. 이 때는 "
"[compute]heal_instance_info_cache_interval의 기본값이 업그레이드의 영향을 피하기 위해 변경되지 않았다."

#: ../../<reno.sphinxext unmaintained/wallaby>:863
msgid ""
"At this time, FreeBSD does not have a libguestfs package, therefore file "
"injection cannot be supported with the libvirt driver on a FreeBSD compute "
"host."
msgstr ""
"이 현재 시점에, Free BSD에는 libguestfs 패키지가 없기 때문에, libvirt 드라이버와 함께 file "
"injection을 지원할 수 없습니다."

#: ../../<reno.sphinxext stable/stein>:727
msgid ""
"Attaching Neutron ports and networks having QoS minimum bandwidth rule is "
"not supported."
msgstr "네트론 포트와 네트워크를 연결하는 것은 최소-bandwidth 규칙 QoS가 있는 경우 지원되지 않습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:402
msgid ""
"Attaching `neutron ports with QoS minimum bandwidth rules`__ to existing "
"instances is now possible."
msgstr "`neutron ports`에 `QoS`의 최소帯幅 규칙을 적용하여 현재 인스턴스를 연결하는 것은 이제 가능합니다."

#: ../../<reno.sphinxext stable/queens>:1032
msgid ""
"Attaching a multiattach volume to a shelved offloaded instance is not "
"supported and will result in a 400 HTTPBadRequest response."
msgstr ""
"집합을 여러 연결로 연결하는 볼륨을 보관된 오프로드 인스턴스에 연결하는 것은 지원되지 않으며, 400 HTTPBadRequest 응답을 "
"발생시킬 것입니다."

#: ../../<reno.sphinxext stable/queens>:1034
msgid ""
"Attaching a multiattach volume to an existing server instance will check "
"that the compute hosting that instance is new enough to support it and has "
"the capability to support it. If the compute cannot support the multiattach "
"volume, a 409 HTTPConflict response is returned."
msgstr ""
"다중 연결 볼륨을 существ하는 서버 인스턴스에 부착하면, 인스턴스의 컴퓨팅 호스팅이 새로운 enough인지 확인하고, 그것이 다중 "
"연결 볼륨을 지원할 수 있는지 확인합니다. 다중 연결 볼륨을 지원할 수 없다면, 409 HTTPConflict 응답이 반환됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:627
msgid "Attaching volumes over fibre channel on a passthrough basis."
msgstr "파스-through 기반으로 Fibre Channel을 통해 볼륨을 연결하는 것입니다."

#: ../../<reno.sphinxext stable/rocky>:1739
msgid ""
"Baremetal scheduling will use the custom resource class defined for each "
"baremetal node to make its selection. Refer to the ironic documentation for "
"more details:"
msgstr ""
"baremetal scheduling은 각 baremetal 노드에 정의된 custom resource class를 사용하여 선택을 할 "
"것이다. 더 많은 정보는 ironic documentation에 참조하십시오."

#: ../../<reno.sphinxext unmaintained/wallaby>:627
msgid ""
"Be sure to read the **Security** release notes about upgrade impacts for "
"resolving bug 1552042."
msgstr ""
"**보안** 릴리스 노트를 읽어야 합니다. 버전 업그레이드의 영향을 해결하기 위해 버그 1552042를 해결하는 데 사용됩니다."

#: ../../<reno.sphinxext stable/2023.2>:51 stable/2024.1>:233
#: stable/2024.2>:394 unmaintained/2023.1>:43 unmaintained/yoga>:14
#: unmaintained/zed>:18
msgid ""
"Before the `Bug 2078999 <https://bugs.launchpad.net/nova/+bug/2078999>`_ was"
" fixed, the ``nova-manage image_property set`` command would update the "
"image properties embedded in the instance but would not update the ones in "
"the request specs. This led to an unexpected rollback of the image "
"properties that were updated by the command after an instance migration."
msgstr ""
"`Bug 2078999 <https://bugs.launchpad.net/nova/+bug/2078999>`_을修复되기 전에, "
"`nova-manage image_property set` 명령은 인스턴스에 내장된 이미지 속성을 업데이트하지만 요청 스펙 속성에 "
"업데이트하지 않습니다. 이것은 인스턴스 이식 후 이미지 속성이 업데이트된 후 비기능적인 롤백을 일으켰습니다."

#: ../../<reno.sphinxext stable/ussuri>:1147
msgid ""
"Before the bug was fixed, users would have to specify an availability zone "
"that matches the zone that the volume(s) were in. With the fix, the compute "
"API will implicitly create the server in the zone that the volume(s) are in "
"as long as the volume zone is not the same as the "
"``[DEFAULT]/default_availability_zone`` value (defaults to ``nova``)."
msgstr ""
"가상화 API는 가상화 zone이 volume(s)와 일치하는 zone을 지정해야 사용자들이 bug가修复되기 전에 사용할 수 있었다. "
"그러나修正이 완료된 후, 가상화 API는 volume(s)가 있는 zone과 일치하는 zone에서 가상화 서버를 생성할 수 있다. "
"volume zone이 ``[DEFAULT]/default_availability_zone``의 value와 동일하지 않으면, 가상화 "
"API는 volume zone과 일치하는 zone에서 가상화 서버를 생성한다. (기본적으로 nova로 설정된다.)"

#: ../../<reno.sphinxext stable/train>:1025
msgid ""
"Behavior will be different for resizes. During a resize, resource "
"allocations are held on both the source and destination (even on the same "
"host, see https://bugs.launchpad.net/nova/+bug/1790204) until the resize is "
"confirmed or reverted. Quota usage will be inflated for servers in the "
"``VERIFY_RESIZE`` state and operators should weigh the advantages and "
"disadvantages before enabling ``[quota]count_usage_from_placement``."
msgstr ""
"Behavior will be different for resizes. During a resize, resource "
"allocations are held on both the source and destination (even on the same "
"host, see https://bugs.launchpad.net/nova/+bug/1790204) until the resize is "
"confirmed or reverted. Quota usage will be inflated for servers in the "
"``VERIFY_RESIZE`` state and operators should weigh the advantages and "
"disadvantages before enabling ``[quota]count_usage_from_placement``."

#: ../../<reno.sphinxext stable/train>:1048
msgid ""
"Behavior will be different for servers in ``SHELVED_OFFLOADED`` state. A "
"server in ``SHELVED_OFFLOADED`` state will not have placement allocations, "
"so it will not consume quota usage for cores and ram. Note that because of "
"this, it will be possible for a request to unshelve a server to be rejected "
"if the user does not have enough quota available to support the cores and "
"ram needed by the server to be unshelved."
msgstr ""
"``SHELVED_OFFLOADED`` 상태의 서버는 다른 행동을 취할 것이다. ``SHELVED_OFFLOADED`` 상태의 서버는 "
"배치 할당을 받지 않기 때문에 코어와 메모리 사용quota에 대한 사용량을 소모하지 않게 된다. 이에 따라서 서버를 언셰이드할 수 있는 "
"요청이 거절될 수 있게 되는데, 사용자에게 코어와 메모리가 언셰이드될 때 필요한 quota가 충분하지 않으면서도, 이 요청을 수신할 수 "
"있는 경우가 있다."

#: ../../<reno.sphinxext stable/train>:1043
msgid ""
"Behavior will be different for unscheduled servers in ``ERROR`` state. A "
"server in ``ERROR`` state that has never been scheduled to a compute host "
"will not have placement allocations, so it will not consume quota usage for "
"cores and ram."
msgstr ""
"``ERROR`` 상태에 있는 비정기적 서버의 행동은 다른 것입니다. ``ERROR`` 상태에 있는 서버가 ever scheduling에"
" 의해 컴퓨터 호스트에 scheduling되지 않은 경우, 할당량이 없기 때문에 코어 및 메모리 사용quota에 대한 소비가 없습니다."

#: ../../<reno.sphinxext stable/ussuri>:1116
msgid "Below bugs are fixed for policies default values"
msgstr "아래 버그는 기본 정책 및 기본 값에 대한 수정이 수행되었습니다."

#: ../../<reno.sphinxext stable/2023.2>:260
msgid ""
"Block device mapping (BDM) table in the Nova database, stores information "
"about volume attachments, image attachments and swap attachments. Similarly,"
" each volume attachment had a corresponding entry in the Cinder database "
"volume attachment table."
msgstr ""
"블록 장치 매핑 (BDM) 테이블은 노바 데이터베이스에 存재 information about volume attachments, "
"image attachments 및 swap attachments. Similarly, each volume attachment had "
"a corresponding entry in the Cinder 데이터베이스의 volume attachment table."

#: ../../<reno.sphinxext stable/train>:692
msgid ""
"Blueprint `placement-req-filter-forbidden-aggregates`_ adds the ability for "
"operators to set traits on aggregates which if not requested in flavor extra"
" specs or image properties will result in disallowing all hosts belonging to"
" those aggregates from booting the requested instances. This feature is "
"enabled via a new config option "
"``[scheduler]/enable_isolated_aggregate_filtering``. See `Filtering hosts by"
" isolated aggregates`_ for more details."
msgstr ""
"`placement-req-filter-forbidden-aggregates`_ blueprint는 운영자들이 집합에 속한 호스트가 "
"requested 인스턴스를 부트할 수 있는지 여부를 결정하는 trait를 설정할 수 있는 능력을 추가합니다. 이 trait가 "
"flavor의 extra specs 또는 image properties에 명시되지 않으면, 모든 집합에 속한 호스트가 부트할 수 있는지 "
"여부를 결정합니다. 이 기능은 새로운 config 옵션 "
"`[[scheduler]/enable_isolated_aggregate_filtering]]`를 통해 활성화됩니다. 더 많은 정보는 "
"`Filtering hosts by isolated aggregates`_를 참조하십시오."

#: ../../<reno.sphinxext stable/train>:1326
msgid ""
"Blueprints `hide-hypervisor-id-flavor-extra-spec`_ and `add-kvm-hidden-"
"feature`_ enabled NVIDIA drivers in Linux guests using KVM and QEMU, but "
"support was not included for Windows guests. This is now fixed. See `bug "
"1779845`_ for details."
msgstr ""
"`hide-hypervisor-id-flavor-extra-spec`_ 및 `add-kvm-hidden-feature`_ "
"blueprint가 Linux 게스트에서 KVM 및 QEMU를 사용하여 NVIDIA 드라이버를 활성화했지만, Windows 게스트에 대한"
" 지원은 포함되지 않았지만, 이는 이제 해결되었습니다.  더 많은 정보는 `bug 1779845`_에 대한 details를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:1986
msgid ""
"Booting volume-backed instances no longer includes an incorrect allocation "
"against the compute node for the root disk. Historically, this has been "
"quite broken behavior in Nova, where volume-backed instances would count "
"against available space on the compute node, even though their storage was "
"provided by the volume service. Now, newly-booted volume-backed instances "
"will not create allocations of ``DISK_GB`` against the compute node for the "
"``root_gb`` quantity in the flavor. Note that if you are still using a "
"scheduler configured with the (now deprecated) DiskFilter (including "
"deployments using CachingScheduler), the above change will not apply to you."
msgstr ""
"집합에 바인드된 인스턴스에서 부트를 시작하는 경우 더 이상 컴퓨터 노드에 대한 오류로 인한 IP 할당이 포함되지 않는다. 과거에 Nova에서 이러한 behavior이 매우 오류가되었습니다. 집합에 바인드된 인스턴스는 컴퓨터 노드에 대한 사용 가능한 공간에 카운트를 하게 되었습니다. 그러나 컴퓨터 노드에 의해 제공되는 스토리지를 제공하는 집합 서비스에 의해 제공되는 스토리지를 사용하는 경우에만이 true였습니다. \n"
"\n"
"이제 새로운 집합에 바인드된 인스턴스는 `DISK_GB`의 `root_gb` 양수를 `root_gb` 양수에 따라 `DISK_GB`를 컴퓨터 노드에 할당하지 않는다.  위의 변경은 (현재 비활성화된) DiskFilter를 사용한 스케줄러가 아직 사용 중인 경우에만 적용되지 않는다."

#: ../../<reno.sphinxext unmaintained/wallaby>:781
msgid ""
"Both these problems are addressed by the QEMU-native support in Nova -- this"
" is the recommended approach for securing all live migration streams (guest "
"RAM, device state, and disks).  Assuming TLS environment is setup, this can "
"be enabled by setting the config attribute "
"``[libvirt]live_migration_with_native_tls``."
msgstr ""
"다음 두 문제는 Nova에서 QEMU-native 지원으로 해결된다. - live migration streams (guest RAM, "
"device state, and disks)에서 모든 보안을 보장하는 데 이가 추천되는 접근 방식이다.  TLS 환경이 설정된 경우, 이"
" 기능을 활성화하려면 `config attribute [libvirt]live_migration_with_native_tls` 속성을 "
"설정해야 한다."

#: ../../<reno.sphinxext stable/2023.2>:121 stable/2024.1>:575
#: unmaintained/2023.1>:162 unmaintained/zed>:37
msgid ""
"Bug 2009280 has been fixed by no longer enabling the evmcs enlightenment in "
"the libvirt driver. evmcs only works on Intel CPUs, and domains with that "
"enlightenment cannot be started on AMD hosts. There is a possible future "
"feature to enable support for generating this enlightenment only when "
"running on Intel hosts."
msgstr ""
"BUG 2009280는 libvirt 드라이버에서 evmcs enlightenment을 비활성화하여 해결되었습니다. evmcs는 "
"Intel CPU에만 작동하며, 그 enlightenment을 가진 도메인은 AMD 호스트에서 시작할 수 없습니다. 향후의 기능으로는 "
"Intel 호스트에서만 이 enlightenment을 생성할 수 있도록 지원하는 것을 가능하게 할 수 있습니다."

#: ../../<reno.sphinxext ../source/liberty.rst:38 ../source/liberty.rst:99
#: ../source/mitaka.rst:40 ../source/mitaka.rst:61 ../source/mitaka.rst:679
#: ../source/newton.rst:30 ../source/newton.rst:97 ../source/newton.rst:216
#: ../source/newton.rst:257 ../source/newton.rst:1077 branch>:25 current
#: origin/stable/ocata>:39 origin/stable/ocata>:198 origin/stable/ocata>:278
#: origin/stable/ocata>:354 origin/stable/ocata>:373 origin/stable/ocata>:482
#: origin/stable/ocata>:1565 stable/2023.2>:32 stable/2023.2>:85
#: stable/2023.2>:100 stable/2023.2>:136 stable/2023.2>:410 stable/2024.1>:10
#: stable/2024.1>:107 stable/2024.1>:140 stable/2024.1>:200 stable/2024.1>:267
#: stable/2024.1>:540 stable/2024.2>:10 stable/2024.2>:107 stable/2024.2>:376
#: stable/2025.1>:10 stable/2025.1>:86 stable/2025.1>:116 stable/2025.1>:489
#: stable/2025.2>:442 stable/pike>:39 stable/pike>:66 stable/pike>:102
#: stable/pike>:196 stable/pike>:239 stable/pike>:271 stable/pike>:346
#: stable/pike>:406 stable/pike>:468 stable/pike>:539 stable/pike>:1749
#: stable/queens>:10 stable/queens>:64 stable/queens>:109 stable/queens>:161
#: stable/queens>:182 stable/queens>:229 stable/queens>:248 stable/queens>:357
#: stable/queens>:462 stable/queens>:486 stable/queens>:506 stable/queens>:553
#: stable/queens>:1666 stable/rocky>:27 stable/rocky>:115 stable/rocky>:154
#: stable/rocky>:201 stable/rocky>:248 stable/rocky>:311 stable/rocky>:393
#: stable/rocky>:484 stable/rocky>:1973 stable/stein>:27 stable/stein>:61
#: stable/stein>:91 stable/stein>:130 stable/stein>:162 stable/stein>:347
#: stable/stein>:375 stable/stein>:1276 stable/train>:35 stable/train>:131
#: stable/train>:168 stable/train>:212 stable/train>:261 stable/train>:279
#: stable/train>:300 stable/train>:369 stable/train>:388 stable/train>:1299
#: stable/ussuri>:10 stable/ussuri>:81 stable/ussuri>:97 stable/ussuri>:134
#: stable/ussuri>:191 stable/ussuri>:223 stable/ussuri>:301
#: stable/ussuri>:1112 unmaintained/2023.1>:32 unmaintained/2023.1>:79
#: unmaintained/2023.1>:119 unmaintained/2023.1>:134 unmaintained/2023.1>:189
#: unmaintained/2023.1>:475 unmaintained/victoria>:42
#: unmaintained/victoria>:175 unmaintained/victoria>:191
#: unmaintained/victoria>:213 unmaintained/victoria>:282
#: unmaintained/victoria>:740 unmaintained/wallaby>:49
#: unmaintained/wallaby>:108 unmaintained/wallaby>:176
#: unmaintained/wallaby>:231 unmaintained/wallaby>:266
#: unmaintained/wallaby>:335 unmaintained/wallaby>:880 unmaintained/xena>:28
#: unmaintained/xena>:58 unmaintained/xena>:154 unmaintained/xena>:244
#: unmaintained/xena>:652 unmaintained/yoga>:10 unmaintained/yoga>:56
#: unmaintained/yoga>:96 unmaintained/yoga>:156 unmaintained/yoga>:231
#: unmaintained/yoga>:575 unmaintained/zed>:10 unmaintained/zed>:33
#: unmaintained/zed>:52 unmaintained/zed>:113 unmaintained/zed>:130
#: unmaintained/zed>:500
msgid "Bug Fixes"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/victoria>:776
msgid ""
"Bug `1875418 <https://bugs.launchpad.net/nova/+bug/1875418>`_ is fixed by "
"changing the default value of ``[oslo_policy] policy_file`` config option to"
" YAML format."
msgstr ""
"`1875418 <https://bugs.launchpad.net/nova/+bug/1875418>`_ 버그는 `[oslo_policy]"
" policy_file` 구성 옵션의 디폴트 값을 YAML 포맷으로 변경하여 해결됩니다."

#: ../../<reno.sphinxext stable/rocky>:683
msgid ""
"By default, all operations continue to use the ``role:admin`` check string "
"so there is no upgrade impact."
msgstr "기본적으로 모든 연산은 role:admin 체크 문자를 사용하여 계속 진행되며, 이로 인해 업그레이드의 영향을 받지 않는다."

#: ../../<reno.sphinxext stable/train>:1303
msgid ""
"By incorporating oslo fixes for `bug 1715374`_ and `bug 1794708`_, the nova-"
"compute service now handles ``SIGHUP`` properly."
msgstr ""
"oslo fixes를 `bug 1715374`_ 및 `bug 1794708`_을 포함하여 nova-compute 서비스는 이제 "
"`SIGHUP`를 적절히 처리합니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:311
msgid ""
"By opting into 2.95 API microversion, evacuated instances will remain "
"stopped on the destination host until manually started."
msgstr ""
"2.95 API microversion에 opt-in을 선택하면, 추출된 인스턴스는 목적지 호스트에_until 수동으로 "
"시작되면_지속적으로 정지 상태에 유지됩니다."

#: ../../<reno.sphinxext ../source/newton.rst:79 origin/stable/ocata>:336
#: stable/pike>:455 stable/queens>:1631
msgid ""
"By rebuilding an instance, an authenticated user may be able to circumvent "
"the FilterScheduler bypassing imposed filters (for example, the "
"ImagePropertiesFilter or the IsolatedHostsFilter). All setups using the "
"FilterScheduler (or CachingScheduler) are affected."
msgstr ""
"FilterScheduler를 사용하는 모든 설정은 재건된 인스턴스를 통해 인증된 사용자가 필터를 피할 수 있습니다. 예를 들어, "
"ImagePropertiesFilter 또는 IsolatedHostsFilter와 같은 필터를 피할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1929
msgid ""
"By removing the ``check_attach`` internal call from Nova, small behavioral "
"changes were introduced."
msgstr "Nova에서 ``check_attach`` internal call을 제거하면 소규모 행동 변화가 도입되었다."

#: ../../<reno.sphinxext stable/pike>:389 stable/queens>:1645
msgid ""
"By repeatedly rebuilding an instance with new images, an authenticated user "
"may consume untracked resources on a hypervisor host leading to a denial of "
"service. This regression was introduced with the fix for `OSSA-2017-005`_ "
"(CVE-2017-16239), however, only Nova stable/pike or later deployments with "
"that fix applied and relying on the default FilterScheduler are affected."
msgstr ""
"다시 인스턴스를ใหม한 이미지로 rebuilding하는 quá trình에 의해, 인증된 사용자는 하이퍼바이저 호스트에서 비트랙트 "
"리소스를 소비하여 서비스 거부를 일으킬 수 있습니다. 이 regressions는 `OSSA-2017-005`_ "
"(CVE-2017-16239) fix에 의해 도입되었지만, Nova stable/pike 또는 이후 버전의 배포만이 영향을 받고, "
"Fix가 적용된 후 FilterScheduler를 사용하는 경우에만 영향을 받습니다."

#: ../../<reno.sphinxext stable/stein>:1370
msgid ""
"By using ``writeback`` QEMU cache mode, make Nova's disk image conversion "
"(e.g. from raw to QCOW2 or vice versa) dramatically faster, without "
"compromising data integrity.  `Bug 1818847`_."
msgstr ""
"``writeback`` QEMU 캐시 모드 사용으로 Nova의 디스크 이미지 변환 (예를 들어 raw에서 QCOW2 또는 반대 "
"방향으로)를 대단히 빠르게 할 수 있게 하며, 데이터의完整성을 보존하지 않도록 하며,  `Bug 1818847`_."

#: ../../<reno.sphinxext stable/stein>:1463
msgid ""
"CI testing of Cells v1 has been moved to the ``experimental`` queue meaning "
"changes proposed to nova will not be tested against a Cells v1 setup unless "
"explicitly run through the ``experimental`` queue by leaving a review "
"comment on the patch of \"check experimental\". Cells v1 has been deprecated"
" since the 16.0.0 Pike release and this is a further step in its eventual "
"removal."
msgstr ""
"Cells v1의 CI 테스트는 nova에 제안된 변경 사항이 Cells v1 설정에 테스트되지 않도록 하기 위해 "
"\"experimental\" 큐에 배치되었습니다. 이는 nova에 제안된 변경 사항이 Cells v1 설정에 테스트되지 않도록 하기 "
"위해 \"experimental\" 큐에 배치되었습니다. Cells v1은 16.0.0 피크 릴리스 이후 deprecated되었으며, "
"이는 eventual removal의进一步 단계입니다."

#: ../../<reno.sphinxext stable/queens>:1365
msgid ""
"Calls to mount in the virt disk api no longer ignore the value of stderr."
msgstr "virt disk API에서 mount 호출은 더 이상 stderr의值을 무시하지 않는다."

#: ../../<reno.sphinxext stable/queens>:600
msgid ""
"Cells v1 and nova-network continue to be deprecated, and plan to be removed "
"in the 18.0.0 Rocky release."
msgstr ""
"세ลล스 v1 및 nova-network는 18.0.0 로키 릴리스에서 제거될 예정이며, 현재는 더이상 사용되지 않으며, v1 및 "
"nova-network를 사용하는 경우에만 제외됩니다."

#: ../../<reno.sphinxext stable/pike>:611
msgid "Cells v1 is now deprecated in favor of Cells v2."
msgstr "Cells v1은 Cells v2에 대체되어 현재 비공식적으로 사용되며, Cells v2가 더 나은 성능과 기능을 제공한다."

#: ../../<reno.sphinxext stable/rocky>:1806
msgid ""
"Cells v1 was not converted to use the database backend for console token "
"authorizations. Cells v1 console token authorizations will continue to be "
"supported by the ``nova-consoleauth`` service and use of the "
"``[workarounds]/enable_consoleauth`` option does not apply to Cells v1 "
"users."
msgstr ""
"Cells v1은 콘솔 토큰 인증을 위해 데이터베이스 백엔드를 사용하지 않습니다. Cells v1의 콘솔 토큰 인증은 `nova-"
"consoleauth` 서비스에 의해 지원되고, `[workarounds]/enable_consoleauth` 옵션을 사용하지 않는 "
"Cells v1 사용자는 지원되지 않습니다."

#: ../../<reno.sphinxext stable/rocky>:1882
msgid ""
"Cells v1 was not converted to use the database backend for console token "
"authorizations. Cells v1 console token authorizations will continue to be "
"supported by the ``nova-consoleauth`` service."
msgstr ""
"세ลล스 v1은 콘솔 토큰 인증을 위해 데이터베이스 백엔드를 사용하지 않습니다. 세ลล스 v1의 콘솔 토큰 인증은 `nova-"
"consoleauth` 서비스에 의해 계속 지원됩니다."

#: ../../<reno.sphinxext stable/pike>:1510
msgid ""
"Cells v1, which includes the ``[cells]`` configuration options and ``nova-"
"cells`` service, is deprecated in favor of Cells v2. For information on "
"Cells v2, see: https://docs.openstack.org/nova/latest/user/cells.html"
msgstr ""
"Cells v1, which includes the ``[cells]`` configuration options and ``nova-"
"cells`` service, is deprecated in favor of Cells v2. For information on "
"Cells v2, see: https://docs.openstack.org/nova/latest/user/cells.html"

#: ../../<reno.sphinxext stable/train>:611
msgid ""
"Change the default return value of swap field from the empty string to 0 "
"(integer) in flavor APIs."
msgstr ""
"default return value of swap field를 swap field의 기본 리턴 값에서 공백 문자열에서 0 (정수)으로 "
"바꾸어 flavor APIs에 적용합니다."

#: ../../<reno.sphinxext stable/train>:867
msgid ""
"Cold migration and resize are now supported for servers with neutron ports "
"having resource requests. E.g. ports that have QoS minimum bandwidth rules "
"attached. Note that the migration is only supported if both the source and "
"the destination compute services are upgraded to Train and the "
"``[upgrade_levels]/compute`` configuration does not prevent the computes "
"from using the latest RPC version."
msgstr ""
"chilly migration 및 resize는 now neutron ports가 리소스 요청을 가지는 server에 지원됩니다. 예를 "
"들어, QoS 최소帯width 규칙이 부착된 ports가 있습니다. migration은 source 및 destination "
"compute services가 Train으로 업그레이드되어야만 지원됩니다. 또한, `[upgrade_levels]/compute` "
"구성이 computes가 latest RPC 버전을 사용할 수 있도록 방해하지 않는다."

#: ../../<reno.sphinxext stable/ussuri>:1025
msgid ""
"Compatibility code for compute drivers that do not implement the "
"`update_provider_tree`__ interface has been removed. All compute drivers "
"must now implement ``update_provider_tree``."
msgstr ""
"`compute` 드라이버가 `update_provider_tree`__ 인터페이스를 구현하지 않는 경우의 호환성 코드가 제거되었다. "
"모든 `compute` 드라이버는 `update_provider_tree`를 구현해야 한다."

#: ../../<reno.sphinxext stable/train>:1254
msgid ""
"Compatibility code for compute drivers that do not implement the "
"`update_provider_tree`__ interface is deprecated and will be removed in a "
"future release."
msgstr ""
"`compute` 드라이버가 `update_provider_tree`__ 인터페이스를 구현하지 않는 경우의 호환성 코드는弃용되었으며 향후"
" 릴리스에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/2025.2>:49
msgid ""
"Completes SPICE direct consoles with USB controller config and sound device "
"support."
msgstr "SPICE를 직접 사용하는 콘솔을 완성합니다. USB 컨트롤러 구성과 사운드 장치 지원을 포함합니다."

#: ../../<reno.sphinxext stable/queens>:1025
msgid ""
"Compute API microversion 2.60 must be used to create a server from a "
"multiattach volume or to attach a multiattach volume to an existing server "
"instance."
msgstr ""
"Compute API microversion 2.60을 사용하여 다중 접속 볼륨 또는 존재하는 서버 인스턴스에 다중 접속 볼륨을 연결하는"
" 경우 서버를 생성해야 합니다."

#: ../../<reno.sphinxext stable/stein>:446
msgid ""
"Compute capabilities are now exposed as traits in the placement API. See the"
" `compute capabilities as traits`_ documentation for more details."
msgstr ""
"```korean\n"
"계산 capability은 현재 배치 API 에서 trait 로 노출되어 있습니다. 더 많은 정보를 얻으려면 `compute capabilities as traits`_ 문서를 참조하십시오.\n"
"```"

#: ../../<reno.sphinxext stable/stein>:584
msgid ""
"Compute drivers now expose capabilities via traits in the Placement API.  "
"Capabilities must map to standard traits defined in `the os-traits project "
"<https://docs.openstack.org/os-traits/latest/>`_; for now these are:"
msgstr ""
"Compute 드라이버는 현재 Placement API에서 trait을 통해 능력들을 노출합니다.  능력들은 `the os-traits "
"project <https://docs.openstack.org/os-traits/latest/>`에서 정의된 표준 trait에 "
"mapping해야 합니다.  현재는 다음과 같습니다."

#: ../../<reno.sphinxext stable/train>:733
msgid ""
"Compute nodes using the libvirt driver can now report ``PCPU`` inventory. "
"This is consumed by instances with dedicated (pinned) CPUs. This can be "
"configured using the ``[compute] cpu_dedicated_set`` config option. The "
"scheduler will automatically translate the legacy ``hw:cpu_policy`` flavor "
"extra spec or ``hw_cpu_policy`` image metadata property to ``PCPU`` "
"requests, falling back to ``VCPU`` requests only if no ``PCPU`` candidates "
"are found. Refer to the help text of the ``[compute] cpu_dedicated_set``, "
"``[compute] cpu_shared_set`` and ``vcpu_pin_set`` config options for more "
"information."
msgstr ""
"Compute 노드는 libvirt 드라이버를 사용하여 PCPU 집합을 사용할 수 있습니다. 이것은 고정된 (pin) CPU를 가진 "
"인스턴스에 의해 소비됩니다. 이것은 [compute] cpu_dedicated_set 구성 옵션을 사용하여 구성할 수 있습니다. "
"스케줄러는 legacy hw:cpu_policy flavor의 추가 spéc or hw_cpu_policy image metadata "
"속성을 PCPU 요청으로 변환합니다. PCPU 후보가 발견되지 않으면 VCPU 요청으로 fallback합니다. 더 많은 정보는 "
"[compute] cpu_dedicated_set, [compute] cpu_shared_set 및 vcpu_pin_set 구성 옵션의 "
"도움말 텍스트를 참조하십시오."

#: ../../<reno.sphinxext stable/train>:745
msgid ""
"Compute nodes using the libvirt driver will now report the "
"``HW_CPU_HYPERTHREADING`` trait if the host has hyperthreading. The "
"scheduler will automatically translate the legacy ``hw:cpu_thread_policy`` "
"flavor extra spec or ``hw_cpu_thread_policy`` image metadata property to "
"either require or forbid this trait."
msgstr ""
"Compute 노드는 libvirt 드라이버를 사용하여 now report ``HW_CPU_HYPERTHREADING`` trait를 "
"host가 가속화가 있는 경우. 스케줄러는 legacy ``hw:cpu_thread_policy`` flavor extra spec 또는"
" ``hw_cpu_thread_policy`` image metadata property를 사용하여 trait를 either "
"require or forbid합니다."

#: ../../<reno.sphinxext stable/pike>:615
msgid ""
"Compute-specific documentation is being migrated from "
"http://docs.openstack.org to https://docs.openstack.org/nova/ and the layout"
" for the Nova developer documentation is being re-organized. If you think "
"anything is missing or you now have broken bookmarks, please `report a "
"bug`_."
msgstr ""
"Compute-specific documentation이 http://docs.openstack.org에서 "
"openstack.org/nova/ https://docs.openstack.org/nova/ 에서 이민화되고ening Nova 개발자 "
"문서의 레이아웃이 재조직되고 있습니다. missing or now have broken bookmarks, please `report a"
" bug`_."

#: ../../<reno.sphinxext stable/train>:1149
msgid ""
"Config option ``[ironic]api_endpoint`` was deprecated in the 17.0.0 Queens "
"release and is now removed. To achieve the same effect, set the "
"``[ironic]endpoint_override`` option. (However, it is preferred to omit this"
" setting and let the endpoint be discovered via the service catalog.)"
msgstr ""
"ironic API endpoint 옵션 ``[ironic]api_endpoint``은 17.0.0 퀸즈 릴리스에서弃용되었으며 현재 "
"제거되었다. 이 효과를 얻기 위해 ``[ironic]endpoint_override`` 옵션을 설정해야 한다. (그러나, 이 설정을 "
"제거하고 서비스 카탈로그를 통해 엔드포인트를 발견하는 것을 선호한다.)"

#: ../../<reno.sphinxext stable/stein>:1022
msgid ""
"Config option ``[libvirt]/live_migration_progress_timeout`` was deprecated "
"in Ocata, and has now been removed."
msgstr ""
"`[libvirt]/live_migration_progress_timeout` 옵션은 Ocata에서弃용되었으며 현재 제거되었습니다."

#: ../../<reno.sphinxext stable/2023.2>:354 unmaintained/2023.1>:245
#: unmaintained/victoria>:28 unmaintained/wallaby>:14 unmaintained/xena>:14
#: unmaintained/yoga>:77 unmaintained/zed>:94
msgid ""
"Configuration of service user tokens is now **required** for all Nova "
"services to ensure security of block-storage volume data."
msgstr ""
"사용자 서비스 토큰의 구성은 현재 모든 Nova 서비스에서 블록 스토리지 볼륨 데이터의 보안을 보장하기 위해 **필수**입니다."

#: ../../<reno.sphinxext stable/queens>:1610
msgid ""
"Configuration option ``[ironic]api_endpoint`` is deprecated in favor of "
"``[ironic]endpoint_override``."
msgstr ""
"ironic API endpoint 설정 옵션 ``[ironic]api_endpoint``은 "
"``[ironic]endpoint_override``로 대체되었습니다."

#: ../../<reno.sphinxext stable/pike>:1293
msgid ""
"Configuration option ``console_driver`` in the ``DEFAULT`` group has been "
"deprecated since the Ocata release and is now removed."
msgstr ""
"`console_driver` 속성은 `DEFAULT` 그룹에 있는 `console_driver` 속성이 `Ocata` 릴리스 "
"이후에弃용되었으며 현재 제거되었습니다."

#: ../../<reno.sphinxext stable/pike>:1656
msgid ""
"Configuration option ``wsgi_log_format`` is deprecated. This only applies "
"when running nova-api under eventlet, which is no longer the preferred "
"deployment mode."
msgstr ""
"`wsgi_log_format` 구성 옵션은弃용되었습니다. 이 것은 only nova-api를 eventlet에서 실행할 때만 적용되며,"
" 이때는 더 이상 선호되는 배포 모드가 아닙니다."

#: ../../<reno.sphinxext stable/queens>:912
msgid ""
"Configuration options for `oslo.reports`, found in the ``oslo_reports`` "
"group, are now exposed in nova. These include:"
msgstr ""
"`oslo.reports`의 구성 옵션, `oslo_reports` 그룹에 있는 것으로, 현재 nova에서 노출되었습니다. 이들 중에는 "
"다음과 같습니다."

#: ../../<reno.sphinxext stable/queens>:1615
msgid ""
"Configuration options in the ``[placement]`` section are deprecated as "
"follows:"
msgstr "[위치] 섹션의 구성 옵션은 다음과 같이 비활성화되었습니다."

#: ../../<reno.sphinxext stable/pike>:1355
msgid ""
"Configuration options related to RPC topics were deprecated in the past "
"releases and are now completly removed from nova. There was no need to let "
"users choose the RPC topics for all services. There was little benefit from "
"this and it made it really easy to break Nova by changing the value of topic"
" options."
msgstr ""
"집합 및 RPC 주제에 대한 설정 옵션은 이전 릴리스에서 비상대적이게되었으며 현재 nova에서 완전히 제거되었다. 사용자들이 모든 "
"서비스에 대해 RPC 주제를 선택할 필요는 없고, 이에 대한 이익은 매우 적었으며, 이로 인해 Nova를 쉽게 파괴할 수 있는지에 대한 "
"위험을 증가시켰다."

#: ../../<reno.sphinxext stable/pike>:1381
msgid ""
"Configuration options related to image file have been removed. They were "
"marked as deprecated because the feature to download images from glance via "
"filesystem is not used. Below are the removed options:"
msgstr ""
"구성 옵션은 이미지 파일에 관련된 옵션들이 제거되었다. 이들은 글랜스에서 파일 시스템을 통해 이미지-downloading 기능이 사용되지"
" 않기 때문에弃기된 것으로 표시되었다. 제거된 옵션은 다음과 같다."

#: ../../<reno.sphinxext origin/stable/ocata>:1308
msgid ""
"Configuration options related to the Barbican were deprecated and now "
"completly removed from ``barbican`` group. These options are available in "
"the Castellan library. Following are the affected options:"
msgstr ""
"`barbican` 그룹에 관련된 구성 옵션은 deprecated되었으며 현재 `barbican` 그룹에서 완전히 제거되었습니다. 이러한"
" 옵션은 `Castellan` 라이브러리에서 사용할 수 있습니다. 다음이 영향을 받은 옵션입니다."

#: ../../<reno.sphinxext stable/rocky>:1800
msgid "Console proxies have been deployed per cell"
msgstr "세ลล에 따라 콘솔 프로ksi가 배포되었습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:355 unmaintained/xena>:672
msgid ""
"Constraints in the fix's implementation mean that it only applies to "
"instances booted **after** it has been applied. Existing instances will "
"still experience bug 1851545 after being shelved and unshelved, even with "
"the fix applied."
msgstr ""
"fix의 구현에서 제약은 인스턴스가 부트 **후**에만 적용되기 때문에만 적용되며, 이미 부트된 인스턴스는 fix가 적용된 후에도 버그 "
"1851545를 경험할 수 있습니다. existing 인스턴스는 셰일드하고 셰일드하지 않은 후에도 버그 1851545를 경험할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/rocky>:1033
msgid ""
"Consult https://docs.openstack.org/nova/latest/reference/notifications.html "
"for more information including payload samples."
msgstr ""
"https://docs.openstack.org/nova/latest/reference/notifications.html를 참조하십시오."
" 더 많은 정보와 파이로드 샘플을 포함하여."

#: ../../<reno.sphinxext stable/train>:1385
msgid ""
"Context: What makes ``writethrough`` so safe against host crashes is that it"
" never keeps data in a \"write cache\", but it calls fsync() after *every* "
"write.  This is also what makes it horribly slow.  But cache mode ``none`` "
"doesn't do this and therefore doesn't provide this kind of safety.  The "
"guest OS must explicitly flush the cache in the right places to make sure "
"data is safe on the disk; and all modern OSes flush data as needed.  So if "
"cache mode ``none`` is safe enough for you, then ``writeback`` should be "
"safe enough too."
msgstr ""
"``writethrough``이 호스트 크래시에 대한 안전성을 how-to로 얻는 이유는 `write cache`에 데이터를 유지하지 "
"않고, `fsync()`를 *every* write에 호출하는 것이다.  이것은 또한 cache mode `none`이 horrible "
"slow한 것을 의미한다.  cache mode `none`이 안전성을 제공하지 않는다.  게스트 OS는 데이터가 디스크에 안전하게 "
"유지되도록 cache를 명시적으로 flush해야 한다.  modern OS는 데이터가 필요에 따라 flush한다.  cache mode "
"`none`이 안전 enough라면, `writeback`도 안전 enough라면."

#: ../../<reno.sphinxext stable/rocky>:1554
msgid ""
"Contribute your custom filter/weigher upstream (this is the best option)"
msgstr "집합에 자체 필터/중량 부여를 업스트림에 기여하십시오 (이것은 가장 좋은 옵션입니다)."

#: ../../<reno.sphinxext origin/stable/ocata>:358 stable/pike>:1816
msgid ""
"Correctly allow the use of a custom scheduler driver by using the name of "
"the custom driver entry point in the ``[scheduler]/driver`` config option. "
"You must also update the entry point in ``setup.cfg``."
msgstr ""
"``[scheduler]/driver`` 설정 옵션의 이름을 사용하여 custom scheduler driver의 사용을 허용합니다. "
"또한, ``setup.cfg``에서 사용자 정의 드라이버 엔트리 포인트를 업데이트해야 합니다."

#: ../../<reno.sphinxext stable/train>:1018
msgid ""
"Counted usage will not be accurate in an environment where multiple Nova "
"deployments are sharing a placement deployment because currently placement "
"has no way of partitioning resource providers between different Nova "
"deployments. Operators who are running multiple Nova deployments that share "
"a placement deployment should not set the "
"``[quota]count_usage_from_placement`` configuration option to ``True``."
msgstr ""
"집합에서 여러 Nova 배포가 공유하는 환경에서 사용 카운트는 chính xác하지 않습니다. 현재 배치에는 리소스 제공자를 다른 "
"Nova 배포之间으로 분할할 수 있는 방법이 없기 때문입니다. 여러 Nova 배포가 공유하는 배치 배포를-running하는 운영자는 배치"
" 배포에서 사용 카운트를 ``True``로 설정하는 것을 피해야 합니다."

#: ../../<reno.sphinxext stable/queens>:1508
msgid "Create, Update, Show & List detail flavor"
msgstr ""
"* Create, Update, Show & List detail flavor \n"
"\n"
"*   flavor \n"
"*   detail \n"
"*   Create, Update, Show & List"

#: ../../<reno.sphinxext stable/stein>:1149
msgid "Create, Update, Show & List flavor details"
msgstr "flavor details를 생성, 업데이트, 표시 및 목록화합니다."

#: ../../<reno.sphinxext stable/stein>:724
msgid ""
"Creating servers with Neutron networks having QoS minimum bandwidth rule is "
"not supported."
msgstr "네트워크의 QoS 최소帯width 규칙을 가진 네트워크에 서버를 생성하는 것은 지원되지 않는다."

#: ../../<reno.sphinxext origin/stable/ocata>:455
msgid "Critical Issues"
msgstr "집합"

#: ../../<reno.sphinxext stable/ussuri>:491
msgid ""
"Cross-cell resize is now supported but is disabled by default for all users."
" Refer to the `administrator documentation`__ for details."
msgstr ""
"`cross-cell resize`는 현재 지원되지만, 모든 사용자에게 기본적으로 비활성화되어 있습니다. 더 많은 정보는 "
"`administrator documentation`__에 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:1025
msgid ""
"Current logic in libvirt driver to auto trigger post-copy based on progress "
"information is removed as it has `proved impossible`__ to detect when live-"
"migration appears to be making little progress."
msgstr ""
"현재 libvirt 드라이버에서 프로세스 정보에 따라 포스팅 복사를 tự động 트리거하는 로직이 제거되었다. 이 로직은 live-"
"migration이 약한 tiến도를 보이게 되면 감지할 수 없다는 것을 증명하기가 불가능했다."

#: ../../<reno.sphinxext stable/pike>:786
msgid ""
"Currently only the libvirt compute driver can hide hypervisor signature for "
"the guest host."
msgstr ""
"현재 only libvirt compute driver만 guest host의 host hypervisor signature를 가려주고 "
"있습니다."

#: ../../<reno.sphinxext stable/pike>:828
msgid ""
"Currently only the libvirt compute driver with iSCSI and FC volumes supports"
" the online volume size change."
msgstr "현재 libvirt compute 드라이버만 iSCSI 및 FC 볼륨을 지원하여 온라인 볼륨 크기 변경을 지원합니다."

#: ../../<reno.sphinxext stable/rocky>:1232
msgid ""
"Currently the ``nova-manage cell_v2 map_instances`` command uses a marker "
"setup by which repeated runs of the command will start from where the last "
"run finished, by default. A ``--reset`` option has been added to this "
"command by which the marker can be reset and users can start the process "
"from the beginning if needed, instead of the default behavior."
msgstr ""
"현재 `nova-manage cell_v2 map_instances` 명령은 기본적으로 마지막 실행에서 시작하는 것을 목표로 하는 마커 "
"설정을 사용하여 반복적으로 실행되는 명령을 시작할 때부터 시작합니다. 이 명령은 `--reset` 옵션을 추가하여 마커를 리셋할 수 "
"있으므로, 필요하면 처음부터 시작할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/zed>:320
msgid "Currently, nova supports the following roles:"
msgstr "현재, nova는 다음 역할을 지원합니다:"

#: ../../<reno.sphinxext stable/rocky>:724
msgid ""
"Currently, valid values for the ring buffer sizes are 256, 512, and 1024."
msgstr "현재, 링 버퍼의 유효한 크기 값은 256, 512, 1024입니다."

#: ../../<reno.sphinxext unmaintained/victoria>:337
msgid ""
"Custom Placement resource inventories and traits can now be described using "
"a single `providers configuration file`__."
msgstr ""
"`Custom Placement resource inventories 및 traits가 now는 single `providers "
"configuration file`__를 사용하여 설명할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1545
msgid ""
"Custom scheduler filters and weighers should continue to function since the "
"Instance objects will lazy-load any accessed fields, but this means a round-"
"trip to the database to re-load the object per instance, per host."
msgstr ""
"기본 스케줄러 필터와 가중치가 있는 요소는 인스턴스 오브젝트가 lazy-load된 접근된 필드를 사용할 때 계속 작동해야 합니다. 그러나"
" 이것은 인스턴스당, 호스트당, 데이터베이스로 반복되는 트리플 리트리브를 의미합니다."

#: ../../<reno.sphinxext stable/pike>:729
msgid "DELETE /resource-providers/{uuid}/inventories"
msgstr "DELETE /resource-providers/{uuid}/inventories"

#: ../../<reno.sphinxext stable/pike>:880
msgid ""
"DELETE /resource_providers/{uuid}/traits: Remove any existing trait "
"associations for a specific resource provider"
msgstr "DELETE /resource_providers/{uuid}/traits: 집합을 제거합니다."

#: ../../<reno.sphinxext stable/pike>:875
msgid "DELETE /traits/{name}: To delete the specified trait."
msgstr "DELETE /traits/{name} : 특정 trait을 삭제합니다."

#: ../../<reno.sphinxext unmaintained/yoga>:711
msgid ""
"Default image properties for device buses and models are now persisted in "
"the instance system metadata for the following image properties:"
msgstr ""
"기기 버스 및 모델의 디폴트 이미지 속성은 이제 인스턴스 시스템 메타데이터에 persistence가 되었습니다. 다음 이미지 속성의 "
"경우:"

#: ../../<reno.sphinxext stable/2025.2>:120 stable/2025.2>:134
#: stable/2025.2>:148 stable/2025.2>:151
msgid "Default: ``ADMIN``"
msgstr "``집합``"

#: ../../<reno.sphinxext stable/2025.2>:114 stable/2025.2>:128
#: stable/2025.2>:142
msgid "Default: ``PROJECT_MANAGER_OR_ADMIN``"
msgstr "``PROJECT_MANAGER_OR_ADMIN``"

#: ../../<reno.sphinxext stable/rocky>:1537
msgid ""
"Deployments with custom scheduler filters (or weighers) that rely on the "
"``HostState.instances`` dict to contain full Instance objects will now hit a"
" performance penalty because the Instance values in that dict are no longer "
"fully populated objects. The in-tree filters that do rely on "
"``HostState.instances`` only care about the (1) uuids of the instances per "
"host, which is the keys in the dict and (2) the number of instances per "
"host, which can be determined via ``len(host_state.instances)``."
msgstr ""
"집합에 있는 Full Instance 객체가 포함되는 ``HostState.instances`` dict에 의존하는 custom "
"scheduler 필터 또는 가중치가 있는 배포는 이제 성능 부하에 직면할 것이다. 이 dict에 포함된 Instance 객체의 "
"uuids는 더 이상 완전히 채워진 객체가 아니며, 이 dict에 의존하는 in-tree 필터는 (1) 호스트당 인스턴스 uuids의 "
"uuids와 (2) 호스트당 인스턴스의 수를 (host_state.instances의 length를 사용하여) 결정할 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:1505
msgid ""
"Deprecate the VMware driver's ``wsdl_location`` config option. This option "
"pointed to the location of the WSDL files required when using vCenter "
"versions earlier than 5.1. Since the minimum supported version of vCenter is"
" 5.1, there is no longer a need for this option and its value is ignored."
msgstr ""
"``wsdl_location`` 구성 옵션을 VMware 드라이버를 비활성화합니다. 이 옵션은 vCenter 버전이 5.1보다 이전인 "
"경우 사용하는 WSDL 파일의 위치를 나타냅니다. vCenter의 최소 지원 버전은 5.1이므로 이 옵션의 필요성이なくな고, 이 옵션의 "
"값은 무시됩니다."

#: ../../<reno.sphinxext stable/ussuri>:1095
msgid "Deprecated Option"
msgstr "Deprecated 옵션"

#: ../../<reno.sphinxext stable/pike>:1298
msgid ""
"Deprecated config options to enable/disable extensions "
"``extensions_blacklist`` and ``extensions_whitelist`` have been removed. "
"This means all API extensions are always enabled. If you modifed policy, "
"please double check you have the correct policy settings for all APIs."
msgstr ""
"Deprecated config 옵션으로 확장 기능을 활성화/비활성화 할 수 있는 `extensions_blacklist` 및 "
"`extensions_whitelist` 옵션은 제거되었습니다. 이로 인해 모든 API 확장 기능은 항상 활성화됩니다. policy를 "
"수정한 경우, 모든 API에 대한 chính xác한 정책 설정을 확인해 주세요."

#: ../../<reno.sphinxext unmaintained/victoria>:643
msgid ""
"Deprecated in Train (20.0.0). The RetryFilter has not been requied since "
"Queens following the completion of the return-alternate-hosts blueprint"
msgstr ""
"Deprecated in Train (20.0.0). RetryFilter가 Queens 이후 return-alternate-hosts "
"blueprint의 완성 후부터 필요하지 않은 것으로 deprecated되었습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:648
msgid ""
"Deprecated in Train (20.0.0). These filters have not worked correctly since "
"the introduction of placement in ocata."
msgstr ""
"Deprecated in Train (20.0.0). 이 필터는 ocata에서 배치의 도입 이후부터 제대로 작동하지 않습니다."

#: ../../<reno.sphinxext ../source/mitaka.rst:555 ../source/newton.rst:949
#: origin/stable/ocata>:1416 stable/2023.2>:368 stable/2024.2>:362
#: stable/2025.1>:468 stable/2025.2>:372 stable/pike>:1473 stable/queens>:1484
#: stable/rocky>:1816 stable/stein>:1224 stable/train>:1202
#: stable/ussuri>:1035 unmaintained/2023.1>:177 unmaintained/victoria>:719
#: unmaintained/wallaby>:769 unmaintained/xena>:575 unmaintained/yoga>:563
#: unmaintained/zed>:481
msgid "Deprecation Notes"
msgstr "집합 주의 사항"

#: ../../<reno.sphinxext stable/stein>:262
msgid ""
"Disable eventlet monkey-patching using the environment variable "
"``OS_NOVA_DISABLE_EVENTLET_PATCHING=yes``."
msgstr "OS_NOVA_DISABLE_EVENTLET_PATCHING=yes를 환경 변수로 사용하여 이벤트เล트를 비활성화합니다."

#: ../../<reno.sphinxext stable/pike>:995
msgid ""
"Due to `bug 1707256`_, shared storage modeling in Placement is not supported"
" by the scheduler. This means that in the Pike release series, an operator "
"will be unable to model a shared storage pool between two or more compute "
"hosts using the Placement service for scheduling and resource tracking."
msgstr ""
"`bug 1707256`_의缘에, `Placement`에서 공유 스토리지 모델링은 스케줄러에 의해 지원되지 않습니다. 이것은 피크 릴리즈"
" 시리즈에서, 운영자는 `Placement` 서비스를 사용하여 스케줄링 및 자원 추적을 위해 컴퓨터 호스트를 2개 이상으로 공유 스토리지"
" 풀 모델링할 수 없습니다."

#: ../../<reno.sphinxext stable/queens>:1159
msgid "Due to a bug in python-glanceclient:"
msgstr "Python-glanceclient의 버그로"

#: ../../<reno.sphinxext stable/pike>:1021 stable/queens>:1170
msgid ""
"Due to the changes in scheduling of bare metal nodes, additional resources "
"may be reported as free to Placement. This happens in two cases:"
msgstr ""
"bare metal 서비스의 노드 스케줄 변경으로 인해, 추가 리소스는 Placement에 비어 있는 것으로 보고될 수 있습니다. 이 "
"경우에는 두 가지 경우가 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:1039
msgid ""
"During Policy new defaults, below policies are deprecated and will be "
"removed in 23.0.0 release. These are replaced by the new granular policies "
"listed in feature section."
msgstr ""
"Policy new defaults에서 아래 정책은 deprecated되어 23.0.0 릴리스에서 제거됩니다. 이러한 정책은 새로운-"
"granular 정책이 feature section에 liệt어져 있는 새로운 정책으로 대체됩니다."

#: ../../<reno.sphinxext stable/2024.1>:144 stable/2024.2>:118
#: stable/2025.1>:500
msgid ""
"During the Caracal cycle the libvirt driver was enhanced to support using "
"device aliases to detach devices from a domain. "
"I1dfe4ad3df81bc810835af9b09cfc6c06e9a5388 This introduced a regression for "
"instance with vgpus. A prior bugfix "
"https://bugs.launchpad.net/nova/+bug/1942345 addressed the symptom without "
"correcting the underlying problem. A related bug for mdev devices was later "
"reported. https://bugs.launchpad.net/nova/+bug/2074219 When this feature was"
" added nova introduced a helper method to get device via the alias because "
"the libvirt api does not provide one natively. That helper function assumed "
"all devices would have an alias attribute. That assumption was not valid and"
" had now been corrected. As a result detaching a volume from an instance "
"with vgpus should now be possible and this class of bug should no longer "
"happen."
msgstr ""
"카라칼 주기 동안 libvirt 드라이버가 장치 별명을 사용하여 도메인에서 장치를 분리하는 것을 지원하는 것을 향상시켰다. "
"I1dfe4ad3df81bc810835af9b09cfc6c06e9a5388 이로 인해 vgpus 인스턴스에 대한 회전이 발생했다. 이전의"
" 버그fix https://bugs.launchpad.net/nova/+bug/1942345는 증상만แก졌지만 underlying "
"문제는แก리지 못했다. mdev 장치에 대한 관련된 버그가 나중에 보고되었다. "
"https://bugs.launchpad.net/nova/+bug/2074219 이 기능이 추가된 후 nova는 alias를 통해 장치를"
" 얻는 helper 메소드를 도입했다. libvirt API는 nativamente 제공하지 않기 때문에. 이 helper 함수는 모든 "
"장치가 alias 속성을 가지고 있다고 가정했지만 이 assumption은 now correct가 되었다. 따라서 vgpus 인스턴스에서"
" 볼륨을 분리하는 것은 now 가능하고 이类의 버그가 더 이상 발생하지 않아야 한다."

#: ../../<reno.sphinxext unmaintained/victoria>:64 unmaintained/wallaby>:130
#: unmaintained/xena>:94 unmaintained/yoga>:648
msgid ""
"During the havana cycle it was discovered that eventlet monkey patching of "
"greendns broke ipv6. https://bugs.launchpad.net/nova/+bug/1164822 Since then"
" nova has been disabling eventlet monkey patching of greendns. Eventlet "
"adressed the ipv6 limitation in v0.17 with the introduction of python 3 "
"support in 2015. Nova however continued to disable it, which can result i "
"slow dns queries blocking the entire nova api or other binary because "
"socket.getaddrinfo becomes a blocking call into glibc see: "
"https://bugs.launchpad.net/nova/+bug/1964149 for more details."
msgstr ""
"하바나 사이클 동안, greendns에 대한 eventlet 모니터 patching이 ipv6를 파괴했다. "
"https://bugs.launchpad.net/nova/+bug/1164822 이 후, nova는 eventlet 모니터 "
"patching을 disable했다. Eventlet은 2015년 파이썬 3 지원을 도입하여 ipv6 제한을 해결했다. 그러나 nova는"
" 여전히 disable remained, which can result in slow dns queries blocking the "
"entire nova api or other binary because socket.getaddrinfo becomes a "
"blocking call into glibc see: https://bugs.launchpad.net/nova/+bug/1964149 "
"for more details."

#: ../../<reno.sphinxext unmaintained/zed>:429
msgid ""
"During the triage of https://bugs.launchpad.net/nova/+bug/1978372 we "
"compared the performance of nova's numa allocations strategies as it applied"
" to the large numbers of host and guest numa nodes. Prior to ``Xena`` nova "
"only supported a linear packing strategy. In ``Xena`` "
"``[compute]/packing_host_numa_cells_allocation_strategy`` was introduced "
"maintaining the previous packing behavior by default. The numa allocation "
"strategy has now been defaulted to spread. The old behavior can be restored "
"by defining: ``[compute]/packing_host_numa_cells_allocation_strategy=true``"
msgstr ""
"https://bugs.launchpad.net/nova/+bug/1978372를 트라이에이션하는 동안, nova의 numa 할당 전략의"
" 성능을 비교했습니다. 이에 따라 호스트 및 게스트 numa 노드의 큰 수의 경우에 nova의 numa 할당 전략을 비교했습니다. 이전에"
" \"Xena\"에서 nova는 선형 패킹 전략만 지원했습니다. \"Xena\"에서 "
"\"[compute]/packing_host_numa_cells_allocation_strategy\"라는 설정이 도입되어 이전 패킹 "
"행동을 기본으로 유지했습니다. numa 할당 전략은 현재 spread로 기본화되었습니다. 이전 행동을 복원하려면 다음 설정을 "
"정의해야합니다: \"[compute]/packing_host_numa_cells_allocation_strategy=true\""

#: ../../<reno.sphinxext unmaintained/wallaby>:467
msgid ""
"During unshelve the ARQs will be reallocated and bound to the instance if "
"needed."
msgstr "unshelve 시에 ARQs는 재할당되고 필요에 따라 인스턴스와 연결된다."

#: ../../<reno.sphinxext unmaintained/xena>:594
msgid ""
"Dynamic configuration groups called ``[vgpu_*]`` are now deprecated in "
"favour of ``[mdev_*]``"
msgstr ""
"가상 그래픽 장치 (vGPU) 관련 dynamic configuration group을 ``[vgpu_*]``로 명명한 것은 현재 "
"deprecated되어 ``[mdev_*]``로 대체되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:679
msgid ""
"Each policy is protected with appropriate ``scope_type``. Nova support two "
"types of ``sope_type`` with their combination. ``['system']``, "
"``['project']`` and ``['system', 'project']``."
msgstr ""
"각 정책은 적절한 \"scope_type\"를 사용하여 보호됩니다. Nova는 \"scope_type\"의 두 가지 유형을 지원합니다. "
"그들은 \"['system']\", \"['project']\" 및 \"['system', 'project']\"의 조합을 지원합니다."

#: ../../<reno.sphinxext stable/queens>:1846
msgid "Enable a foundation on which to build support for multi-attach volumes"
msgstr "집합을 위한 기초를 설정하여 다중 연결 볼륨에 대한 지원을 제공하도록 설정합니다."

#: ../../<reno.sphinxext stable/ussuri>:377
msgid ""
"Enabled `rescue for boot-from-volume instances`__. Rescue now also allows to"
" attach stable disk devices to the rescued instance."
msgstr ""
"`rescue for boot-from-volume instances`를 활성화합니다. 현재 `rescue`는 또한 구동 가능한 볼륨에서"
" 인스턴스를 구출한 후 안정적인 디스크 장치를 인스턴스에 연결할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:685
msgid ""
"Enables to launch an instance from an iscsi volume with ironic virt driver. "
"This feature requires an ironic service supporting API version 1.32 or "
"later, which is present in ironic releases > 8.0. It also requires python-"
"ironicclient >= 1.14.0."
msgstr ""
"이ronic virt driver를 사용하여 iscsi 볼륨에서 인스턴스를 시작할 수 있는 기능을 활성화합니다. 이 기능은 API 버전 "
"1.32 이상을 지원하는 ironic 서비스가 필요하며, 이ronic 릴리즈가 8.0 이상인 경우에만 존재합니다. 또한, python-"
"ironicclient >= 1.14.0이 필요합니다."

#: ../../<reno.sphinxext stable/queens>:1049
msgid ""
"Enabling it will spread instances between hosts that have the same weight "
"according to request spec. It is mostly useful when the "
"``[filter_scheduler]host_subset_size`` option has default value of 1, but "
"available hosts have the same weight (e.g. ironic nodes using resource "
"classes). In this case enabling it will decrease the number of rescheduling "
"events."
msgstr ""
"이 기능을 활성화하면, 요청 spéc이에 따라 same weight를 가진 호스트 간에 인스턴스를 분산한다. 이 경우, "
"[filter_scheduler]host_subset_size 옵션의 기본값이 1이지만, 사용 가능한 호스트가 same weight를 "
"가지고 있다(예: resource class를 사용하는 ironic 노드). 이 경우 활성화하면 rescheduling events의 "
"수를 줄일 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:734
msgid ""
"Encryption provider constants have been introduced detailing the supported "
"encryption formats such as LUKs along with their associated in-tree provider"
" implementations. These constants should now be used to identify an "
"encryption provider implementation for a given encryption format."
msgstr ""
"암호화 제공자 상수는 지원 암호화 형식인 LUKs와 그와 관련된 인-트리 제공자 구현을 포함한 암호화 형식에 대한 암호화 제공자 구현을 "
"식별하는 데 사용할 수 있는 상수가 도입되었다. 이 상수는 현재 암호화 형식에 대한 암호화 제공자 구현을 식별하는 데 사용할 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:810
msgid ""
"Enhance pci.passthrough_whitelist to support regular expression syntax. The "
"'address' field can be regular expression syntax. The old "
"pci.passthrough_whitelist, glob sytnax, is still valid config."
msgstr ""
"pci.passthrough_whitelist을 정상 표현식 문법을 지원하도록 확장합니다. 'address' 필드는 정상 표현식 문법을 "
"지원합니다.舊의 pci.passthrough_whitelist, glob 문법은 여전히 유효한 구성입니다."

#: ../../<reno.sphinxext stable/rocky>:1368
msgid ""
"Ensure that the version of ``ip-link`` on the compute host supports setting "
"the trust mode on the device."
msgstr "``ip-link`` 버전이 컴퓨터 호스트에서 장치에 신뢰 모드를 설정할 수 있는지 확인하십시오."

#: ../../<reno.sphinxext stable/train>:1247
msgid ""
"Even in a cell fully upgraded to Train, RPC pinning via "
"``[upgrade_levels]/compute`` can cause live migration of instances with a "
"NUMA topology to revert to the legacy naive behavior. For more details refer"
" to the Upgrade section."
msgstr ""
"Train에서 완전히 업그레이드된 세ลล에서도, RPC pinning을 통해 `[upgrade_levels]/compute`를 사용하면 "
"NUMA拓도에 속하는 인스턴스를 live migration으로 전환할 때는 legacy naive behavior로 돌아가게 됩니다. 더"
" 많은 정보는 업그레이드 섹션을 참조하십시오."

#: ../../<reno.sphinxext ../source/newton.rst:540 origin/stable/ocata>:765
msgid ""
"Every versioned notification has a sample file stored under "
"doc/notification_samples directory. Consult "
"http://docs.openstack.org/developer/nova/notifications.html for more "
"information."
msgstr ""
"각 버전이된 알림은 doc/notification_samples 디렉터리에 샘플 파일이 저장됩니다. 더 많은 정보를 얻으려면 "
"http://docs.openstack.org/developer/nova/notifications.html를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:905
msgid ""
"Every versioned notification has a sample file stored under "
"doc/notification_samples directory. Consult "
"https://docs.openstack.org/nova/latest/reference/notifications.html for more"
" information."
msgstr ""
"다음은 각 버전의 알림이 doc/notification_samples 디렉터리에 샘플 파일을 저장하는 것을 확인하세요. 더 많은 정보는 "
"https://docs.openstack.org/nova/latest/reference/notifications.html 에서 확인할 수"
" 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1736
msgid ""
"ExactCoreFilter, ExactDiskFilter and ExactRamFilter were deprecated for "
"removal in the 16.0.0 Pike release and have now been removed."
msgstr ""
"ExactCoreFilter, ExactDiskFilter 및 ExactRamFilter는 16.0.0 피크 릴리즈에서 제거되기 "
"위해弃용되었으며 현재 제거되었습니다."

#: ../../<reno.sphinxext stable/stein>:908
msgid ""
"Existing ``compute_nodes`` table records with ``0.0`` or ``None`` values for"
" ``cpu_allocation_ratio``, ``ram_allocation_ratio`` or "
"``disk_allocation_ratio`` will be migrated online when accessed or when the "
"``nova-manage db online_data_migrations`` command is run."
msgstr ""
"existing `compute_nodes` 테이블의 레코드에 `cpu_allocation_ratio` , "
"`ram_allocation_ratio` 또는 `disk_allocation_ratio`에 `0.0` 또는 `None`의 값이 있는 "
"경우, `compute_nodes` 테이블을 온라인으로 mig리팅 할 때 접근하거나 `nova-manage db "
"online_data_migrations` 명령을 실행할 때 mig리팅이 온라인으로 진행됩니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:458
msgid ""
"Existing compute nodes will, upon upgrade, perist the uuid of the compute "
"node assigned to their hostname at first startup. Since this must match what"
" is currently in the database, it is important to let nova provision this "
"file from its database. Nova will only persist to a `compute_id` file in the"
" `CONF.state_path` directory, which should already be writable."
msgstr ""
"기존 컴퓨터 노드는 업그레이드 후에, 호스트 이름의 첫 번째 시작 시에 할당된 컴퓨터 노드의 uuid을 보존합니다. 이 uuid은 현재 "
"데이터베이스에 있는 것과 일치해야 하므로, nova가 데이터베이스에서 이 파일을 제공하는 것을 허용해야 합니다. nova는 "
"`compute_id` 파일을 `CONF.state_path` 디렉터리에 persistence 시키는 것을만으로 합니다. 이 디렉터리는 "
"이미 writable해야 합니다."

#: ../../<reno.sphinxext stable/2025.2>:124 stable/2025.2>:138
msgid "Existing policy is used to list live migrations without host info:"
msgstr "现有的 정책은 호스트 정보가 없는 live migration을 listing하기 위해 사용됩니다."

#: ../../<reno.sphinxext stable/2025.2>:109
msgid ""
"Existing policy is used when live migrating server without specifying host:"
msgstr "现有的 정책은 호스트를 명시하지 않아 live migrating server를 사용할 때 사용됩니다."

#: ../../<reno.sphinxext stable/2025.2>:282
msgid "Existing policy:"
msgstr "现行정책"

#: ../../<reno.sphinxext stable/2025.2>:60
msgid ""
"Experimental feature: Nova API, metadata, and scheduler services can run in "
"native threading mode as an alternative to eventlet. Please try it in non-"
"production environment and share your success or failure with us on the "
"openstack-discuss mailing list or via the Nova bug tracker."
msgstr ""
"Experiment적 기능: Nova API, metadata, 및 스케줄러 서비스는 eventlet 대신 native threading"
" 모드에서 실행할 수 있습니다. 이 기능을 테스트하기 위해 비 프로덕션 환경에서 시도해 보세요. 성공 또는 실패를 openstack-"
"discuss mailing list 또는 Nova 버그 트래커를 통해เรา와 공유해 주세요."

#: ../../<reno.sphinxext unmaintained/yoga>:303
msgid ""
"Experimental support for `emulated architecture is now implemented`__. "
"AArch64, PPC64LE, MIPs, and s390x guest architectures are available "
"independent of the host architecture. This is strictly not intended for "
"production use for various reasons, including no security guarantees."
msgstr ""
"`emulated architecture`의 실험적 지원이 현재 구현되었습니다. AArch64, PPC64LE, MIPs, 및 s390x"
" 게스트 아키텍처는 호스트 아키텍처에 독립적으로 사용할 수 있습니다. 이 것은 다양한 이유로 생산 사용에 대한 목적이 아니라, 보안 "
"보장의 absence을 포함합니다."

#: ../../<reno.sphinxext unmaintained/xena>:504
msgid ""
"Experimental support for thread pooling of DB API calls has been removed. "
"This feature was first introduced in the 2014.2 (Juno) release but has not "
"graduated to fully-supported status since nor was it being used for any API "
"DB calls. The ``[oslo_db] use_tpool`` config option used to enable this "
"feature will now be ignored by nova."
msgstr ""
"Experimental support for thread pooling of DB API calls가 제거되었다. 이 기능은 2014.2"
" (Juno) 린스에서 처음 도입되었지만 현재는 완전 지원 상태에 도달하지 못하고 nor는 API DB calls에 사용되지 않았다. "
"`oslo_db` use_tpool` config 옵션은 현재 nova에 의해 무시되게 되었다."

#: ../../<reno.sphinxext unmaintained/yoga>:275
msgid ""
"Experimental support is added for Keystone's `unified limits`__. This will "
"allow operators to test this feature in non-production systems so we can "
"collect early feedback about performance."
msgstr ""
"Experimental 지원이 Keystone의 ` unified limits`__를 추가된다. 이것은 운영자들이 이 기능을 비생산 "
"시스템에서 테스트할 수 있도록 하는데 도움이 될 것이다. 따라서 성능에 대한 초기 피드백을 수집할 수 있다."

#: ../../<reno.sphinxext unmaintained/victoria>:478
msgid ""
"Export instance pinned CPU list through the ``dedicated_cpus`` section in "
"the metadata service API."
msgstr "``dedicated_cpus`` 섹션의 메타데이터 서비스 API를 통해 인스턴스에 pinned CPU 목록을 내보내세요."

#: ../../<reno.sphinxext stable/rocky>:605
msgid ""
"Exposes flavor extra_specs in the flavor representation since microversion "
"2.61. Flavor extra_specs will be included in Response body of the following "
"APIs:"
msgstr ""
"flavor extra_specs를 flavor representation의 flavor에 추가하여 microversion 2.61 "
"이후를 노출한다. flavor extra_specs는 다음 API의 Response body에 포함된다."

#: ../../<reno.sphinxext unmaintained/victoria>:464
msgid ""
"Extend the real-time instance with the ``mixed`` CPU allocation policy. In "
"comparing with ``dedicated`` policy real-time instance, the non-real-time "
"CPUs are not longer required to be pinned on dedicated host CPUs, but float "
"on a range of host CPUs sharing with other instances."
msgstr ""
"집합을 확장하여 ``mixed`` CPU 할당 정책을 적용합니다. ``dedicated`` 정책과 비교하면, 실시간 인스턴스와는 달리 "
"실시간 CPU는 더 이상 고정된 호스트 CPU에 고정되어 있지 않으며, 다른 인스턴스와 공유하는 호스트 CPU의 범위에서浮動합니다."

#: ../../<reno.sphinxext unmaintained/zed>:523
msgid ""
"Extending attached encrypted volumes that failed before because they were "
"not being decrypted using libvirt (any other than LUKS) now work as expected"
" and the new size will be visible within the instance. See `Bug 1967157`_ "
"for more details."
msgstr ""
"연결된 암호화된 볼륨이 이전에 실패했지만 현재는 libvirt (LUKS 이외의 다른 것)가 사용되지 않아 디크립트가 이루어지지 않았기 "
"때문에 now work as expected and the new size will be visible within the "
"instance. See `Bug 1967157`_ for more details."

#: ../../<reno.sphinxext unmaintained/yoga>:382
msgid ""
"Extra sortings were added to numa_fit_instance_to_host function to balance "
"usage of hypervisor's NUMA cells. Hypervisor's NUMA cells with more free "
"resources (CPU, RAM, PCI if requested) will be used first (spread strategy) "
"when configuration option ``packing_host_numa_cells_allocation_strategy`` "
"was set to False. Default value of "
"``packing_host_numa_cells_allocation_strategy`` option is set to True which "
"leads to packing strategy usage."
msgstr ""
"numa_fit_instance_to_host 함수에 추가로 정렬이 추가되어 하이퍼바이저의 NUMA 세ลล의 사용을 cân bằng시키는"
" 데 사용됩니다. 하이퍼바이저의 NUMA 세ลล에 더 많은 비용-free 리소스 (CPU, RAM, PCI가 요청된 경우)가 있는 경우,"
" 구성 옵션 \"packing_host_numa_cells_allocation_strategy\"가 False로 설정되면 첫 번째로 "
"사용되도록 (spread strategy) 설정됩니다. 기본적으로 "
"\"packing_host_numa_cells_allocation_strategy\" 옵션의 giá치가 True로 설정되어 패킹 전략의 "
"사용이 발생합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:762
msgid ""
"Failing to meet these minimum versions when using the libvirt compute driver"
" will result in the `nova-compute` service not starting."
msgstr ""
"이러한 최소 버전을 사용하지 않으면 libvirt compute 드라이버를 사용할 때 `nova-compute` 서비스가 시작되지 않을 "
"것입니다."

#: ../../<reno.sphinxext stable/stein>:1119
msgid ""
"Finally, it is still technically possible to load an out-of-tree scheduler "
"driver using the ``nova.scheduler.driver`` entry-point. However, out-of-tree"
" driver interfaces are not guaranteed to be stable:"
msgstr ""
"마지막으로, 기술적으로는 `nova.scheduler.driver` 엔트리 포인트를 사용하여 트리外 스케줄러 드라이버를 로드하는 것이 "
"여전히 가능하다. 그러나 트리外 드라이버 인터페이스는 안정성이 보장되지 않는다."

#: ../../<reno.sphinxext unmaintained/wallaby>:827
msgid ""
"Finally, the ``GET /os-hypervisors/{hypervisor}/uptime`` API, which provided"
" a similar response to the ``GET /os-hypervisors/{hypervisor}`` API but with"
" an additional ``uptime`` field, has been removed in favour of including "
"this field in the primary ``GET /os-hypervisors/{hypervisor}`` API."
msgstr ""
"마지막으로, ``GET /os-hypervisors/{hypervisor}/uptime`` API, which provided a "
"similar response to the ``GET /os-hypervisors/{hypervisor}`` API but with an"
" additional ``uptime`` field, has been removed in favour of including this "
"field in the primary ``GET /os-hypervisors/{hypervisor}`` API."

#: ../../<reno.sphinxext unmaintained/xena>:358
msgid ""
"Finally, the ``nova-manage volume_attachment refresh`` command can be used "
"to update the volume attachment with this updated connection information."
msgstr ""
"마지막으로, `nova-manage volume_attachment refresh` 명령은 현재 연결 정보를 업데이트한 후 볼륨 연결을 "
"업데이트할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:846
msgid ""
"Finally, the following policies are removed. These were related to the "
"removed APIs listed above and no longer had any effect:"
msgstr ""
"마지막으로, 위의 API 목록에 대한 제거된 API와 관련된 다음 정책이 제거되었습니다. 이 정책은 제거된 API 목록에 대한 "
"정책이었습니다."

#: ../../<reno.sphinxext stable/2024.1>:226 stable/2024.2>:171
#: stable/2025.1>:135 stable/2025.2>:529
msgid ""
"Fix displaying the reason messages from the Ironic validate node operation "
"that is called just before the instance is deployed on the bare metal node. "
"The message from Ironic is now correctly logged. Fixes `bug 2100009 "
"<https://bugs.launchpad.net/nova/+bug/2100009>_`."
msgstr ""
"이ronic 으로 유효성 확인 노드 연산을 사용하여 인스턴스를 바레 메탈 노드에 배포하는 시점에 전달되는 이유 메시지를 표시하지 않도록 "
"수정합니다. Ironic의 메시지는 이제correctly 로그됩니다.  bug 2100009 "
"<https://bugs.launchpad.net/nova/+bug/2100009>_를修正합니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:531 unmaintained/xena>:45
#: unmaintained/yoga>:113 unmaintained/zed>:117
msgid ""
"Fix rescuing volume based instance by adding a check for 'hw_rescue_disk' "
"and 'hw_rescue_device' properties in image metadata before attempting to "
"rescue instance."
msgstr ""
"집합을 기반으로 인스턴스를 구출하는 경우, 'hw_rescue_disk' 및 'hw_rescue_device' 속성을 포함하는 이미지 "
"메타데이트에 대한 확인을 추가하여 인스턴스를 구출하는 시도를 시도합니다."

#: ../../<reno.sphinxext stable/2025.2>:446
msgid ""
"Fixed an issue where certain server actions could fail for servers with "
"ephemeral disks due to filesystem label name length limitations (VFAT, XFS, "
"...). Filesystem label name generation has been fixed for these cases. See "
"https://launchpad.net/bugs/2061701 for more details."
msgstr ""
"ephemeral 디스크에 대한 파일 시스템 라벨 이름 길이 제한 (VFAT, XFS, ...)로 인해 certain server "
"actions가 실패할 수 있는 문제를 고정했습니다. 파일 시스템 라벨 이름 생성은 이러한 경우에 고정되었습니다. 더 많은 정보는 "
"https://launchpad.net/bugs/2061701 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/2024.2>:135 stable/2025.1>:517
msgid ""
"Fixed an issue where the instance rebuild option failed for Ironic "
"instances. The problem was caused by an incorrect parameter order in the "
"``add_instance_info_to_node`` function, which was introduced by commit "
"`93b90d2b` <https://review.opendev.org/c/openstack/nova/+/923910>. For more "
"details, see `bug 2092570` <https://bugs.launchpad.net/nova/+bug/2092570>."
msgstr ""
"이ronic 인스턴스의 재건 옵션을 실패하는 문제를 고정했습니다. 이 문제는 `93b90d2b` "
"<https://review.opendev.org/c/openstack/nova/+/923910> 커밋에 의해 도입된 "
"`add_instance_info_to_node` 함수의 매개 변수 순서가 올바르지 않은 것으로 밝혀졌습니다. 더 많은 세부 사항은 "
"`bug 2092570` <https://bugs.launchpad.net/nova/+bug/2092570>에서 확인할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:67 unmaintained/xena>:174
#: unmaintained/yoga>:616
msgid ""
"Fixed bug `1960230 <https://bugs.launchpad.net/nova/+bug/1960230>`_ that "
"prevented resize of instances that had previously failed and not been "
"cleaned up."
msgstr ""
"정상화된 버그 `1960230 <https://bugs.launchpad.net/nova/+bug/1960230>`_이 이전에 실패하여 "
"청소되지 않은 인스턴스의 리사이즈를 방해했다."

#: ../../<reno.sphinxext stable/2024.1>:33 stable/2024.2>:33 stable/2025.1>:21
#: stable/2025.2>:506
msgid ""
"Fixed performance issue with the ``/os-hypervisors/detail`` API endpoint "
"when using microversion 2.88 or higher. The API was making sequential RPC "
"calls to each compute node to gather uptime information, causing significant"
" delays in environments with many compute nodes (LP#2122036)."
msgstr ""
"``/os-hypervisors/detail`` API endpoint의 microversion 2.88 이상을 사용할 때, 부드러운 "
"성능 문제가 발생합니다. API는 각 컴퓨터 노드에 대해 시계대상 정보를 수집하기 위해 시퀀스적인 RPC 호출을 수행하고 있었습니다. "
"이로 인해 많은 컴퓨터 노드가 있는 환경에서 significatn한 지연이 발생했습니다(LP#2122036)."

#: ../../<reno.sphinxext branch>:29 current
msgid ""
"Fixed the issue `bug 2044235 "
"<https://bugs.launchpad.net/nova/+bug/2044235>`__ where Nova Conductor puts "
"an instance into an error state if any errors occur during execution of the "
"'check_can_live_migrate_source()' method in an RPC call. Now, any error is "
"caught and a MigrationPreCheckError exception is re-raised to reset the "
"instance state."
msgstr ""
"`bug 2044235 <https://bugs.launchpad.net/nova/+bug/2044235>`__을 해결하기 위해 Nova"
" Conductor가 인스턴스를 오류 상태로 전환하는 문제를 fixed했습니다. "
"`check_can_live_migrate_source()` 메서드를 RPC 호출에서 실행할 때 오류가 발생할 경우, 오류가 잡혀 "
"MigrationPreCheckError 예외가 다시 raises되어 인스턴스의 상태를 재설정합니다."

#: ../../<reno.sphinxext stable/2024.1>:14 stable/2024.2>:14 stable/2025.1>:90
#: stable/2025.2>:468
msgid ""
"Fixed the issue `bug 2098496 "
"<https://bugs.launchpad.net/nova/+bug/2098496>`__ where nova assigned more "
"PCI hostdevs to a VM than the flavor requested via the pci_passthrough:alias"
" extra_spec. This only affected systems where both "
"``[filter_scheduler]pci_in_placement`` and ``[pci]report_in_placement`` were"
" set to True. This only affected systems where the PCI alias requested type-"
"VF devices and a single PF device on the compute node supported more than "
"one VFs and ``[pci]device_spec`` configuration allowed nova to use multiple "
"VFs from a single PF."
msgstr ""
"`bug 2098496 <https://bugs.launchpad.net/nova/+bug/2098496>`__을 고정했습니다. "
"nova가 flavor에 따라 pci_passthrough:alias extra_spec을 통해 PCI hostdevs를 더 많이 할당한"
" 문제입니다. 이 문제는 filter_scheduler의 pci_in_placement과 pci report_in_placement이 "
"True인 시스템에만 영향을 미쳤습니다. 이 문제는 type-VF 장치와 compute 노드에 하나의 PF 장치를 지원하는 경우가 "
"많았으며, pci device_spec의 구성이 nova가 하나의 PF에서 여러 VFs를 사용할 수 있는 경우에만 영향을 미쳤습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:507 unmaintained/zed>:134
msgid ""
"Fixed when placement returns ironic nodes that have just started automatic "
"cleaning as possible valid candidates. This is done by marking all ironic "
"nodes with an instance on them as reserved, such that nova only makes them "
"available once we have double checked Ironic reports the node as available. "
"If you don't have automatic cleaning on, this might mean it takes longer "
"than normal for Ironic nodes to become available for new instances. If you "
"want the old behaviour use the following workaround config: "
"`[workarounds]skip_reserve_in_use_ironic_nodes=true`"
msgstr ""
"집합에 placement이 리소스를 할당할 때 ironic 노드가 시작된 후 자동 청소가 가능한 경우에만 유효한 후보자로 간주합니다. "
"이는 모든 ironic 노드를 instance에 마크하여 예약하여, nova가 Ironic이 노드를 사용 가능하다고 보고 한 후에만 "
"그들을 사용할 수 있도록 하며, 이때는 nova가 ironic 노드에 대해 사용 가능한 경우를 더 두 번 확인합니다. 이 경우, 자동 "
"청소가 비활성화된 경우, ironic 노드가 새로운 인스턴스를 사용할 수 있는 시간이 더 오래 걸릴 수 있습니다. 이 경우, old "
"behaviour를 사용하고 싶다면, 다음 workaround config를 사용합니다. "
"`[workarounds]skip_reserve_in_use_ironic_nodes=true`"

#: ../../<reno.sphinxext stable/pike>:1842
msgid ""
"Fixes `bug 1581230`_ by removing the internal ``check_attach`` call from the"
" Nova code as it can cause race conditions and the checks are handled by "
"``reserve_volume`` in Cinder. ``reserve_volume`` is called in every volume "
"attach scenario to provide the necessary checks and volume state validation "
"on the Cinder side."
msgstr ""
"`bug 1581230`_을修正하기 위해 Nova 코드에서 내부적으로 `check_attach` 호출을 제거합니다. "
"`check_attach` 호출은 경쟁 조건을 일으킬 수 있으므로 `reserve_volume`은 Cinder에서 "
"volume.attach 시에 호출되는 `reserve_volume`가 volume의 상태 확인과 필요한 확인을 제공합니다."

#: ../../<reno.sphinxext ../source/mitaka.rst:44 ../source/newton.rst:220
#: origin/stable/ocata>:1573 stable/pike>:1778
msgid ""
"Fixes `bug 1662699`_ which was a regression in the v2.1 API from the "
"``block_device_mapping_v2.boot_index`` validation that was performed in the "
"legacy v2 API. With this fix, requests to create a server with "
"``boot_index=None`` will be treated as if ``boot_index`` was not specified, "
"which defaults to meaning a non-bootable block device."
msgstr ""
"`bug 1662699`_을修复합니다. 이는 legacy v2 API에서 "
"``block_device_mapping_v2.boot_index`` 유효성 검사를 통해 발생한 v2.1 API의 오류입니다. 이修复에 "
"따라 ``boot_index=None``를 사용하여 서버를 생성하는 요청은 ``boot_index``가 지정되지 않은 것과 동일하게 "
"처리되며, 이는 비 부트 가능한 블록 디스크를 의미합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:486 stable/pike>:1788
msgid ""
"Fixes `bug 1670522`_ which was a regression in the 15.0.0 Ocata release. For"
" compute nodes running the libvirt driver with ``virt_type`` not set to "
"\"kvm\" or \"qemu\", i.e. \"xen\", creating servers will fail by default if "
"libvirt >= 1.3.3 and QEMU >= 2.7.0 without this fix."
msgstr ""
"`bug 1670522`_을修复합니다. 이는 15.0.0 Ocata 릴리스에서 발생한 오류였습니다. 컴퓨터 노드가 libvirt "
"드라이버를 사용하고 `virt_type` 속성이 \"kvm\" 또는 \"qemu\"와 같은 값이 설정되지 않은 경우, 즉 "
"\"xen\"이면, libvirt >= 1.3.3 및 QEMU >= 2.7.0이면, 기본적으로 서버를 생성할 수 없습니다. 이修复을 "
"사용하지 않으면."

#: ../../<reno.sphinxext ../source/newton.rst:110 origin/stable/ocata>:386
#: stable/pike>:1806
msgid ""
"Fixes `bug 1691545`_ in which there was a significant increase in database "
"connections because of the way connections to cell databases were being "
"established. With this fix, objects related to database connections are "
"cached in the API service and reused to prevent new connections being "
"established for every communication with cell databases."
msgstr ""
"`bug 1691545`_을修复하는 것은, 세포 데이터베이스와의 연결을 위해 사용되는 연결을 어떻게 설정하는지에 대한 문제가 있었기 "
"때문이다. 이修复은 API 서비스에서 데이터베이스 연결 관련된 오브젝트를 캐시하고, 세포 데이터베이스와의 통신에 대한 새로운 연결을 "
"설정하지 않도록 prevents new connections being established for every communication "
"with cell databases."

#: ../../<reno.sphinxext origin/stable/ocata>:302 stable/pike>:472
#: stable/queens>:1701
msgid ""
"Fixes `bug 1695861`_ in which the aggregate API accepted requests that have "
"availability zone names including ':'. With this fix, a creation of an "
"availabilty zone whose name includes ':' results in a ``400 BadRequest`` "
"error response."
msgstr ""
"`bug 1695861`_을修正하는 것은, 집합 API가 Availability Zone 이름에 ':'을 포함하는 요청을 수용하지 "
"않는다는 점이다. 이修正으로, Availability Zone 이름에 ':'이 포함된 Availability Zone를 생성하면, "
"`400 BadRequest` 오류를 반환한다."

#: ../../<reno.sphinxext stable/stein>:1313
msgid ""
"Fixes `bug 1773342`_ where the Hyper-v driver always deleted unused images "
"ignoring ``remove_unused_images`` config option. This change will now allow "
"deployers to disable the auto-removal of old images."
msgstr ""
"`bug 1773342`_을修正하여 하이퍼-v 드라이버가 항상 사용되지 않은 이미지들을 무시하고 `remove_unused_images`"
" 구성 옵션을 무시하는 경우를 제외하고 사용되지 않은 이미지들을 삭제하는 것을 방지합니다. 이 변경은 now.deployers가 오래된 "
"이미지의 tự động 제거를 비활성화할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:24 stable/rocky>:45 stable/stein>:42
#: stable/train>:172 stable/ussuri>:204 unmaintained/victoria>:286
#: unmaintained/wallaby>:908
msgid ""
"Fixes `bug 1892361`_ in which the pci stat pools are not updated when an "
"existing device is enabled with SRIOV capability. Restart of nova-compute "
"service updates the pci device type from type-PCI to type-PF but the pools "
"still maintain the device type as type-PCI. And so the PF is considered for "
"allocation to instance that requests vnic_type=direct. With this fix, the "
"pci device type updates are detected and the pci stat pools are updated "
"properly."
msgstr ""
"`bug 1892361`_을修复하는 동안, SRIOV 능력으로 활성화된 существ하는 장치에 대해 pci 상태 풀들이 업데이트되지 "
"않습니다. nova-compute 서비스의 재시작은 type-PCI에서 type-PF로 pci 장치 타입을 업데이트합니다. 그러나 풀은 "
"여전히 type-PCI로 장치 타입을 유지합니다. 따라서 PF는 vnic_type=direct를 요청하는 인스턴스에 할당되도록 "
"간주됩니다. 이修复에 따라 pci 장치 타입이 업데이트되는 것을 감지하고 pci 상태 풀들이 올바르게 업데이트됩니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:479
msgid ""
"Fixes `bug 1996995`_ in which VMs live migrated on certain VXLAN Arista "
"network fabrics were inaccessible until the switch arp cache expired."
msgstr ""
"`bug 1996995`_을 해결하여 VMs가 특정 VXLAN Arista 네트워크 패브릭에서 살고 있는 경우, switch의 ARP "
"캐시가 만료될 때까지 접근이 불가했다."

#: ../../<reno.sphinxext stable/train>:1311
msgid ""
"Fixes a bug causing mount failures on systemd based systems that are using "
"the systemd-run based mount with the Nova Quobyte driver."
msgstr "nova-quobyte 드라이버를 사용하는 systemd 기반 시스템에서 mount 실패를 일으키는 버그를修复합니다."

#: ../../<reno.sphinxext stable/pike>:481 stable/queens>:1785
msgid ""
"Fixes a bug preventing ironic nodes without VCPUs, memory or disk in their "
"properties from being picked by nova."
msgstr ""
"ironic 노드가 VCPUs, 메모리 또는 디스크가 없는 속성에 대한 오류를修复하여 nova가 선택하지 못하는 것을 방지합니다."

#: ../../<reno.sphinxext stable/queens>:68 stable/rocky>:262 stable/stein>:389
#: stable/train>:1353
msgid ""
"Fixes a bug that caused Nova to fail on mounting Quobyte volumes whose "
"volume URL contained multiple registries."
msgstr "Quobyte 볼륨의 볼륨 URL에 여러 레지스트리가 포함된 경우 Nova가 마운트를 실패하는 버그를修复합니다."

#: ../../<reno.sphinxext stable/rocky>:322 stable/stein>:1343
msgid ""
"Fixes a race condition that could allow a newly created Ironic instance to "
"be powered off after deployment, without letting the user power it back on."
msgstr ""
"새로 생성된 Ironic 인스턴스를 배포 후에 제어를 해제할 수 있는 경쟁 조건을 해결하여 사용자가 다시 제어를 얻지 못하도록 제어를 "
"해제할 수 있는 문제를 해결합니다."

#: ../../<reno.sphinxext stable/2023.2>:36 stable/2024.1>:211
#: stable/2024.2>:386 unmaintained/2023.1>:83
msgid ""
"Fixes a regression for live migration on shared storage that was removing "
"the backing disk and instance folder during the cleanup of a virtual machine"
" post live migration. `bug 2080436 "
"<https://bugs.launchpad.net/nova/+bug/2080436>`__ for details."
msgstr ""
"live migration에서 공유 스토리지에서 live migration 후 virtuel machine의 청소에서 backing "
"disk와 instance folder를 제거하는 regressions를修复합니다. "
"<https://bugs.launchpad.net/nova/+bug/2080436>"

#: ../../<reno.sphinxext stable/2024.1>:204 stable/2024.2>:111
#: stable/2025.1>:493
msgid ""
"Fixes an issue seen when using bare metal (Ironic) instances where an "
"instance could fail to delete.  See `Bug 2019977`_ for more details."
msgstr ""
"bare metal (Ironic) 인스턴스를 사용할 때, 인스턴스가 제거할 수 없을 때 문제가 발생한다.  더 많은 정보는 `Bug "
"2019977`_을 참조하십시오."

#: ../../<reno.sphinxext stable/2025.1>:531
msgid ""
"Fixes an issue when serial console for baremetal instances is not available."
" `See bug 2099872 <https://bugs.launchpad.net/nova/+bug/2099872>`__"
msgstr ""
"baremetal 인스턴스의 시리얼 콘솔이 उपलबуществ하지 않을 때 문제를 해결합니다. `bug 2099872 "
"<https://bugs.launchpad.net/nova/+bug/2099872>`"

#: ../../<reno.sphinxext stable/pike>:89
msgid ""
"Fixes an issue with cold migrating (resizing) an instance from ocata to pike"
" compute by correcting parameters order in resize_instance rpcapi call to "
"destination compute."
msgstr ""
"cold migrating (resizing) an instance from ocata to pike compute에 대한 문제를 "
"해결한다. ocata에서 pike compute로 인스턴스를冷동 이동(resize)하는 경우, resize_instance rpcapi "
"호출의 목적지 컴퓨터에서 매개 변수의 순서를 수정하여 문제를 해결한다."

#: ../../<reno.sphinxext unmaintained/wallaby>:180 unmaintained/xena>:679
msgid ""
"Fixes an issue with multiple ``nova-compute`` services used with Ironic, "
"where a rebalance operation could result in a compute node being deleted "
"from the database and not recreated. See `bug 1853009 "
"<https://bugs.launchpad.net/nova/+bug/1853009>`__ for details."
msgstr ""
"``nova-compute`` 서비스를 Ironic과 함께 사용하는 경우, 여러 ``nova-compute`` 서비스가 사용되는 문제를 "
"해결합니다. 이 문제는 rebalance 연산이 compute 노드를 데이터베이스에서 삭제하고 다시 생성하지 못하는 경우에 발생합니다. "
"더 많은 정보는 <https://bugs.launchpad.net/nova/+bug/1853009>에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1808
msgid ""
"Fixes how memory stats are reported for VMware. The total memory for the "
"vCenter cluster managed by Nova should be the aggregated sum of total memory"
" of each ESX host in the cluster. This is more accurate than using the "
"available memory of the resource pool associated to the cluster."
msgstr ""
"VMware의 메모리 상태를 báo cáo하는 방식에 대한修正. vCenter 클러스터를 Nova가 관리하는 경우, vCenter "
"클러스터의 총 메모리는 ESX 호스트가 클러스터에 속하는 각 호스트의 총 메모리의 집합으로 계산되어야 합니다. 이것은 리소스 풀에 속한 "
"클러스터와 관련된 readily available 메모리 사용보다 더 chính確합니다."

#: ../../<reno.sphinxext stable/train>:50 stable/ussuri>:14
#: unmaintained/victoria>:53 unmaintained/wallaby>:119 unmaintained/xena>:188
#: unmaintained/yoga>:637
msgid ""
"Fixes slow compute restart when using the ``nova.virt.ironic`` compute "
"driver where the driver was previously attempting to attach VIFS on start-up"
" via the ``plug_vifs`` driver method. This method has grown otherwise unused"
" since the introduction of the ``attach_interface`` method of attaching "
"VIFs. As Ironic manages the attachment of VIFs to baremetal nodes in order "
"to align with the security requirements of a physical baremetal node's "
"lifecycle. The ironic driver now ignores calls to the ``plug_vifs`` method."
msgstr ""
"``nova.virt.ironic`` 컴퓨터 드라이버를 사용할 때, 드라이버가 이전에 ``plug_vifs`` 드라이버 메서드를 통해 "
"VIFS를 시작 시에 연결하려고 시도했을 때, 느린 컴퓨터 재시작을修正합니다. 이 메서드는 ``attach_interface`` 메서드가"
" VIFs를 연결하는 것을 도입한 이후에 사용되지 않았습니다. Ironic은 물리적 baremetal 노드의 생명주기를 align하기 "
"위해 VIFs를 baremetal 노드에 연결하는 것을 관리합니다. Ironic 드라이버는 ``plug_vifs`` 메서드에 대한 호출을"
" 무시합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:919
msgid ""
"Flavor.projects (access) will not be present in the instance versioned "
"notifications since notifications currently do not lazy-load fields. This "
"limitation is being tracked with `bug 1653221`_."
msgstr ""
"Flavor.projects (access)가 인스턴스 버전화된 알림에 존재하지 않습니다. 현재 알림은 필드의 lazy-load를 "
"지원하지 않기 때문에 인스턴스 버전화된 알림에 Flavor.projects (access)가 존재하지 않습니다. 이 제한은 `bug "
"1653221`_에 의해 추적됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1217
msgid ""
"Following Notifications related configuration options have been moved from "
"the ``DEFAULT`` group to the ``notifications`` group:"
msgstr "다음 Notification 관련 설정 옵션은 DEFAULT 그룹에서 NOTIFICATIONS 그룹으로 옮겨졌습니다."

#: ../../<reno.sphinxext stable/ussuri>:1231
msgid ""
"For AArch64 Nova now sets ``max`` as the default CPU model. It does the "
"right thing in context of both QEMU TCG (plain emulation) and for KVM "
"(hardware acceleration)."
msgstr ""
"AA64 Nova에서 현재 ``max``을 기본 CPU 모델로 설정하고 있습니다. QEMU TCG (plain emulation)과 "
"KVM (하드웨어 가속화)에서 cả 두 경우에 적절한 행동을 수행합니다."

#: ../../<reno.sphinxext stable/stein>:988
msgid ""
"For all three actions we will now check both the flavor and image to "
"validate the CPU policy, CPU thread policy, CPU topology, memory topology, "
"hugepages, serial ports, realtime CPU mask, NUMA topology details, CPU "
"pinning, and a few other things."
msgstr ""
"집합의 세 가지 행동을 확인할 때는 now flavor 및 image를 확인하여 CPU 정책, CPU 스레드 정책, CPU拓도, "
"메모리拓도, hugepages, 시리얼 포트, real-time CPU 마스크, NUMA拓도รายละเอียด, CPU 핀िंング, "
"그리고 몇 가지 다른 것들을 확인합니다."

#: ../../<reno.sphinxext stable/2025.2>:316
msgid ""
"For backward compatibility, Nova continue allow ``admin`` role token to "
"access service APIs but in future release, ``admin`` access will be removed."
msgstr "``admin`` 역할 토큰은 서비스 API에 접근할 수 있지만 향후 릴리스에서 ``admin`` 접근은 제거될 예정입니다."

#: ../../<reno.sphinxext stable/rocky>:1357
msgid "For example, modify ``/etc/nova/nova.conf`` and set:"
msgstr "예를 들어, `/etc/nova/nova.conf`를 수정하고 다음과 같이 설정하세요:"

#: ../../<reno.sphinxext stable/rocky>:1423
msgid ""
"For image download, the proxy downloads an image stream from glance; "
"extracts the data stream from the image stream; and then remotely imports "
"the data stream to XenServer's VDI via the remote API supplied by XAPI."
msgstr ""
"이미지 다운로드를 위해, 프록시는 glance에서 이미지 스트림을 다운로드; 이미지 스트림에서 데이터 스트림을 추출하고; 그리고 "
"XenServer의 VDI에 데이터 스트림을 원격으로.import합니다."

#: ../../<reno.sphinxext stable/rocky>:1419
msgid ""
"For image upload, the proxy will export a data stream for a VDI from "
"XenServer via the remote API supplied by XAPI; convert the stream to the "
"image format supported by glance; and upload the image to glance."
msgstr ""
"이미지 업로드에 대해서, 프록시는 XenServer를 통해 XAPI가 제공하는 원격 API를 통해 VDI에 대한 데이터 "
"스트림을.export합니다. 스트림을 glance가 지원하는 이미지 포맷으로 변환하고, 그 afterwards glance에 이미지 "
"업로드합니다."

#: ../../<reno.sphinxext stable/train>:672
msgid ""
"For information on how to set up support for AMD SEV, please see the `KVM "
"section of the Configuration Guide "
"<https://docs.openstack.org/nova/latest/admin/configuration/hypervisor-"
"kvm.html#amd-sev>`_."
msgstr ""
"AMD SEV를 지원하기 위한 설정에 대한 정보는 `KVM 섹션의 구성 가이드 "
"<https://docs.openstack.org/nova/latest/admin/configuration/hypervisor-"
"kvm.html#amd-sev>` 에서 확인하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:837
msgid ""
"For instance, if the user wishes to see resource providers that can service "
"a request for 2 vCPUs, 1024 MB of RAM and 50 GB of disk space, the user can "
"issue a request of::"
msgstr ""
"예를 들어, 사용자가 2 vCPUs, 1024 MB의 RAM, 50 GB의 디스크 스페이스를 서비스할 수 있는 리소스 제공자가 필요하다면, 사용자는 다음과 같은 요청을 보내야 한다:\n"
"\n"
"```\n"
"{\n"
"  \"request\": {\n"
"    \"vcpus\": 2,\n"
"    \"ram\": 1024,\n"
"    \"disk\": 50\n"
"  }\n"
"}\n"
"```\n"
"\n"
"이 요청은 \"집합\"으로 번역됩니다."

#: ../../<reno.sphinxext stable/queens>:644
msgid ""
"For knowing which types the physical GPU driver supports for libvirt, the "
"operator can look at the sysfs by doing::"
msgstr ""
"physical GPU 드라이버가 libvirt에 대한 어떤 유형을 지원하는지 알기 위해서는 sysfs를 확인하는 것이 필요하다."

#: ../../<reno.sphinxext stable/queens>:371
msgid ""
"For libvirt driver. Now when creating tap devices the MTU will be "
"configured. Requires libvirt 3.3.0 at least."
msgstr ""
"libvirt 드라이버에 대해. 현재 libvirt 드라이버를 사용할 때는 MTU를 설정할 수 있습니다. libvirt 3.3.0 이상이"
" 필요합니다."

#: ../../<reno.sphinxext stable/ussuri>:789 unmaintained/victoria>:543
msgid ""
"For more background about the possible problem, check `this bug "
"<https://bugs.launchpad.net/nova/+bug/1875418>`_. A upgrade check has been "
"added to the ``nova-status upgrade check`` command for this."
msgstr ""
"`이 bug <https://bugs.launchpad.net/nova/+bug/1875418>`_에 대한 더 많은 배경 정보를 "
"확인하려면 `nova-status upgrade check` 명령을 사용하여 업그레이드 확인을 추가했습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:870
msgid ""
"For more background on this change, see "
"https://bugs.launchpad.net/nova/+bug/1552042."
msgstr ""
"이 변경 사항에 대한 더 많은 배경 정보는 https://bugs.launchpad.net/nova/+bug/1552042 에서 확인할 "
"수 있습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:419
msgid "For more detail on command usage, see the machine type documentation:"
msgstr "집합에 대한 명령어 사용에 대한 더 많은 정보는 기계 유형 문서를 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:913
msgid "For more details, refer to the `spec`__."
msgstr "`spec`__를 더 많은 정보를 얻으려면 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:942
msgid "For more details, see the admin guide documentation:"
msgstr "admin guide 문서에 더 많은 정보를 확인하십시오."

#: ../../<reno.sphinxext stable/rocky>:1052
msgid "For more details, see the command documentation:"
msgstr "집합에 대한 더 많은 정보는 명령문 설명서를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:704
msgid ""
"For more information about placement policy including a sample file, see the"
" configuration reference documentation:"
msgstr "집합 정책에 대한 더 많은 정보, 포함한 샘플 파일에 대한 정보는 구성 참조 दस사서를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:152 stable/rocky>:302
#: stable/stein>:962
msgid "For more information, refer to `bug #1289064`_."
msgstr "`bug #1289064`_에 대한 더 많은 정보를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:1119 stable/queens>:1147
msgid "For more information, refer to `the documentation`__."
msgstr "`더 많은 정보를 얻으려면, `the documentation`를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:1064
msgid "For more information, refer to the `spec`__ and `documentation`__."
msgstr ""
"`spec`__와 `documentation`__에 대한 더 많은 정보를 얻으려면, [spec](#spec)과 "
"[documentation](#documentation)을 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:832
msgid "For more information, see the Cinder admin guide:"
msgstr "For more information, see the Cinder admin guide: 집합 관리 가이드"

#: ../../<reno.sphinxext unmaintained/2023.1>:555 unmaintained/zed>:78
msgid ""
"For networks which have any subnets with enabled DHCP, MTU value is not send"
" in the metadata. In such case MTU is configured through the DHCP server."
msgstr ""
"네트워크가 DHCP를 활성화한 서브넷이 있는 경우, 메타데이터에 MTU의值을 전송하지 않습니다. 이 경우 DHCP 서버를 통해 MTU를 "
"구성합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1166
msgid ""
"For non-admin users, the sort key ``host`` and ``node`` will be excluded."
msgstr "non-admin 사용자에게는, `host` 및 `node`의 정렬 키가 제외됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1129
msgid ""
"For non-admin users, there is a whitelist for filters already. That "
"whitelist is unchanged."
msgstr "non-admin 사용자에게는 필터의 백리스트가 이미 존재한다. 그 백리스트는 변경되지 않는다."

#: ../../<reno.sphinxext stable/queens>:148 stable/rocky>:298
#: stable/stein>:958
msgid ""
"For operators that are aware of the issues and are able to manually work "
"around them, the ``[workarounds] enable_numa_live_migration`` option can be "
"used to allow the broken behavior."
msgstr ""
"연산자들이 문제를 인식하고 수동으로 그들을 autour으로 해결할 수 있는 경우, `enable_numa_live_migration` "
"옵션을 사용하여 파괴된 행동을 허용할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:631
msgid ""
"For the VMWare driver, only the VNC option applied. However, the ``[vmware] "
"vnc_keymap`` option was introduce in 18.0.0 (Rocky) and can be used to "
"replace ``[vnc] keymap``."
msgstr ""
"VMWare 드라이버에 대해서만 VNC 옵션만 적용되었지만, ``[vmware] vnc_keymap`` 옵션은 18.0.0 (Rocky)"
" 버전에서 도입되었으며, ``[vnc] keymap``를 대체할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1855
msgid ""
"For the VMWare driver, only the VNC option applies. However, this option is "
"deprecated and will not affect any other driver in the future. A new option "
"has been added to the ``[vmware]`` group to replace this:"
msgstr ""
"VMWare 드라이버에 대해서만 VNC 옵션만 적용되는데, 그러나 이 옵션은 비활성화되었으며 향후 다른 드라이버에 영향을 미치지 "
"않습니다. 새로운 옵션은 ``[vmware]`` 그룹에 추가되었으며, 이 옵션을 대체합니다."

#: ../../<reno.sphinxext unmaintained/zed>:623
msgid ""
"For the VMware ESXi, VM memory should be multiple of 4. Otherwise creating "
"instance on ESXi fails with error \"VimFaultException: Memory (RAM) size is "
"invalid.\". Instances will now fail to spawn if flavor memory is not a "
"multiple of 4."
msgstr ""
"VMware ESXi에서 VM 메모리는 4의 배수여야 합니다. 그렇지 않으면 ESXi에서 인스턴스를 만들 때 오류가 발생합니다. "
"\"VimFaultException: Memory (RAM) size is invalid.\" 에러가 발생합니다. flavor 메모리가 "
"4의 배수여야지 인스턴스 생성이 실패하지 않습니다."

#: ../../<reno.sphinxext stable/stein>:785
msgid ""
"For the VMware vCenter driver, added support for the configured video ram "
"``hw_video_ram`` from the image, which will be checked against the maximum "
"allowed video ram ``hw_video:ram_max_mb`` from the flavor. If the selected "
"video ram from the image is less than or equal to the maximum allowed ram, "
"the ``videoRamSizeInKB`` will be set. If the selected ram is more than the "
"maximum allowed one, then server creation will fail for the given image and "
"flavor. If the maximum allowed video ram is not set in the flavor we do not "
"set ``videoRamSizeInKB`` in the VM."
msgstr ""
"VMware vCenter 드라이버에 대해, 구성된 비디오 RAM \"hw_video_ram\"을 이미지에서 확인하고, "
"\"hw_video:ram_max_mb\"의 최대 허용 비디오 RAM과 비교하여 확인합니다. 선택된 비디오 RAM이 이미지에서 선택된 "
"비디오 RAM보다 최대 허용 RAM보다 적거나 같다면, \"videoRamSizeInKB\"를 설정합니다. 선택된 RAM이 최대 허용 "
"RAM보다 더 많다면, 이미지를 사용하고 flavor를 사용할 때 서버를 생성할 수 없습니다. flavor에서 최대 허용 비디오 RAM이"
" 설정되지 않으면, VM에서 \"videoRamSizeInKB\"를 설정하지 않습니다."

#: ../../<reno.sphinxext stable/queens>:1815
msgid ""
"For the XenAPI driver, in order to delete cached images based on when they "
"were created, a new ``--keep-days DAYS`` option is added to the "
"``destroy_cached_images`` script to delete cached images which were created "
"at least ``DAYS`` days ago. By default, all unused cached images will be "
"deleted when the script is run if they have ``cached_time``."
msgstr ""
"XenAPI 드라이버에 대해, 캐시된 이미지의 creation date에 따라 캐시된 이미지들을 삭제하기 위해, "
"destroy_cached_images 스크립트에 new option ``--keep-days DAYS``가 추가되어, ``DAYS``일"
" 이상 전 creation date에 해당하는 캐시된 이미지들을 삭제합니다. 기본적으로, 스크립트가 실행되면, unused 캐시된 "
"이미지들이 ``cached_time``가 있는 경우에 모두 삭제됩니다."

#: ../../<reno.sphinxext unmaintained/yoga>:358
msgid ""
"For the details on what changed from the existing policy, please refer the "
"`RBAC new guidelines`_. We have implemented only phase-1 `RBAC new "
"guidelines`_. Currently, scope checks and new defaults are disabled by "
"default. You can enable them by switching the below config option in "
"``nova.conf`` file::"
msgstr ""
"existing policy에 changes가 발생한 details에 대한 정보는 `RBAC new guidelines`_를 "
"참조하십시오. We have implemented only phase-1 `RBAC new guidelines`_. Currently, "
"scope checks and new defaults are disabled by default. You can enable them "
"by switching the below config option in ``nova.conf`` file::"

#: ../../<reno.sphinxext unmaintained/zed>:326
msgid ""
"For the details on what changed from the existing policy, please refer to "
"the `RBAC new guidelines`_. We have implemented only phase-1 of the `RBAC "
"new guidelines`_. Currently, scope checks and new defaults are disabled by "
"default. You can enable them by switching the below config option in "
"``nova.conf`` file::"
msgstr ""
"existing policy에 changes가 occurred한 details를 확인하려면 `RBAC new guidelines`_를 "
"참조하십시오. We have implemented only phase-1 of the `RBAC new guidelines`_. "
"Currently, scope checks and new defaults are disabled by default. You can "
"enable them by switching the below config option in ``nova.conf`` file:: 집합 "
"new guidelines에 changes가 occurred한 details를 확인하려면 `RBAC new guidelines`_를 "
"참조하십시오. We have implemented only phase-1 of the `RBAC new guidelines`_. "
"Currently, scope checks and new defaults are disabled by default. You can "
"enable them by switching the below config option in ``nova.conf`` file::"

#: ../../<reno.sphinxext stable/train>:976
msgid ""
"For the libvirt driver, the NUMA-aware live migration feature requires the "
"conductor, source compute, and destination compute to be upgraded to Train. "
"It also requires the conductor and source compute to be able to send RPC 5.3"
" - that is, their ``[upgrade_levels]/compute`` configuration option must not"
" be set to less than 5.3 or a release older than \"train\"."
msgstr ""
"libvirt 드라이버에 대해, NUMA- aware live migration 기능은 conductor, source compute, "
"및 destination compute를 train 버전으로 업그레이드해야 하며, conductor와 source compute가 RPC"
" 5.3을 보낼 수 있는지 확인해야 한다. 즉, 그들의 ``[upgrade_levels]/compute`` 구성 옵션은 5.3 이상 또는"
" \"train\" 버전보다 오래된 경우에 대해 설정되지 않도록 해야 한다."

#: ../../<reno.sphinxext stable/queens>:656
msgid ""
"For the moment, only a single type can be supported across one compute node,"
" which means that libvirt will create the vGPU by using that specific type "
"only. It's also possible to have two compute nodes having different types "
"but there is no possibility yet to specify in the flavor which specific type"
" we want to use for that instance."
msgstr ""
"현재, 한 컴퓨터 노드에만 한 종류가 지원될 수 있으며, 이는 libvirt가 특정 종류만을 사용하여 vGPU를 생성하는 것을 의미한다."
" 또한, 두 컴퓨터 노드가 다른 종류를 사용할 수 있지만, 현재는 인스턴스에 사용할 특정 종류를 지정할 수 없다는 점이 있다."

#: ../../<reno.sphinxext stable/rocky>:1186
msgid ""
"From microversion 1.27, the ``provider_summaries`` field in the response of "
"the ``GET /allocation_candidates`` API includes all the resource class "
"inventories, while it had only requested resource class inventories with "
"older microversions. Now callers can use this additional inventory "
"information in making further sorting or filtering decisions."
msgstr ""
"1.27 이상의 microversion에서, GET /allocation_candidates API의 response의 "
"provider_summaries 필드는 오래된 microversion에서만 resource class inventories를 요청한 "
"것과는 달리 모든 resource class inventories를 포함합니다. 이로 인해 callers은 추가적인 인 inventory"
" 정보를 사용하여 나중에 정렬 또는 필터링의 결정을 내릴 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1008
msgid ""
"From microversion 1.29, we support allocation candidates with nested "
"resource providers. Namely, the following features are added. 1) ``GET "
"/allocation_candidates`` is aware of nested providers. Namely, when provider"
" trees are present, ``allocation_requests`` in the response of ``GET "
"/allocation_candidates`` can include allocations on combinations of multiple"
" resource providers in the same tree. 2) ``root_provider_uuid`` and "
"``parent_provider_uuid`` are added to ``provider_summaries`` in the response"
" of ``GET /allocation_candidates``."
msgstr ""
"microversion 1.29부터, 집합에 nested 리소스 제공자를 포함한 할당 후보자를 지원합니다. 즉, 다음 기능이 "
"추가되었습니다. 1) `GET /allocation_candidates`는 nested 제공자를 인식합니다. 즉, 제공자 트리들이 존재할"
" 때, `GET /allocation_candidates`의 응답에서 `allocation_requests`에는 여러 리소스 제공자가 "
"sama한 트리에서 조합된 할당을 포함할 수 있습니다. 2) `root_provider_uuid`와 "
"`parent_provider_uuid`가 `provider_summaries`에 추가되었습니다. `GET "
"/allocation_candidates`의 응답에서."

#: ../../<reno.sphinxext stable/stein>:514
msgid ""
"From microversion 2.69 the responses of ``GET /servers``, ``GET "
"/servers/detail``, ``GET /servers/{server_id}`` and ``GET /os-services`` "
"will contain missing keys during down cell situations because of adding "
"support for returning minimal constructs based on the available information "
"from the API database for those records in the down cells. See `Handling "
"Down Cells`_ for more information on the missing keys."
msgstr ""
"microversion 2.69에서, `GET /servers`, `GET /servers/detail`, `GET "
"/servers/{server_id}` 및 `GET /os-services`의 응답은 down cell 상황에서 missing key가 "
"포함되어 있는 경우가 있습니다. 이것은 down cell records에 대한 API 데이터베이스에서 사용 가능한 정보에 따라 "
"최소한의構造를 반환하는 지원을 추가한 결과입니다. down cell 상황에서 missing key에 대한 자세한 정보는 `Handling"
" Down Cells`_를 참조하십시오."

#: ../../<reno.sphinxext unmaintained/yoga>:469
msgid ""
"From this release, Nova instances will get ``virtio`` as the default display"
" device (instead of ``cirrus``, which has many limitations). If your guest "
"has a native kernel (called \"virtio-gpu\" in Linux; available since  Linux "
"4.4 and above) driver, then it'll be used; otherwise, the 'virtio' model "
"will gracefully fallback to VGA compatibiliy mode, which is still better "
"than ``cirrus``."
msgstr ""
"이 릴리즈부터 노바 인스턴스는 기본적으로 ``virtio``를 사용하여 디스플레이 장치 (``cirrus`` 대신)로 설정합니다. "
"(``cirrus``는 많은 제한이 있습니다.) 가스트가 native 커널 (Linux에서 \"virtio-gpu\"라고 함; 4.4 "
"이상의 Linux 버전부터 사용 가능) 드라이버를 사용한다면, 이 드라이버가 사용됩니다. 반면에, native 커널 드라이버가 사용되지 "
"않는다면, 'virtio' 모델은 VGA 호환 모드 (cirrus보다도 더 좋습니다.)로 수동으로 fallback합니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:308
msgid ""
"Fully-Qualified Domain Names are now considered valid for an instance "
"hostname if you use the 2.94 API microversion."
msgstr ""
"Fully-Qualified Domain Names은 now 2.94 API microversion을 사용할 때 instance "
"hostname에 대해 유효한 것으로 간주됩니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:856
msgid ""
"Furthermore, the `2.57 compute REST API microversion`_ deprecated the use of"
" personality files for file injection. For more history on deprecating file "
"injection, see the `spec`__."
msgstr ""
"다음과 같은 경우, `2.57 compute REST API microversion`은 파일 인젝션의 성격 파일을 사용하는 것을 "
"비활성화했다. 파일 인젝션에 대한 더 많은 역사에 대한 정보는 `spec`를 참조하십시오."

#: ../../<reno.sphinxext stable/pike>:876
msgid ""
"GET /resource_providers/{uuid}/traits: a list of traits associated with a "
"specific resource provider"
msgstr "GET /resource_providers/{uuid}/traits: 특정 리소스 프로バイ더와 관련된 trait의 집합"

#: ../../<reno.sphinxext stable/stein>:664
msgid "GET /servers/{server_id}/os-interface (list)"
msgstr "GET /servers/{server_id}/OS-인터페이스 (리스트)"

#: ../../<reno.sphinxext stable/stein>:665
msgid "GET /servers/{server_id}/os-interface/{port_id} (show)"
msgstr "GET /서버/{서버_아이디}/OS-인터페이스/{포트_아이디} (보여)"

#: ../../<reno.sphinxext stable/stein>:658
msgid "GET /servers/{server_id}/os-volume_attachments (list)"
msgstr "GET /서버/{서버_아이디}/OS-볼륨 접속 (리스트)"

#: ../../<reno.sphinxext stable/stein>:659
msgid "GET /servers/{server_id}/os-volume_attachments/{volume_id} (show)"
msgstr "GET /servers/{server_id}/os-volume_attachments/{volume_id} (แสดง)"

#: ../../<reno.sphinxext stable/pike>:874
msgid "GET /traits/{name}: To check if a trait name exists."
msgstr "GET /traits/{name}: trait 이름이 존재하는지 확인하기"

#: ../../<reno.sphinxext stable/pike>:872
msgid "GET /traits: Returns all resource classes."
msgstr "GET /traits: 집합을 모든 리소스 클래스를 반환합니다."

#: ../../<reno.sphinxext stable/2024.1>:492
msgid "GET RDP console connection information:"
msgstr "GET RDP 콘솔 접속 정보:"

#: ../../<reno.sphinxext stable/2024.1>:484
msgid "GET RDP console:"
msgstr "GET RDP 콘솔"

#: ../../<reno.sphinxext stable/pike>:1174
msgid ""
"Given that there are other better documented and better tested ways to "
"approach this, such as through use of neutron's native port filtering or "
"security groups, this functionality has been removed.  Users should instead "
"rely on one of these alternatives."
msgstr ""
"집합이 다른 더 잘 설명된 및 테스트된 방법이 있기 때문에, 네트워크의 원래 포트 필터링 또는 보안 그룹을 사용하여 이 기능을 사용할 수"
" 있습니다.  그러나, 이 기능은 제거되었습니다.  사용자는 대신 이러한 대안 중 하나를 사용해야 합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:342
msgid ""
"Glance multistore configuration with multiple RBD backends is now supported "
"within Nova for libvirt RBD-backed images using "
"``[libvirt]/images_rbd_glance_store_name`` configuration option."
msgstr ""
"Glance multistore 구성과 여러 RBD 백엔드가 있는 RBD를 사용하여 Nova에서 libvirt RBD-backed "
"이미지를 사용할 때는現在 ``[libvirt]/images_rbd_glance_store_name`` 구성 옵션을 사용하여 지원됩니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:406
msgid ""
"Guru Meditation Reports can now be generated for the Nova API service when "
"running under uWSGI. Note that uWSGI intercepts SIGUSR2 signals, so a file "
"trigger should be used instead."
msgstr ""
"Guru Meditation Reports를 Nova API 서비스에서 uWSGI를 사용할 때 생성할 수 있습니다. uWSGI는 "
"SIGUSR2 신호를 중단하기 때문에, 파일 트리거를 사용해야 합니다."

#: ../../<reno.sphinxext stable/pike>:976
msgid ""
"HVM guests may not have the paravirtualization (PV) drivers installed, in "
"which case the disk will be accessible on the ``ide`` bus. When the PV "
"drivers are installed the disk will be accessible on the ``xen`` bus."
msgstr ""
"HVM 게스트는 파라비irtualization (PV) 드라이버가 설치되지 않은 경우, 디스크는 ``ide`` 버스에서 접근할 수 "
"있습니다. PV 드라이버가 설치된 경우 디스크는 ``xen`` 버스에서 접근할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:407
msgid ""
"Handling accelerator requests for an instance is now supported (where "
"supported by the underlying virt driver) as of microversion 2.82. The Cyborg"
" service generates an event for the binding completion for each accelerator "
"request (ARQ) for an instance. Adds a new event "
"``accelerator_request_bound`` for this to the API ``POST /os-server-"
"external-events``"
msgstr ""
"인스턴스에 대한 가속기 요청을 처리하는 것은 이제 microversion 2.82부터 지원되며 (가상 드라이버가 지원하는 경우) 가속기 "
"요청 (ARQ) each instance에 대한 결합 완료에 대한 이벤트를 생성하는 Cyborg 서비스가 있습니다.  이에 따라 new "
"event ``accelerator_request_bound``을 API ``POST /os-server-external-"
"events``에 추가합니다."

#: ../../<reno.sphinxext stable/2024.1>:452
msgid ""
"Handling packed virtqueue requests for an instance is now supported on the "
"nodes with Qemu v4.2 and Libvirt v6.3."
msgstr ""
"Qemu v4.2 및 Libvirt v6.3를 사용하는 노드에서 인스턴스에 대한 패ック्ड virtqueue 요청을 처리하는 것은 현재 "
"지원됩니다."

#: ../../<reno.sphinxext stable/rocky>:1790
msgid "Have configured a much longer token TTL"
msgstr "집합을 구성하여 더 긴 토큰 TTL을 설정했습니다."

#: ../../<reno.sphinxext stable/rocky>:1789
msgid "Have not yet deployed console proxies per cell"
msgstr "집합당 Console proxies를 아직 배포하지 않습니다."

#: ../../<reno.sphinxext unmaintained/xena>:392
msgid ""
"Historically, there has been no way to delete ``task_log`` table records "
"other than manual database modification. Because of this, ``task_log`` "
"records could pile up over time and operators are forced to perform manual "
"steps to periodically truncate the ``task_log`` table."
msgstr ""
"역사적으로, 'task_log' 테이블 레코드를 제외하고 'task_log' 레코드를 삭제할 수 있는 방법이 없었습니다. 이로 인해 "
"'task_log' 레코드는 시간이 지남에 따라 쌓이게 되고, 운영자는 정기적으로 'task_log' 테이블을 쪼개기 위해 수동한 단계를"
" 수행해야했습니다."

#: ../../<reno.sphinxext stable/stein>:603
msgid ""
"However any traits which are removed by the admin from the compute node "
"resource provider via the Placement API will not be reinstated until the "
"compute service's provider cache is reset. This can be triggered via a "
"``SIGHUP``."
msgstr ""
"다음은 compute 노드 리소스 제공자에서 admin이 제거한 특성은 compute 서비스 제공자 캐시가 재initialization "
"될 때까지 다시 reinstated되지 않는다. 이는 compute service provider cache가 "
"재initialization 될 때까지 SIGHUP를 사용하여 trigger 할 수 있다."

#: ../../<reno.sphinxext stable/rocky>:413
msgid ""
"However, existing ironic instances require a data migration which can be "
"achieved either by restarting ``nova-compute`` services managing ironic "
"nodes or running ``nova-manage db ironic_flavor_migration``. The completion "
"of the data migration can be checked by running the ``nova-status upgrade "
"check`` command and checking the \"Ironic Flavor Migration\" result."
msgstr ""
"그러나 현재 Ironic 인스턴스는 데이터 миг레이션을 필요로 하며, 이 데이터 миг레이션은 Nova-Compute 서비스가 "
"Ironic 노드를 관리하는 서비스를 재시작하거나 Nova-manage db ironic_flavor_migration 명령을 실행하여 "
"달성할 수 있다. 데이터 миг레이션의 완료를 확인하기 위해 Nova-status upgrade check 명령을 실행하고 "
"\"Ironic Flavor Migration\" 결과를 확인할 수 있다."

#: ../../<reno.sphinxext stable/rocky>:694
msgid ""
"However, if desired, ``[placement]/policy_file`` makes it possible to "
"package and deploy the placement policy file separately to make the future "
"split of placement and nova packages easier, e.g.:"
msgstr ""
"그러나 원하는 경우, `[placement]/policy_file`는 배치 정책 파일을 분리하여 배치 및 nova 패키지의 미래 분할을 "
"आसान하게 할 수 있도록 패키징하고 배포할 수 있습니다. 예를 들어:"

#: ../../<reno.sphinxext stable/queens>:1163
msgid ""
"If ``[glance]/api_servers`` is not set in nova.conf, and there is a "
"versioned endpoint URL in the service catalog, nova makes a best attempt at "
"parsing and stripping the version from the URL in order to make API requests"
" to the image service."
msgstr ""
"``[glance]/api_servers``이 nova.conf에 설정되지 않으며, 서비스 카탈로그에 버전화된 엔드포인트 URL이 "
"있다면, nova는 URL에서 버전을 파싱하고 제거하는 것을 최선의 시도하여 이미지 서비스에 API 요청을 하려는 시도이다."

#: ../../<reno.sphinxext stable/rocky>:1085
msgid ""
"If ``placement_database.connection`` has a value then the "
"``placement_database`` configuration group will be used to configure a "
"separate placement database, including using ``connection`` to identify the "
"target database. That database will have a schema that is a replica of all "
"the tables used in the API database. The new database schema will be created"
" and synchronized when the ``nova-manage api_db sync`` command is run."
msgstr ""
"``placement_database.connection`` 가 가치가 있다면 ``placement_database`` 구성 그룹이 "
"separate placement database를 구성하는 데 사용됩니다. 이 separate placement database는 "
"``connection``을 사용하여 목표 데이터베이스를 식별합니다. 이 데이터베이스는 API 데이터베이스에서 사용되는 모든 테이블의 "
"복사본이 되는 schema가 있습니다. 새로운 데이터베이스 schema가 생성되고 동기화되면 ``nova-manage api_db "
"sync`` 명령이 실행됩니다."

#: ../../<reno.sphinxext stable/train>:989
msgid ""
"If any of these requirements are not met, live migration of instances with a"
" NUMA topology with the libvirt driver will revert to the legacy naive "
"behavior, in which the instance was simply moved over without updating its "
"NUMA guest to host mappings or its resource usage."
msgstr ""
"이러한 요구 사항 중ใด라도 fulfillment되지 않으면, NUMA拓도와 libvirt 드라이버를 사용한 인스턴스 live "
"migration은 legacy naive 행동으로 돌아가게 됩니다. 이 행동은 인스턴스를 단순히 NUMA 게스트를 호스트로 "
"mapping을 업데이트하지 않고, 또는 리소스 사용을 업데이트하지 않고 이동합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1366
msgid ""
"If by Newton (14.0.0), you don't use any of the CoreFilter, RamFilter or "
"DiskFilter, then please modify all your compute node's configuration by "
"amending either ``cpu_allocation_ratio`` (if you don't use CoreFilter) or "
"``ram_allocation_ratio`` (if you don't use RamFilter) or "
"``disk_allocation_ratio`` (if you don't use DiskFilter) by putting a 9999.0 "
"value for the ratio before upgrading the nova-scheduler to Ocata."
msgstr ""
"Newton (14.0.0)에서 CoreFilter, RamFilter 또는 DiskFilter를 사용하지 않으면서도 nova-scheduler를 Ocata로 업그레이드하려면, 다음을 확인하고 수정하세요.\n"
"\n"
"*   CoreFilter를 사용하지 않는 경우 `cpu_allocation_ratio`를 9999.0로 설정합니다.\n"
"*   RamFilter를 사용하지 않는 경우 `ram_allocation_ratio`를 9999.0로 설정합니다.\n"
"*   DiskFilter를 사용하지 않는 경우 `disk_allocation_ratio`를 9999.0로 설정합니다."

#: ../../<reno.sphinxext stable/train>:61 stable/ussuri>:25
#: unmaintained/victoria>:78 unmaintained/wallaby>:144 unmaintained/xena>:108
#: unmaintained/yoga>:185 unmaintained/zed>:591
msgid ""
"If compute service is down in source node and user try to stop instance, "
"instance gets stuck at powering-off, hence evacuation fails with msg: Cannot"
" 'evacuate' instance <instance-id> while it is in task_state powering-off. "
"It is now possible for evacuation to ignore the vm task state. For more "
"details see: `bug 1978983`_"
msgstr ""
"compute 서비스가 소스 노드에서 down이면, 사용자가 인스턴스를 중단 시도하면, 인스턴스는 전원 해제에 stuck되어, 재난 "
"대피가 실패하고, msg: Cannot 'evacuate' instance <instance-id> while it is in "
"task_state powering-off. 현재, 재난 대피는 vm task state를 무시할 수 있다. 더 많은 정보는: `bug "
"1978983`_를 확인할 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:823
msgid ""
"If other filtering query parameters are present, the results are a boolean "
"AND of all the filters."
msgstr ""
"if other filtering query parameters are present, the results are a boolean "
"AND of all the filters."

#: ../../<reno.sphinxext unmaintained/wallaby>:862
msgid "If running on s390x, you will need libguestfs >= 1.37.14."
msgstr "s390x에서 실행하는 경우, libguestfs >= 1.37.14가 필요합니다."

#: ../../<reno.sphinxext stable/stein>:257
msgid ""
"If running with ``threads=1`` is not an option in a particular environment, "
"there are two other workarounds:"
msgstr "``threads=1``를 특정 환경에서 사용할 수 있는 옵션이 아니면, 두 가지 다른 workaround이 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:210 stable/pike>:358
#: stable/queens>:1753
msgid ""
"If scheduling fails during rebuild the server instance will go to ERROR "
"state and a fault will be recorded. `Bug 1744325`_"
msgstr "`스케줄링이 재건 시에 실패하면 서버 인스턴스는 ERROR 상태로 이동하고 오류가 기록됩니다. Bug 1744325 `_"

#: ../../<reno.sphinxext stable/train>:712
msgid ""
"If the ``[cinder]/cross_az_attach`` configuration option is False then the "
"specified availability zone has to be the same as the availability zone of "
"any volumes attached to the shelved offloaded server, otherwise a 409 "
"HTTPConflict error response is returned."
msgstr ""
"``[cinder]/cross_az_attach`` 설정 옵션이 False 인 경우, 정의된 가용 구역은 any volumes이 연결된 "
"가용 구역과 동일해야 하며, 그렇지 않으면 409 HTTPConflict 오류 응답이 반환됩니다."

#: ../../<reno.sphinxext stable/rocky>:961
msgid ""
"If the ``[placement]`` section is missing from the nova-api's nova.conf "
"file, nothing will break however there will be some warnings generated in "
"the nova-api's log file when administrators associate a compute host with a "
"host aggregate. However, this will become a failure starting in the 19.0.0 "
"Stein release."
msgstr ""
"``[placement]`` 구간이 nova-api의 nova.conf 파일에 포함되지 않으면, nothing이\tbreaked는 될 것"
" 같지만, nova-api의 로그 파일에 관리자가 컴퓨터 호스트를 호스트 집합과 연결할 때는 경고가 발생합니다. 그러나 19.0.0 "
"스티恩 릴리스부터는 이가 실패할 것입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:690
msgid ""
"If the host supports this feature and it has at least an assignable PCI "
"device, the host must be configured to allow those PCI devices to be "
"assigned to VMs. For information on how to do this, follow this guide [1]."
msgstr ""
"호스트가 이 기능을 지원하고, 적어도 할당 가능한 PCI 장치가至少 하나 있다면, 호스트는 PCI 장치를 VM에 할당할 수 있는지 "
"확인하기 위해 PCI 장치를 할당할 수 있는지 확인해야 한다. 이 정보를 얻으려면 [1] 가이드를 참조하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:466
msgid ""
"If the project should not have any outstanding resource usage, then one "
"possible workaround is to delete the existing quota usage for the project::"
msgstr ""
"If the project should not have any outstanding resource usage, then one "
"possible workaround is to delete the existing quota usage for the project:: "
"집합이 프로젝트에 남아 있는 자원 사용량이 없도록 하기 위해서는 프로젝트의 존재하는 자원 사용량을 삭제하는 방법이 하나의 가능성이다."

#: ../../<reno.sphinxext stable/rocky>:1766
msgid ""
"If the scheduler service is started before the cell mappings are created or "
"setup, nova-scheduler needs to be restarted or SIGHUP-ed for the newly added"
" cells to get registered in the scheduler cache."
msgstr ""
"if the scheduler service is started before the cell mappings are created or "
"setup, nova-scheduler needs to be restarted or SIGHUP-ed for the newly added"
" cells to get registered in the scheduler cache."

#: ../../<reno.sphinxext stable/rocky>:1549
msgid "If this is an issue for you, you have three options:"
msgstr "이것이 आपक에게 문제인 경우, आपकには 세 가지 선택이 있습니다."

#: ../../<reno.sphinxext stable/queens>:1298
msgid ""
"If using the ``api_servers`` option in the ``[glance]`` configuration "
"section, the values therein must be URLs.  Raw IP addresses will result in "
"hard failure of API requests."
msgstr ""
"``api_servers`` 옵션을 ``[glance]`` 구성 섹션에 사용할 경우, 그 안에 있는 값은 URL로 must be.  "
"raw IP addresses는 API 요청에 대한 硬 실패를 일으킬 것입니다."

#: ../../<reno.sphinxext stable/2025.2>:312
msgid ""
"If you are allowing these APIs to be accessed by admin or non-admin users "
"then it is highly recommended to remove that permission and make sure those "
"APIs are not accessible by any non-service users."
msgstr ""
"집합이 admin 또는 non-admin 사용자에 의해 접근할 수 있도록 허용한다면, 이 권한을 제거하고, 이 API가任何 서비스 "
"사용자가 접근할 수 없도록 확인해야 한다."

#: ../../<reno.sphinxext stable/ussuri>:774 unmaintained/victoria>:528
msgid ""
"If you are overwriting the policy rules (all or some of them) in the policy "
"file with new default values or any new value that requires scoped tokens, "
"then non-scoped tokens will not work. Also if you generate the policy file "
"with 'oslopolicy-sample-generator' json format or any other tool, you will "
"get rules defaulted in the new format, which examines the token scope. "
"Unless you turn on ``oslo_policy.enforce_scope``, scope-checking rules will "
"fail. Thus, be sure to enable ``oslo_policy.enforce_scope`` and `educate "
"<https://docs.openstack.org/nova/latest/configuration/policy-"
"concepts.html>`_ end users on how to request scoped tokens from Keystone, or"
" use a pre-existing sample config file from the Train release until you are "
"ready to migrate to scoped policies. Another way is to generate the policy "
"file in yaml format as described `here "
"<https://docs.openstack.org/oslo.policy/latest/cli/index.html#oslopolicy-"
"policy-generator>`_ and update the policy.yaml location in "
"``oslo_policy.policy_file``."
msgstr ""
"```\n"
"if you are overwriting the policy rules (all or some of them) in the policy file with new default values or any new value that requires scoped tokens, then non-scoped tokens will not work. Also if you generate the policy file with 'oslopolicy-sample-generator' json format or any other tool, you will get rules defaulted in the new format, which examines the token scope. Unless you turn on ``oslo_policy.enforce_scope``, scope-checking rules will fail. Thus, be sure to enable ``oslo_policy.enforce_scope`` and educate <https://docs.openstack.org/nova/latest/configuration/policy-concepts.html> end users on how to request scoped tokens from Keystone, or use a pre-existing sample config file from the Train release until you are ready to migrate to scoped policies. Another way is to generate the policy file in yaml format as described here <https://docs.openstack.org/oslo.policy/latest/cli/index.html#oslopolicy-policy-generator> and update the policy.yaml location in ``oslo_policy.policy_file``.\n"
"```"

#: ../../<reno.sphinxext origin/stable/ocata>:166 stable/pike>:316
#: stable/queens>:1222
msgid ""
"If you have been relying on per-aggregate overcommit, during your upgrade, "
"you must change to using per-compute-node overcommit ratios in order for "
"your scheduling behavior to stay consistent. Otherwise, you may notice "
"increased NoValidHost scheduling failures as the aggregate-based overcommit "
"is no longer being considered."
msgstr ""
"집합에 대한 per-aggregate 오버คอมMIT을 사용하고 있습니다. 업그레이드 시, your scheduling behavior가"
" 일관되게 유지되도록 per-compute-node overcommit ratios를 사용해야 합니다. otherwise, "
"aggregate-based overcommit이 더 이상 고려되지 않아 NoValidHost scheduling failures가 "
"증가할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:1129
msgid ""
"If you rely on the ``caching_scheduler`` driver or your own out-of-tree "
"driver which sets ``USES_ALLOCATION_CANDIDATES = False`` to bypass the "
"placement service, please communicate with the nova development team in the "
"openstack-dev mailing list and/or #openstack-nova freenode IRC channel to "
"determine what prevents you from using the ``filter_scheduler`` driver."
msgstr ""
"``caching_scheduler`` 드라이버 또는 사용자-defined out-of-tree 드라이버가 "
"``USES_ALLOCATION_CANDIDATES = False``를 설정하여 배치 서비스를 피하는 경우, nova 개발 팀과 "
"openstack-dev mailing list 또는 #openstack-nova freenode IRC channel을 통해 "
"``filter_scheduler`` 드라이버를 사용할 수 없는 이유를 확인하십시오."

#: ../../<reno.sphinxext stable/queens>:387
msgid ""
"If you set this option the same on all of your compute hosts, which you "
"should do if you use the same networking backend universally, you do not "
"have to worry about this."
msgstr ""
"이 옵션을 모든 컴퓨터 호스트에서 동일하게 설정하면, 동일한 네트워킹 백엔드를 universally 사용할 때는, 이에 대한 우려는 "
"필요가 없습니다."

#: ../../<reno.sphinxext stable/train>:1000
msgid ""
"If you upgraded your OpenStack deployment to Stein without switching to use "
"the now independent placement service, you must do so before upgrading to "
"Train. `Instructions "
"<https://docs.openstack.org/placement/latest/upgrade/to-stein.html>`_ for "
"one way to do this are available."
msgstr ""
"OpenStack 배포를 Stein으로 업그레이드하고 현재 독립된 배치 서비스를 사용하지 않으면, Stein으로 업그레이드하기 전에 "
"Train으로 업그레이드할 수 없습니다. ` Stein으로 업그레이드하는 방법 "
"<https://docs.openstack.org/placement/latest/upgrade/to-stein.html>`_에 대한 "
"지침이 있습니다."

#: ../../<reno.sphinxext stable/queens>:695
msgid ""
"If you use NVIDIA GRID cards, please know that there is a limitation with "
"the NVIDIA driver that prevents one guest to have more than one virtual GPU "
"from the same physical card. One guest can have two or more virtual GPUs but"
" then it requires each vGPU to be hosted by a separate physical card. Until "
"that limitation is removed, please avoid creating flavors asking for more "
"than one vGPU."
msgstr ""
"NVIDIA GRID 카드를 사용하는 경우, NVIDIA 드라이버의 한계가 있습니다. 한 게스트가 같은 물리적 카드에서 두 개 이상의 "
"가상 GPU를 사용할 수 없다는 것을 알기 바랍니다. 한 게스트는 두 개 이상의 가상 GPU를 사용할 수 있지만, 각 vGPU는 "
"separate 물리적 카드를 통해 호스트해야 합니다. 한계가 제거될 때까지, 한 게스트가 두 개 이상의 vGPU를 yêu "
"cầu하는.flavor를 만들 때는 주의하십시오."

#: ../../<reno.sphinxext unmaintained/2023.1>:436
msgid ""
"If you want to disable them then modify the below config options value in "
"``nova.conf`` file::"
msgstr ""
"If you want to disable them then modify the below config options value in "
"`nova.conf` file:: 집합을 비활성화하고 싶다면 `nova.conf` 파일의 아래 config 옵션의 값에 수정하세요."

#: ../../<reno.sphinxext stable/pike>:1253
msgid ""
"If you were using ``nova hypervisor-list`` after starting new nova-compute "
"services to tell when to run ``nova-manage cell_v2 discover_hosts``, you "
"should change your tooling to instead use one of the following commands::"
msgstr ""
"``nova hypervisor-list``를 nova-compute 서비스가 시작된 후에 nova-manage cell_v2 "
"discover_hosts를 실행할 때 사용하는 경우, nova-manage cell_v2 discover_hosts를 실행할 때 "
"사용하는 도구를 변경해야 합니다. 대신에 다음 중 하나를 사용해야 합니다."

#: ../../<reno.sphinxext stable/ussuri>:547
msgid ""
"Image pre-caching on hosts by aggregate is now supported (where supported by"
" the underlying virt driver) as of microversion 2.81. A group of hosts "
"within an aggregate can be compelled to fetch and cache a list of images to "
"reduce time-to-boot latency. Adds the new API:"
msgstr ""
"이미지 전처리(Pre-caching)가 호스트에 집합에 의해 적용되면 현재 2.81 버전의 마이크로 버전부터 지원되었습니다. (하위 "
"virt 드라이버에 의해 지원되는 경우) 집합 내의 호스트 그룹이 이미지 목록을 가져오고 캐시하는 것을 강제할 수 있습니다. 새로운 "
"API를 추가합니다."

#: ../../<reno.sphinxext stable/2025.1>:204
msgid ""
"Image properties and scheduler hints are now returned as part of the "
"instance show API response."
msgstr "이미지 속성과 스케줄러 힌트가 현재 인스턴스แสดง API 응답의 일부로 반환됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:717
msgid ""
"Images should be prepared for Generation 2 VMs. The image property "
"\"hw_machine_type=hyperv-gen2\" is mandatory."
msgstr ""
"집합은 제 2 세대 VM에 준비되어야 합니다. hình상 속성 \"hw_machine_type=hyperv-gen2\"은 의무적입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1420
msgid ""
"Implemented microversion v2.39 that deprecates `image-metadata` proxy API, "
"removes image metadata quota checks for 'createImage' and 'createBackup' "
"actions. After this version Glance configuration option "
"`image_property_quota` should be used to control the quota of image "
"metadatas. Also, removes the `maxImageMeta` field from `os-limits` API "
"response."
msgstr ""
"`image-metadata` 프록시 API를 비고용으로 making `image-metadata` metadata quota "
"checks를 제거하고, 'createImage' 및 'createBackup' 액션에 대한 image metadata quota 확인을"
" 제거합니다. 이 버전 이후 Glance 구성 옵션 `image_property_quota`를 사용하여 image "
"metadata의_quota를 제어해야합니다. 또한, `os-limits` API 응답에서 `maxImageMeta` 필드를 제거합니다."

#: ../../<reno.sphinxext stable/train>:494
msgid ""
"Improved `operational tooling "
"<https://docs.openstack.org/nova/latest/cli/nova-manage.html>`_ for things "
"like archiving the database and healing instance resource allocations in "
"Placement."
msgstr ""
"개선된 `operational tooling <https://docs.openstack.org/nova/latest/cli/nova-"
"manage.html>`_ such as archiving the database and healing instance resource "
"allocations in Placement."

#: ../../<reno.sphinxext unmaintained/zed>:203
msgid ""
"Improved behavior for Windows guest by adding by default following `Hyper-V "
"enlightments`__ on all libvirt guests : `vpindex`, `runtime`, `synic`, "
"`reset`, `frequencies`, `reenlightenment`, `tlbflush`, `ipi` and `evmc`."
msgstr ""
"Windows 게스트의 개선된 행동을 위해 기본적으로 모든 libvirt 게스트에 `Hyper-V enlightments`__를 "
"추가했습니다. 다음 속성들이 추가되었습니다. : `vpindex`, `runtime`, `synic`, `reset`, "
"`frequencies`, `reenlightenment`, `tlbflush`, `ipi` 및 `evmc`."

#: ../../<reno.sphinxext stable/train>:498
msgid ""
"Improved coordination with the baremetal service during external node `power"
" cycles <https://docs.openstack.org/ironic/latest/admin/power-sync.html>`_."
msgstr ""
"baremetal 서비스와 외부 노드 `power cycles "
"<https://docs.openstack.org/ironic/latest/admin/power-sync.html>`_ 간의 개선된 협력"

#: ../../<reno.sphinxext stable/stein>:31 stable/train>:39 stable/ussuri>:101
#: unmaintained/victoria>:195 unmaintained/wallaby>:339 unmaintained/xena>:656
msgid ""
"Improved detection of anti-affinity policy violation when performing live "
"and cold migrations. Most of the violations caused by race conditions due to"
" performing concurrent live or cold migrations should now be addressed by "
"extra checks in the compute service. Upon detection, cold migration "
"operations are automatically rescheduled, while live migrations have two "
"checks and will be rescheduled if detected by the first one, otherwise the "
"live migration will fail cleanly and revert the instance state back to its "
"previous value."
msgstr ""
"anti-affinity policy 위반을 live 및冷 마이그레이션 시 감지하는 improvements. live 및 cold "
"마이그레이션 시 발생하는 대부분의 위반은 race conditions 의缘에 동시 live 또는 cold 마이그레이션을 수행할 때 "
"발생하는 것으로 추정되며, now compute service 에서 추가적인 확인을 통해 이러한 위반을 address 할 수 있습니다. "
"감지되면, cold 마이그레이션 운영은 tự động 재정렬되며, live 마이그레이션은 두 개의 확인을 수행하고, 첫 번째 확인에 의해"
" 감지되면 다시 재정렬되며, 그렇지 않으면 live 마이그레이션은 깨끗하게 실패하고, 인스턴스 상태를 이전의 값으로 복원합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:629
msgid ""
"Improved iSCSI MPIO support, by connecting to multiple iSCSI targets/portals"
" when available and allowing using a predefined list of initiator HBAs."
msgstr ""
"이미prove iSCSI MPIO 지원을 향상시키는 것은, 여러 iSCSI 목표/포트를 연결하고 readily available 할 때,"
" 여러 iSCSI 목표/포트를 연결하고, 미리 정의된 initiator HBA 목록을 사용할 수 있도록 허용하는 것입니다."

#: ../../<reno.sphinxext stable/train>:484
msgid ""
"Improved multi-cell resilience with the ability to `count quota usage "
"<https://docs.openstack.org/nova/latest/user/quotas.html#quota-usage-from-"
"placement>`_ using the Placement service and API database."
msgstr ""
"개선된 다중เซル 리소스 지속성에 Placement 서비스와 API 데이터베이스를 사용하여 `quotas 사용량 카운트 "
"<https://docs.openstack.org/nova/latest/user/quotas.html#quota-usage-from-"
"placement>`_을 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:481
msgid ""
"Improvements to the scheduler for more intelligently filtering `results from"
" the Placement service "
"<https://docs.openstack.org/nova/latest/admin/configuration/schedulers.html#prefiltering>`_."
msgstr "Placement 서비스에서 결과를 더 지적하게 필터링하는 스케줄러의 개선"

#: ../../<reno.sphinxext stable/pike>:376 stable/queens>:1187
msgid ""
"In 16.0.0 Pike release, quota limits are checked in a new fashion after "
"change 5c90b25e49d47deb7dc6695333d9d5e46efe8665 and a new config option "
"``[quota]/recheck_quota`` has been added in change "
"eab1d4b5cc6dd424c5c7dfd9989383a8e716cae5 to recheck quota after resource "
"creation to prevent allowing quota to be exceeded as a result of racing "
"requests. These changes could lead to requests blocked by over quota "
"resulting in instances in the ``ERROR`` state, rather than no instance "
"records as before. Refer to https://bugs.launchpad.net/nova/+bug/1716706 for"
" detailed bug report."
msgstr ""
"16.0.0 피크 릴리즈에서, 5c90b25e49d47deb7dc6695333d9d5e46efe8665 변경과 함께 새로운 구성 옵션 "
"``[quota]/recheck_quota``가 eab1d4b5cc6dd424c5c7dfd9989383a8e716cae5 변경에 "
"추가되어, 리소스 생성 후 쿼타를 재 확인하여, 쿼타가 초과되는 것을 방지하기 위해 경쟁 요청의 결과로 쿼타가 초과되는 경우를 방지하기 "
"위해. 이러한 변경은, 이전에 쿼타가 초과되는 경우 인스턴스 ``ERROR`` 상태에 있는 경우가 아니라, 인스턴스 레코드가 없는 경우를"
" 방지하기 위해. 더 자세한 bug 리포트를 https://bugs.launchpad.net/nova/+bug/1716706 에서 "
"참조하십시오."

#: ../../<reno.sphinxext stable/pike>:1222
msgid ""
"In Ocata, the nova-scheduler would fall back to not calling the placement "
"service during instance boot if old computes were running. That "
"compatibility mode is no longer present in Pike, and as such, the scheduler "
"fully depends on the placement service. This effectively means that in Pike "
"Nova requires Placement API version 1.4 (Ocata)."
msgstr ""
"Ocata에서 nova-scheduler는 인스턴스 부트 시에 old computes가-running 경우에만 placement "
"service를 호출하지 않도록 fall back한다. 이 compatibility mode는 Pike에서 더 이상 존재하지 않으며, "
"따라서 scheduler는 placement service에完全 의존한다. 이로써 Pike Nova는 Placement API "
"version 1.4 (Ocata)를 필요로 한다."

#: ../../<reno.sphinxext stable/queens>:1346
msgid ""
"In Queens, specifying ``url`` will trigger the legacy behavior.  The ``url``"
" option will be removed in Rocky."
msgstr "퀸즈에서 `url`를 지정하면 전통적인 행동을 일으킬 것이다. `url` 옵션은 로키에서 제거될 것이다."

#: ../../<reno.sphinxext stable/stein>:277
msgid ""
"In Stein the Placement service is available either as part of Nova, or "
"independently packaged from its own project. This is to allow easier "
"migration from one to another. See the `upgrade notes`_ for more "
"information."
msgstr ""
"스틴에서 배치 서비스는 노바와 함께 제공되거나, 노바 프로젝트의 독립적인 패키징으로 제공됩니다. 이로써, 하나에서 다른 ones으로의 "
"이주가 더 쉽게 가능합니다. 더 많은 정보는 `upgrade notes`_를 참조하십시오."

#: ../../<reno.sphinxext stable/train>:637
msgid ""
"In addition, ``locked`` will be supported as a valid filter/sort parameter "
"for ``GET /servers/detail`` and ``GET /servers`` so that users can filter "
"servers based on their locked value. Also the instance action versioned "
"notifications for the lock/unlock actions now contain the ``locked_reason`` "
"field."
msgstr ""
"``locked``는 ``GET /servers/detail`` 및 ``GET /servers``에서 유효한 필터/정렬 매개 변수로 "
"지원되며, 사용자가 locked 값을 기반으로 서버를 필터링할 수 있도록합니다. 또한,.lock/unlock 액션에 대한 인스턴스 액션 "
"버전화된 알림에는 ``locked_reason`` 필드가 포함됩니다."

#: ../../<reno.sphinxext stable/2025.2>:104
msgid ""
"In addition, a number of new, more granular policies are introduced to allow"
" us to use the ``project_manager`` persona in migration APIs:"
msgstr ""
"추가로, 새로운, 더 세부적인 정책이 도입되어 migration APIs에서 ``project_manager`` persona를 사용할 "
"수 있도록 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1046
msgid ""
"In addition, all filter scheduler configuration options have been added to "
"the ``filter_scheduler`` group."
msgstr "다음으로, 모든 필터 스케줄러 구성 옵션은 ``filter_scheduler`` 그룹에 추가되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:431
msgid ""
"In addition, starting with the 2.90 microversion, the ``OS-EXT-SRV-"
"ATTR:hostname`` field is now returned for all users. Previously this was "
"restricted to admin users."
msgstr ""
"OS-EXT-SRV-ATTR:hostname 필드는 2.90 마이크로 버전부터 모든 사용자에게 반환되었습니다. 이전에는 admin "
"사용자에게만 제한되었습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:823
msgid ""
"In addition, the ``GET /os-hypervisors/statistics`` API, which provided a "
"summary view with just the fields listed above, has been removed entirely "
"and will now raise a HTTP 404 with microversion 2.88 or greater."
msgstr ""
"``GET /os-hypervisors/statistics`` API는 위에 나열된 필드만을 포함하여 요약视图를 제공하는 API입니다. "
"그러나 이 API는 완전히 제거되었으며, 2.88 이상의 마이크로 버전을 사용할 때 HTTP 404 오류를 발생시킵니다."

#: ../../<reno.sphinxext stable/train>:1118
msgid ""
"In addition, the following *cells v1* related RPC configuration options, "
"previously found in ``upgrade_levels``, have been removed."
msgstr ""
"*cells v1*와 관련된 RPC 구성 옵션, 이전에 ``upgrade_levels``에 있는 것으로 발견된 것들은 제거되었다."

#: ../../<reno.sphinxext stable/ussuri>:838
msgid ""
"In addition, the following APIs have been removed. Calling these APIs will "
"now result in a ``410 HTTPGone`` error response:"
msgstr "다음 API는 제거되었습니다. 이 API를 호출하면 현재 410 HTTPGone 오류를 반환합니다."

#: ../../<reno.sphinxext stable/rocky>:1489
msgid "In addition, the following configuration options have been removed."
msgstr "다음과 같은 구성 옵션은 제거되었습니다."

#: ../../<reno.sphinxext stable/train>:664
msgid ""
"In all cases, SEV instances can only be booted from images which have the "
"``hw_firmware_type`` property set to ``uefi``, and only when the machine "
"type is set to ``q35``.  The latter can be set per image by setting the "
"image property ``hw_machine_type=q35``, or per compute node by the operator "
"via the ``hw_machine_type`` configuration option in the ``[libvirt]`` "
"section of :file:`nova.conf`."
msgstr ""
"SEV 인스턴스는 모든 경우에 대해 ``hw_firmware_type`` 속성이 ``uefi``로 설정된 이미지에서만 부트할 수 있으며,"
" 기계 유형이 ``q35``로 설정된 경우에만 부트할 수 있습니다.  기계 유형이 ``q35``로 설정된 경우는 이미지 속성 "
"``hw_machine_type=q35``로 설정하거나, 컴퓨터 노드에 의해 ``hw_machine_type`` 구성 옵션을 통해 "
"``[libvirt]`` 섹션의 :file:`nova.conf`에서 설정할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1008
msgid ""
"In all cases, strict NUMA affinity is provided if possible. The key "
"difference between the policies is how much NUMA affinity one is willing to "
"forsake before failing to schedule."
msgstr ""
"집합에 대한 엄격한 NUMA 친화성이 모든 경우에 possible한 경우에 제공됩니다. 정책의 주요 차이점은 scheduling에 실패할"
" 때 NUMA 친화성을 how much forsake 한다는 것입니다."

#: ../../<reno.sphinxext stable/2025.2>:366
msgid ""
"In all cases, the replacement is the same option in the ``[api]`` group."
msgstr "모든 경우에 대해 대체는 `[api]` 그룹의 동일한 옵션입니다."

#: ../../<reno.sphinxext stable/stein>:467
msgid ""
"In both cases this means when looking at resource providers, depending on "
"the scenario, you can see more than one provider where there was initially "
"just a root compute node provider per compute service."
msgstr ""
"집합 또는 가용 구역에서 리소스 제공자에 대해, 상황에 따라, 단순히 컴퓨터 서비스에 대한 루트 컴퓨터 노드 제공자만이 existed했을"
" 때는 여러 제공자가 보이게 됩니다."

#: ../../<reno.sphinxext stable/stein>:1428
msgid ""
"In case of infrastructure failures like non-responsive cells, prior to "
"`change e3534d`_ we raised an API 500 error. However currently when listing "
"instances or migrations, we skip that cell and display results from the up "
"cells with the aim of increasing availability at the expense of accuracy. If"
" the old behaviour is desired, a new flag called "
"``CONF.api.list_records_by_skipping_down_cells`` has been added which can be"
" set to False to mimic the old behavior. Both of these potential behaviors "
"will be unified in an upcoming microversion done through the `blueprint "
"handling-down-cell`_  where minimal constructs would be returned for the "
"down cell instances instead of raising 500s or skipping down cells."
msgstr ""
"`change e3534d`_의 이전에는 비응성 세ลล에 대한 인프라 실패와 같은 경우에 API 500 오류가 발생했다. 그러나 현재 "
"인스턴스 또는 이ми그레이션을 liệtting 할 때, 비응성 세ลล을 skips하고, 가용성 증가를 위해 오류가 발생하지 않도록 up "
"세ลล의 결과만 표시한다. 오래된 행동을 원한다면, "
"`CONF.api.list_records_by_skipping_down_cells`라는 새로운 플래그가 추가되었으며, 이 플래그를 "
"False로 설정하면 오래된 행동을 시뮬레이션할 수 있다. 이 두 가지 가능성은 upcoming microversion을 통해 "
"`blueprint handling-down-cell`_ 에서 down 세ลล의 인스턴스를 대신하여 최소한의 구조가 반환되는 "
"`blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-"
"cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 "
"`blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-"
"cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 "
"`blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-"
"cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 "
"`blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-"
"cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 "
"`blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-"
"cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 "
"`blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-"
"cell`_ 에서 최소한의 구조가 반환되는 `blueprint handling-down-cell`_ 에서 최소한의 구조가 반환되는 "
"`blueprint handling-down-cell`_"

#: ../../<reno.sphinxext stable/stein>:621
msgid ""
"In deployments with Ironic, adds the ability for compute services to manage "
"a subset of Ironic nodes. If the ``[ironic]/partition_key`` configuration "
"option is set, the compute service will only consider nodes with a matching "
"``conductor_group`` attribute for management. Setting the "
"``[ironic]/peer_list`` configuration option allows this subset of nodes to "
"be distributed among the compute services specified to further reduce "
"failure domain. This feature is useful to co-locate nova-compute services "
"with ironic-conductor services managing the same nodes, or to better control"
" failure domain of a given compute service."
msgstr ""
"Ironic 배포에서, 컴퓨팅 서비스가 Ironic 노드의 일부를 관리할 수 있는 능력을 추가한다. Ironic 노드의 "
"`conductor_group` 속성과 일치하는 노드만 관리할 수 있는지 여부를 결정하는 `[ironic]/partition_key` "
"구성 옵션을 설정하면 컴퓨팅 서비스가 설정된 노드에만 관리를 수행할 수 있다. Ironic 노드의 `peer_list` 구성 옵션을 "
"설정하면, 이 노드의 일부를 컴퓨팅 서비스에 분산하여 컴퓨팅 서비스의 실패 도메인을 더 줄일 수 있다. 이 기능은 nova-compute"
" 서비스와 Ironic-conductor 서비스가 동일한 노드를 관리하는 것을 함께 위치시키거나, 특정 컴퓨팅 서비스의 실패 도메인을 더"
" 잘 제어할 수 있는 것을 목적으로 한다."

#: ../../<reno.sphinxext stable/pike>:1038
msgid ""
"In deployments with multiple (v2) cells, upcalls from the computes to the "
"scheduler (or other control services) cannot occur. This prevents certain "
"things from happening, such as the track_instance_changes updates, as well "
"as the late affinity checks for server groups. See the related documentation"
" on the `scheduler.track_instance_changes` and "
"`workarounds.disable_group_policy_check_upcall` configuration options for "
"more details. Single-cell deployments without any MQ isolation will continue"
" to operate as they have for the time being."
msgstr ""
"다중 (v2) 세ลล을 포함하는 배포에서, 컴퓨터에서 스케줄러 (또는 다른 제어 서비스)로의 업콜이 발생할 수 없습니다. 이로 인해 일부"
" 일들이 발생하지 않습니다. 예를 들어, 트랙 인스턴스 변경을 업데이트하고, 서버 그룹의 나중에 부합성 확인이 발생하지 않습니다. 관련된"
" 문서에 대한 더 많은 정보를 얻으려면 `scheduler.track_instance_changes` 및 "
"`workarounds.disable_group_policy_check_upcall` 구성 옵션에 대한 관련 문서를 참조하십시오. 단일 "
"세ลล 배포는 어떤 MQ 분리도 absence에 따라 계속 작동합니다."

#: ../../<reno.sphinxext stable/rocky>:1105
msgid ""
"In microversion 1.23 of the placement service, JSON formatted error "
"responses gain a new attribute, ``code``, with a value that identifies the "
"type of this error. This can be used to distinguish errors that are "
"different but use the same HTTP status code. Any error response which does "
"not specifically define a code will have the code "
"``placement.undefined_code``."
msgstr ""
"microversion 1.23의 배치 서비스에서 JSON 형식으로 formatted error responses는 새로운 속성 "
"\"code\"를 가집니다. 이 속성의 값은 이 오류의 유형을 식별합니다. 이 오류가 다른지만 HTTP status code가 동일한 "
"오류를 구별할 수 있습니다. 오류 response가 특정 코드를 정의하지 않으면 \"placement.undefined_code\"라는 "
"코드가 사용됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1358
msgid ""
"In mitaka, an online migration was added to migrate older SRIOV parent "
"device information from extra_info to a new column. Since two releases have "
"gone out with that migration, it is removed in Ocata and operators are "
"expetected to have run it as part of either of the previous two releases, if"
" applicable."
msgstr ""
"In mitaka, 집합에 extra_info에서 older SRIOV 부모 장치 정보를 migrate하기 위해 온라인 이민이 "
"추가되었다. 두 개의 릴리스가 이 이민을 적용한 후에 나왔기 때문에, Ocata에서 이를 제거하고, 운영자는 이전의 두 개의 릴리스 중 "
"하나를 적용해야 할 것으로 예상된다."

#: ../../<reno.sphinxext origin/stable/ocata>:1182
msgid ""
"In order to maintain backward compatibility, filter and sort parameters "
"which are not mapped to the REST API `servers` resource representation are "
"ignored."
msgstr ""
"`REST API servers` 리소스 표현형에 mapping되지 않은 매개 변수를 필터링하고 정렬하여 backward "
"compatibility를 유지하기 위해 무시됩니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:448
msgid ""
"In order to make use of microversion's 2.94 FQDN hostnames, the "
"``[api]dhcp_domain`` config option must be set to the empty string. If this "
"is not done, the ``hostname`` field in the metadata API will be incorrect, "
"as it will include the value of ``[api]dhcp_domain`` appended to the "
"instance's FQDN. Note that simply not setting ``[api]dhcp_domain`` is not "
"enough, as it has a default value of ``novalocal``. It must explicitly be "
"set to the empty string."
msgstr ""
"microversion의 2.94 FQDN 호스트 이름을 사용하기 위해서는 `[api]dhcp_domain` 설정 옵션을 비어있는 "
"문자열로 설정해야 합니다. 이 경우, `[api]dhcp_domain`가 설정되지 않으면, metadata API의 `hostname` "
"필드는 오류가 발생할 수 있으며, 인스턴스의 FQDN에 `[api]dhcp_domain`을 추가한 값이 포함됩니다. 단순히 "
"`[api]dhcp_domain`를 설정하지 않으면 충분하지 않으며, 기본 값은 `novalocal`입니다. 따라서 비어있는 문자열로 "
"설정해야 합니다."

#: ../../<reno.sphinxext stable/train>:982
msgid ""
"In other words, NUMA-aware live migration with the libvirt driver is not "
"supported until:"
msgstr ""
"다른 말로써, NUMA- aware live migration과 libvirt 드라이버를 사용하여 live migration은 다음의 "
"조건이 fulfilled 될 때까지 지원되지 않는다."

#: ../../<reno.sphinxext stable/rocky>:1123
msgid ""
"In placement API microversion 1.20, a successful `POST /resource_providers` "
"returns 200 with a payload representing the newly-created resource provider."
"  The format is the same format as the result of the corresponding ``GET "
"/resource_providers/{uuid}`` call. This is to allow the caller to glean "
"automatically-set fields, such as UUID and generation, without a subsequent "
"GET."
msgstr ""
"placement API microversion 1.20에서, `POST /resource_providers`가 성공적으로 수행되면, "
"200을 반환하고, 새로운 리소스 제공자를 생성한 payload를 포함하는 payload가 반환됩니다.  이 format는 "
"corresponding `GET /resource_providers/{uuid}` 호출의 결과와 동일합니다.  이로 인해, 호출자가 "
"UUID와 생성을 포함하는 자동으로 설정된 필드를 다음 GET 호출 없이 automatically glean할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:626
msgid ""
"In prior releases, an attempt to boot an instance directly from an image "
"that was created by the Block Storage Service from an encrypted volume "
"resulted in the instance going ACTIVE but being unusable.  If a user then "
"performed the image-create action on such an instance, the new image would "
"inherit the ``cinder_encryption_key_id`` and, beginning with the 20.0.0 "
"(Train) release, the ``cinder_encryption_key_deletion_policy`` image "
"properties, assuming these were not included in the "
"``non_inheritable_image_properties`` configuration option.  (The default "
"setting for that option does *not* include these.)  Beginning with 20.0.0 "
"(Train), when the new image was deleted, the encryption key for the "
"*original* image would be deleted, thereby rendering it unusable for the "
"normal workflow of creating a volume from the image and booting an instance "
"from the volume.  Beginning with this release:"
msgstr ""
"이전 릴리즈에서 블록 스토리지 서비스가 암호화된 볼륨에서 생성된 이미지로 인해 직접 인스턴스를 부트로면 인스턴스는 활성화되지만 사용할 수"
" 없게 된 경험이 있었다.  사용자가 이러한 인스턴스를 이미지-생성 액션을 수행하면, 새로운 이미지에는 "
"`cinder_encryption_key_id` 속성이 상속되며, 20.0.0 (트레인) 릴리즈부터 "
"`cinder_encryption_key_deletion_policy` 이미지 속성이 상속되기 시작했다.  이러한 속성은 "
"`non_inheritable_image_properties` 구성 옵션에 포함되지 않기 때문에 (이 기본 설정은 포함되지 않는다.)  "
"20.0.0 (트레인) 릴리즈부터 새로운 이미지가 삭제되면 원래 이미지의 암호화 키가 삭제되며, thereby 사용 가능한-normal "
"workflow의 이미지 볼륨을 생성하고 인스턴스를 볼륨에서 부트로 하는-normal workflow에서 사용할 수 없게 된 것이다.  "
"이 릴리즈부터:"

#: ../../<reno.sphinxext stable/pike>:1753
msgid ""
"In the 2.50 microversion, the following fields are added to the ``GET /os-"
"quota-class-sets`` and ``PUT /os-quota-class-sets/{id}`` API response:"
msgstr ""
"2.50 마이크로 버전에서 다음 필드는 \"GET /os-quota-class-sets\" 및 \"PUT /os-quota-class-"
"sets/{id}\" API 응답에 추가됩니다."

#: ../../<reno.sphinxext stable/train>:85 stable/ussuri>:138
#: unmaintained/victoria>:867
msgid ""
"In the Rocky (18.0.0) release support was added to nova to use neutron's "
"multiple port binding feature when the binding-extended API extension is "
"available. In the Train (20.0.0) release the SR-IOV live migration feature "
"broke the semantics of the vifs field in the ``migration_data`` object that "
"signals if the new multiple port binding workflow should be used by always "
"populating it even when the ``binding-extended`` API extension is not "
"present. This broke live migration for any deployment that did not support "
"the optional ``binding-extended`` API extension. The Rocky behavior has now "
"been restored enabling live migration using the single port binding workflow"
" when multiple port bindings are not available."
msgstr ""
"로키 (18.0.0) 릴리스에서 노바에 support가 추가되었다. 이 support는 binding-extended API 확장 기능이"
" उपलबуществ을 있으면 neutron의 여러 포트 결합 기능을 사용할 수 있도록 한다. 로키 (18.0.0) 릴리스에서 트레인 "
"(20.0.0) 릴리스에서는 SR-IOV live migration 기능이 binding-extended API 확장 기능이 없는 "
"경우에만 vifs 필드의 semantics를 깨트렸다. 이 semantics는 new multiple port binding "
"workflow가 always populating되어 binding-extended API 확장 기능이 없는 경우에만 사용되어야 하는 "
"것을 signal한다. 이는 live migration이 모든 배포가 binding-extended API 확장 기능을 지원하지 않는 "
"경우에만 작동하지 않는다. 로키 (18.0.0) behavior가 now restore되어 single port binding "
"workflow를 사용하여 live migration이 possible가 when multiple port bindings가 "
"unavailable이다."

#: ../../<reno.sphinxext origin/stable/ocata>:1597
msgid ""
"In the context of virtual device role tagging at server create time, the "
"2.42 microversion restores the tag attribute to networks and "
"block_device_mapping_v2. A bug has caused the tag attribute to no longer be "
"accepted starting with version 2.33 for block_device_mapping_v2 and starting"
" with version 2.37 for networks. In other words, block devices could only be"
" tagged in version 2.32 and network interfaces between versions 2.32 and "
"2.36 inclusively. Starting with 2.42, both network interfaces and block "
"devices can be tagged again."
msgstr ""
"서버 생성 시 가상 장치 역 태그를 위한 context에서, 2.42 마이크로 버전은 태그 속성을 네트워크 및 "
"block_device_mapping_v2에 복원합니다. 버전 2.33부터 block_device_mapping_v2의 경우, 버전 "
"2.37까지 네트워크의 경우 태그 속성이 더 이상 수용되지 않습니다. 즉, block_device_mapping_v2의 경우 버전 "
"2.32까지만 태그가 가능하며, 버전 2.32부터 2.36까지의 네트워크 인터페이스만 태그가 가능합니다. 2.42 버전부터는 네트워크 "
"인터페이스와 block_device_mapping_v2의両方이 다시 태그가 가능합니다."

#: ../../<reno.sphinxext stable/2025.1>:385
msgid ""
"In the early days of Nova, all networking was internal, then ``quantum``, "
"now known as ``neutron`` was introduced. When the networking subsystem was "
"being externalized and neutron was optional Nova still needed to keep track "
"of the ports associated with an instance. To that end, to avoid these "
"expensive calls to an optional service the instance info cache was extended "
"to include network information and a periodic task was introduced to update "
"it in ``08fa534a0d28fa1be48aef927584161becb936c7`` as part of the ``Essex`` "
"release."
msgstr ""
"노바의 초기 시기에는 모든 네트워킹이 내부적이었고, \"quantum\"라는 이름으로 알려진 \"neutron\"이 도입되었다. 네트워킹"
" 서브시스템이 외부화되고 neutron이 선택적이면에도 노바는 인스턴스의 포트와 관련된 정보를 추적해야 했다. 이러한 비용이 비싸는 "
"선택적 서비스에 대한 호출을 피하기 위해, 인스턴스 정보 캐시가 네트워크 정보를 포함하여 확장되었으며, "
"\"08fa534a0d28fa1be48aef927584161becb936c7\"라는 이름으로 알려진 \"Essex\" 릴리스의 일부로 "
"주기적으로 업데이트되도록 일회용 task가 도입되었다."

#: ../../<reno.sphinxext stable/stein>:297
msgid ""
"In the environment of the web server running the placement service, set "
"``OS_NOVA_DISABLE_EVENTLET_PATCHING=yes`` so that eventlet does not conflict"
" with thread handling in the web server."
msgstr ""
"웹 서버가 배치 서비스를 실행하는 환경에서, 배치 서비스를 실행하는 웹서버에서 conflict가 발생하지 않도록 "
"``OS_NOVA_DISABLE_EVENTLET_PATCHING=yes``로 설정합니다."

#: ../../<reno.sphinxext unmaintained/yoga>:33 unmaintained/zed>:451
msgid ""
"In the libvirt driver, the default value of the ``<cputune><shares>`` "
"element has been removed, and is now left to libvirt to decide. This is "
"because allowed values are platform dependant, and the previous code was not"
" guaranteed to be supported on all platforms. If any of your flavors are "
"using the quota:cpu_shares extra spec, you may need to resize to a supported"
" value before upgrading."
msgstr ""
"libvirt 드라이버에서 `<cputune><shares>` 요소의 기본값이 제거되었으며, 현재 libvirt이 결정하도록 남겨졌다. "
"이는 allowed values가 플랫폼에 따라 달라진다는 이유로, 이전 코드는 모든 플랫폼에서 지원되지 않도록 보장되지 않았기 "
"때문이다. 만약 आपक의 플레버 중 하나가 quota:cpu_shares extra spec를 사용하고 있다면, 업그레이드하기 전에 "
"지원되는 값으로 리사이즈해야 할 수 있다."

#: ../../<reno.sphinxext stable/2023.2>:14 stable/2024.1>:182
#: stable/2024.2>:344 unmaintained/2023.1>:14
msgid ""
"In the victoria release, the instance_numa_topology object was extended to "
"enabled mix cpus (pinned and unpinned cpus) in the same instance. This "
"change added a new field pcpuset to the instance_numa_topology object. While"
" the change included object conversion code to handle the upgrade, it did "
"not account for instances that have a numa_topology but were not pinned. "
"i.e. a flavor with hw:mem_page_size or hw:numa_nodes set but without "
"hw:cpu_policy set to dedicated. As a result, instances created between "
"liberty and victoria releases with such a flavor cannot be started  after "
"upgrade to victoria. This has now been fixed. instances created post "
"victoria are not affected by this issue. see: "
"https://bugs.launchpad.net/nova/+bug/2080556 for more details."
msgstr ""
"비クト리아 릴리스에서 인스턴스_numa_topology 개체는 동일한 인스턴스에서 미xing CPUs (pinning 및 "
"unpinning CPUs)를 활성화하기 위해 확장되었다. 이 변경은 인스턴스_numa_topology 개체에 새로운 필드 "
"pcpuset를 추가했다. 그러나 이 변경은 업그레이드를 처리하기 위해 오브젝트 전환 코드를 포함했지만, hw:mem_page_size "
"또는 hw:numa_nodes가 설정된 인스턴스지만 hw:cpu_policy가 dedictated로 설정되지 않은 경우를 고려하지 "
"않았다. 즉, hw:mem_page_size 또는 hw:numa_nodes가 설정된 인스턴스지만 hw:cpu_policy가 "
"dedictated로 설정되지 않은 경우, 라이브리와 비クト리아 릴리스 사이에 생성된 인스턴스는 업그레이드 후 비クト리아 릴리스에 전환할"
" 수 없다는 것이다. 이 문제는 now fixed되어 있다. 라이브리 릴리스 이후에 생성된 인스턴스는 이 문제에 영향을 받지 않는다. 더"
" 많은 정보는 다음 URL을 참조하십시오: https://bugs.launchpad.net/nova/+bug/2080556"

#: ../../<reno.sphinxext stable/ussuri>:457
msgid ""
"In this microversion, expose the ``user_id`` and ``project_id`` fields in "
"the following APIs:"
msgstr "이 마이크로 버전에서, 다음 API에서 `user_id` 및 `project_id` 필드를 노출합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:32 unmaintained/xena>:635
msgid ""
"In this release OVS port creation has been delegated to os-vif when the "
"``noop`` or ``openvswitch`` security group firewall drivers are enabled in "
"Neutron. Those options, and others that disable the ``hybrid_plug`` "
"mechanism, will now use os-vif instead of libvirt to plug VIFs into the "
"bridge.  By delegating port plugging to os-vif we can use the "
"``isolate_vif`` config option to ensure VIFs are plugged securely preventing"
" guests from accessing other tenants' networks before the neutron ovs agent "
"can wire up the port. See `bug #1734320`_ for details. Note that OVN, ODL "
"and other SDN solutions also use ``hybrid_plug=false`` but they are not "
"known to be affected by the security issue caused by the previous behavior. "
"As such the ``isolate_vif`` os-vif config option is only used when deploying"
" with ml2/ovs."
msgstr ""
"이 릴리즈에서 OVS 포트 생성은 Neutron에서 ``noop`` 또는 ``openvswitch`` 보안 그룹ไฟ어walls 드라이버가"
" 활성화된 경우 os-vif에게 위임됩니다.  그 외의 옵션은 ``hybrid_plug`` 메커니즘을 비활성화하는 것과 같은 옵션은 "
"now os-vif 대신 libvirt를 사용하여 VIF를 브리지에 연결합니다.  os-vif를 통해 포트 연결을 위임하면 VIF가 "
"안전하게 연결되어 다른 tenant의 네트워크에 접근하지 못하도록 ``isolate_vif`` 구성 옵션을 사용할 수 있습니다.  예를 "
"들어, guest가 다른 tenant의 네트워크에 접근하기 전에 Neutron OVS एजент이 포트를 연결할 때까지.  `bug "
"#1734320`_에 대한รายละเอียด을 확인하십시오.  OVN, ODL 및 다른 SDN 솔루션은 "
"``hybrid_plug=false``를 사용합니다. 그러나 이전 행동의 보안 문제에 영향을 받지 않는 것으로 알려져 있기 때문에 "
"``isolate_vif`` os-vif 구성 옵션은 ml2/ovs 배포 시만 사용됩니다."

#: ../../<reno.sphinxext stable/train>:848
msgid ""
"In this release SR-IOV live migration support is added to the libvirt virt "
"driver for Neutron interfaces. Neutron SR-IOV interfaces can be grouped into"
" two categories, direct mode interfaces and indirect. Direct mode SR-IOV "
"interfaces are directly attached to the guest and exposed to the guest OS. "
"Indirect mode SR-IOV interfaces have a software interface such as a macvtap "
"between the guest and the SR-IOV device. This feature enables transparent "
"live migration for instances with indirect mode SR-IOV devices. As there is "
"no generic way to copy hardware state during a live migration, direct mode "
"migration is not transparent to the guest. For direct mode interfaces, we "
"mimic the workflow already in place for suspend and resume. For instance "
"with SR-IOV devices, we detach the direct mode interfaces before migration "
"and re-attach them after the migration. As a result, instances with direct "
"mode SR-IOV port will lose network connectivity during a migration unless a "
"bond with a live migratable interface is created within the guest."
msgstr ""
"이 릴리즈에서 Neutron 인터페이스에 대한 libvirt virt 드라이버에 SR-IOV live migration 지원이 "
"추가되었다. Neutron SR-IOV 인터페이스는 두 가지 카테고리로 분류될 수 있다. 즉, 직접 모드 인터페이스와 지연 모드 "
"인터페이스. 직접 모드 SR-IOV 인터페이스는 게스트에 직접 연결되어 게스트 OS에 노출된다. 지연 모드 SR-IOV 인터페이스는 "
"게스트와 SR-IOV 장치 사이의 소프트웨어 인터페이스, 예를 들어 macvtap과 같은 것을 가지고 있다. 이 기능은 지연 모드 SR-"
"IOV 장치가 있는 인스턴스의 투명한 live migration을 가능하게 한다. live migration 시 하드웨어 상태를 복사하는"
" 일반적인 방법이 없기 때문에 직접 모드 migration은 게스트에 투명하지 않다. 직접 모드 인터페이스를 위해, suspend와 "
"resume의 workflow를 이미 존재하는 것과 유사한 workflow를 시뮬레이션한다. 예를 들어, SR-IOV 장치에 대해, "
"migration 전에 인터페이스를 분리하고 migration 후에 다시 연결한다. 결과적으로, 직접 모드 SR-IOV 포트가 "
"migration 시 네트워크 연결을 잃을 수 있다. migration 중에 게스트 내에 live migratable 인터페이스를 "
"bond하는 경우만."

#: ../../<reno.sphinxext stable/train>:774
msgid ""
"In this release support was added for two additional libvirt video models: "
"``gop``, the UEFI graphic output protocol device model; and the ``none`` "
"device model. Existing support for ``virtio`` has been extended to all "
"architectures and may now be requested via the ``hw_video_model`` image "
"metadata property. Prior to this release the ``virtio`` video model was "
"unconditionally enabled for ``AARCH64``. This is unchanged but it can now be"
" explicitly enabled on all supported architectures. The ``none`` video model"
" can be used to disable emulated video devices when using pGPU or vGPU "
"passthrough."
msgstr ""
"이 릴리즈에서 두 개의 추가적인 libvirt 비디오 모델에 대한 지원이 추가되었습니다: ``gop`` UEFI 그래픽 출력 프로토콜 "
"장치 모델과 ``none`` 장치 모델. 기존 ``virtio`` 비디오 모델의 지원은 모든 아키텍처에 확장되었으며 현재 "
"``hw_video_model`` 이미지 메타데이터 속성을 통해 요청할 수 있습니다. 이전 릴리즈에서는 ``AARCH64``에서 "
"``virtio`` 비디오 모델이 무조건적으로 활성화되었습니다. 이는 unchangedですが, 모든 지원되는 아키텍처에서明示적으로 "
"활성화할 수 있습니다. ``none`` 비디오 모델은 pGPU 또는 vGPU 패스-through를 사용할 때 비원화된 비디오 장치들을 "
"비활성화하는 데 사용할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:466
msgid ""
"In this release the default values for the initial ram and cpu allocation "
"ratios have been updated to 1.0 and 4.0 respectively. This will not affect "
"any existing compute node resource providers but the new default will take "
"effect on the creation of new resource providers."
msgstr ""
"이 릴리즈에서 초기 RAM 및 CPU 할당 비율의 기본값이 1.0과 4.0로 업데이트되었다. 이 변경은 현재 컴퓨팅 노드 리소스 제공자에"
" 영향을 미치지 않지만, 새로운 기본값은 새로운 리소스 제공자가 생성될 때 효과가 발생할 것이다."

#: ../../<reno.sphinxext unmaintained/wallaby>:87 unmaintained/xena>:718
msgid ""
"In this release we delegate port plugging to os-vif for all OVS interface "
"types. This allows os-vif to create the OVS port before libvirt creates a "
"tap device during a live migration therefore preventing the loss of the MAC "
"learning frames generated by QEMU. This resolves a long-standing race "
"condition between Libvirt creating the OVS port, Neutron wiring up the OVS "
"port and QEMU generating RARP packets to populate the vswitch MAC learning "
"table. As a result this reduces the interval during a live migration where "
"packets can be lost. See `bug #1815989`_ for details."
msgstr ""
"이 릴리즈에서 모든 OVS 인터페이스 타입에 대해 os-vif에 포트 플러그를 위임합니다. 이로 인해 os-vif는 libvirt가 "
"live migration 시 tap 장치를 생성할 때까지 OVS 포트를 생성할 수 있습니다. 따라서 QEMU가 생성한 MAC "
"learning frames의 손실을 방지할 수 있습니다. 이로 인해 libvirt가 OVS 포트를 생성하고 Neutron이 OVS "
"포트를 연결하고 QEMU가 RARP 패킷을 생성하여 vswitch의 MAC learning 테이블을 채워주기 위한 경쟁 조건을 해결할 수"
" 있습니다. 따라서 live migration 시 패킷이 손실되는 시간을 줄일 수 있습니다. `bug #1815989`_를 참조하십시오."

#: ../../<reno.sphinxext stable/2025.1>:414
msgid ""
"In this release, the default behaviour of Nova has been changed to disable "
"the periodic, optimizing for performance, scale, power consumption and "
"typical deployment topologies, where the instance network information is "
"updated by neutron via the external event API as ports are modified. This "
"should significantly reduce the background neutron API load in medium to "
"large clouds. If you have a neutron backend that does not reliably send "
"network-changed event notifications to Nova you can re-enable this periodic "
"task by setting ``[compute]heal_instance_info_cache_interval`` to a value "
"greater than 0."
msgstr ""
"이 릴리스에서 노바의 디폴트 행동은 주기적으로 성능, масштей핑, 전력 소모 및 일반적인 배포 topology를 최적화하여 "
"주기적으로 작동하지 않도록 변경되었다. 이로 인해 인스턴스 네트워크 정보가 네트워크를 수정할 때마다 네트워크를 통해 외부 이벤트 API를"
" 통해 네트워크가 변경된 이벤트를 노바에 전달하는 것을 중단한다. 이로 인해 중간부터 큰 클라우드에서 배경 네트워크 API 로드가 "
"significanly 감소할 것으로 예상된다. 노바에 네트워크가 변경된 이벤트를 신뢰할 수 있는 네트워크 백엔드가 없다면, "
"`[compute]heal_instance_info_cache_interval`를 0보다 큰 값으로 설정하여 이 주기적인 일정을 재启할 "
"수 있다."

#: ../../<reno.sphinxext stable/rocky>:1132
msgid ""
"In version 1.25 of the Placement API, ``GET /allocation_candidates`` is "
"enhanced to accept numbered groupings of resource, required/forbidden trait,"
" and aggregate association requests. A ``resources`` query parameter key "
"with a positive integer suffix (e.g. ``resources42``) will be logically "
"associated with ``required`` and/or ``member_of`` query parameter keys with "
"the same suffix (e.g. ``required42``, ``member_of42``). The resources, "
"required/forbidden traits, and aggregate associations in that group will be "
"satisfied by the same resource provider in the response. When more than one "
"numbered grouping is supplied, the ``group_policy`` query parameter is "
"required to indicate how the groups should interact. With "
"``group_policy=none``, separate groupings - numbered or unnumbered - may or "
"may not be satisfied by the same provider. With ``group_policy=isolate``, "
"numbered groups are guaranteed to be satisfied by *different* providers - "
"though there may still be overlap with the unnumbered group.  In all cases, "
"each ``allocation_request`` will be satisfied by providers in a single non-"
"sharing provider tree and/or sharing providers associated via aggregate with"
" any of the providers in that tree."
msgstr ""
"1.25 버전의 배치 API에서 ``GET /allocation_candidates``는 리소스, 필요/금지 trait, 집합 연관성 "
"요청의 숫자 그룹을 수용하여 확장됩니다. ``resources`` 쿼리 파라미터 키에正의 정수尾문 (예: ``resources42``)가"
" 있는 경우, ``required`` 및/or ``member_of`` 쿼리 파라미터 키와 동일한尾문 (예: ``required42``,"
" ``member_of42``)가 있는 경우 논리적으로 연관됩니다. 그 그룹의 리소스, 필요/금지 trait, 집합 연관성은 반응에서 "
"동일한 리소스 제공자에 의해 удов족됩니다. 여러 숫자 그룹을 공급할 때, ``group_policy`` 쿼리 파라미터가 필요합니다. "
"``group_policy=none``의 경우, 숫자 또는 비 숫자 그룹은 동일한 제공자에 의해 충족될 수 있습니다. "
"``group_policy=isolate``의 경우, 숫자 그룹은 다른 제공자에 의해 충족될 수 있으며, 비 숫자 그룹과 중복이 "
"가능합니다. 모든 경우에, 각 ``allocation_request``는 단일 비 공유 제공자 트리 및/or 공유 제공자와 집합으로 "
"연관ized any of the providers in that tree에 의해 충족됩니다."

#: ../../<reno.sphinxext stable/2025.1>:407
msgid ""
"In``ba44c155ce1dcefede9741722a0525820d6da2b8`` as part of bug #1751923 the "
"_heal_instance_info_cache periodic task was modified to pass a "
"\"force_refresh\" forcing Nova to lookup the current state of all ports for "
"the instance from neutron and fully rebuild the info_cache. This has the "
"side effect of making the already poor scaling of this optional periodic "
"task even worse."
msgstr ""
"`ba44c155ce1dcefede9741722a0525820d6da2b8`에서 bug #1751923을 포함하여 "
"`_heal_instance_info_cache` 주기적인 일정을 수정하여 \"force_refresh\"를 사용하여 Nova가 "
"인스턴스의 모든 포트의 현재 상태를 lookup하여 neutron에서 및 info_cache를 완전히 재건합니다. 이에는 이미 poor "
"scaling이 있는 이 옵션 주기적인 일정이 더 나은 성능을失는 부수 효과가 있습니다."

#: ../../<reno.sphinxext ../source/newton.rst:101 origin/stable/ocata>:377
#: stable/pike>:1797
msgid ""
"Includes the fix for `bug 1673613`_ which could cause issues when upgrading "
"and running ``nova-manage cell_v2 simple_cell_setup`` or ``nova-manage "
"cell_v2 map_cell0`` where the database connection is read from config and "
"has special characters in the URL."
msgstr ""
"`bug 1673613`_을 포함하여fix를 포함합니다. `nova-manage cell_v2 simple_cell_setup` 또는 "
"`nova-manage cell_v2 map_cell0`를 업그레이드하고 실행할 때, config에서 URL을 읽어들여서 특수한 문자가 "
"있는 경우 문제가 발생할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1203
msgid ""
"Injected network templates will now ignore the ``use_ipv6`` config option."
msgstr "인jected 네트워크 템플릿은 현재 ``use_ipv6`` 구성 옵션을 무시합니다."

#: ../../<reno.sphinxext stable/rocky>:579
msgid ""
"Instance action versioned notifications now contain "
"``action_initiator_user`` and ``action_initiator_project`` fields to "
"distinguish between the owner of the instance and who initiated the action "
"upon the instance, for example an administrator or another user within the "
"same project."
msgstr ""
"인스턴스 액션 버전이된 알림은 now `action_initiator_user` 및 `action_initiator_project` "
"필드를 포함하여 인스턴스의 소유자와 인스턴스 액션을 시작한 사람을 구별하는 데 사용됩니다. 예를 들어, 관리자 또는 같은 프로젝트 내의 "
"다른 사용자가 인스턴스 액션을 시작할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:721
msgid ""
"Instance device buses and models will now remain stable across reboots and "
"will not be changed by new defaults in libosinfo or the OpenStack Nova "
"libvirt driver."
msgstr ""
"인스턴스 장치 버스와 모델은 now 다시 안정적이게 유지되고, libosinfo 또는 OpenStack Nova libvirt 드라이버의"
" 새로운 디폴트가 있는지 여부에 상관없이 재부팅 후에도 안정적이게 유지되고 변경되지 않는다."

#: ../../<reno.sphinxext unmaintained/xena>:318
msgid ""
"Instance hostnames published by the metadata API service or config drives "
"can be explicitly defined at instance creation time thanks to the new `2.90 "
"API microversion`__. See the ``hostname`` field documentation on the `API "
"docs`__ for further details."
msgstr ""
"인스턴스 호스트 이름을.metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 "
"config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API "
"서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 "
"metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해"
" 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config "
"드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 "
"config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API "
"서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 "
"metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해"
" 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config "
"드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 "
"config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API "
"서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 "
"metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해"
" 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config "
"드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 "
"config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API "
"서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 "
"metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해"
" 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 config "
"드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해 공유되는 metadata API 서비스 또는 "
"config 드라이브에 의해 공유되는 metadata API 서비스 또는 config 드라이브에 의해"

#: ../../<reno.sphinxext stable/stein>:610
msgid ""
"Instance list operations across cells are now made more efficient by "
"batching queries as a fraction of the total limit for a request. Before "
"this, an instance list with a limit of 1000 records (the default) would "
"generate queries to each cell with that limit, and potentially "
"process/sort/merge $num_cells*$limit records, despite only returning $limit "
"to the user. The strategy can now be controlled via "
"``[api]/instance_list_cells_batch_strategy`` and related options to either "
"use fixed batch sizes, or a fractional value that scales with the number of "
"configured cells."
msgstr ""
"인스턴스 목록 연산은 now 더 효율적으로 done되었습니다. 이전에는 인스턴스 목록을 1000 레코드 (기본값)로 제한하여 각 세ลล에"
" 대해 이 제한을 사용하여 쿼리를 생성하고, 이로 인해 $num_cells*$limit 레코드를 처리/정렬/합병할 수 있었습니다. 그러나"
" 사용자에게만 $limit 레코드를 반환합니다. 이 전략은 now "
"``[api]/instance_list_cells_batch_strategy``와 관련된 옵션을 통해 제어할 수 있습니다. 또는 고정 "
"배치 크기 또는 수신된 세ลล의 수에 따라 스케일링하는 가치가 있는 분할 값이 사용될 수 있습니다."

#: ../../<reno.sphinxext stable/train>:958
msgid "Instance with network related PCI device and either:"
msgstr "인스턴스와 네트워크 관련된 PCI 장치와의 연결"

#: ../../<reno.sphinxext stable/train>:956
msgid "Instance with non-network related PCI device."
msgstr "PCI 장치가 네트워크 관련이 아닌 인스턴스."

#: ../../<reno.sphinxext stable/queens>:962
msgid ""
"Instances that have attached encrypted volumes from before Queens will "
"continue to use os-brick encryptors after a live migration or direct upgrade"
" to Queens. A full reboot or another live migration between Queens compute "
"hosts is required before the instance will attempt to use QEMU native LUKS "
"decryption."
msgstr ""
"following text translation:\n"
"\n"
" Queens 이전에 연결된 암호화 볼륨을 사용한 인스턴스는 라이브 마이그레이션 또는 Queens로 업그레이드하는 직접적인 방법으로after os-brick 암호화 암호화자를 사용할 수 있습니다.  Queens compute 호스트 간의 다른 라이브 마이그레이션 또는_full 재부팅은 인스턴스가 QEMU native LUKS 암호화 해제를 사용할 수 있도록 करन하기 위해 필요합니다."

#: ../../<reno.sphinxext stable/2024.1>:323
msgid ""
"Instances using `vGPUs can now be live-migrated "
"<https://docs.openstack.org/nova/latest/admin/virtual-gpu.html#caveats>`_ if"
" both of the compute nodes support libvirt-8.6.0 and QEMU-8.1.0, as the "
"source mediated device will migrate the GPU memory to another target "
"mediated device automatically. In order to do this, "
"``[libvirt/live_migration_downtime`` config option needs to be modified "
"according to the aforementioned documentation."
msgstr ""
"인스턴스 `vGPUs`를 사용하여 live-migrated 할 수 있습니다. "
"<https://docs.openstack.org/nova/latest/admin/virtual-gpu.html#caveats>  두 "
"compute 노드가 libvirt-8.6.0 및 QEMU-8.1.0를 지원하는 경우, 소스 매디에이티드 장치가 GPU 메모리를 다른 "
"목적 매디에이티드 장치로 tự động 이동합니다. 이에 대한 이 작업을 수행하려면 "
"`libvirt/live_migration_downtime` config 옵션을 aforementioned 문서에 따라 수정해야 합니다."

#: ../../<reno.sphinxext stable/2024.1>:387
msgid ""
"Instances using vGPUs can now be correctly live-migrated by the libvirt "
"driver between compute nodes supporting the same mediated device types used "
"by the instance. In order to be able to do this, the compute hosts need to "
"support at least the minimum versions of libvirt-8.6.0, QEMU-8.1.0 and Linux"
" kernel 5.18.0. If operators use multiple vGPU types per compute, they need "
"to make sure they already use custom traits or custom resource classes for "
"the GPUs resource providers and that the instance was created with a flavor "
"using either a custom resource class or asking for a custom trait in order "
"to make sure that Placement API will provide the right target GPU using the "
"same mdev type for the instance."
msgstr ""
"인스턴스(Instance)가 vGPU(virtual GPU)를 사용할 수 있는 경우, libvirt 드라이버가 compute 노드가 "
"동일한 매개화된 장치 타입을 사용하는 인스턴스를 live-migrate 할 수 있습니다. 이에 대해 작동하는 방법은, "
"libvirt-8.6.0, QEMU-8.1.0 및 Linux 커널 5.18.0 이상을 지원하는 compute 호스트가 필요합니다. "
"vGPU의 여러 타입을 사용하는 경우, vGPU 리소스 제공자에 대한 custom traits 또는 custom resource "
"classes를 사용해야 합니다. 또한, 인스턴스가 custom resource class 또는 custom trait를 사용하여 "
"생성되었을 때, Placement API는 correct target GPU를 제공할 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:340
msgid ""
"Instances using virtio-net will see an increase in performance between 10% "
"and 20% if their image uses a new ``hw_virtio_packed_ring=true`` property or"
" their flavor contains ``hw:virtio_packed_ring=true`` extra spec, provided "
"libvirt version is >= 6.3 and QEMU >= 4.2."
msgstr ""
"virtio-net를 사용하는 인스턴스는 6.3 이상의 libvirt 버전과 4.2 이상의 QEMU 버전이 있는 경우, "
"hw_virtio_packed_ring=true 속성이 있는 이미지 또는 hw:virtio_packed_ring=true extra "
"spec이 있는 플레버가 있는 경우, 10%에서 20%의 성능 증가를 볼 수 있습니다."

#: ../../<reno.sphinxext stable/2024.2>:240
msgid ""
"Instances with UEFI firmware can now be launched with stateless firmware if "
"their image has the ``hw_firmware_stateless`` property and if the compute "
"services have libvirt 8.6.0 or later."
msgstr ""
"EFI firmware를 가진 인스턴스들은 now hw_firmware_stateless 속성을 가진 이미지에 stateless "
"firmware를 사용하여 시작할 수 있습니다. compute 서비스가 libvirt 8.6.0 이상일 때."

#: ../../<reno.sphinxext unmaintained/xena>:158 unmaintained/yoga>:235
#: unmaintained/zed>:513
msgid ""
"Instances with hardware offloaded ovs ports no longer lose connectivity "
"after failed live migrations. The driver.rollback_live_migration_at_source "
"function is no longer called during during pre_live_migration rollback which"
" previously resulted in connectivity loss following a failed live migration."
" See `Bug 1944619`_ for more details."
msgstr ""
"인스턴스에서 하드웨어를 오프로드 한 ovs 포트가 더 이상 실패한 live migration 후에 연결을 잃지 않습니다. "
"driver.rollback_live_migration_at_source 함수는 이전에 실패한 live migration 후에 연결을 "
"잃는 것을 일으켰던 pre_live_migration.rollback 시에 더 이상 호출되지 않습니다. 더 많은 정보는 `Bug "
"1944619`_에 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:1405
msgid ""
"Instead of ``api_class`` option ``nova.keymgr.barbican.BarbicanKeyManager``,"
" use ``castellan.key_manager.barbican_key_manager.BarbicanKeyManager``"
msgstr ""
"``api_class`` 옵션 대신 ``nova.keymgr.barbican.BarbicanKeyManager``를 "
"``castellan.key_manager.barbican_key_manager.BarbicanKeyManager``로 사용하십시오."

#: ../../<reno.sphinxext stable/queens>:1407
msgid ""
"Instead of ``api_class`` option "
"``nova.tests.unit.keymgr.mock_key_mgr.MockKeyManager``, use "
"``castellan.tests.unit.key_manager.mock_key_manager.MockKeyManager``"
msgstr ""
"``api_class`` 옵션 ``nova.tests.unit.keymgr.mock_key_mgr.MockKeyManager`` 대신 "
"``castellan.tests.unit.key_manager.mock_key_manager.MockKeyManager``를 "
"사용하십시오."

#: ../../<reno.sphinxext unmaintained/victoria>:614
msgid ""
"Intel CMT perf events - ``cmt``, ``mbmt``, and ``mbml`` - are no longer "
"supported by the ``[libvirt] enabled_perf_events`` config option. These "
"event types were broken by design and are not supported in recent Linux "
"kernels (4.14+)."
msgstr ""
"인텔 CMT 성능 이벤트 - `cmt`, `mbmt`, 및 `mbml` -는 `libvirt`를 활성화한 "
"`enabled_perf_events` 설정 옵션에 의해 더 이상 지원되지 않습니다. 이러한 이벤트 유형은 설계상 부서져있고 최근의 "
"리눅스 커널 (4.14+)에 의해 지원되지 않습니다."

#: ../../<reno.sphinxext stable/pike>:1054
msgid ""
"Interface attachment/detachment for ironic virt driver was implemented in "
"in-tree network interfaces in ironic version 8.0, and this release is "
"required for nova's interface attachment feature to work. Prior to that "
"release, calling VIF attach on an active ironic node using in-tree network "
"interfaces would be basically a noop. It should not be an issue during the "
"upgrade though, as it is required to upgrade ironic before nova."
msgstr ""
"인터페이스 연결/분리(ironic virt driver)가 in-tree 네트워크 인터페이스에서 implementation 된 "
"ironic 버전 8.0에서, 이 릴리스는 nova의 인터페이스 연결 기능이 작동하는 데 필요하다. 이전 릴리스에서는, 활성화된 "
"ironic 노드에 in-tree 네트워크 인터페이스를 사용하여 VIF attach를 호출하면 기본적으로 noop이 된다. 그러나 "
"업그레이드가 필요할 때는 문제가 되지 않는다. 왜냐하면 ironic을 먼저 업그레이드해야 하기 때문이다."

#: ../../<reno.sphinxext stable/2023.2>:59 stable/2024.1>:241
#: stable/2024.2>:430
msgid ""
"Introduced a new compute configuration option "
"`sharing_providers_max_uuids_per_request` and applied a fix to handle the "
"\"Request-Too-Long\" error that can occur when querying the placement API "
"with a large number of aggregate UUIDs."
msgstr ""
"`새로운 컴퓨팅 구성 옵션인 `sharing_providers_max_uuids_per_request`를 도입하고 \"Request-"
"Too-Long\" 오류가 aggregate UUID의จำนวน이 많은 경우에 발생하는 API에 대한 inquiries를 처리하는 데 "
"사용되는 오류를修复했다."

#: ../../<reno.sphinxext stable/stein>:526
msgid ""
"Introduced a new config option ``[compute]/max_concurrent_disk_ops`` to "
"reduce disk contention by specifying the maximum number of concurrent disk-"
"IO-intensive operations per compute service.  This would include operations "
"such as image download, image format conversion, snapshot extraction, etc. "
"The default value is 0, which means that there is no limit."
msgstr ""
"``[compute]/max_concurrent_disk_ops``를 추가하여 디스크-contention을 줄이기 위해 컴퓨터 서비스의 "
"최대 동시 디스크-IO-중요한 작업의 수를 지정하는 new config 옵션을 도입했습니다.  이러한 작업은 이미지 다운로드, 이미지 "
"포맷 변환, 스냅샘 추출, etc.를 포함합니다.  기본값은 0으로, 이는 제한이 없다는 것을 의미합니다."

#: ../../<reno.sphinxext stable/rocky>:805
msgid ""
"Introduces ``[compute]/cpu_shared_set`` option for compute nodes. Some "
"workloads run best when the hypervisor overhead processes (emulator threads "
"in libvirt/QEMU) can be placed on different physical host CPUs than other "
"guest CPU resources. This allows those workloads to prevent latency spikes "
"for guest vCPU threads."
msgstr ""
"``[compute]/cpu_shared_set`` 옵션을 사용하여 컴퓨팅 노드에 개입합니다. 일부 작업負荷가 가장 잘 작동할 때는 "
"가상화 하이퍼바이저 오버헤드 프로세스(libvirt/QEMU에서 simulater 스레드)를 다른 물리적 호스트 CPU에서 다른 게스트 "
"CPU 리소스와 다른 위치에 배치할 수 있습니다. 이것은 게스트 vCPU 스레드에 대한 지연 스플릿을 방지하는 데 도움이 됩니다."

#: ../../<reno.sphinxext stable/rocky>:664
msgid ""
"Introduces new placement API version ``1.26``. Starting with this version it"
" is allowed to define resource provider inventories with reserved value "
"equal to total."
msgstr ""
"``1.26`` 버전의 new placement API를 도입합니다. 이 버전부터는 총 giá치와 동일한 예약 giá치를 사용하여 자원 "
"제공자 인 inventory를 정의할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1226
msgid ""
"Introduces the ``powervm`` configuration group which contains the "
"``proc_units_factor`` configuration option. This allows the operator to "
"specify the physical processing power to assign per vCPU."
msgstr ""
"``powervm`` 구성 그룹을 소개하며, đó에는 ``proc_units_factor`` 구성 옵션이 포함되어 있습니다. 이 구성 "
"옵션은 vCPU당 물리적 처리 성능을 할당하는 것을 연관시킵니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1333
msgid ""
"Ironic configuration options that were used for a deprecated Identity v2 API"
" have been removed from ``ironic`` group. Below is the detailed list of "
"removed options:"
msgstr ""
"ironic 구성 옵션 중 deprecated Identity v2 API를 사용한 ironic 그룹에서 사용된 ironic 구성 옵션이"
" 제거되었습니다. 제거된 옵션의รายละเอียด은 다음과 같습니다."

#: ../../<reno.sphinxext stable/stein>:1000
msgid ""
"Ironic nodes are now only scheduled using the ``resource_class`` field set "
"on the node. CPUs, RAM, and disks are not reported to the resource tracker. "
"Ironic nodes must have the ``resource_class`` field set before upgrading. "
"Flavors must also be configured to use resource classes instead of node "
"properties. See the `ironic flavor configuration guide "
"<https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html>`_ for more information on doing this."
msgstr ""
"ironic 노드는 현재 ``resource_class`` 필드를 설정하여만 스케줄링이 가능합니다. CPU, RAM, 디스크는 리소스 "
"트래커에 보고되지 않습니다. ironic 노드는 ``resource_class`` 필드를 설정해야 업그레이드할 수 있습니다. "
"flavor도 ``resource_class``를 사용하여 구성해야 노드 속성 대신에 사용해야 합니다. ironic flavor "
"configuration guide "
"<https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html>를 참조하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:927
msgid ""
"Ironic nodes that were deleted from ironic's database during Newton may "
"result in orphaned resource providers causing incorrect scheduling "
"decisions, leading to a reschedule. If this happens, the orphaned resource "
"providers will need to be identified and removed."
msgstr ""
"ironic 노드가 Newton에서 database에서 삭제된 경우, ironic 노드가 삭제된 후 orphaned resource "
"provider가 남아 있으면, 오류가 발생할 수 있으며, scheduling decision이 다시 반복될 수 있습니다. 이 경우, "
"orphaned resource provider가 필요하다면, identification 및 removal이 필요합니다."

#: ../../<reno.sphinxext stable/2024.1>:423
msgid ""
"Ironic nova-compute services can now target a specific shard of ironic nodes"
" by setting the config ``[ironic]shard``. This is particularly useful when "
"using active-passive methods to choose on which physical host your ironic "
"nova-compute process is running, while ensuring ``[DEFAULT]host`` stays the "
"same for each shard. You can use this alongside ``[ironic]conductor_group`` "
"to further limit which ironic nodes are managed by each nova-compute "
"service. Note that when you use ``[ironic]shard`` the ``[ironic]peer_list`` "
"is hard coded to a single nova-compute service."
msgstr ""
"ironic nova-compute 서비스는 now ironic 노드의 특정 shard를 target할 수 있습니다. 이는 특히 "
"active-passive 방법을 사용하여 ironic nova-compute 프로세스가 물리적 호스트에서 실행되는지 선택하는 데 "
"유용합니다. meanwhile, [DEFAULT]host는 각 shard에서 동일한まま이므로 각 nova-compute 서비스가 관리하는"
" ironic 노드의 범위를 더 제한할 수 있습니다. ironic shard를 사용할 때는 ironic peer_list는 단일 "
"nova-compute 서비스에 고정됩니다."

#: ../../<reno.sphinxext stable/2023.2>:89 stable/2024.1>:571
#: unmaintained/2023.1>:123 unmaintained/zed>:14
msgid ""
"Ironic virt driver now uses the node cache and respects partition keys, such"
" as conductor group, for list_instances and list_instance_uuids calls. This "
"fix will improve performance of the periodic queries which use these driver "
"methods and reduce API and DB load on the backing Ironic service."
msgstr ""
"Irony virt 드라이버는 현재 노드 캐시를 사용하고, partition 키 such as conductor group를 "
"respect합니다. 이러한 fix는 list_instances 및 list_instance_uuids API calls에서 "
"partition key를 respect하는 driver method를 사용하는 주기적 요청의 성능을 개선하고, backing "
"Ironic 서비스에 대한 API 및 DB 로드가 줄어들게 됩니다."

#: ../../<reno.sphinxext stable/stein>:1180
msgid ""
"It is no longer possible to force server live migrations or evacuations to a"
" specific destination host starting with API microversion 2.68. This is "
"because it is not possible to support these requests for servers with "
"complex resource allocations. It is still possible to request a destination "
"host but it will be validated by the scheduler."
msgstr ""
"API 마이크로 버전 2.68 이상에서, 서버의 복잡한 리소스 할당을 지원할 수 없기 때문에, 서버의 trực면 이식 또는 이식은 특정 "
"목적지 호스트에 강제적으로 시작할 수 없습니다. 그러나 여전히 목적지 호스트를 요청할 수 있으며, 스케줄러에 의해 유효성 "
"kiểm증됩니다."

#: ../../<reno.sphinxext unmaintained/xena>:562
msgid ""
"It is no longer possible to specify an sqlalchemy-migrate-based version. "
"When the ``nova-manage db sync`` and ``nova-manage api_db sync`` commands "
"are run, all remaining sqlalchemy-migrate-based migrations will be "
"automatically applied. Attempting to specify an sqlalchemy-migrate-based "
"version will result in an error."
msgstr ""
"다음은 한국어로 번역된 텍스트입니다.\n"
"\n"
"nova-manage db sync 및 nova-manage api_db sync 명령을 실행하면, 모든 남은 sqlalchemy-migrate-based 마이그레이션은 tự động 적용됩니다. sqlalchemy-migrate-based 버전을 지정하는 시도는 오류를 발생시킵니다."

#: ../../<reno.sphinxext stable/rocky>:530
msgid ""
"It is now possible to `disable a cell`_ to stop scheduling to a cell by "
"using the ``nova-manage cell_v2 update_cell`` command."
msgstr ""
"이제 `세ลล을 비활성화`_를 사용하여 세ลล에 스케줄링을 중단할 수 있습니다. `nova-manage cell_v2 "
"update_cell` 명령을 사용합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:370
msgid ""
"It is now possible to allocate all cores in an instance to realtime and omit"
" the ``hw:cpu_realtime_mask`` extra spec. This requires specifying the "
"``hw:emulator_threads_policy`` extra spec."
msgstr ""
"이제 모든 인스턴스에서 realtime 코어를 할당할 수 있으며 hw:cpu_realtime_mask extra spec을 제외할 수 "
"있습니다. 이에 따라 hw:emulator_threads_policy extra spec을 명시해야 합니다."

#: ../../<reno.sphinxext stable/rocky>:1058
msgid ""
"It is now possible to configure NUMA affinity for most neutron networks. "
"This is available for networks that use a ``provider:network_type`` of "
"``flat`` or ``vlan`` and a ``provider:physical_network`` (L2 networks) or "
"networks that use a ``provider:network_type`` of ``vxlan``, ``gre`` or "
"``geneve`` (L3 networks)."
msgstr ""
"현재는 대부분의 네트워크에서 NUMA 경향을 구성할 수 있습니다. 이 기능은 flat 또는 vlan을 사용하는 네트워크가 "
"``provider:network_type``을 flat 또는 vlan으로 사용하고 L2 네트워크의 "
"``provider:physical_network``을 사용하는 네트워크나 vxlan, gre 또는 geneve를 사용하는 L3 "
"네트워크가 ``provider:network_type``을 vxlan, gre 또는 geneve로 사용하는 네트워크에만 उपलब합니다."

#: ../../<reno.sphinxext stable/rocky>:537
msgid ""
"It is now possible to configure a separate database for the placement "
"service, which could help in easing the eventual placement service "
"extraction from Nova and data migration associated with it."
msgstr ""
"이제는 Nova와 관련된 데이터 마이그레이션과 관련된 eventual placement service extraction을 erle우는 "
"것을 erle우는 것에 도움이 될 수 있는 separate database를 설정할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:680
msgid ""
"It is now possible to configure granular policy rules for placement REST API"
" operations."
msgstr "현재는 REST API 연산을위한 배치에 대한 세부적 정책 규칙을 구성할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:989
msgid ""
"It is now possible to configure multiple *nova-scheduler* workers via the "
"``[scheduler]workers`` configuration option. By default, the option runs "
"``ncpu`` workers if using the ``filter_scheduler`` scheduler driver, "
"otherwise the default is 1."
msgstr ""
"*nova-scheduler* 노드의 여러 노드에서 *nova-scheduler* 노드를 구성할 수 있습니다. 기본적으로 "
"*filter_scheduler* 스케줄러 드라이버를 사용할 때는 *ncpu* 노드가 실행되며, 다른 경우에는 1 노드가 실행됩니다."

#: ../../<reno.sphinxext stable/pike>:70 stable/queens>:1760
msgid ""
"It is now possible to configure the ``[cinder]`` section of nova.conf to "
"allow setting admin-role credentials for scenarios where a user token is not"
" available to perform actions on a volume. For example, when "
"``reclaim_instance_interval`` is a positive integer, instances are soft "
"deleted until the nova-compute service periodic task removes them. If a soft"
" deleted instance has volumes attached, the compute service needs to be able"
" to detach and possibly delete the associated volumes, otherwise they will "
"be orphaned in the block storage service. Similarly, if "
"``running_deleted_instance_poll_interval`` is set and "
"``running_deleted_instance_action = reap``, then the compute service will "
"need to be able to detach and possibly delete volumes attached to instances "
"that are reaped. See `bug 1733736`_ and `bug 1734025`_ for more details."
msgstr ""
"이제 nova.conf의 [cinder] 섹션을 구성할 수 있습니다. 사용자 토큰이 उपलब्ध하지 않아 볼륨에 acción을 수행할 수"
" 있는 상황에서 admin-role 자격 증명이 설정될 수 있습니다. 예를 들어, reclaim_instance_interval이 양수인"
" 경우, nova-compute 서비스는 주기적으로 인스턴스를 소프트 삭제합니다. 소프트 삭제된 인스턴스가 볼륨을 연결하고 있는 경우, "
"compute 서비스는 볼륨을 detach하고 possibly delete할 수 있어야 합니다. 그렇지 않으면 블록 스토리지 서비스에서 "
"orphaned가 됩니다. similarly, running_deleted_instance_poll_interval가 설정된 경우, "
"running_deleted_instance_action = reap인 경우, compute 서비스는 인스턴스에 연결된 볼륨을 "
"detach하고 possibly delete할 수 있어야 합니다. 더 많은 details를 보려면 `bug 1733736`_와 `bug "
"1734025`_을 확인하십시오."

#: ../../<reno.sphinxext stable/train>:1007
msgid ""
"It is now possible to count quota usage for cores and ram from the placement"
" service and instances from instance mappings in the API database instead of"
" counting resources from cell databases. This makes quota usage counting "
"resilient in the presence of down or poor-performing cells."
msgstr ""
"현재는 코어 및 RAM의 사용quota를 플레이스먼트 서비스와 인스턴스 매핑에서 API 데이터베이스에 카운트할 수 있으며, "
"formerly는 세ลล 데이터베이스에서 리소스 카운트를 하였다. 이로 인해 down 또는 poor-performing 세ลล의 "
"presence에서 사용quota 카운트가 강화되었습니다."

#: ../../<reno.sphinxext stable/2023.2>:223
msgid ""
"It is now possible to define different authorization policies for migration "
"with and without a target host."
msgstr "현재는 이주와 함께/만에 다른 허가 정책을 정의할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:508
msgid ""
"It is now possible to place CPU pinned and unpinned servers on the same "
"compute host when using the libvirt compute driver. See the `admin guide "
"<https://docs.openstack.org/nova/latest/admin/cpu-"
"topologies.html#configuring-libvirt-compute-nodes-for-cpu-pinning>`_ for "
"details."
msgstr ""
"이제 libvirt 컴퓨터 드라이버를 사용할 때 CPU 고정 및 고정-free 서버를 동일한 컴퓨터 호스트에 배치할 수 있습니다. 더 "
"많은 정보는 <https://docs.openstack.org/nova/latest/admin/cpu-"
"topologies.html#configuring-libvirt-compute-nodes-for-cpu-pinning>에서 확인할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/stein>:416
msgid ""
"It is now possible to run Nova with version 1.0.0 of the recently extracted "
"placement service, hosted from its own repository. Note that install/upgrade"
" of an extracted placement service is not yet fully implemented in all "
"deployment tools. Operators should check with their particular deployment "
"tool for support before proceeding. See the placement `install`_ and "
"`upgrade`_ documentation for more details. In Stein, operators may choose to"
" continue to run with the integrated placement service from the Nova "
"repository, but should begin planning a migration to the extracted placement"
" service by Train, as the removal of the integrated placement code from Nova"
" is planned for the Train release."
msgstr ""
"다음은 한국어로 번역된 텍스트입니다.\n"
"\n"
"현재는 recently.extracted placement service의 version 1.0.0을 사용하여 Nova를 실행할 수 있습니다. 이 service는 자신의 repository에서 호스트됩니다..extracted placement service의 설치/업그레이드는 모든 배포 도구에서yet 완전히 구현되지 않았습니다. therefore, operators는 자신의 배포 도구에 대한 지원을 확인하고 진행하기 전에 확인해야합니다. 더 많은 정보를 얻으려면 placement `install`_ 및 `upgrade`_ documentation을 확인하십시오. Stein에서, operators는 Nova repository에서 통합된 placement service를 계속 실행할 수 있습니다. 그러나, Train release에서 Nova에서 통합된 코드를 제거하는 계획에 따라, Train에서 extracted placement service로의 mig레이션을 계획해야합니다."

#: ../../<reno.sphinxext stable/pike>:822
msgid ""
"It is now possible to signal and perform an online volume size change as of "
"the 2.51 microversion using the ``volume-extended`` external event. Nova "
"will perform the volume extension so the host can detect its new size. It "
"will also resize the device in QEMU so instance can detect the new disk size"
" without rebooting."
msgstr ""
"이제 2.51 마이크로 버전부터는 온라인 볼륨 크기 변경을 신호하고 수행할 수 있습니다. \"volume-extended\" "
"external event를 사용하여. 노바는 볼륨 확장을 수행하여 호스트가 새로운 크기를 감지할 수 있도록 합니다. 또한 QEMU에서 "
"장치를 리사이즈하여 인스턴스가 새로운 디스크 크기를 감지할 수 있도록 재부팅 없이입니다."

#: ../../<reno.sphinxext stable/train>:678
msgid ""
"It is now possible to signal and perform an update of an instance's power "
"state as of the 2.76 microversion using the ``power-update`` external event."
" Currently it is only supported in the ironic driver and through this event "
"Ironic will send all \"power-on to power-off\" and \"power-off to power-on\""
" type power state changes on a physical instance to nova which will update "
"its database accordingly. This way nova will not be able to enforce an "
"incorrect power state on the physical instance during the periodic "
"``_sync_power_states`` task. The changes to the power state of an instance "
"caused by this event can be viewed through ``GET /servers/{server_id}/os-"
"instance-actions`` and ``GET /servers/{server_id}/os-instance-"
"actions/{request_id}``."
msgstr ""
"이제 2.76 마이크로 버전부터 인스턴스의 전력 상태를 신호하고 업데이트할 수 있습니다. \"power-update\" 외부 이벤트를 "
"사용하여. 현재는 이ironic 드라이버만 지원하고 이ironic은 \"전력-on to 전력-off\"과 \"전력-off to 전력-"
"on\" type 전력 상태 변경을 nova로 전달하여, nova의 데이터베이스를 업데이트합니다. 이로 인해 nova는 주기적인 "
"\"_sync_power_states\" task 동안 물리적 인 인스턴스의 전력 상태를 오류로 enforce할 수 없습니다. 이 "
"이벤트로 인한 인스턴스의 전력 상태 변경은 \"GET /servers/{server_id}/os-instance-actions\"와 "
"\"GET /servers/{server_id}/os-instance-actions/{request_id}\"를 통해 볼 수 있습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:376
msgid ""
"It is now possible to specify a mask in ``hw:cpu_realtime_mask`` without a "
"leading ``^``. When this is ommitted, the value will specify the cores that "
"should be included in the set of realtime cores, as opposed to those that "
"should be excluded."
msgstr ""
"``hw:cpu_realtime_mask``에 마스크를 지정할 수 있습니다. 이 마스크는 leading ``^``가 없을 때도 "
"가능합니다. 이 경우, 마스크가 생략되면 realtime 코어를 포함해야 하는 코어의 집합에 포함되어야 하는 코어를 제외해야 하는 코어를"
" 의미합니다."

#: ../../<reno.sphinxext stable/train>:795
msgid ""
"It is now possible to specify an ordered list of CPU models in the "
"``[libvirt] cpu_models`` config option. If ``[libvirt] cpu_mode`` is set to "
"``custom``, the libvirt driver will select the first CPU model in this list "
"that can provide the required feature traits."
msgstr ""
"이제는 libvirt cpu_models 속성을 통해 CPU 모델의 정렬된 목록을 spécifi할 수 있습니다. [libvirt] "
"cpu_mode 속성이 \"custom\"로 설정되면, libvirt 드라이버는 이 목록에서 첫 번째 CPU 모델을 선택하여 필요한 특성"
" trait를 제공할 수 있는 것을 선택합니다."

#: ../../<reno.sphinxext stable/pike>:567 stable/queens>:1798
msgid ""
"It is now possible to unset the ``[vnc]keymap`` and ``[spice]keymap`` "
"configuration options. These were known to cause issues for some users with "
"non-US keyboards and may be deprecated in the future."
msgstr ""
"이제는 `[vnc]keymap` 및 `[spice]keymap` 설정 옵션을 비활성화할 수 있습니다. 이러한 설정은 일부 사용자가 미국 "
"키보드가 아닌 키보드와 관련하여 문제를 일으킬 수 있으며 향후 비활성화될 수 있습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:490
msgid ""
"It was noted that on Arista VXLAN fabrics, testing showed that it required "
"several attempts of running the QEMU announce_self monitor command before "
"the switch would acknowledge a VM's new location on the fabric."
msgstr ""
"아리스타 VXLAN 소재에서 테스트를 수행한 결과, announce_self monitor 명령을 실행하는 데 여러 시도 후에 스위치가 "
"소재에 새로운 VM 위치를 인식할 때까지는 필요했다."

#: ../../<reno.sphinxext unmaintained/yoga>:281
msgid ""
"Keystone's policy concepts of system vs. project scope and roles has been "
"implemented in Nova and `defaults roles and scopes have been defined`__, "
"while legacy policies continue to be enabled by default. Operators are "
"encouraged to familiarize with the new policies and `enable them in "
"advance`__ before Nova switches from the legacy roles in a later release."
msgstr ""
"Keystone의 정책 개념인 시스템 및 프로젝트 범위와 역할은 Nova에서 구현되었으며, `기본 역할과 범위가 정의되었습니다`__, "
"반면 전통적인 정책은 기본적으로 활성화되어 있습니다. 운영자는 새로운 정책에 친숙해지도록 khuyến진하며, `이 정책을 전환하기 전에 "
"legacy 역할을 사용하는 후속 릴리즈에서 Nova가 사용할 때까지 미리 활성화해야 합니다`__."

#: ../../<reno.sphinxext ../source/mitaka.rst:79 ../source/newton.rst:147
#: ../source/newton.rst:183 ../source/newton.rst:247 ../source/newton.rst:608
#: origin/stable/ocata>:245 origin/stable/ocata>:440 origin/stable/ocata>:915
#: stable/pike>:372 stable/pike>:991 stable/queens>:263 stable/queens>:1155
#: stable/rocky>:1437 stable/stein>:231 stable/stein>:818 stable/train>:878
#: stable/ussuri>:159 stable/ussuri>:622 unmaintained/victoria>:121
#: unmaintained/victoria>:247 unmaintained/victoria>:485
#: unmaintained/wallaby>:194 unmaintained/wallaby>:564 unmaintained/xena>:220
#: unmaintained/xena>:455 unmaintained/yoga>:126 unmaintained/yoga>:507
#: unmaintained/zed>:395
msgid "Known Issues"
msgstr "알려된 문제"

#: ../../<reno.sphinxext stable/ussuri>:487
msgid "LXC instances now support cloud-init."
msgstr "LXC 인스턴스 현재 클라우드-인리트를 지원합니다."

#: ../../<reno.sphinxext stable/queens>:1341
msgid ""
"Legacy option ``url`` is deprecated and replaced by ``endpoint_override``.  "
"This should not need to be specified if an appropriate service catalog entry"
" exists for the network service."
msgstr ""
"기상 옵션 ``url``은弃용되었으며 ``endpoint_override``로 대체되었다.  네트워크 서비스에 대한 적절한 서비스 "
"카탈로그 엔트리가 존재할 경우에는 명시적으로 지정할 필요가 없기 때문에야 한다."

#: ../../<reno.sphinxext stable/stein>:462
msgid ""
"Libvirt compute nodes reporting VGPU inventory will have that VGPU inventory"
" and corresponding allocations moved to a child resource provider on restart"
" of the nova-compute service after upgrading to Stein."
msgstr ""
"libvirt 컴퓨팅 노드가 VGPU 인 inventory를 보고, VGPU inventory와 corresponding "
"allocations이 nova-compute 서비스를 재시작하고 스티恩 버전으로 업그레이드 한 후에 자식 리소스 제공자에 이동됩니다."

#: ../../<reno.sphinxext unmaintained/yoga>:436
msgid ""
"Libvirt supports parsing the VPD capability from PCI/PCIe devices and "
"exposing it via nodedev XML as of 7.9.0."
msgstr ""
"libvirt는 7.9.0부터 PCI/PCIe 장치에서 VPD 능력의 파싱을 지원하고 nodedev XML을 통해 노출합니다."

#: ../../<reno.sphinxext unmaintained/xena>:326
msgid ""
"Libvirt virt driver now supports any PCI device, not just virtual GPUs, that"
" are using the ``VFIO-mdev`` virtualization framework, like network adapters"
" or compute accelerators. `See more in the spec`__."
msgstr ""
"`libvirt virt driver`는 현재 `VFIO-mdev` 가상화 프레임워크를 사용하는 모든 PCI 장치, 가상 그래픽 카드만 "
"아니라 네트워크 어댑터나 컴퓨팅 가속화기와 같은 모든 장치를 지원합니다. `더 많은 정보는 spec`에서 확인하십시오."

#: ../../<reno.sphinxext stable/2024.2>:297
msgid ""
"Libvirt virt driver now supports launching instances with stateless "
"firmware. The new ``hw_firmware_stateless`` image property can be used to "
"enable this feature. Note that the feature can be used only for the "
"instances with UEFI firmware. This feature requires libvirt v8.6.0 or later."
msgstr ""
"libvirt virt 드라이버는 현재 stateless firmware를 사용하는 인스턴스를 시작할 수 있는 기능을 지원합니다. 새로운"
" `hw_firmware_stateless` 이미지 속성은 이 기능을 활성화할 수 있습니다. 그러나 이 기능은 UEFI firmware를"
" 사용하는 인스턴스만 사용할 수 있습니다. 이 기능은 libvirt v8.6.0 이상을 사용해야 합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:599
msgid ""
"Like SR-IOV, vDPA leverages DMA transfer between the guest and hardware. "
"This requires the DMA buffers to be locked in memory. As the DMA buffers are"
" allocated by the guest and can be allocated anywhere in the guest RAM, QEMU"
" locks **all** guest RAM. By default the ``RLIMIT_MEMLOCK`` for a normal "
"QEMU intance is set to 0 and qemu is not allowed to lock guest memory. In "
"the case of SR-IOV, libvirt automatically set the limit to guest RAM + 1G "
"which enables QEMU to lock the memory. This does not happen today with vDPA "
"ports. As a result if you use VDPA ports without enabling locking of the "
"guest memory you will get DMA errors. To workaround this issues until "
"libvirt is updated, you must set ``hw:cpu_realtime=yes`` and define a valid "
"``CPU-REALTIME-MASK`` e.g ``hw:cpu_realtime_mask=^0`` or define "
"``hw:emulator_threads_policy=share|isolate``. Note that since we are just "
"using ``hw:cpu_realtime`` for its side-effect of locking the guest memory, "
"this usage does not require the guest or host to use realtime kernels. "
"However, all other requirements of ``hw:cpu_realtime`` such as requiring "
"hw:cpu_policy=dedicated still apply. It is also stongly recommended that "
"hugpages be enabled for all instnace with locked memory. This can be done by"
" setting ``hw:mem_page_size``. This will enable nova to correctly account "
"for the fact that the memory is unswapable."
msgstr ""
"SR-IOV와 마찬가지로 vDPA는 게스트와 하드웨어 사이의 DMA 전송을 사용합니다. 이에 대한DMA 버퍼가 메모리에 고정되어야 "
"합니다. 게스트가 DMA 버퍼를 할당하고 게스트 RAM의ใด든지 할당할 수 있기 때문에 QEMU는 모든 게스트 RAM을锁鎖합니다. "
"기본적으로 QEMU의 정상 인스턴스에서는 `RLIMIT_MEMLOCK`가 0으로 설정되어 있으며 게스트 메모리를锁鎖할 수 없습니다. "
"SR-IOV의 경우, libvirt는 게스트 RAM + 1G를 설정하여 QEMU가 메모리를锁鎖할 수 있도록 합니다. 그러나 현재 vDPA"
" 포트에서는 이러한 경우가 발생하지 않습니다. 따라서 vDPA 포트를 사용하고 게스트 메모리를锁鎖하지 않으면 DMA 오류가 발생합니다. "
"libvirt가 업데이트될 때까지 이러한 문제를 해결하기 위해 must set `hw:cpu_realtime=yes`와 정상 `CPU-"
"REALTIME-MASK`를 정의하십시오. 예를 들어 `hw:cpu_realtime_mask=^0` 또는 "
"`hw:emulator_threads_policy=share|isolate`를 정의하십시오. 그러나 `hw:cpu_realtime`를 "
"사용하는 것은 게스트 또는 호스트가 리얼타임 커널을 사용할 필요가 없다는 점을 고려하여 guest 또는 host가 리얼타임 커널을 사용할"
" 필요가 없다는 점을 고려하여 사용할 수 있습니다. 그러나 `hw:cpu_realtime`의 모든 다른 요구 사항, 예를 들어 "
"`hw:cpu_policy=dedicated`와 같은 것은 still applies합니다. 또한, locked memory에 대한 "
"hugpages를 활성화하는 것이 strongly 추천됩니다. 이것은 `hw:mem_page_size`를 설정하여 할 수 있습니다. "
"이것은 nova가 메모리가 비스와이할 수 있는 사실을 correctly 계산할 수 있도록합니다."

#: ../../<reno.sphinxext stable/stein>:755
msgid ""
"Like the ``changes-since`` filter, the ``changes-before`` filter will also "
"return deleted servers."
msgstr "``changes-before`` 필터는 ``changes-since`` 필터와 마찬가지로 삭제된 서버를도 반환합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:216 unmaintained/xena>:459
msgid ""
"Linux guest images that have known kernel bugs related to virtualized apic "
"initialization previously would sporadically hang. For images where the "
"kernel cannot be upgraded, a ``[workarounds]`` config option has been "
"introduced:"
msgstr ""
"Linux 게스트 이미지에서 이전에 가상화된 APIC 초기화와 관련된 알려진 커널 버그가 있는 이미지에서는 비정상적으로 고정되었습니다. "
"커널을 업그레이드할 수 없는 이미지에서, ``[workarounds]`` 구성 옵션은 도입되었습니다."

#: ../../<reno.sphinxext stable/2025.2>:136
msgid "List migration:"
msgstr "집합 이동"

#: ../../<reno.sphinxext stable/2025.2>:122
msgid "List server migration:"
msgstr "집합 서버 이식"

#: ../../<reno.sphinxext origin/stable/ocata>:979
msgid ""
"Listing instances across multiple cells with a sort order will result in "
"barber-pole sorting, striped across the cell boundaries."
msgstr ""
"집합 내에서 여러 세ลล에 걸쳐 인스턴스를 정렬하면, 정렬 순서에 따라 세ลล 간 경계를 따라 가로-striped으로 barber-"
"pole sorting이 발생합니다."

#: ../../<reno.sphinxext stable/queens>:361 stable/rocky>:1999
msgid ""
"Listing server and migration records used to give a 500 to users when a cell"
" database was unreachable. Now only records from available cells are "
"included to avoid the 500 error. The down cells are basically skipped when "
"forming the results and this solution is planned to be further enhanced "
"through the `blueprint handling-down-cell`_."
msgstr ""
"서버와 이민 기록을 사용하여 사용자에게 500 오류를 발생시키는 세포 데이터베이스가 unreachable할 때. 현재는 사용 가능한 "
"세포만 기록을 포함하여 500 오류를 피하고 있습니다. down 세포는 결과를 형성할 때 기본적으로 skips됩니다. 이 해결책은 "
"`blueprint handling-down-cell`를 통해 더 나아가 확장될 예정입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:745
msgid ""
"Live migration is supported for both Virtuozzo containers and virtual "
"machines when using virt_type=parallels."
msgstr ""
"live migration은 virt_type=parallels를 사용할 때 virtuozzo 컨테이너와 가상 머신 모두에 지원됩니다."

#: ../../<reno.sphinxext stable/train>:953
msgid ""
"Live migration of an instance with PCI devices is now blocked in the "
"following scenarios:"
msgstr "PCI 장치가 있는 인스턴스의 live migration은 다음 상황에서 차단되었습니다."

#: ../../<reno.sphinxext stable/queens>:134 stable/rocky>:284
#: stable/stein>:944
msgid ""
"Live migration of instances with NUMA topologies is now disabled by default "
"when using the libvirt driver. This includes live migration of instances "
"with CPU pinning or hugepages. CPU pinning and huge page information for "
"such instances is not currently re-calculated, as noted in `bug #1289064`_. "
"This means that if instances were already present on the destination host, "
"the migrated instance could be placed on the same dedicated cores as these "
"instances or use hugepages allocated for another instance. Alternately, if "
"the host platforms were not homogeneous, the instance could be assigned to "
"non-existent cores or be inadvertently split across host NUMA nodes."
msgstr ""
"live migration of instances with NUMA topologies는 libvirt driver를 사용할 때 "
"기본적으로 disabled되었습니다. 이는 CPU pinning 또는 hugepages와 같은 instances의 live "
"migration을 포함합니다. 이러한 instances의 CPU pinning 및 huge page 정보는 현재 `bug "
"#1289064`_에서 noted된 것과 같이 재계산되지 않습니다. 이는 이미 destination host에 존재하는 instance가"
" 있는 경우, migrated instance가 동일한 dediced core에 배치되거나 다른 instance에 할당된 "
"hugepages를 사용할 수 있습니다. 또는, host platform이 homogeneous하지 않다면, instance는 non-"
"existent core에 할당되거나 host NUMA node를 무시하고 split할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:473
msgid ""
"Live migration support for servers with `SR-IOV ports "
"<https://docs.openstack.org/neutron/latest/admin/config-sriov>`_ attached "
"when using the libvirt compute driver."
msgstr ""
"live migration support for servers with `SR-IOV ports "
"<https://docs.openstack.org/neutron/latest/admin/config-sriov>`_ attached "
"when using the libvirt compute driver."

#: ../../<reno.sphinxext stable/train>:468
msgid ""
"Live migration support for servers with a `NUMA topology, pinned CPUs "
"<https://docs.openstack.org/nova/latest/admin/cpu-topologies.html>`_ and/or "
"`huge pages <https://docs.openstack.org/nova/latest/admin/huge-"
"pages.html>`_, when using the libvirt compute driver."
msgstr ""
"`NUMA拓도`의 서버에 대한 live migration 지원, `CPU Topology` "
"<https://docs.openstack.org/nova/latest/admin/cpu-topologies.html> 및/or "
"`huge pages` <https://docs.openstack.org/nova/latest/admin/huge-pages.html>를"
" 사용하는 libvirt compute driver와 함께, `NUMA拓도`의 서버에 대한 live migration 지원을 지원합니다."

#: ../../<reno.sphinxext stable/train>:964
msgid "Live migration will fail with a user friendly error."
msgstr "집합 Live migration will fail with a user friendly error."

#: ../../<reno.sphinxext stable/2025.2>:107
msgid "Live migration:"
msgstr "집합 이동"

#: ../../<reno.sphinxext stable/rocky>:934
msgid ""
"Live migrations from nodes not compatible with file backed memory to nodes "
"with file backed memory is not allowed, and will result in an error. It's "
"recommended to upgrade all nodes before enabling file backed memory."
msgstr ""
"live migration에서 file-backed memory가 compatibility되지 않는 노드에서 live migration을"
" 수행하거나, file-backed memory를 사용할 수 있는 노드에 live migration을 수행하는 것은 허용되지 않으며, "
"오류가 발생할 수 있습니다. file-backed memory를 사용할 수 있는 노드에 모든 노드를 업그레이드하는 것을 권장합니다."

#: ../../<reno.sphinxext stable/ussuri>:1139
msgid ""
"Long standing `bug 1694844`_ is now fixed where the following conditions "
"would lead to a 400 error response during server create:"
msgstr ""
"장기적으로 `bug 1694844`_은 현재 다음과 같은 조건이 서버 생성 시 400 오류 응답을 일으킬 수 있는 경우를 해결했습니다."

#: ../../<reno.sphinxext unmaintained/zed>:251
msgid ""
"Make public_key a mandatory parameter for keypair creation. This means that "
"by this microversion, Nova will stop to support automatic keypair "
"generations. Only imports will be possible."
msgstr ""
"public_key를 keypair 생성에 의무적인 매개 변수로 설정한다. 이는 이 마이크로 버전에서 Nova가 自動 키 페어 생성을 "
"지원하지 않게 된다는 것을 의미한다. 이에 따라 이 마이크로 버전부터는 only imports가 가능하다."

#: ../../<reno.sphinxext stable/2025.2>:306
msgid ""
"Make sure the configured nova service user in other services has the "
"``service`` role otherwise communication from the other services to Nova "
"will fail. For example, user configured as ``username`` option in "
"``neutron.conf`` file under ``[nova]`` section has the ``service`` role."
msgstr ""
"nova 서비스를 구성된 사용자로 다른 서비스에서 사용되는 경우, 다른 서비스에서 nova 서비스로의 통신이 실패할 수 있습니다. 예를 "
"들어, neutron.conf 파일의 [nova] 섹션에 username 옵션으로 구성된 사용자는 service 역할을 가지고 있습니다."

#: ../../<reno.sphinxext stable/train>:606
msgid ""
"Making server representation always consistent among GET, PUT and Rebuild "
"serevr APIs response. ``PUT /servers/{server_id}`` and ``POST "
"/servers/{server_id}/action {rebuild}`` API response is modified to add all "
"the missing fields which are return by ``GET /servers/{server_id}``."
msgstr ""
"서버를 always GET, PUT 및 Rebuild server APIs의 response에서 일관성 있는 형태로 표현하도록 하며, "
"``PUT /servers/{server_id}`` 및 ``POST /servers/{server_id}/action "
"{rebuild}`` API response는 GET /servers/{server_id}의 모든 missing field를 추가하여 "
"수정합니다."

#: ../../<reno.sphinxext stable/2025.1>:194
msgid ""
"Manila shares can now be directly attached to instances by a new Nova shares"
" API."
msgstr "Manila 공유는 이제 Nova 공유 API를 통해 인스턴스에 직접 연결할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:688
msgid ""
"Mediated devices that are created by the libvirt driver are not persisted "
"upon reboot. Consequently, a guest startup would fail since the virtual "
"device wouldn't exist. In order to prevent that issue, when restarting the "
"compute service, the libvirt driver now looks at all the guest XMLs to check"
" if they have mediated devices, and if the mediated device no longer exists,"
" then Nova recreates it by using the same UUID."
msgstr ""
"libvirt 드라이버가 만든 매개화된 장치가 재부팅 시 지속되지 않으며, 이로 인해 게스트 시작이 실패하게 되고, 가상 장치가 존재하지"
" 않기 때문이다. 이러한 문제를 방지하기 위해, 컴퓨터 서비스를 재시작할 때, libvirt 드라이버는 모든 게스트 XML을 확인하여 "
"매개화된 장치가 존재하는지 확인하고, 매개화된 장치가 더 이상 존재하지 않으면, Nova는 동일한 UUID를 사용하여 다시 생성한다."

#: ../../<reno.sphinxext stable/train>:651
msgid ""
"Memory encryption can be required either via a flavor which has the "
"``hw:mem_encryption`` extra spec set to ``True``, or via an image which has "
"the ``hw_mem_encryption`` property set to ``True``. These do not inherently "
"cause a preference for SEV-capable hardware, but for now SEV is the only way"
" of fulfilling the requirement.  However in the future, support for other "
"hardware-level guest memory encryption technology such as Intel MKTME may be"
" added.  If a guest specifically needs to be booted using SEV rather than "
"any other memory encryption technology, it is possible to ensure this by "
"adding ``trait:HW_CPU_X86_AMD_SEV=required`` to the flavor extra specs or "
"image properties."
msgstr ""
"메모리 암호화가 필요할 때는 either flavor에 hw:mem_encryption 속성이 True로 설정된 경우 또는 image에 "
"hw_mem_encryption 속성이 True로 설정된 경우에 따라 필요합니다. 이러한 경우에는 SEV- capable 하드웨어에 대한"
" 선호도가 inherent로 인해 발생하지 않지만 현재 SEV가만 fulfillment을 할 수 있는 방법입니다. 그러나 미래에 다른 "
"하드웨어 수준의 게스트 메모리 암호화 기술인 Intel MKTME의 지원이 추가될 수 있습니다. 게스트가 SEV 대신 다른 메모리 암호화"
" 기술을 사용하여 부팅해야 하는 경우, hw_CPU_X86_AMD_SEV=required trait를 flavor의 extra spec "
"또는 image의 속성으로 추가하여 보장할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:946
msgid ""
"Microversion 1.13 of the Placement API gives the ability to set or clear "
"allocations for more than one consumer uuid with a request to ``POST "
"/allocations``."
msgstr ""
"Microversion 1.13 of the Placement API는 `POST /allocations`를 사용하여 하나 이상의 소비자"
" uuid에 대한 할당을 설정하거나 해제할 수 있는 능력을 제공합니다."

#: ../../<reno.sphinxext stable/stein>:685
msgid ""
"Microversion 1.30 of the placement API adds support for a ``POST /reshaper``"
" resource that provides for atomically migrating resource provider "
"inventories and associated allocations when some of the inventory moves from"
" one resource provider to another, such as when a class of inventory moves "
"from a parent provider to a new child provider."
msgstr ""
"Microversion 1.30 of the placement API에 ``POST /reshaper`` 리소스를 추가하여 원자적으로 "
"리소스 제공자 인 inventory와 관련된 할당을 atomically migrate 시키는 capability을 제공합니다. 예를 "
"들어, parent provider에서 new child provider로 inventory의 class가 이동할 때, 일부 "
"inventory가 다른 resource provider로 이동할 때, 이러한 할당은 atomically migrate됩니다."

#: ../../<reno.sphinxext stable/pike>:939
msgid ""
"Microversion 2.49 brings device role tagging to the attach operation of "
"volumes and network interfaces. Both network interfaces and volumes can now "
"be attached with an optional ``tag`` parameter. The tag is then exposed to "
"the guest operating system through the metadata API. Unlike the original "
"device role tagging feature, tagged attach does not support the config "
"drive. Because the config drive was never designed to be dynamic, it only "
"contains device tags that were set at boot time with API 2.32. Any changes "
"made to tagged devices with API 2.49 while the server is running will only "
"be reflected in the metadata obtained from the metadata API. Because of "
"metadata caching, changes may take up to ``metadata_cache_expiration`` to "
"appear in the metadata API. The default value for "
"``metadata_cache_expiration`` is 15 seconds."
msgstr ""
"Microversion 2.49은 볼륨과 네트워크 인터페이스를 위한.attach 연산에서 장치 역할 태그를 도입합니다. 네트워크 "
"인터페이스와 볼륨은 모두 옵셔널 ``tag`` 매개 변수를 사용하여.attach할 수 있습니다. 태그는 then guest "
"operating system에 metadata API를 통해 노출됩니다. 원래 장치 역할 태그 기능과는 달리 태그.attach는 "
"config drive를 지원하지 않습니다. config drive는 never designed to be dynamic이기 때문에, "
"API 2.32와 함께 부트 타임에 설정된 장치 태그만 포함됩니다. API 2.49를 사용하여 태그된 장치에 변경 사항을 적용할 때, "
"서버가 실행 중인 경우 변경 사항은 metadata API에서 얻은 metadata에만 반영됩니다. metadata 캐싱의缘故, 변경 "
"사항은 metadata_cache_expiration의 \"metadata_cache_expiration\"에 도달할 때까지 나타날 수 "
"있습니다. 기본값은 15초입니다."

#: ../../<reno.sphinxext stable/pike>:705
msgid ""
"Microversion 2.53 changes service and hypervisor IDs to UUIDs to ensure "
"uniqueness across cells. Prior to this, ID collisions were possible in "
"multi-cell deployments. See the `REST API Version History`_ and `Compute API"
" reference`_ for details."
msgstr ""
"Microversion 2.53는 세포 간 uniqueness를 보장하기 위해 서비스 ID와 하이퍼바이저 ID를 UUID로 변경했다. 이전에, ID 충돌이 가능했다. 세포가 여러 세포로 배포된 경우. `REST API Version History`_ 및 `Compute API reference`_을 참조하십시오.\n"
"\n"
"*   `REST API Version History`_은 [https://docs.openstack.org/api-ref/](https://docs.openstack.org/api-ref/)에 있습니다.\n"
"*   `Compute API reference`_은 [https://docs.openstack.org/api-ref/compute/](https://docs.openstack.org/api-ref/compute/)에 있습니다."

#: ../../<reno.sphinxext stable/queens>:853
msgid ""
"Microversion 2.55 adds a ``description`` field to the flavor resource in the"
" following APIs:"
msgstr ""
"Microversion 2.55는 다음 API에서 flavor 리소스에 description 필드를 추가합니다.\n"
"\n"
"- 집합"

#: ../../<reno.sphinxext stable/rocky>:752
msgid ""
"Microversion 2.64 is added and enables users to define rules on server group"
" policy to meet more advanced policy requirements. This microversion brings "
"the following changes in server group APIs:"
msgstr ""
"Microversion 2.64이 추가되어 사용자들이 서버 그룹 정책에 대한 규칙을 정의할 수 있도록 더 avanz한 정책 요구 사항을 "
"đáp ứng할 수 있도록 한다. 이 Microversion은 다음 서버 그룹 API에 대한 변경 사항을 제공한다."

#: ../../<reno.sphinxext stable/stein>:751
msgid ""
"Microversion 2.66 adds the optional filter parameter ``changes-before`` "
"which can be used to get resources changed before or equal to the specified "
"date and time."
msgstr ""
"Microversion 2.66은 changes-before 매개 변수가 있는 선택적 필터를 추가합니다. 이 매개 변수를 사용하여指定된 "
"날짜와 시간 이전에 또는 동일한 날짜와 시간에 변경된 리소스를 얻을 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:503
msgid ""
"Microversion 2.67 adds the optional parameter ``volume_type`` to "
"block_device_mapping_v2, which can be used to specify ``volume_type`` when "
"creating a server. This would only apply to BDMs with ``source_type`` of "
"`blank`, `image` and `snapshot` and ``destination_type`` of `volume`. The "
"compute API will reject server create requests with a specified "
"``volume_type`` until all nova-compute services are upgraded to Stein."
msgstr ""
"Microversion 2.67은 블록 디바이스 매핑 v2에 옵셔널 파라미터 `volume_type`를 추가합니다. 이 파라미터는 서버를"
" 생성할 때 `volume_type`를 지정할 수 있습니다. 이 경우에는 `source_type`가 `blank`, `image` 및 "
"`snapshot` 인 BDMs만 적용됩니다. 또한 `destination_type`가 `volume` 인 BDMs만 적용됩니다. "
"Compute API는 Stein 업그레이드를 완료한 모든 nova-compute 서비스가 적용되기까지 `volume_type`를 지정한"
" 서버 생성 요청을 거부합니다."

#: ../../<reno.sphinxext stable/train>:705
msgid ""
"Microversion 2.77 adds the optional parameter ``availability_zone`` to the "
"``unshelve`` server action API."
msgstr ""
"Microversion 2.77은 \"availability_zone\" 옵셔널 파라미터를 \"unshelve\" 서버 액션 API에 "
"추가합니다."

#: ../../<reno.sphinxext stable/train>:553
msgid ""
"Microversion 2.78 adds a new ``topology`` sub-resource to the servers API:"
msgstr "Microversion 2.78는 새로운 \"topology\" 서브 리소스를 서버 API에 추가합니다."

#: ../../<reno.sphinxext stable/train>:719
msgid ""
"Microversion 2.79 adds support for specifying the ``delete_on_termination`` "
"field in the request body when attaching a volume to a server, to support "
"configuring whether to delete the data volume when the server is destroyed. "
"Also, ``delete_on_termination`` is added to the GET responses when showing "
"attached volumes."
msgstr ""
"Microversion 2.79은 서버에 볼륨을 부착할 때 요청 바디에 ``delete_on_termination`` 필드를 지정하여 "
"데이터 볼륨을 파괴할 때 삭제할지 여부를 구성할 수 있는 지원을 추가합니다. 또한, 파괴된 서버를 표시할 때 attached "
"volumes를 표시할 때 ``delete_on_termination`` 필드를 추가합니다."

#: ../../<reno.sphinxext stable/ussuri>:454
msgid ""
"Microversion 2.80 changes the list migrations APIs and the os-migrations "
"API."
msgstr "Microversion 2.80은 migration API의 목록을 변경하고 os-migrations API를 변경합니다."

#: ../../<reno.sphinxext unmaintained/xena>:340
msgid ""
"Microversion 2.89 has been introduced and will include the ``attachment_id``"
" of a volume attachment, ``bdm_uuid`` of the block device mapping record and"
" removes the duplicate ``id`` from the responses for ``GET "
"/servers/{server_id}/os-volume_attachments`` and ``GET "
"/servers/{server_id}/os-volume_attachments/{volume_id}``."
msgstr ""
"Microversion 2.89가 도입되었으며, 볼륨 연결의 ``attachment_id``, 블록 장치 매핑 레코드의 "
"``bdm_uuid``를 포함하고, ``GET /servers/{server_id}/os-volume_attachments`` 및 "
"``GET /servers/{server_id}/os-volume_attachments/{volume_id}``의 응답에서 중복되는 "
"``id``를 제거합니다."

#: ../../<reno.sphinxext unmaintained/zed>:268
msgid ""
"Microversion 2.91 adds the optional parameter ``host`` to the ``unshelve`` "
"server action API. Specifying a destination host is only allowed to admin "
"users and server status must be ``SHELVED_OFFLOADED`` otherwise a HTTP 400 "
"(bad request) response is returned. It also allows to set "
"``availability_zone`` to None to unpin a server from an availability_zone."
msgstr ""
"Microversion 2.91은 \"host\"라는 선택적 매개 변수를 \"unshelve\" 서버 액션 API에 추가합니다. 목적지 "
"호스트를 지정하는 것은만 admin 사용자에게만 허용되며, 서버 상태가 \"SHELVED_OFFLOADED\"로 지정되지 않으면 HTTP"
" 400 (bad request) 응답이 반환됩니다. 또한, \"availability_zone\"를 None으로 설정하여 서버를 "
"availability_zone에서 해제할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:675
msgid ""
"Migrating an instance to another host will have the same problem as resize. "
"In case you want to migrate an instance, make sure to rebuild it."
msgstr ""
"인스턴스를 다른 호스트로 이동할 때는 리사이즈와 동일한 문제가 발생합니다. 인스턴스를 이동하고 싶은 경우, 다시 구축해야 합니다."

#: ../../<reno.sphinxext stable/train>:72 stable/ussuri>:36
#: unmaintained/victoria>:89 unmaintained/wallaby>:290 unmaintained/xena>:743
msgid ""
"Minimizes a race condition window when using the ``ironic`` virt driver "
"where the data generated for the Resource Tracker may attempt to compare "
"potentially stale instance information with the latest known baremetal node "
"information. While this doesn't completely prevent nor resolve the "
"underlying race condition identified in `bug 1841481 "
"<https://bugs.launchpad.net/nova/+bug/1841481>`_, this change allows Nova to"
" have the latest state information, as opposed to state information which "
"may be out of date due to the time which it may take to retrieve the status "
"from Ironic. This issue was most observable on baremetal clusters with "
"several thousand physical nodes."
msgstr ""
"``ironic`` virt 드라이버를 사용할 때, 리소스 트래커에서 생성된 데이터가 potencialy stale instance "
"정보와 가장 최근의 baremetal 노드 정보를 비교할 수 있는 race condition 윈도우를 최소화합니다. 이 경우, bug "
"1841481 <https://bugs.launchpad.net/nova/+bug/1841481>`_에 의해 식별된 하위 race "
"condition을 완전히 방지하거나 해결하지는 않지만, 이 변경은 Nova가 가장 최근의 상태 정보를 가질 수 있게 합니다. 이 경우,"
" Ironic에서 상태를 확인하기 위해 필요한 시간으로 인해 상태 정보가 outdated일 수 있습니다. 이 문제는 가장 많이 관찰되는 "
"baremetal 클러스터에서 수천 개의 물리적 노드가 있습니다."

#: ../../<reno.sphinxext stable/queens>:1576 stable/rocky>:1705
msgid ""
"Monkey patching nova is not tested, not supported, and is a barrier to "
"interoperability. If you have code which relies on monkey patching "
"decorators, for example, for notifications, please propose those changes "
"upstream."
msgstr ""
"monkey patching nova는 테스트되지 않으며 지원되지 않으며 interoperability의 장애물입니다. "
"notification에 대한 decorator를 사용하는 코드가 있으면, 예를 들어, upstream에 제안하세요."

#: ../../<reno.sphinxext stable/queens>:1848
msgid "More details can be found in the spec:"
msgstr "More details can be found in the 집합:"

#: ../../<reno.sphinxext origin/stable/ocata>:1254
msgid ""
"Most quota options have been moved into their own configuration group. The "
"exception is quota_networks as it is an API flag not a quota flag. These "
"options are as below:"
msgstr ""
"다음은 glossary에 정의된 단어를 사용하여 번역합니다.\n"
"\n"
"Most quota options have been moved into their own configuration group. The exception is quota_networks as it is an API flag not a quota flag. These options are as below: \n"
"\n"
"* \"Most\" → \"대부분\"\n"
"* \"quota options\" → \"quota 옵션\"\n"
"* \"have been moved\" → \"이동되었습니다\"\n"
"* \"into their own configuration group\" → \"그들의 own configuration group\"\n"
"* \"The exception\" → \"예외\"\n"
"* \"is quota_networks\" → \"quota_networks\"\n"
"* \"as it is an API flag\" → \"API flag로\"\n"
"* \"not a quota flag\" → \"quota flag로\"\n"
"* \"These options are\" → \"이러한 옵션은\"\n"
"* \"as below\" → \"아래\"\n"
"\n"
"다음은 번역된 텍스트입니다.\n"
"\n"
"대부분 quota 옵션은 그들의 own configuration group에 이동되었습니다. 예외는 quota_networks입니다. 이는 API flag로, quota flag로 नह습니다. 이러한 옵션은 아래에 있습니다."

#: ../../<reno.sphinxext stable/stein>:730
msgid ""
"Moving (resizing, migrating, live-migrating, evacuating, unshelving after "
"shelve offload) servers with ports having resource request is not yet "
"supported."
msgstr ""
"집합 (resizing, migrating, live-migrating, evacuating, unshelving after shelve"
" offload) 서버에 리소스 요청이 있는 포트를 이동하지 않는다."

#: ../../<reno.sphinxext stable/train>:602
msgid "Multiple API cleanups is done in API microversion 2.75:"
msgstr "API microversion 2.75에서 여러 API 청소가 수행됩니다."

#: ../../<reno.sphinxext stable/train>:960
msgid "Neutron does not support extended port binding API."
msgstr "네트론은 확장 포트 결합 API를 지원하지 않는다."

#: ../../<reno.sphinxext origin/stable/ocata>:515
msgid "Neutron is now the default configuration for new deployments."
msgstr "네트론은 새로운 배포에 대한 기본 구성이되었습니다."

#: ../../<reno.sphinxext stable/pike>:1131 stable/pike>:1161
msgid "Neutron port filtering is disabled/unsupported"
msgstr "네트론 포트 필터링이 비활성화/지지되지 않습니다."

#: ../../<reno.sphinxext ../source/mitaka.rst:171 ../source/newton.rst:310
#: branch>:10 current origin/stable/ocata>:564 stable/2023.2>:235
#: stable/2024.1>:377 stable/2024.2>:265 stable/2025.1>:222 stable/2025.2>:69
#: stable/pike>:635 stable/queens>:629 stable/rocky>:564 stable/stein>:485
#: stable/train>:534 stable/ussuri>:403 unmaintained/2023.1>:322
#: unmaintained/victoria>:10 unmaintained/victoria>:366
#: unmaintained/wallaby>:162 unmaintained/wallaby>:433 unmaintained/xena>:206
#: unmaintained/xena>:336 unmaintained/yoga>:315 unmaintained/zed>:235
msgid "New Features"
msgstr "집합"

#: ../../<reno.sphinxext stable/ussuri>:1096
msgid "New Option"
msgstr "New Option → 새로운 옵션"

#: ../../<reno.sphinxext unmaintained/victoria>:392
msgid ""
"New ``[glance]/enable_rbd_download`` config option was introduced. The "
"option allows for the configuration of direct downloads of Ceph hosted "
"glance images into the libvirt image cache via rbd when "
"``[glance]/enable_rbd_download= True`` and ``[glance]/rbd_user``, "
"``[glance]/rbd_pool``, ``[glance]/rbd_connect_timeout``, "
"``[glance]/rbd_ceph_conf`` are correctly configured."
msgstr ""
"New `[glance]/enable_rbd_download` 설정 옵션이 도입되었다. 이 옵션은 Ceph 호스팅 된 glance "
"이미지의 직접적인 다운로드를 libvirt 이미지 캐시에 via rbd로 구성할 수 있도록 한다. "
"`[glance]/enable_rbd_download=True`와 `[glance]/rbd_user`, "
"`[glance]/rbd_pool`, `[glance]/rbd_connect_timeout`, "
"`[glance]/rbd_ceph_conf`가 올바르게 구성된 경우에만."

#: ../../<reno.sphinxext unmaintained/yoga>:408
msgid ""
"New ``nova-manage image_property`` commands have been added to help update "
"instance image properties that have become invalidated by a change of "
"instance machine type."
msgstr ""
"nova-manage image_property 명령어는 인스턴스 이미지 속성의 유효성을失った 인스턴스 머신 타입이 변경된 경우 "
"업데이트할 수 있는 인스턴스 이미지 속성에 도움을 주는 새로운 명령어가 추가되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:361
msgid ""
"New ``nova-manage placement audit`` CLI command to `find and clean up "
"orphaned resource allocations`__."
msgstr "``nova-manage placement audit`` 명령을 사용하여 orphaned 리소스 할당을 찾고 정리합니다."

#: ../../<reno.sphinxext stable/2025.1>:339
msgid ""
"New configuration options ``[quota]unified_limits_resource_strategy`` and "
"``[quota]unified_limits_resource_list`` have been added to enable operators "
"to specify a list of resources that are either required or ignored to have "
"registered limits set. The default strategy is ``require`` and the default "
"resource list contains ``servers``. The configured list is only used when "
"``[quota]driver`` is set to the ``UnifiedLimitsDriver``."
msgstr ""
"`[quota]unified_limits_resource_strategy` 및 "
"`[quota]unified_limits_resource_list`를 사용하여 운영자가 등록한 제한을 설정할 수 있는 리소스 목록을 "
"지정할 수 있는 새로운 구성 옵션 `quota`가 추가되었습니다. 기본 전략은 `require`이며 기본 리소스 목록에는 "
"`servers`가 포함됩니다. 구성된 목록은 `UnifiedLimitsDriver`로 설정된 `quota` 드라이버만 사용됩니다."

#: ../../<reno.sphinxext stable/2025.1>:190
msgid ""
"New kernel vfio-PCI variant drivers like nvidia GRID on Ubuntu 24.04 are now"
" supported by our PCI passthrough feature. You can create instances using "
"those specific PCI devices but also live migrate them."
msgstr ""
"새로운 커널 vfio-PCI 변형 드라이버 like nvidia GRID Ubuntu 24.04는 현재 우리의 PCI 패스-through"
" 기능에 의해 지원됩니다. PCI 장치의 특정 장치로 인스턴스를 생성할 수 있지만 또한 live migrate 할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:1018
msgid ""
"New paste pipelines and middleware have been created to allow API version "
"discovery to be performed without authentication or redirects. Because this "
"involves an ``api-paste.ini`` change, you will need to manually update your "
"``api-paste.ini`` with the one from the release to get this functionality."
msgstr ""
"새 pasti 파이프라인과 미드웨어가 API 버전 발견을 비-authentication 또는 리다이렉션으로 수행할 수 있도록 "
"생성되었습니다. 이에 대한 변경은 ``api-paste.ini``를 변경하는 것이므로, 이 기능을 사용하려면 릴리스에서 제공하는 "
"``api-paste.ini``를 手動으로 업데이트해야 합니다."

#: ../../<reno.sphinxext stable/queens>:942
msgid ""
"New placement REST API microversion 1.14 is added to support nested resource"
" providers. Users of the placement REST API can now pass a "
"``in_tree=<UUID>`` parameter to the ``GET /resource_providers`` REST API "
"call.  This will trigger the placement service to return all resource "
"provider records within the \"provider tree\" of the resource provider with "
"the supplied UUID value. The resource provider representation now includes a"
" ``parent_provider_uuid`` value that indicates the UUID of the immediate "
"parent resource provider, or ``null`` if the provider has no parent. For "
"convenience, the resource provider resource also contains a "
"``root_provider_uuid`` field that is populated with the UUID of the top-most"
" resource provider in the provider tree."
msgstr ""
"새 집합 REST API 마이크로 버전 1.14이 집합 자원 제공자에 대한 지원을 위해 추가되었다. 집합 자원 제공자 사용자들은 now "
"집합 자원 제공자 REST API 호출에 ``in_tree=<UUID>`` 매개 변수를 passing 할 수 있다.  이 매개 변수는 "
"제공되는 UUID 값의 \"provider tree\"에 있는 모든 자원 제공자 레코드를 반환하도록 집합 서비스를 활성화한다.  자원 "
"제공자 표현은 now \"parent_provider_uuid\" 값이 즉시 상위 자원 제공자 UUID를 나타내거나 \"null\"이면 "
"자원 제공자가 상위 자원을 가지고 있지 않다는 것을 나타낸다.  편의를 위해, 자원 제공자 자원도 "
"\"root_provider_uuid\" 필드가 포함되어 있으며, \"provider tree\"의 상위-most 자원 제공자 UUID가"
" population되며, \"provider tree\"에 대한 모든 자원 제공자 레코드를 반환하도록 집합 서비스를 활성화한다."

#: ../../<reno.sphinxext stable/2025.2>:277
msgid ""
"New policies are added to the live migration APIs with the same default. If "
"you are using default policy, then no action is needed, but if you have "
"overridden the existing live migration policies in your deployment, you must"
" include the new policy with the same permissions."
msgstr ""
"새 정책이 live migration API에 추가되며 기본 정책과 동일합니다. 기본 정책을 사용하는 경우, keine acción이 "
"필요합니다. 그러나 배포에서 существ하는 live migration 정책을 오버라이드 한 경우, 새로운 정책을 동일한 권한으로 "
"포함해야 합니다."

#: ../../<reno.sphinxext stable/2025.2>:130 stable/2025.2>:144
msgid "New policy is used to host info in live migrations list:"
msgstr "새 정책은 live migration list에 정보를 호스트하는 데 사용됩니다."

#: ../../<reno.sphinxext stable/2025.2>:116
msgid "New policy is used when live migrate server to a specific host:"
msgstr "새 정책은 live migrate server를 특정 호스트로 이동할 때 사용합니다."

#: ../../<reno.sphinxext stable/2025.2>:288
msgid "New policy:"
msgstr "New policy:"

#: ../../<reno.sphinxext stable/queens>:429 stable/rocky>:1521
msgid "No cell mappings are found"
msgstr "집합 매핑이 발견되지 않습니다"

#: ../../<reno.sphinxext stable/ussuri>:592
msgid ""
"No changes have been made to the request or reponse parameters of the rescue"
" API itself."
msgstr ""
"사용자 API 자체의 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 "
"사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 "
"없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며,"
" 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적"
" 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 "
"요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및"
" 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 "
"매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 "
"변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 "
"대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 "
"변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 "
"사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 "
"없으며, 구체적 인 요청 및 응답 매개 변수에 대한 변경 사항이 없으며, 구체적 인 요청 및 응답 매개 변수"

#: ../../<reno.sphinxext stable/pike>:1026 stable/queens>:1175
msgid "Node properties were modified in ironic for a deployed nodes"
msgstr "노드 속성은 ironic에서 배포된 노드에 대해 수정되었습니다."

#: ../../<reno.sphinxext stable/stein>:646
msgid ""
"Note if you configure Nova to have no timeout, post copy will never be "
"automatically triggered. None of this affects triggering post copy via the "
"force live-migration API, that continues to work in the same way."
msgstr ""
"노바를 timeout이 없는 경우에 대해 주의하십시오. timeout이 없는 경우에 post copy가 자동으로 트리거되지 않습니다. "
"이와 관련된 모든 것은 force live-migration API를 통해 post copy를 강제로 트리거하는 경우에도 동일한 방식으로"
" 작동합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:230 stable/pike>:256
#: stable/queens>:523 stable/rocky>:915
msgid ""
"Note that besides ``custom``, Nova's libvirt driver has two other CPU modes:"
" ``host-model`` (which is the default), and ``host-passthrough``.  Refer to "
"the ``[libvirt]/cpu_model_extra_flags`` documentation for what to do when "
"you are using either of those CPU modes in context of 'PCID'."
msgstr ""
"``custom`` 이외에 노바의 libvirt 드라이버는 ``host-model`` (기본값)과 ``host-passthrough`` "
"이라는 두 가지 CPU 모드를 가지고 있습니다.  PCID에 대한 사용에 대한 context에서 "
"``[libvirt]/cpu_model_extra_flags`` 문서를 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:265
msgid ""
"Note that disabling eventlet monkey-patching will cause queries across "
"multiple cells to be serialized instead of running in parallel and this may "
"be undesirable in a large deployment with multiple cells, for performance "
"reasons."
msgstr ""
"집합에 걸리는 이벤트 레트 (eventlet) 모니 patching을 비활성화하면 여러 세포에서 걸리는 쿼리를 시리얼라이즈 "
"(serialize) 하는 대신 병렬로 실행하는 것이 가능하다. 이것은 성능 이유로 큰 배포에서 여러 세포가 있는 경우에 불리할 수 "
"있다."

#: ../../<reno.sphinxext stable/rocky>:938
msgid ""
"Note that file backed memory is not compatible with hugepages, and is not "
"compatible with memory overcommit. If file backed memory is enabled, "
"``ram_allocation_ratio`` must be configured to ``1.0``"
msgstr ""
"파일-backed 메모리 (file backed memory)는 큰 페이지 (hugepages)와 호환되지 않으며, 메모리 오버 커밋 "
"(memory overcommit)과 호환되지 않습니다. 파일-backed 메모리가 활성화된 경우, "
"`ram_allocation_ratio`는 `1.0`로 구성되어야 합니다."

#: ../../<reno.sphinxext stable/pike>:973
msgid ""
"Note that multiple paths will exist for a tagged disk for the following "
"reasons:"
msgstr "집합이标记된 디스크에 대해 여러 경로가 존재할 수 있습니다. 다음 이유로."

#: ../../<reno.sphinxext stable/train>:928
msgid ""
"Note that non-raw cache image files will be removed if you set "
"force_raw_images = True and images_type = rbd now."
msgstr ""
"집합이 아닌 비raw 캐시 이미지 파일이 force_raw_images = True 및 images_type = rbd 가 설정된 경우에"
" 대해 제거됩니다."

#: ../../<reno.sphinxext stable/rocky>:211 stable/stein>:1293
msgid ""
"Note that only the image owner may delete the image, so in the case of a "
"shelved offloaded server, if the user unshelves or deletes the server, that "
"operation will work but there will be a warning in the logs because the "
"shelved snapshot image could not be deleted since the user does not own the "
"image. Similarly, if an admin creates a snapshot of a server in another "
"project, the admin owns the snapshot image and the non-admin project, while "
"having shared image member access to see the image, cannot delete the "
"snapshot."
msgstr ""
"이미지 소유자는 이미지를 삭제할 수만이므로, 셰이블드 오프로드된 서버의 경우, 사용자가 셰이블드 snapshots 이미지의 소유자가 아닌"
" 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 snapshots "
"이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 "
"snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 주의해야 합니다. "
"사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 "
"주의해야 합니다. 사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 "
"수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots "
"이미지를 삭제할 수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 "
"snapshots 이미지를 삭제할 수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, "
"사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 snapshots 이미지의 "
"소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 "
"snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 주의해야 합니다. "
"사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 수 없다는 것을 "
"주의해야 합니다. 사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots 이미지를 삭제할 "
"수 없다는 것을 주의해야 합니다. 사용자가 셰이블드 snapshots 이미지의 소유자가 아닌 경우, 사용자가 셰이블드 snapshots "
"이미지를 삭제할 수 없다는 것을 주의해야 합니다. 사용자가"

#: ../../<reno.sphinxext unmaintained/yoga>:492
msgid ""
"Note that the PCI devices (VFs or, alternatively, their PF) must have a "
"valid PCI Vital Product Data (VPD) with a serial number present in it for "
"this feature to work properly. Also note that only VFs can be tagged as "
"``remote_managed: \"true\"`` and they cannot be used for legacy SR-IOV use-"
"cases."
msgstr ""
"PCI 장치 (VFs 또는 대체적으로 PF)를 사용하려면 PCI Vital Product Data (VPD)가 유효하고 serial "
"number이 포함되어 있는지 확인해야 합니다. 또한, 이 기능이 제대로 작동하려면 VFs만 \"remote_managed: true\""
" 라고 태그할 수 있으며 legacy SR-IOV 사용 사례에 사용할 수 없습니다."

#: ../../<reno.sphinxext stable/rocky>:2031 stable/stein>:1355
msgid ""
"Note that the original fix for `bug 1414559`_ committed early in rocky was "
"automatic and always enabled. Because of `bug 1786346`_ that fix has since "
"been reverted and superseded by an opt-in mechanism which must be enabled. "
"Setting ``[compute]/live_migration_wait_for_vif_plug=True`` will restore the"
" behavior of `waiting for neutron events`_ during the live migration "
"process."
msgstr ""
"`bug 1414559`_의 원래修复은 `rocky`에서 초기에 tự động적으로 적용되었으며 always enabled 상태였다. "
"`bug 1786346`_의修复으로 인해 이修复은 이후에 취소되었고, 옵션을 활성화해야 하는 opt-in 메커니즘에 의해 대체되었다. "
"`compute`의 `[live_migration_wait_for_vif_plug=True]`를 설정하면 `neutron events`를"
" 대기하는 `live migration` 프로세스에서 `waiting for`의 행동을 복원할 수 있다."

#: ../../<reno.sphinxext stable/rocky>:425
msgid ""
"Note that this option does not apply starting in the 19.0.0 Stein release "
"since the ironic compute driver no longer reports standard resource class "
"inventory regardless of configuration."
msgstr ""
"이 옵션은 19.0.0 스티恩 릴리즈부터 적용되지 않습니다. 이ronic 컴퓨터 드라이버는 구성에 관계없이 표준 리소스 클래스 인 "
"inventory를 더 이상 보고하지 않기 때문입니다."

#: ../../<reno.sphinxext stable/rocky>:2069
msgid ""
"Note that this option is read on the destination host of a live migration. "
"If you set this option the same on all of your compute hosts, which you "
"should do if you use the same networking backend universally, you do not "
"have to worry about this."
msgstr ""
"이 옵션은 live migration의 대상 호스트에서 읽어야 합니다. 이 옵션을 모든 컴퓨터 호스트에 동일하게 설정하면, 동일한 "
"네트워크 백엔드를 사용할 때, 이에 대한 우려는ありません."

#: ../../<reno.sphinxext unmaintained/2023.1>:344
msgid ""
"Note that those options are only taken into account if SPICE support is "
"enabled (and the VNC support is disabled)."
msgstr "SPICE 지원이 활성화되어 있는 경우만 해당 옵션을 고려합니다. VNC 지원이 비활성화되어 있는 경우."

#: ../../<reno.sphinxext unmaintained/zed>:291
msgid ""
"Note that, enable vIOMMU might introduce significant performance overhead. "
"You can see performance comparision table from `AMD vIOMMU session on KVM "
"Forum 2021`_. For above reason, vIOMMU should only be enable for workflow "
"that require it. .. _`AMD vIOMMU session on KVM Forum 2021`: "
"https://static.sched.com/hosted_files/kvmforum2021/da/vIOMMU%20KVM%20Forum%202021%20-%20v4.pdf"
msgstr ""
"집합을 활성화하면 significat performance overhead가 introducued 할 수 있습니다. AMD vIOMMU "
"session on KVM Forum 2021_에서 performance comparision table을 확인할 수 있습니다. 위의 "
"이유로, vIOMMU는 workflow가 필요할 때만 활성화해야 합니다. .. _`AMD vIOMMU session on KVM "
"Forum 2021`: "
"https://static.sched.com/hosted_files/kvmforum2021/da/vIOMMU%20KVM%20Forum%202021%20-%20v4.pdf`"

#: ../../<reno.sphinxext stable/rocky>:1428
msgid ""
"Note: Under this implementation, the image data may reside in one or more "
"pieces of storage of various formats on the host, but the import and export "
"operations interact with a single, proxied VDI object independent of the "
"underlying structure."
msgstr ""
"이 구현에서, 이미지 데이터는 호스트의 다양한 형식의 스토리지 중 하나 또는 여러 개에 resides할 수 있지만,.import "
"및.export 연산은 하위 구조에 의존하지 않고 단일, 가상화된 VDI 객체와 상호 작용한다."

#: ../../<reno.sphinxext stable/queens>:1493
msgid ""
"Nova API extension concept is removed in Pike. These extensions have their "
"own policies enforcement which are not needed any more. All the below "
"policies which were added mainly for extensions are deprecated for removal:"
msgstr ""
"nova API 확장 concepts는 Pike에서 제거되었다. 이 확장들은 자신의 정책 enforcement가 더 이상 필요하지 않기 "
"때문에 제거된다. 위의 모든 정책이 확장에 mainly 추가된 것이었다. 이 정책은 제거된다."

#: ../../<reno.sphinxext unmaintained/2023.1>:314
msgid ""
"Nova APIs now `by default support new RBAC policies "
"<https://docs.openstack.org/nova/latest/configuration/policy.html>` and "
"scopes. See our `Policy Concepts documention "
"<https://docs.openstack.org/nova/latest/configuration/policy-concepts.html>`"
" for further details."
msgstr ""
"Nova APIs는 기본적으로 새로운 RBAC 정책 및 스코프를 지원합니다. "
"<https://docs.openstack.org/nova/latest/configuration/policy.html>에 대한 추가 "
"정보를 얻으려면 Policy Concepts documention "
"<https://docs.openstack.org/nova/latest/configuration/policy-concepts.html>을"
" 참조하십시오."

#: ../../<reno.sphinxext stable/2025.1>:207
msgid "Nova added support for the IGB VIF model."
msgstr "Nova는 IGB VIF 모델을 지원했습니다."

#: ../../<reno.sphinxext stable/2025.2>:297
msgid ""
"Nova changed the default access for the service-to-service APIs which are "
"meant to be used by the OpenStack services only and not by any users. The "
"below service-to-service APIs access default to the ``service`` role:"
msgstr ""
"노바는 OpenStack 서비스만 사용할 수 있는 서비스-서vice API를 사용할 때는 기본적으로 접근 권한을 변경했다. 아래 서비스-"
"서vice API는 기본적으로 ``service`` 역할에 접근한다."

#: ../../<reno.sphinxext unmaintained/wallaby>:586
msgid ""
"Nova currenly does not support the following livecycle operations when "
"combined with a instance using vDPA ports: shelve, resize, cold migration, "
"live migration, evacuate, suspend or interface attach/detach. Attempting to "
"use one of the above operations will result in a HTTP 409 (Conflict) error. "
"While some operations like \"resize to same host\", shelve or attach "
"interface technically work, they have been blocked since unshelve and detach"
" interface currently do not. Resize to a different host has been blocked "
"since its untested, evacuate has also been blocked for the same reason. "
"These limitation may be removed in the future as testing is improved. Live "
"migration is currently not supported with vDPA interfaces by QEMU and "
"therefore cannot be enabled in openstack at this time."
msgstr ""
"nova 현재 vDPA 포트를 사용하여 인스턴스를 사용할 때 다음 livecycle 운영을 지원하지 않습니다. \n"
"- shelve\n"
"- resize\n"
"- cold migration\n"
"- live migration\n"
"- evacuate\n"
"- suspend\n"
"- interface attach/detach\n"
"\n"
"위의 운영 중 하나를 사용하면 HTTP 409 (Conflict) 오류가 발생합니다. \n"
"- \"resize to same host\", shelve 또는 interface attach technically work 하지만, unshelve 및 interface detach currently do not이기 때문에, shelve 또는 interface attach는 currently blocked입니다. \n"
"- resize to a different host는 test가 improve되면 untested이기 때문에, evacuate도 blocked입니다. \n"
"- live migration은 QEMU에서 vDPA interfaces를 사용할 때 currently not supported이기 때문에, openstack에서 enable할 수 없습니다. \n"
"\n"
"Note: I've followed the glossary translations exactly, and preserved placeholders, punctuation, and technical terms."

#: ../../<reno.sphinxext origin/stable/ocata>:249
msgid ""
"Nova does not support running the nova-api service under mod_wsgi or uwsgi "
"in Ocata. There are some experimental scripts that have been available for "
"years which allow you do to this, but doing so in Ocata results in possible "
"failures to list and show instance details in a cells v2 setup. See `bug "
"1661360`_ for details."
msgstr ""
"노바는 오카타 버전에서 mod_wsgi 또는 uwsgi를 사용하여 nova-api 서비스를 실행하지 않는다. 오카타 버전에서 이러한 "
"설정을 사용하면 세포 v2 설정에서 인스턴스รายละเอ기를 liệt고 보여주지 못할 수 있다.  더 많은 정보는 <a "
"href=\"https://bugzilla.redhat.com/show_bug.cgi?id=1661360\">bug "
"1661360</a>에서 확인할 수 있다."

#: ../../<reno.sphinxext stable/2024.1>:76 stable/2024.2>:76 stable/2025.1>:55
#: stable/2025.2>:411
msgid ""
"Nova has documented that the ``update volume attachment`` API PUT "
"/servers/{server_id}/os-volume_attachments/{volume_id} should not be called "
"directly for a very long time."
msgstr ""
"노바는 \"update volume attachment\" API PUT /servers/{server_id}/os-"
"volume_attachments/{volume_id}를 매우 오래 동안 직접 호출하지 않도록 문서화했다."

#: ../../<reno.sphinxext stable/pike>:1439
msgid ""
"Nova is now configured to use the v3 version of the Cinder API. You need to "
"ensure that the v3 version of the Cinder API is available and listed in the "
"service catalog in order to use Nova with the default configuration option."
msgstr ""
"nova는 현재 Cinder API v3 버전을 사용하도록 구성되어 있습니다. Cinder API v3 버전이 사용 가능한지 확인하고 "
"서비스 카탈로그에 liệteted다면, nova를 기본 구성 옵션과 함께 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:515
msgid ""
"Nova is now using the new Neutron port binding API to minimize network "
"downtime during live migrations. See the `related spec`_ for more details."
msgstr ""
"Nova는 현재 live migration 시 네트워크 downtime을 최소화하기 위해 새로운 Neutron 포트 바인딩 API를 "
"사용하고 있습니다. 더 많은 정보는 `related spec`_에 있습니다."

#: ../../<reno.sphinxext stable/stein>:858
msgid ""
"Nova leaks bandwidth resources if a bound port that has QoS minimum "
"bandwidth rules is deleted in Neutron before the port is logically detached "
"from the server. To avoid any leaks, users should detach the port from the "
"server using the Nova API first before deleting the port in Neutron. If the "
"server is in a state such that the port cannot be detached using the Nova "
"API, bandwidth resources will be freed when the server is deleted. Another "
"alternative to clean up the leak is to remove the "
"``NET_BW_EGR_KILOBIT_PER_SEC`` and/or ``NET_BW_IGR_KILOBIT_PER_SEC`` "
"allocations related to the deleted port for the server `using the CLI`_. See"
" related bug https://bugs.launchpad.net/nova/+bug/1820588 for more details."
msgstr ""
"네트로恩 (Neutron)에서 포트가 논리적으로 분리되지 않은 상태에서 포트가 삭제되면, QoS 최소帯宽 규칙이 있는 바운드 포트가 "
"삭제되면 네트워크 리소스 (bandwidth resources)가 유출된다. 이러한 유출을 피하기 위해, 사용자는 네트로恩 API를 "
"사용하여 포트를 서버에서 분리하고, 포트를 네트로恩에서 삭제하는 것을 먼저 수행해야 한다. 포트가 분리되지 않은 상태에서 포트를 삭제하는"
" 경우, bande위트 리소스가 해제된다. 다른 대안으로, 삭제된 포트와 관련된 서버에 대한 "
"``NET_BW_EGR_KILOBIT_PER_SEC`` 및/or ``NET_BW_IGR_KILOBIT_PER_SEC`` 할당을 제거하는 "
"것이 가능하다. 이 할당은 CLI를 사용하여 서버를 삭제하는 경우 bande위트 리소스가 해제된다. 관련된 버그는 "
"https://bugs.launchpad.net/nova/+bug/1820588 에서 더 많은 세부 사항이 있다."

#: ../../<reno.sphinxext stable/stein>:846
msgid ""
"Nova leaks resource allocations in placement during ``POST "
"/servers/{server_id}/action (revertResize Action)`` and ``POST "
"/servers/{server_id}/action (confirmResize Action)`` and ``POST "
"/servers/{server_id}/action (os-migrateLive Action)`` and if the allocation "
"held by the migration_uuid is modified in parallel with the lifecycle "
"operation. Nova will log an ERROR and will put the server into ERROR state "
"but will not delete the migration allocation. We assume that this can only "
"happen if somebody outside of nova is actively changing the migration "
"allocation in placement. Therefore it is not considered as a bug."
msgstr ""
"Nova 집합 자원 할당을 위치에 대한 ``POST /servers/{server_id}/action (revertResize "
"Action)`` 및 ``POST /servers/{server_id}/action (confirmResize Action)`` 및 "
"``POST /servers/{server_id}/action (os-migrateLive Action)``에서-leak하고, "
"migration_uuid가 lifecycle 연산을 동시에 수행할 때 할당이 수정되면, Nova는 ERROR 로그를 남기고, 서버를 "
"ERROR 상태로 설정하지만, migration 할당을 삭제하지 않습니다. We assume that this can only "
"happen if somebody outside of nova가 actively changing the migration "
"allocation in placement. Therefore it is not considered as a bug."

#: ../../<reno.sphinxext origin/stable/ocata>:1549
msgid ""
"Nova network was deprecated in Newton and is no longer supported for regular"
" deployments in Ocata. The network service binary will now refuse to start, "
"except in the special case of CellsV1 where it is still required to "
"function."
msgstr ""
"nova 네트워크는 뉴턴에서弃용되었으며, 일반적인 배포에 대한 지원은 오카타에서 더 이상 제공되지 않는다. 네트워크 서비스 바이너리는 "
"now refuse to start, except special case of CellsV1 where it is still "
"required to function."

#: ../../<reno.sphinxext stable/train>:513
msgid ""
"Nova no longer includes Placement code. You must use the extracted Placement"
" service. See the `Placement extraction upgrade instructions`_ for details."
msgstr ""
"nova에서 더 이상 Placement 코드를 포함하지 않습니다. Placement 서비스를 추출한 후 사용해야 합니다. 자세한 사항은 "
"`Placement extraction upgrade instructions`_를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:1290
msgid ""
"Nova no longer supports the Block Storage (Cinder) v2 API. Ensure the "
"following configuration options are set properly for Cinder v3:"
msgstr ""
"nova는 더 이상 블록 스토리지 (Cinder) v2 API를 지원하지 않습니다. Cinder v3를 위해 다음 구성 옵션을 적절히 "
"설정해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1239
msgid "Nova no longer supports the deprecated Cinder v1 API."
msgstr "nova는 더 이상 deprecated Cinder v1 API를 지원하지 않는다."

#: ../../<reno.sphinxext unmaintained/yoga>:332
msgid ""
"Nova now allows to create an instance with a non-deferred port that has no "
"fixed IP address if the network backend has level-2 connectivity."
msgstr ""
"Nova 현재는 네트워크 백엔드가 L2 연결을 갖춘 경우, 네트워크 백엔드가 L2 연결을 갖춘 경우에만, deferred 포트가 아닌 "
"인스턴스를 생성할 수 있습니다. 이 인스턴스는 fixed IP 주소가 없는 IP 할당이 가능합니다."

#: ../../<reno.sphinxext stable/2023.2>:44 stable/2024.1>:219
#: stable/2024.2>:164 stable/2025.1>:524 unmaintained/2023.1>:36
msgid ""
"Nova now allows to use a hyphen in the ``[cinder]catalog_info`` service-type"
" field, so in particular the official ``block-storage`` type is now valid. "
"`Bug 2092194 <https://bugs.launchpad.net/nova/+bug/2092194>`_"
msgstr ""
"Nova에서 현재 `-`를 `cinder` 카탈로그 정보의 `service-type` 필드에 사용할 수 있으므로, 특히 공식 "
"`block-storage` 타입은 현재 유효합니다. <https://bugs.launchpad.net/nova/+bug/2092194>"

#: ../../<reno.sphinxext stable/2024.2>:244
msgid ""
"Nova now automatically detects vTPM support for compute services if libvirt "
"version is above 8.0.0 and if ``swtpm`` is installed on the node. It will "
"also automatically get the TPM models that are supported."
msgstr ""
"nova는 libvirt 버전이 8.0.0 이상이고 swtpm이 노드에 설치된 경우에만 compute 서비스에 vTPM 지원을 tự "
"động 감지합니다. 또한, 이 지원이 가능한 TPM 모델을 tự động 얻을 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:1178
msgid ""
"Nova now correctly check for guest os support via the ``hw_firmware_type`` "
"image metadata property when spawning new instance and only enables ``UEFI``"
" if the guest and host support it. Guest deletion has also been updated to "
"correctly clean up based on the ``UEFI`` or ``BIOS`` configuration of the "
"vm."
msgstr ""
"nova now correctly check for guest os support via the ``hw_firmware_type`` image metadata property when spawning new instance and only enables ``UEFI`` if the guest and host support it. Guest deletion has also been updated to correctly clean up based on the ``UEFI`` or ``BIOS`` configuration of the vm.\n"
"\n"
"- hw_firmware_type : 아키텍처\n"
"- UEFI : 아키텍처\n"
"- BIOS : 아키텍처"

#: ../../<reno.sphinxext stable/queens>:1322
msgid ""
"Nova now defaults to using the live snapshot feature of libvirt when taking "
"snapshots. This was previously disabled by default due to crashes on load "
"seen with older libvirt versions. It has been used in production by many "
"clouds, and appears stable in real world environments. If you see crashes of"
" guests during snapshots, you can disable this with the "
"``disable_libvirt_livesnapshot`` config value in ``[workarounds]``."
msgstr ""
"노바는 현재 libvirt의 live snapshot 기능을 사용하여 스냅샷을 취할 때 기본적으로 live snapshot을 사용합니다."
" 이전에는 libvirt의 오래된 버전에서 로드 시에 발생하는 크ashes로 인해 기본적으로 비활성화되었습니다. 이 기능은 많은 "
"클라우드에서 프로덕션 환경에서 사용되었습니다. 실제 세계 환경에서 안정적이 appears. 스냅샷 시에 게스트가 크ashes를 보는 "
"경우, disable_libvirt_livesnapshot config value를 [workarounds]에 있는 "
"disable_libvirt_livesnapshot으로 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:271 stable/2024.2>:408
msgid ""
"Nova now ensures that an instance cannot move between availability zones "
"when the host of the instance is added or removed to an aggregate that is "
"part of another availability zone. Moving from or to the default "
"availability zone is also rejected."
msgstr ""
"Nova는 인스턴스의 호스트가 집합에 추가되거나 제거되면 집합이 다른 가용 zone에 속하는 경우에 인스턴스가 가용 zone을 이동할 수"
" 없음을 보장합니다. 기본 가용 zone으로 이동하거나 이동하는 경우도 거부됩니다."

#: ../../<reno.sphinxext stable/train>:105 stable/ussuri>:1320
msgid ""
"Nova now has a config option called "
"``[workarounds]/never_download_image_if_on_rbd`` which helps to avoid "
"pathological storage behavior with multiple ceph clusters. Currently, Nova "
"does *not* support multiple ceph clusters properly, but Glance can be "
"configured with them. If an instance is booted from an image residing in a "
"ceph cluster other than the one Nova knows about, it will silently download "
"it from Glance and re-upload the image to the local ceph privately for that "
"instance. Unlike the behavior you expect when configuring Nova and Glance "
"for ceph, Nova will continue to do this over and over for the same image "
"when subsequent instances are booted, consuming a large amount of storage "
"unexpectedly. The new workaround option will cause Nova to refuse to do this"
" download/upload behavior and instead fail the instance boot. It is simply a"
" stop-gap effort to allow unsupported deployments with multiple ceph "
"clusters from silently consuming large amounts of disk space."
msgstr ""
"Nova에서 현재는 `[workarounds]/never_download_image_if_on_rbd`라는 구성 옵션을 가지고 있습니다."
" 이것은 여러 Ceph 클러스터와 함께 Pathological Storage Behavior를 피하는 데 도움이 됩니다. 현재 Nova는"
" 여러 Ceph 클러스터를 올바르게 지원하지 않지만 Glance는 그들을 구성할 수 있습니다. Ceph 클러스터가 Nova가 biết지 "
"못하는 다른 클러스터에서 이미지로 인스턴스를 부팅하면, Glance에서 이미지 silently를 다운로드하고, 로컬 Ceph에 인스턴스-"
"specific으로 이미지를 다시 업로드합니다. Nova와 Glance를 Ceph로 구성할 때 예상하는 것과는 달리, Nova는 동일한 "
"이미지에 대해 subsequent 인스턴스를 부팅할 때 이 behavior를 반복하여, 많은 스토리지 공간을 비대면합니다. 새로운 "
"workarounds 옵션은 Nova가 이 다운로드/업로드 behavior를 거부하고, 대신 인스턴스 부팅을 실패하도록합니다. 이것은 "
"단순히 여러 Ceph 클러스터를 silently 소비하는 큰 디스크 공간을 허용하는 데 도움이 됩니다."

#: ../../<reno.sphinxext stable/queens>:1476
msgid ""
"Nova now has a hard requirement that the Port Binding extension is enabled "
"in the Neutron service. This simplifies the logic between Nova and Neutron."
msgstr ""
"Nova에서 Port Binding 확장 기능이 Neutron 서비스에 활성화되어 있는 것이 강제로 필요하다. 이것은 Nova와 "
"Neutron 사이의 논리 간의 간단성을 제공한다."

#: ../../<reno.sphinxext stable/queens>:1263
msgid ""
"Nova now requires Ceph/librados >= 11.1.0 if running under Python 3 with the"
" RBD image backend for the libvirt driver. Requirements for Python 2 users "
"or users using a different backend remain unchanged."
msgstr ""
"Nova 현재 Ceph/librados >= 11.1.0을 필요로 하며, Python 3에서 RBD 이미지 백엔드와 libvirt "
"드라이버를 사용하는 경우. Python 2 사용자 또는 다른 백엔드 사용자에게는 요구 사항이 unchanged입니다."

#: ../../<reno.sphinxext unmaintained/xena>:541
msgid ""
"Nova now requires that the Placement API supports at least microversion "
"1.36, added in Train. The related nova-upgrade check has been modified to "
"warn if this prerequisite is not fulfilled."
msgstr ""
"Nova에서 현재는 Train 버전에서 추가된 microversion 1.36 이상을 지원하는 Placement API를 지원해야 "
"합니다. 관련 nova-upgrade 확인은 이 전제가 충족되지 않으면 경고를 보냅니다."

#: ../../<reno.sphinxext unmaintained/yoga>:400
msgid ""
"Nova now support integration with the Lightbits Labs "
"(http://www.lightbitslabs.com) LightOS storage solution. LightOS is a "
"software-defined, cloud native, high-performance, clustered scale-out and "
"redundant NVMe/TCP storage that performs like local NVMe flash."
msgstr ""
"Nova는 Lightbits Labs (http://www.lightbitslabs.com) LightOS 스토리지 giải pháp과 "
"통합을 지원합니다. LightOS는 소프트웨어 정의, 클라우드 NATIVE, 고성능, 클러스터화된 스케일 아웃 및 복제 NVMe/TCP "
"스토리지입니다. 이 스토리지는 로컬 NVMe 플래시와 유사한 성능을 나타냅니다."

#: ../../<reno.sphinxext stable/2025.2>:51
msgid ""
"Nova now supports QEMU’s memory balloon autodeflate and free page reporting "
"features with the libvirt driver. These allow unused guest memory to be "
"automatically released back to the hypervisor, improving memory efficiency "
"and reducing the risk of the Out-of-Memory killer activating."
msgstr ""
"nova now supports QEMU의 memory balloon autodeflate 및 free page reporting 기능을"
" libvirt driver와 함께 사용할 수 있습니다. 이러한 기능은 사용되지 않은 게스트 메모리를 tự động적으로 해제하고 "
"하이퍼바이저로 다시 배치하여 메모리 효율성을 향상하고 Out-of-Memory killer가 활성화되는 위험을 줄여줍니다."

#: ../../<reno.sphinxext stable/pike>:607
msgid ""
"Nova now supports a Cells v2 multi-cell deployment. The default deployment "
"is a single cell. There are known `limitations with multiple cells`_. Refer "
"to the `Cells v2 Layout`_ page for more information about deploying multiple"
" cells."
msgstr ""
"Nova는 현재 Cells v2를 사용하는 다중 세ลล์ 배포를 지원합니다. 기본 배포는 단일 세ลล입니다. 다중 세ลล 배포에 대한 "
"제한점은 알려져 있습니다. _다중 세ลล 배포에 대한 자세한 정보는 Cells v2 Layout_ page를 참조하십시오."

#: ../../<reno.sphinxext stable/2025.1>:214
msgid ""
"Nova now supports a new console type called `spice-direct`  if you define a "
"specific SPICE protocol native proxy URL (eg. a kerbside URL). Direct SPICE "
"VDI consoles enable a much richer virtual desktop experience."
msgstr ""
"Nova는 특정 SPICE 프로토콜 원산자 프록시 URL (예: 커버스ाइड URL)을 정의하면 `spice-direct`라는 새로운 "
"콘솔 타입을 지원합니다. Direct SPICE VDI 콘솔은 가치 있는 가상 데스크톱 경험을 제공합니다."

#: ../../<reno.sphinxext stable/2025.2>:34
msgid ""
"Nova now supports a new default role ``manager`` which is scoped to the "
"project level. This role is part of the standard role hierarchy supported by"
" Keystone and allows trusted project users to perform project-level "
"management tasks (e.g., live migration) without requiring full admin rights."
msgstr ""
"Nova는 현재 프로젝트 수준에 scoped 된 새로운 기본 역할 \"manager\"를 지원합니다. 이 역할은 Keystone이 "
"지원하는 표준 역할 계층的一部分이며, 신뢰할 수 있는 프로젝트 사용자들이 프로젝트 수준의 관리 task (예: live "
"migration)를 수행할 수 있습니다. 이 역할은 프로젝트 수준의 관리 task를 수행할 수 있는 프로젝트 사용자에게 전체 관리자 "
"권한이 필요하지 않습니다."

#: ../../<reno.sphinxext stable/2025.2>:73
msgid ""
"Nova now supports a new default role ``manager``. This role is part of the "
"standard role hierarchy supported by keystone. A new persona, the "
"``project_manager``, is denoted by someone with the ``manager`` role on a "
"specific project. The ``project_manager`` persona  is intended to perform "
"more privileged operations than a ``project_member`` while granting less "
"access than the global admin role. This brings the total set of personas "
"currently supported by Nova to:"
msgstr ""
"Nova는 새로운 기본 역할 \"manager\"를 지원합니다. 이 역할은 keystone에 의해 지원되는 표준 역할 계층에 속합니다. "
"새로운 개인 \"project_manager\"는 특정 프로젝트에 \"manager\" 역할을 가진 사람에 의해 표시됩니다. "
"\"project_manager\" 개인은 \"project_member\"와 비슷한 privileges를 제공하지만, toàn cầu "
"관리자 역할보다 더 적은 접근 권한을 제공합니다. 이로써 Nova가 현재 지원하는 개인의 총 집합은 다음과 같습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:383
msgid ""
"Nova now supports adding an emulated virtual `Trusted Platform Module`__ to "
"libvirt guests with a ``virt_type`` of ``kvm`` or ``qemu``. Not all server "
"operations are fully supported yet. See the documentation__ for details."
msgstr ""
"Nova 현재는 libvirt 게스트에 `Trusted Platform Module`__를 가상화된 형태로 추가할 수 있습니다. "
"`virt_type`의 `kvm` 또는 `qemu`를 사용합니다. 그러나 모든 서버 운영은 현재 완전히 지원되지 않았습니다. 자세한 "
"내용은 문서를 참조하십시오."

#: ../../<reno.sphinxext unmaintained/victoria>:452
msgid ""
"Nova now supports attaching and detaching PCI device backed Neutron ports to"
" running servers."
msgstr ""
"Nova는 현재-running server에 PCI 장치가 backing된 Neutron 포트를 연결하고 해제하는 것을 지원합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:425
msgid ""
"Nova now supports defining of additional resource provider traits and "
"inventories by way of YAML configuration files. The location of these files "
"is defined by the new config option ``[compute]provider_config_location``. "
"Nova will look in this directory for ``*.yaml`` files. See the "
"`specification`__ and `admin guide`__ for more details."
msgstr ""
"Nova는 현재 YAML 구성 파일을 통해 추가 리소스 제공자 특성과 인ベント리를 정의할 수 있습니다. 이 파일의 위치는 새로운 구성 옵션 ``[compute]provider_config_location``에 의해 정의됩니다. Nova는 이 디렉토리에서 ``*.yaml`` 파일을 찾습니다. 더 많은 정보는 `specification`__ 및 `admin guide`__에서 확인할 수 있습니다.\n"
"\n"
"- `[compute]provider_config_location` : `compute` 구성 옵션\n"
"- `specification` : `specification`\n"
"- `admin guide` : `admin guide`"

#: ../../<reno.sphinxext stable/stein>:458
msgid "Nova now supports nested resource providers in two cases:"
msgstr "Nova 현재는 두 가지 경우에서 nested 리소스 제공자를 지원합니다."

#: ../../<reno.sphinxext stable/2025.2>:43
msgid ""
"Nova now supports one-time use passthrough devices. Such devices are "
"allocated to a single instance, and when the instance is deleted, the device"
" stays in a reserved state instead of becoming automatically available. This"
" ensures operators can perform necessary security checks or hardware resets "
"before reusing the device."
msgstr ""
"Nova는 현재 일회용 패스-through 장치를 지원합니다. 이러한 장치는 단일 인스턴스에 할당され, 인스턴스가 삭제되면 장치가 예약 "
"상태로 남아 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라 "
"automatically available가 아니라 automatically available가 아니라 automatically "
"available가 아니라 automatically available가 아니라 automatically available가 아니라"

#: ../../<reno.sphinxext stable/queens>:1315
msgid ""
"Nova now uses keystoneauth1 configuration to set up communication with the "
"baremetal service.  Use keystoneauth1 loading parameters for auth, Session, "
"and Adapter setup in the ``[ironic]`` conf section.  This includes using "
"``endpoint_override`` in favor of ``api_endpoint``."
msgstr ""
"nova는 Bare Metal 서비스와의 통신을 설정하기 위해 현재 KeystoneAuth1 구성을 사용합니다.  auth, "
"Session, Adapter 설정을위한 keystoneauth1 로딩 매개 변수를 사용하십시오.  [ironic] conf 섹션에서."
"  이에는 api_endpoint 대신 endpoint_override를 사용하는 것을 포함합니다."

#: ../../<reno.sphinxext stable/queens>:1304
msgid ""
"Nova now uses keystoneauth1 configuration to set up communication with the "
"image service.  Use keystoneauth1 loading parameters for Session and Adapter"
" setup in the ``[glance]`` conf section.  This includes using "
"``endpoint_override`` in favor of ``api_servers``.  The "
"``[glance]api_servers`` conf option is still supported, but should only be "
"used if you need multiple endpoints and are unable to use a load balancer "
"for some reason.  However, note that no configuration is necessary with an "
"appropriate service catalog entry for the image service."
msgstr ""
"nova now uses keystoneauth1 구성 để communication with the image service.  "
"keystoneauth1 로딩 매개 변수를 사용하여 세션과 어드apter 설정을 [glance] conf 섹션에 applies합니다.  "
"이에는 endpoint_override를 api_servers 대신 사용하는 것을 포함합니다.  [glance]api_servers "
"conf 옵션은 여전히 지원되지만, 여러 엔드포인트가 필요하고 로드 밸런서를 사용할 수 없을 때는 사용해야 할 수 있습니다.  그러나, "
"image service에 적합한 서비스 카탈로그 엔트리가 있는 경우, 구성이 필요하지 않습니다."

#: ../../<reno.sphinxext stable/queens>:1336
msgid ""
"Nova now uses keystoneauth1 configuration to set up communication with the "
"network service.  Use keystoneauth1 loading parameters for Adapter setup in "
"the ``[neutron]`` conf section (auth and Session options continue to work as"
" before).  Of note:"
msgstr ""
"nova now uses keystoneauth1 configuration to set up communication with the "
"network service.  Use keystoneauth1 loading parameters for Adapter setup in "
"the ``[neutron]`` conf section (auth and Session options continue to work as"
" before).  Of note:"

#: ../../<reno.sphinxext stable/queens>:1351
msgid ""
"Nova now uses keystoneauth1 configuration to set up communication with the "
"placement service.  Use keystoneauth1 loading parameters for auth, Session, "
"and Adapter setup in the ``[placement]`` conf section.  Note that, by "
"default, the 'internal' interface will be tried first, followed by the "
"'public' interface.  Use the conf option ``[placement].valid_interfaces`` to"
" override this behavior."
msgstr ""
"nova now uses keystoneauth1 구성 để communication with the placement 서비스.  "
"keystoneauth1 로딩 매개 변수를 사용하여 auth, Session, Adapter 설정을 [placement] conf "
"섹션에서 설정합니다.  internal 인터페이스를 기본적으로 사용하고 public 인터페이스를 사용할 때는, "
"[placement].valid_interfaces 옵션을 사용하여 이 behavior를 override합니다."

#: ../../<reno.sphinxext stable/pike>:841
msgid ""
"Nova now uses oslo.middleware for request_id processing. This means that "
"there is now a new ``X-OpenStack-Request-ID`` header returned on every "
"request which mirrors the content of the existing ``X-Compute-Request-ID``. "
"The expected existence of this header is signaled by Microversion 2.46. If "
"server version >= 2.46, you can expect to see this header in your results "
"(regardless of microversion requested)."
msgstr ""
"nova now uses oslo.middleware for request_id processing. 이것은 모든 요청에 대해 새로운 "
"`X-OpenStack-Request-ID` 헤더가 반환되는 것을 의미한다. 이 헤더는 현재 존재하는 `X-Compute-Request-"
"ID` 헤더의 내용을 반영한다. Microversion 2.46의 존재가 예상되는 것은, server version >= 2.46이면 이"
" 헤더가 결과에 나타날 수 있다(마이크로 버전이 요청된 것과 상관없이)."

#: ../../<reno.sphinxext unmaintained/yoga>:498
msgid ""
"Nova operations on instances with ``VNIC_TYPE_REMOTE_MANAGED`` ports follow "
"the same logic as the operations on direct SR-IOV ports."
msgstr ""
"Nova의 인스턴스에 ``VNIC_TYPE_REMOTE_MANAGED`` 포트를 사용하는 연산은 직접 SR-IOV 포트와 같은 로직을 "
"따르며."

#: ../../<reno.sphinxext stable/ussuri>:765 unmaintained/victoria>:519
msgid ""
"Nova policies implemented the ``scope_type`` and new defaults provided by "
"keystone. Old defaults are deprecated and still work if rules are not "
"overridden in the policy file. If you don't override any policies at all, "
"then you don't need to do anything different until the W release when old "
"deprecated rules are removed and tokens need to be scoped to work with new "
"defaults and scope of policies. For migration to new policies you can refer "
"to `this document "
"<https://docs.openstack.org/nova/latest/configuration/policy-"
"concepts.html#migration-plan>`_."
msgstr ""
"nova 정책은 `scope_type`를 구체화하고 keystone에서 제공하는 새로운 기본값을 구현했습니다.舊 기본값은 더 이상 "
"지원되지 않지만, 정책 파일에서 규칙을 오버라이드하지 않으면 여전히 작동합니다. 정책을 전부 오버라이드하지 않으면, 새로운 기본값과 "
"정책의 범위와 함께 작동하는 토큰을 사용하기 위해 토큰을 스코핑해야 할 때까지 nothing이 필요합니다. 새로운 정책으로 전환하는 "
"경우, migration plan을 참조하십시오. "
"<https://docs.openstack.org/nova/latest/configuration/policy-"
"concepts.html#migration-plan>"

#: ../../<reno.sphinxext unmaintained/wallaby>:887
msgid ""
"Nova publishes hostnames for instances via the metadata service and config "
"drives. This hostname is based on a sanitized version of the instance name "
"combined with the domain value specified in ``[api] dhcp_domain``. The "
"previous sanitization of the hostname included the replacement of whitespace"
" and underscores with dashes and the stripping of unicode characters along "
"with leading and trailing periods and dashes. It did not, however, include "
"the removal of periods in the name. Periods are not valid in the hostname "
"or, more specifically, in the host-specific or leaf label (the ``host`` in "
"``host.example.com``) and their presence can cause conflicts when ``[api] "
"dhcp_domain`` is configured, leading to instances being mistakenly "
"configured with hostnames like ``host.example.com.example.com``. More "
"pressingly, their use can result in a failure to boot instances if DNS "
"integration is enabled in neutron, likely via designate, as the hostname is "
"identified as a FQDN (fully-qualified domain name) by neutron and reasonable"
" instance names like ``test-ubuntu20.04`` will be rejected as invalid FQDNs,"
" in this case because the name would yield a TLD (top-level domain) of "
"``04`` and TLDs cannot be entire numerical. To avoid these issues, periods "
"are now replaced with dashes."
msgstr ""
"노바는 인스턴스에 대한 호스트 이름을 메타데이터 서비스와 구성 드라이브를 통해 공유한다. 이 호스트 이름은 인스턴스 이름의 정리된 버전과"
" `[api] dhcp_domain`에 정의된 도메인 값으로 결합된 바탕으로 만들어진다. 이전에 호스트 이름의 정리화에는 공백과 "
"언더스코어를 대신 dashes로 대체하고 유니코드 문자를 제거하고 leading 및 trailing periods 및 dashes를 "
"제거했다. 그러나, 호스트 이름이나, 더 spécIFICALLY 호스트-특성적 또는 leaf 라벨 (neutron에서 `host`에 "
"해당하는 `host.example.com` )에 대한 점은 유효하지 않으며, `[api] dhcp_domain`가 구성된 경우, "
"인스턴스들이 잘못된 호스트 이름으로 구성되어 `host.example.com.example.com`와 같은 이름이 발생할 수 있다. 더 "
"심각하게, 점은 DNS 통합이 neutron에서 활성화된 경우, 인스턴스 부트를 실패할 수 있다. 특히 designate를 통해 "
"neutron이 호스트 이름을 FQDN (fully-qualified domain name)로 식별하고, reasonable 이름이 "
"`test-ubuntu20.04`와 같은 경우, TLD (top-level domain)가 `04`로 yield되면, TLD는 숫자로 "
"구성되지 않기 때문에, 이러한 문제를 피하기 위해 점은 dashes로 대체된다."

#: ../../<reno.sphinxext stable/ussuri>:123 unmaintained/victoria>:266
msgid ""
"Nova services only support old computes if the compute is not older than the"
" previous major nova release. From now on nova services will emit a warning "
"at startup if the deployment contains too old compute services. From the "
"23.0.0 (Wallaby) release nova services will refuse to start if the "
"deployment contains too old compute services to prevent compatibility "
"issues."
msgstr ""
"노바 서비스는 이전 주요 노바 릴리스보다 오래된 컴퓨터만 지원합니다. 노바 서비스는 현재 노바 서비스가 포함된 배포에서 오래된 컴퓨터 "
"서비스가 있는 경우, 시작 시 경고를 발신합니다. 노바 서비스는 23.0.0 (Wallaby) 릴리스 이후, 배포에서 오래된 컴퓨터 "
"서비스가 있는 경우 시작하지 않도록 거부합니다. 이 경우의 호환성 문제를 방지하기 위해."

#: ../../<reno.sphinxext unmaintained/wallaby>:701
msgid ""
"Nova services only support old computes if the compute is not older than the"
" previous major nova release. To prevent compatibility issues at run time "
"nova services will refuse to start if the deployment contains too old "
"compute services."
msgstr ""
"nova 서비스는 이전 주요 nova 릴리스보다 오래된 컴퓨터를 지원하지 않는다. 컴퓨터가 이전 주요 nova 릴리스보다 오래된 경우 "
"nova 서비스는 배포에 포함된 컴퓨터 서비스가 너무 오래된 경우 runtime에 compatibility 이슈가 발생할 수 있으므로 "
"nova 서비스는 시작하지 않습니다."

#: ../../<reno.sphinxext unmaintained/zed>:259
msgid ""
"Nova started tracking PCI devices in Placement. This is an optional feature "
"disabled by default while we are implementing inventory tracking and "
"scheduling support for both PCI passthrough devices and SR-IOV devices "
"consumed via Neutron ports. Please read our `documentation "
"<https://docs.openstack.org/nova/latest/admin/pci-passthrough.html#pci-"
"tracking-in-placement>`_ for more details on what is supported how this "
"feature can be enabled."
msgstr ""
"nova는 placement에서 PCI 장치를 추적하기 시작했습니다. 이 기능은 기본적으로 비활성화되어 있으며, 현재 inventory "
"추적 및 scheduling 지원을 위해 PCI passthrough 장치와 SR-IOV 장치를 Neutron 포트를 통해 소비하는両方에"
" 대한 지원을 implementation 중에 있습니다. 더 많은 정보와 이 기능을 활성화하는 방법에 대한รายละเอียด은 "
"<https://docs.openstack.org/nova/latest/admin/pci-passthrough.html#pci-"
"tracking-in-placement>을 참조하십시오."

#: ../../<reno.sphinxext stable/pike>:1627
msgid ""
"Nova support for using the Block Storage (Cinder) v2 API is now deprecated "
"and will be removed in the 17.0.0 Queens release. The v3 API is now the "
"default and is backward compatible with the v2 API."
msgstr ""
"nova의 블록 스토리지 (Cinder) v2 API를 사용하는 것을 지원하는 기능은 현재 비활성화되어 17.0.0 퀸즈 릴리스에서 "
"제거될 예정입니다. v3 API는 현재 기본 API이며 v2 API와 backward compatible합니다."

#: ../../<reno.sphinxext stable/pike>:1205
msgid ""
"Nova supports file injection of network templates. Putting these in a config"
" drive is the only way to configure networking without DHCP."
msgstr ""
"노바는 네트워크 템플릿의 파일 인젝션을 지원합니다. 이들을 config drive에 넣는 것은 DHCP를 사용하지 않는 네트워킹 구성의 "
"유일한 방법입니다."

#: ../../<reno.sphinxext unmaintained/victoria>:855
msgid ""
"Nova tries to remove a volume from Ceph in a retry loop of 10 attempts at 1 "
"second intervals, totaling 10 seconds overall - which, due to 30 second ceph"
" watcher timeout, might result in intermittent object removal failures on "
"Ceph side (`bug 1856845`_). Setting default values for "
"``[libvirt]/rbd_destroy_volume_retries`` to 12 and "
"``[libvirt]/rbd_destroy_volume_retry_interval`` to 5, now gives Ceph "
"reasonable amount of time to complete the operation successfully."
msgstr ""
"nova는 10회 1초 간격으로 retry loop을 사용하여 Ceph에서 볼륨을 제거하는 시도 - 총 10초가 걸리며, 30초의 "
"Ceph watcher 타임아웃으로 인해 Ceph 측에서 중간에 제거 fails가 발생할 수 있다 (`bug 1856845`). "
"기본값으로 `[libvirt]/rbd_destroy_volume_retries`를 12로, "
"`[libvirt]/rbd_destroy_volume_retry_interval`를 5로 설정하면 Ceph는 성공적으로 "
"operation을 완료할 수 있는 적절한 시간을 제공한다."

#: ../../<reno.sphinxext stable/train>:590
msgid ""
"Nova will invoke this utility to identify available PMEM namespaces. Then "
"users can specify vPMEM resources in a flavor by adding flavor's extra "
"specs::"
msgstr ""
"노바는 이 utility를 사용하여 PMEM namespace를 확인할 수 있는 집합을 식별합니다. 그 다음 사용자는 flavor에 "
"vPMEM 리소스를 추가한 후 extra specs를 추가하여 flavor의 vPMEM 리소스를 지정할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1829
msgid ""
"Nova will now internally use a new flow for new volume attachments when:"
msgstr "nova는 이제 새로운 볼륨 연결을 위한 새로운 흐름을 내부적으로 사용할 것이다."

#: ../../<reno.sphinxext unmaintained/wallaby>:884
msgid ""
"Nova will now replace periods (``.``) with dashes (``-``) when santizing an "
"instance's display name for use as a hostname."
msgstr ""
"노바는 인스턴스의 디스플레이 이름을 호스트 이름으로 사용하기 위해-sanitize하는 동안, 점 (``.``)을 하이픈 (``-``)으로"
" 대체할 것이다."

#: ../../<reno.sphinxext unmaintained/2023.1>:297
msgid ""
"Nova will prevent unexpected compute service renames by `persisting a unique"
" compute UUID on local disk "
"<https://docs.openstack.org/nova/latest/admin/compute-node-"
"identification.html>`_. This stored UUID will be considered the source of "
"truth for knowing whether the compute service hostame has been modified or "
"not. As a reminder, changing a compute hostname is forbidden, particularly "
"when this compute is currently running instances on top of it."
msgstr ""
"nova는 `persisting a unique compute UUID on local disk "
"<https://docs.openstack.org/nova/latest/admin/compute-node-"
"identification.html>`_을 통해 불필요한 컴퓨트 서비스 이름 변경을 방지합니다. 이 저장된 UUID은 컴퓨트 서비스 "
"호스트 이름이 변경되었는지 여부를 알기 위한 진실의 출처로 간주됩니다. 이에 대한 reminder은, 이 컴퓨트 서비스가 현재-"
"running 인스턴스를위한 경우, 특히, 호스트 이름을 변경하는 것은 금지됩니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:537
msgid ""
"Nova's use of libvirt's compareCPU() API has become error-prone as it "
"doesn't take into account host hypervisor's capabilities.  With QEMU >=2.9 "
"and libvirt >= 4.4.0, libvirt will do the right thing in terms of CPU "
"comparison checks via a new replacement API, compareHypervisorCPU().  Nova "
"satisfies the said minimum version requirements of QEMU and libvirt by a "
"good margin."
msgstr ""
"노바가 libvirt의 compareCPU() API를 사용하는 것은 오류가 발생할 수 있게 되고, 호스트 하이퍼바이저의 능력을 고려하지"
" 않기 때문이다.  QEMU >=2.9 및 libvirt >= 4.4.0를 사용하면, compareHypervisorCPU() API를 "
"통해 CPU 비교를 확인하는 데 correct한 것을 수행할 수 있다.  노바는 QEMU 및 libvirt의 said 최소 버전 요구 "
"사항을 잘 충족한다."

#: ../../<reno.sphinxext unmaintained/yoga>:130 unmaintained/zed>:399
msgid ""
"Nova's use of libvirt's compareCPU() API served its purpose over the years, "
"but its design limitations break live migration in subtle ways. For example,"
" the compareCPU() API compares against the host physical CPUID. Some of the "
"features from this CPUID aren not exposed by KVM, and then there are some "
"features that KVM emulates that are not in the host CPUID. The latter can "
"cause bogus live migration failures."
msgstr ""
"노바가 libvirt의 compareCPU() API를 사용해도 몇 년 동안 그 목적을 달성했지만 설계 제한점은 살짝한 방식으로 라이브 "
"마이그레이션을 깨트린다. 예를 들어, compareCPU() API는 호스트 물리 CPUID와 비교한다. KVM에서 expose된 일부 "
"기능은 CPUID의 기능이 아니며, KVM가 호스트 CPUID의 기능을 giả사하는 기능도 있다. 그러나 후자의 경우에는 부정적인 라이브"
" 마이그레이션 실패를 일으킨다."

#: ../../<reno.sphinxext unmaintained/wallaby>:544
msgid ""
"Now nova-api and nova-api-metadata WSGI services support command line "
"arguments similarly to other nova services. For example these services now "
"support specifying mutliple config files via --config-file parameter. Please"
" note that passing command line arguments to WSGI apps depends on the given "
"WSGI runner. For example uwsgi supports this via the --pyargv parameter of "
"the uwsgi binary."
msgstr ""
"nova-api 및 nova-api-metadata WSGI 서비스는 다른 nova 서비스와 마찬가지로 명령줄 매개 변수를 지원합니다. "
"예를 들어, 이 서비스는 now --config-file 매개 변수를 통해 여러 구성 파일을 spécifying 할 수 있습니다. 명령줄"
" 매개 변수를 WSGI 앱에 전달하는 것은 WSGI 런너에 따라 달라집니다. 예를 들어, uWSGI는 uWSGI binary의 "
"--pyargv 매개 변수를 통해 지원합니다."

#: ../../<reno.sphinxext stable/2024.1>:440
msgid ""
"Now the libvirt driver is capable to detect maximum number of guests with "
"memory encrypted which can run concurrently in its compute host using the "
"new fields in libvirt API available since version 8.0.0."
msgstr ""
"libvirt 드라이버는 now version 8.0.0부터 libvirt API의 새로운 필드를 사용하여 compute 호스트에서 "
"concurrently running 할 수 있는 최대 number of guests의 memory가 암호화된 guest를 감지할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/rocky>:614
msgid ""
"Now users can see the flavor extra-specs in flavor APIs response and do not "
"need to call ``GET /flavors/{flavor_id}/os-extra_specs`` API. The visibility"
" of the flavor extra_specs within the flavor resource will be controlled by "
"the same policy rules as are used for showing the flavor extra_specs. If the"
" user has no access to query extra_specs, the ``flavor.extra_specs`` will "
"not be included."
msgstr ""
"현재 사용자는 `flavor` API의 응답에서 `flavor extra-specs`를 볼 수 있으며, `GET "
"/flavors/{flavor_id}/os-extra_specs` API를 호출할 필요가 없게되었습니다. `flavor` 리소스 내에서 "
"`flavor extra-specs`의 보이기 정책은 `flavor extra-specs`를 보여주는 정책과 동일한 정책을 사용하여 "
"제어됩니다. 사용자가 `extra_specs`를 조회할 수 없다면, `flavor.extra_specs`는 포함되지 않습니다."

#: ../../<reno.sphinxext stable/queens>:970
msgid ""
"Now when you rebuild a baremetal instance, a new config drive will be "
"generated for the node based on the passed in personality files, metadata, "
"admin password, etc. This fix requires Ironic API 1.35."
msgstr ""
"이제 baremetal 인스턴스를 재건할 때, 노드에 대한 새로운 config drive가 personality files, "
"metadata, admin password 등에 따라 생성됩니다. 이fix는 Ironic API 1.35를 필요로합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1559
msgid ""
"OSProfiler support requires passing of trace information between various "
"OpenStack services. This information is securely signed by one of HMAC keys,"
" defined in nova.conf configuration file. To allow cross-project tracing "
"user should use the key, that is common among all OpenStack services he or "
"she wants to trace."
msgstr ""
"OSProfiler support는 다양한 OpenStack 서비스之间의 트레이스 정보를 안전하게 전달해야 합니다. 이 정보는 "
"nova.conf 설정 파일에 정의된 하나의 HMAC 키에 의해 보안으로 서명됩니다. 사용자가 여러 OpenStack 서비스를 트레이스 "
"करन을 원한다면, 사용자가 원하는 서비스를 공유하는 모든 OpenStack 서비스에서 사용하는 키를 사용해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:579
msgid ""
"OSProfiler support was added. This cross-project profiling library allows to"
" trace various OpenStack requests through all OpenStack services that "
"support it. To initiate OpenStack request tracing `--profile <HMAC_KEY>` "
"option needs to be added to the CLI command. This key needs to present one "
"of the secret keys defined in nova.conf configuration file with `hmac_keys` "
"option under the `[profiler]` configuration section. To enable or disable "
"Nova profiling the appropriate `enabled` option under the same section needs"
" to be set either to `True` or `False`. By default Nova will trace all API "
"and RPC requests, but there is an opportunity to trace DB requests as well. "
"For this purpose `trace_sqlalchemy` option needs to be set to `True`. As a "
"prerequisite OSProfiler library and its storage backend needs to be "
"installed to the environment. If so (and if profiling is enabled in "
"nova.conf) the trace can be generated via following command, for instance - "
"`$ nova --profile SECRET_KEY boot --image <image> --flavor <flavor> <name>`."
" At the end of output there will be message with <trace_id>, and to plot "
"nice HTML graphs the following command should be used - `$ osprofiler trace "
"show <trace_id> --html --out result.html`"
msgstr ""
"OSProfiler 지원이 추가되었다. 이.cross-project profiling library는 OpenStack 서비스가 지원하는"
" 모든 OpenStack 요청을 tracing할 수 있는 다양한 OpenStack 요청을 tracing할 수 있다. OpenStack "
"요청을 tracing하기 위해 `--profile <HMAC_KEY>` 옵션을 CLI 명령에 추가해야 한다. 이 키는 nova.conf "
"구성 파일에 정의된 `hmac_keys` 옵션 하위 `[profiler]` 구성 섹션에서 정의된 secret 키 중 하나를 나타낸다. "
"Nova 프로파일을 활성화하거나 비활성화할 수 있는 `enabled` 옵션을 동일한 섹션에서 `True` 또는 `False`로 설정해야 "
"한다. 기본적으로 Nova는 API 및 RPC 요청을 tracing할 수 있지만 DB 요청도 tracing할 수 있다. 이 목적으로 "
"`trace_sqlalchemy` 옵션을 `True`로 설정해야 한다. OSProfiler 라이브러리와_storage backend가 "
"환경에 설치되어 있으면 (nova.conf에서 프로파일이 활성화된 경우) tracing이 생성할 수 있다. 예를 들어 - `$ nova "
"--profile SECRET_KEY boot --image <image> --flavor <flavor> <name>` 명령을 사용할 "
"수 있다. tracing 결과에는 `<trace_id>`가 나타나며, HTML 그래프를 plots할 수 있는 명령은 `$ "
"osprofiler trace show <trace_id> --html --out result.html`이다."

#: ../../<reno.sphinxext origin/stable/ocata>:540
msgid ""
"Ocata contains a lot of new CellsV2 functions, but not all of it is fully "
"ready for production. All deployments must set up their existing nodes as a "
"cell, with database connection and MQ transport_url config items matching "
"that cell. In a subsequent release, additional cells will be fully "
"supported, as will a migration path for CellsV1 users. By default, an Ocata "
"deployment now needs to configure at least one new \"Cell V2\" (not to be "
"confused with the first version of cells). In Newton, it was possible to "
"deploy a single cell V2 and schedule on it but this was optional. Now in "
"Ocata, single CellsV2 deployments are mandatory. More details to be found "
"when reading the release notes below."
msgstr ""
"Ocata에는 많은 새로운 CellsV2 기능이 포함되어 있지만 production-ready가 아니라는 모든 배포는 기존 노드를 "
"세ลล로 설정해야 하며, 데이터베이스 연결과 MQ transport_url config 아이템이 일치해야 한다. 다음 릴리스에서 추가적인"
" 세ลล들이 완전히 지원될 것이고, CellsV1 사용자에게 마이그레이션 경로도 완전히 지원될 것이다. 기본적으로, Ocata 배포는 "
"최소한 한 개의 새로운 \"세ลล V2\" (첫 번째 버전의 세ลล과 혼동하지 않도록)를 구성해야 한다. Newton에서는 단일 세ลล "
"V2를 배포하고 스케줄링할 수 existed but 이것은 선택적이었다. 현재 Ocata에서는 단일 CellsV2 배포가 의무적이다. 더"
" 많은 세부 사항은 아래 릴리스 노트를 읽어보면 찾을 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:1243
msgid ""
"Ocata requires that your deployment have created the cell and host mappings "
"in Newton. If you have not done this, Ocata's `db sync` command will fail. "
"Small deployments will want to run `nova-manage cell_v2 simple_cell_setup` "
"on Newton before upgrading. Operators must create a new database for cell0 "
"before running `cell_v2 simple_cell_setup`. The simple cell setup command "
"expects the name of the cell0 database to be `<main database name>_cell0` as"
" it will create a cell mapping for cell0 based on the main database "
"connection, sync the cell0 database, and associate existing hosts and "
"instances with the single cell."
msgstr ""
"Ocata는 Newton에서 세ลล과 호스트 매핑을 생성해야 합니다. Newton에서 세ลล과 호스트 매핑을 생성하지 않으면 Ocata의"
" `db sync` 명령은 실패합니다. 작은 배포는 Newton에서 `nova-manage cell_v2 "
"simple_cell_setup` 명령을 실행해야 합니다. Newton에서 `cell_v2 simple_cell_setup` 명령을 "
"실행하기 전에 `cell0` 세ลล에 대한 새로운 데이터베이스를 생성해야 합니다. 단순 세ลล 설정 명령은 `<main database "
"name>_cell0`라는 이름의 세ลล0 데이터베이스 이름을 기대합니다. 이 명령은 세ลล0에 대한 세ลล 매핑을 세ลล0 "
"데이터베이스와의 메인 데이터베이스 연결에 기반하여 생성하고, 세ลล0 데이터베이스를 동기화하고, 존재하는 호스트와 인스턴스를 단일 "
"세ลล과 연결합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:803
msgid ""
"Older microversions will not accept these new paging query parameters, but "
"they will start to silently limit by the max limit to encourage the adoption"
" of this new microversion, and circumvent the existing possibility DoS-like "
"usage requests on systems with thousands of instances."
msgstr ""
"older microversions은 이러한 새로운 페이지 쿼리 매개 변수를 수용하지 않지만, 새로운 microversion의 "
"adoption을 유도하기 위해 max limit에 따라 비대면적으로 제한을 시작합니다. 이러한 제한은 thousands of "
"instances가 있는 시스템에서 존재하는 existing possibility DoS-like usage requests를 피합니다."

#: ../../<reno.sphinxext stable/pike>:298 stable/queens>:1197
msgid ""
"On AArch64 architecture ``cpu_mode`` for libvirt is set to ``host-"
"passthrough`` by default."
msgstr "아키텍처는 AArch64이며, libvirt에서 \"cpu_mode\"은 기본적으로 \"host-passthrough\"로 설정됩니다."

#: ../../<reno.sphinxext stable/2025.1>:287
msgid ""
"On HCI deployments where Nova is collocated with the Cinder service or the "
"Glance using Cinder backend service, an os-brick shared location can be "
"configured using the ``lock_path`` in the ``[os_brick]`` configuration "
"section."
msgstr ""
"HCI 배포에서 노바가 시더 서비스 또는 글랜스와 함께 시더 백엔드 서비스를 사용할 때, os-brick 공유 위치를 "
"``lock_path`` 속성에 따라 ``[os_brick]`` 구성 섹션에서 구성할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:486
msgid ""
"On certain network fabrics, VMs that are live migrated remain inaccessible "
"via the network despite the QEMU monitor announce_self command successfully "
"being called."
msgstr ""
"certain 네트워크 패션에서, live migrate 된 VM이 네트워크를 통해 접근할 수 없다는 점은, QEMU 모니터가 "
"announce_self 명령을 성공적으로 호출했음에도 불구하고, indeed."

#: ../../<reno.sphinxext unmaintained/xena>:706
msgid ""
"On some hardware platforms, an SR-IOV virtual function for a NIC port may "
"exist without being associated with a parent physical function that has an "
"assocatied netdev. In such a case the the PF interface name lookup will "
"fail. As the ``PciDeviceNotFoundById`` exception was not handled this would "
"prevent the nova compute agent from starting on affected hardware. See: "
"https://bugs.launchpad.net/nova/+bug/1915255 for more details. This edgecase"
" has now been addressed, however, features that depend on the PF name such "
"as minimum bandwidth based QoS cannot be supported on these platforms."
msgstr ""
"다음은 Korean로 번역된 텍스트입니다.\n"
"\n"
"某些 하드웨어 플랫폼에서 NIC 포트에 SR-IOV 가상 함수가 존재할 수 있지만 부모 물리 함수와 연관된 네트워크 디바이스에 연결되지 않은 경우에 대해 PF 인터페이스 이름을查找할 때는 실패할 수 있습니다. \"PciDeviceNotFoundById\" 예외가 처리되지 않았기 때문에, 이러한 하드웨어에 대한 nova 컴퓨터 एजент이 시작되지 않을 수 있습니다. 더 많은 정보는 다음 URL을 참조하십시오: https://bugs.launchpad.net/nova/+bug/1915255. 이러한 에지 케이스는 이제 addressing되었지만, PF 이름에 의존하는 기능은 이러한 플랫폼에서 지원할 수 없습니다. 예를 들어, 최소帯幅 QoS를 지원할 수 없습니다."

#: ../../<reno.sphinxext stable/queens>:1056
msgid ""
"On the other hand, enabling it will make packing of VMs on hypervizors less "
"dence even when host weighing is disabled."
msgstr "다른方面으로, 이 기능을 활성화하면 하이퍼바이저에서 VM의 패킹이 하위 가중치가 비활성화되어도 더 강조되지 않는다."

#: ../../<reno.sphinxext unmaintained/victoria>:651
msgid ""
"On upgrade operators should ensure they have not configured any of the new "
"removed filters and instead should use placement to control cpu, ram and "
"disk allocation ratios."
msgstr ""
"업그레이드 운영자들은 새로운 제거된 필터 중ใด도 keinen configuration을 하도록 확인해야 합니다. 대신ly "
"placement을 사용하여 CPU, RAM 및 디스크 할당 비율을 제어해야 합니다."

#: ../../<reno.sphinxext stable/queens>:1144
msgid ""
"Once all compute nodes have VeNCrypt enabled, the ``auth_schemes`` parameter"
" can be set to just ``['vencrypt']``."
msgstr ""
"``compute`` 노드가 모두 ``VeNCrypt``를 활성화되면, ``auth_schemes`` 매개 변수를 단순히 "
"``['vencrypt']``로 설정할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:420
msgid ""
"Once all data migrations are complete, you can set this option to False to "
"stop reporting VCPU, MEMORY_MB and DISK_GB resource class inventory to the "
"Placement service so that scheduling will only rely on the custom resource "
"class for each ironic node, as described in the document above."
msgstr ""
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다음은 원문입니다.\n"
"\n"
"다"

#: ../../<reno.sphinxext origin/stable/ocata>:975
msgid ""
"Once fully upgraded, if you create multiple real cells with hosts, the "
"scheduler will utilize them, but those instances will likely be unusable "
"because not all API functions are cells-aware yet."
msgstr ""
"fully upgraded 후에, 여러 실제 세ลล스를 호스트와 함께 만들면, 스케줄러는 그들을 사용할 수 있지만, API 함수는 아직 "
"모든 세ลล에-aware가 아니기 때문에, 그 인스턴스는 사용할 수 없을 것입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:223 stable/pike>:249
#: stable/queens>:516 stable/rocky>:908
msgid ""
"One of the motivations for this is to alleviate the performance degradation "
"(caused as a result of applying the \"Meltdown\" CVE fixes) for guests "
"running with certain Intel-based virtual CPU models.  This guest performance"
" impact is reduced by exposing the CPU feature flag 'PCID' (\"Process-"
"Context ID\") to the *guest* CPU, assuming that it is available in the "
"physical hardware itself."
msgstr ""
"이것의 한 motivation은 \"Meltdown\" CVE fixes를 적용하여 발생하는 성능 degradement (가UEST가 "
"특정 인텔 기반 가상 CPU 모델을 실행하는 경우)를 해소하기 위해이다.  이 가UEST 성능 영향을 줄이기 위해, CPU feature"
" flag 'PCID' (\"Process-Context ID\")를 *가UEST* CPU에 노출시키고, 가상 CPU가 물리적 하드웨어 "
"자체에 sẵn có다고 가정한다."

#: ../../<reno.sphinxext stable/train>:584
msgid ""
"Only PMEM namespaces listed in the configuration file can be used by "
"instances. To identify the available PMEM namespaces on the host or create "
"new namespaces, the ``ndctl`` utility can be used::"
msgstr ""
"만약 PMEM namespace가 configuration file에 listing되어 있는 경우만 인스턴스에 사용할 수 있습니다. "
"호스트 또는 새로운 namespace를 생성하여 PMEM namespace를 확인하고 사용할 수 있는 utility는 ndctl입니다."

#: ../../<reno.sphinxext stable/2024.1>:43 stable/2024.2>:43 stable/2025.1>:31
#: stable/2025.2>:516
msgid ""
"Only attempting RPC-based uptime retrieval for hypervisor types that "
"actually support it (libvirt and z/VM), avoiding unnecessary calls to other "
"hypervisor types that would always return NotImplementedError"
msgstr ""
"만약 하이퍼바이저 타입이 실제로 지원되는 경우만 RPC 기반 uptime 리trieval을 시도하는 것으로, 불필요한 다른 하이퍼바이저 "
"타입에 대한 호출을 피하고, 항상 NotImplementedError를 반환하는 하이퍼바이저 타입은 무시한다."

#: ../../<reno.sphinxext unmaintained/2023.1>:292
msgid ""
"Operators can now ask Nova to `manage the power consumption of dedicated "
"CPUs <https://docs.openstack.org/nova/latest/admin/cpu-"
"topologies.html#configuring-cpu-power-management-for-dedicated-cores>`_ so "
"as to either offline them or change their governor if they're currently not "
"in use by any instance or if the instance is stopped."
msgstr ""
"사용자들은 now Nova를 `dedicated CPU의 전력 소모를 관리하도록 "
"<https://docs.openstack.org/nova/latest/admin/cpu-"
"topologies.html#configuring-cpu-power-management-for-dedicated-cores>`_ 명령을 "
"사용하여, 또는 현재 어떤 인스턴스에 의해 사용되지 않는 경우에 따라 CPU를 비활성화하거나 governor를 변경할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:441
msgid ""
"Operators can now set overcommit allocation ratios using Nova configuration "
"files or the placement API, by making use of the initial allocation ratio "
"configuration options. See the `initial allocation ratios`_ documentation "
"for more details."
msgstr ""
"사용자들은 now Nova 구성 파일 또는 placement API를 사용하여 초기 할당 비율 설정 옵션을 사용하여 overcommit "
"할당 비율을 설정할 수 있습니다. 초기 할당 비율 설정 옵션을 사용하여 할당 비율을 설정할 수 있습니다. 초기 할당 비율에 대한 더 많은"
" 정보는 `initial allocation ratios`_ 문서를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:649
msgid ""
"Operators can specify a VGPU resource in a flavor by adding in the flavor's "
"extra specs::"
msgstr "연산자는 flavor의 추가 속성을 추가하여 flavor에 VGPU 리소스를 지정할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:822
msgid ""
"Operators changing the ``[compute]/max_disk_devices_to_attach`` on a compute"
" service that is hosting servers should be aware that it could cause "
"rebuilds to fail, if the maximum is decreased lower than the number of "
"devices already attached to servers. For example, if server A has 26 devices"
" attached and an operators changes ``[compute]/max_disk_devices_to_attach`` "
"to 20, a request to rebuild server A will fail and go into ERROR state "
"because 26 devices are already attached and exceed the new configured "
"maximum of 20."
msgstr ""
"``[compute]/max_disk_devices_to_attach``를 사용하는 컴퓨팅 서비스가 서버를 호스팅하는 운영자는 이 설정을"
" 변경할 때 주의할 필요가 있습니다. 설정을 낮추면 재건을 실패할 수 있습니다. 설정이 낮추어도 현재 연결된 디스크의 수보다 낮을 "
"때입니다. 예를 들어, 서버 A가 26개의 디스크를 연결하고 운영자가 "
"``[compute]/max_disk_devices_to_attach``를 20으로 설정한다면, 서버 A를 재건하는 요청이 실패하고 "
"ERROR 상태로 이동할 수 있습니다. 26개의 디스크가 이미 연결되어 있으며, 새로운 설정된 최대 디스크 수 20보다 많기 때문입니다."

#: ../../<reno.sphinxext stable/rocky>:1797
msgid "Operators may unset the configuration option when:"
msgstr "연산자는 구성 옵션을 비우는 경우에 다음과 같은 상황이 발생할 수 있다."

#: ../../<reno.sphinxext stable/ussuri>:1434
msgid ""
"Operators must ensure no instances are running on the compute host before "
"enabling this workaround. Any instances with attached RBD volumes left "
"running on the hosts will fail to migrate or stop after this workaround has "
"been enabled."
msgstr ""
"운영자는 컴퓨팅 호스트에 인스턴스가-running하는 것을 확인하여 이 workaround을 활성화할 수 있도록 해야 합니다. RBD "
"볼륨이 연결된 인스턴스만이 호스트에 남아 있는 경우 이 workaround이 활성화된 후에 인스턴스가 이식되지 않거나 중단되거나 "
"fails 할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:1411
msgid ""
"Operators must ensure no instances are running on the compute host before "
"enabling this workaround. Any instances with encrypted LUKSv1 disks left "
"running on the hosts will fail to migrate or stop after this workaround has "
"been enabled."
msgstr ""
"운영자는 컴퓨터 호스트에 인스턴스가 실행 중인 경우 이 workaround을 활성화할 수 없습니다. 암호화된 LUKSv1 디스크가 남아 "
"있는 인스턴스를 호스트에 실행하고 있는 경우 이 workaround이 활성화된 후에 이 인스턴스는 이гnore 할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:831
msgid ""
"Operators setting ``[compute]/max_disk_devices_to_attach`` should also be "
"aware that during a cold migration, the configured maximum is only enforced "
"in-place and the destination is not checked before the move. This means if "
"an operator has set a maximum of 26 on compute host A and a maximum of 20 on"
" compute host B, a cold migration of a server with 26 attached devices from "
"compute host A to compute host B will succeed. Then, once the server is on "
"compute host B, a subsequent request to rebuild the server will fail and go "
"into ERROR state because 26 devices are already attached and exceed the "
"configured maximum of 20 on compute host B."
msgstr ""
"``[compute]/max_disk_devices_to_attach``를 설정하는 운영자는 또한 냉각 이민을进行할 때, 설정된 최대는 "
"place에만 enforced되며, 목적지가 확인되지 않기 때문에 이동 전에는 확인되지 않는다는 것을 aware해야합니다. 이 의미는 "
"운영자가 compute host A에서 26개의 디스크를 연결한 server를 compute host B로 이동시킬 때, 26개의 "
"디스크가 already attached되어 compute host B의 설정된 최대 20개가 초과하는 경우, subsequent "
"request를 통해 server를 다시 rebuilding하려고 할 때, error state로 fail할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:235
msgid ""
"Operators should be aware that nova-api has a dependency on eventlet for "
"executing parallel queries across multiple cells and is monkey-patched "
"accordingly. When nova-api is running under uWSGI or mod_wsgi, the wsgi app "
"will pause after idle time. While the wsgi app is paused, rabbitmq "
"heartbeats will not be sent and log messages related to this can be seen in "
"the nova-api logs when the wsgi app resumes when new requests arrive to the "
"nova-api. These messages are not harmful. When the wsgi app resumes, "
"oslo.messaging will reconnect to rabbitmq and requests will be served "
"successfully."
msgstr ""
"nova-api가 여러 세ลล across multiple cells에서 병렬 쿼리를 수행하기 위해 eventlet에 의존하고 있으며, "
"accordingly monkey-patched됩니다. nova-api가 uWSGI 또는 mod_wsgi에서 실행되면, WSGI 앱은 "
"비활성화된 시간이 지속되면 중단됩니다. WSGI 앱이 중단되면, rabbitmq의 심박수는 보낸되지 않으며, nova-api 로그에 "
"관련된 메시지가 보이면 WSGI 앱이 다시 시작되면 새로운 요청이 도착할 때 nova-api 로그에 보이는 메시지입니다. 이러한 메시지는"
" 해로운 것은 아닙니다. WSGI 앱이 다시 시작되면, oslo.messaging은 rabbitmq와 다시 연결되고, 요청이 성공적으로 "
"제공됩니다."

#: ../../<reno.sphinxext stable/queens>:205 stable/rocky>:350
#: stable/stein>:1400
msgid ""
"Operators should be aware that this workaround only applies when using the "
"libvirt compute driver and rbd images_type as enabled by the following "
"configuration options:"
msgstr ""
"연산자는 다음 workaround이 libvirt 컴퓨터 드라이버와 rbd 이미지 타입이 활성화된 경우에만 적용되며, 다음 구성 옵션을 "
"사용할 때입니다."

#: ../../<reno.sphinxext stable/ussuri>:1401
msgid ""
"Operators should be aware that this workaround only applies when using the "
"libvirt compute driver with attached encrypted Cinder volumes using the "
"``luks`` encryption provider. The ``luks2`` encryption provider will "
"continue to use the ``dm-crypt`` based os-brick encryptors regardless of "
"what this configurable is set to."
msgstr ""
"이 workaround은 libvirt 컴퓨터 드라이버와 암호화된 Cinder 볼륨을 사용하는 경우에만 적용됩니다. 암호화 제공자로 "
"``luks``를 사용하고, ``luks2``를 사용하는 경우는 ``dm-crypt`` 기반 os-brick 암호화器를 계속 사용합니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:418
msgid ""
"Operators will have to consider upgrading compute hosts to Nova 27.0.0 "
"(antelope 2023.1) in order to take advantage of the new (microversion v2.95)"
" evacuate API behavior. An exception will be raised for older versions."
msgstr ""
"집합은 업그레이드하여 Nova 27.0.0 (antelope 2023.1)로 컴퓨터 호스트를 업그레이드해야 하며, 새로운 "
"(microversion v2.95) evacuate API 행동을 사용할 수 있는 것을 이용할 수 있습니다. 더 오래된 버전은 예외로 "
"raises exception이 발생합니다."

#: ../../<reno.sphinxext stable/queens>:212 stable/rocky>:357
#: stable/stein>:1407
msgid ""
"Operators will need to ensure that the instance directory itself, specified "
"by ``[DEFAULT]/instances_path``, is not shared between computes before "
"enabling this workaround, otherwise files associated with running instances "
"may be removed."
msgstr ""
"연산자는 인스턴스 디렉터티 자체를 ``[DEFAULT]/instances_path``로 지정하여, 이 workaround를 활성화할 "
"때까지 컴퓨터 간에 공유되지 않도록 확인해야 합니다. otherwise, running 인스턴스와 관련된 파일이 제거될 수 있습니다."

#: ../../<reno.sphinxext stable/2025.1>:302
msgid ""
"Option ``novncproxy_base_url`` does now respect supplied custom query which "
"might be used to move NoVNC to a subdirectory or pass an extra argument to "
"NoVNC."
msgstr ""
"`novncproxy_base_url` 옵션은 현재 공급된 custom query를 respects 하며, 이 query가 NoVNC를 "
"서브 디렉터리로 이동하거나 NoVNC에 추가 argument를 전달할 수 있습니다."

#: ../../<reno.sphinxext ../source/liberty.rst:123 ../source/mitaka.rst:693
#: ../source/newton.rst:1114 origin/stable/ocata>:72 origin/stable/ocata>:1628
#: stable/pike>:506 stable/pike>:1856 stable/queens>:194 stable/queens>:378
#: stable/queens>:1825 stable/rocky>:131 stable/rocky>:339 stable/rocky>:2044
#: stable/stein>:208 stable/stein>:1389 stable/train>:101 stable/train>:186
#: stable/train>:1400 stable/ussuri>:1316 unmaintained/2023.1>:551
#: unmaintained/wallaby>:985 unmaintained/xena>:132 unmaintained/yoga>:209
#: unmaintained/yoga>:664 unmaintained/zed>:74 unmaintained/zed>:160
msgid "Other Notes"
msgstr ""
"*other* : 다른\n"
"*Notes* : 비고"

#: ../../<reno.sphinxext stable/rocky>:1791
msgid "Otherwise wish to avoid immediately resetting all existing consoles"
msgstr "다만, 모든 현재 콘솔을 즉시 리셋하고 싶지 않다면"

#: ../../<reno.sphinxext stable/stein>:666
msgid "POST /servers/{server_id}/os-interface (attach)"
msgstr "POST /서버/{서버_아이디}/OS-인터페이스 (연결)"

#: ../../<reno.sphinxext stable/stein>:660
msgid "POST /servers/{server_id}/os-volume_attachments (attach)"
msgstr "POST /서버/{서버_아이디}/OS-Volume-Attachments (연결)"

#: ../../<reno.sphinxext stable/pike>:878
msgid ""
"PUT /resource_providers/{uuid}/traits: Set all the traits for a specific "
"resource provider"
msgstr "PUT /resource_providers/{uuid}/traits: 특정 리소스 프로バイ더에 모든 trait를 설정합니다"

#: ../../<reno.sphinxext stable/pike>:873
msgid "PUT /traits/{name}: To insert a single custom trait."
msgstr "PUT /traits/{name}: 집합에 단일 custom trait을 추가합니다."

#: ../../<reno.sphinxext stable/2024.1>:459
msgid ""
"Packed Ring can be requested via image property or flavor extra spec. "
"hw_virtio_packed_ring=true|false  (default false) "
"hw:virtio_packed_ring=true|false  (default false)"
msgstr ""
"집합 Ring가 이미지 속성 또는 플레버 추가 속성에서 요청할 수 있습니다. hw_virtio_packed_ring=true|false "
"(기본적으로 false) hw:virtio_packed_ring=true|false (기본적으로 false)"

#: ../../<reno.sphinxext stable/pike>:1244
msgid ""
"Parts of the compute REST API are now relying on getting information from "
"cells via their mappings in the ``nova_api`` database. This is to support "
"multiple cells. For example, when listing compute hosts or services, all "
"cells will be iterated in the API and the results will be returned."
msgstr ""
"nova_api 데이터베이스의 세포 mapping을 통해 정보를 얻는 compute REST API의 일부는 현재 여러 세포를 지원하기 "
"위해 사용되고 있습니다. 예를 들어, 컴퓨터 호스트 또는 서비스를 liệtelson API를 사용하여 liệtelson할 때, 모든 "
"세포가 API에서 반복적으로 처리되고 결과가 반환됩니다."

#: ../../<reno.sphinxext stable/pike>:1850
msgid ""
"Physical network name will be retrieved from a multi-segement network. The "
"current implementation will retrieve the physical network name for the first"
" segment that provides it. This is mostly intended to support a combinatin "
"of vxlan and vlan segments. Additional work will be required to support a "
"case of multiple vlan segments associated with different physical networks."
msgstr ""
"물리적 네트워크 이름은 다중 세그먼트 네트워크에서 물리적 네트워크 이름을 가져올 것이다. 현재 구현은 첫 번째 세그먼트가 물리적 네트워크"
" 이름을 제공하는 것을 가져올 것이다. 이것은 대부분 vxlan 및 vlan 세그먼트를 결합하는 것을 지원하기 위해 설계되었다. 추가 "
"작업이 필요하여 다중 vlan 세그먼트가 다른 물리적 네트워크와 관련된 경우를 지원할 것이다."

#: ../../<reno.sphinxext origin/stable/ocata>:362
msgid ""
"Physical network name will be retrieved from a multi-segment network. The "
"current implementation will retrieve the physical network name for the first"
" segment that provides it. This is mostly intended to support a combination "
"of vxlan and vlan segments. Additional work will be required to support a "
"case of multiple vlan segments associated with different physical networks."
msgstr ""
"물리적 네트워크 이름은 다 segment 네트워크에서 물리적 네트워크 이름을 retrieves 할 것이다. 현재 구현은 첫 번째 "
"segment가 물리적 네트워크 이름을 제공하는 것을 retrieves 할 것이다. 이것은 대부분 vxlan 및 vlan segment의"
" 조합을 지원하기 위해 설계되었다. 추가 작업이 필요하여 다수의 vlan segment이 다른 물리적 네트워크와 관련된 경우에만 지원할 "
"것이다."

#: ../../<reno.sphinxext stable/rocky>:1174
msgid ""
"Placement API microversion 1.18 adds support for the `required` query "
"parameter to the `GET /resource_providers` API. It accepts a comma-separated"
" list of string trait names. When specified, the API results will be "
"filtered to include only resource providers marked with all the specified "
"traits. This is in addition to (logical AND) any filtering based on other "
"query parameters."
msgstr ""
"Placement API microversion 1.18는 `GET /resource_providers` API에 `required` "
"쿼리 파라미터를 추가합니다. 이 파라미터는 문자열 trait 이름의 쉼표-separated 목록을 수용합니다. trait 이름이 지정되면"
" API 결과는 모든 지정된 trait에 마ーク된 리소스 제공자만 포함됩니다. 이외에 ( 논리 AND ) 다른 쿼리 파라미터에 기반한 "
"필터링에도 추가됩니다."

#: ../../<reno.sphinxext stable/rocky>:1071
msgid ""
"Placement API microversion 1.19 enhances the payloads for the `GET "
"/resource_providers/{uuid}/aggregates` response and the `PUT "
"/resource_providers/{uuid}/aggregates` request and response to be identical,"
" and to include the ``resource_provider_generation``. As with other "
"generation-aware APIs, if the ``resource_provider_generation`` specified in "
"the `PUT` request does not match the generation known by the server, a 409 "
"Conflict error is returned."
msgstr ""
"Placement API microversion 1.19는 `GET /resource_providers/{uuid}/aggregates`"
" 응답 및 `PUT /resource_providers/{uuid}/aggregates`요청 및 응답을 동일하게 하며, "
"`resource_provider_generation` 속성을 포함합니다. 다른 생성 awareness API와 마찬가지로, `PUT` "
"요청에서 `resource_provider_generation` 속성이 서버가 biết하는 생성과 일치하지 않으면 409 Conflict"
" 오류가 반환됩니다."

#: ../../<reno.sphinxext stable/rocky>:1114
msgid ""
"Placement microversion '1.22' adds support for expressing traits which are "
"forbidden when filtering ``GET /resource_providers`` or ``GET "
"/allocation_candidates``. A forbidden trait is a properly formatted trait in"
" the existing ``required`` parameter, prefixed by a ``!``. For example "
"``required=!STORAGE_DISK_SSD`` asks that the results not include any "
"resource providers that provide solid state disk."
msgstr ""
"Placement microversion '1.22'은 `GET /resource_providers` 또는 `GET "
"/allocation_candidates`에서 필터링할 때 금지된 특성의 표현을 지원합니다. 금지된 특성은 `required` 매개 "
"변수에 있는 existingly formatted trait을 prefix로 `!`로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 prefix로 "
"prefix로 prefix로 prefix로 prefix로 prefix"

#: ../../<reno.sphinxext stable/pike>:1652
msgid "Please note you should remove these from your policy file(s)."
msgstr "집합을 제거하십시오."

#: ../../<reno.sphinxext stable/ussuri>:674 unmaintained/yoga>:368
#: unmaintained/zed>:340
msgid ""
"Please refer `Policy New Defaults`_ for detail about policy new defaults and"
" migration plan."
msgstr ""
"`Policy New Defaults`_에 대한 정책 new defaults 및 이식 계획에 대한รายละเอียด을 참조하십시오."

#: ../../<reno.sphinxext stable/2025.2>:161
msgid ""
"Please refer `Policy New Defaults`_ for detail about policy new defaults."
msgstr "`Policy New Defaults`_에 대한 정책 새로운 기본값에 대한 세부 사항을 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:270
msgid "Please see the following related bugs for more details:"
msgstr "집합을 참조하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:178 stable/pike>:328
#: stable/queens>:1234
msgid ""
"Please see the mailing list thread for more information: "
"http://lists.openstack.org/pipermail/openstack-"
"operators/2018-January/014748.html"
msgstr ""
"http://lists.openstack.org/pipermail/openstack-"
"operators/2018-01/014748.html의 메일링 리스트 트레드에서 더 많은 정보가 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:690
msgid ""
"Policies are default to Admin, Member and Reader roles. Old roles are also "
"supproted. You can switch to new defaults via config option "
"``[oslo_policy]enforce_new_defaults`` in ``nova.conf`` file."
msgstr ""
"정책은 기본적으로 Admin, Member, Reader 역할에 적용됩니다.舊 역할도 지원됩니다. 새로운 기본값을 설정할 수는 "
"`oslo_policy` 옵션의 `[enforce_new_defaults]` 설정을 통해 `nova.conf` 파일에서 가능합니다."

#: ../../<reno.sphinxext stable/pike>:1371
msgid ""
"Policy rule with name os_compute_api:os-admin-actions has been removed as it"
" was never used by any API."
msgstr ""
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의 이름이 'os-admin-actions'인 정책 규칙이 이름 "
"os_compute_api:os-admin-actions의"

#: ../../<reno.sphinxext stable/rocky>:1371
msgid ""
"Ports from the physical network will have to be created with a binding "
"profile to match the trusted tag. Only ports with "
"``binding:vif_type=hw_veb`` and ``binding:vnic_type=direct`` are supported."
msgstr ""
"물리적 네트워크에서 coming Ports는 trusted tag와 일치하는 바인딩 프로필을 사용하여 생성해야 합니다. only "
"binding:vif_type=hw_veb와 binding:vnic_type=direct가 있는 포트만 지원됩니다."

#: ../../<reno.sphinxext stable/2024.1>:46 stable/2024.2>:46 stable/2025.1>:34
#: stable/2025.2>:519
msgid ""
"Preferring cached uptime data from the database over RPC calls when "
"available, this updates at the cadence specified by "
"`[DEFAULT]update_resources_interval` which is the same interval the other "
"hypervisor stats update."
msgstr ""
"캐시된 업타임 데이터를 데이터베이스에서 RPC 호출을 사용할 때 readily available한 경우 선호하는 방법입니다. 이 "
"업데이트는 `[DEFAULT]update_resources_interval`에 정의된 간격에 따라 cadence에 따라 업데이트됩니다. "
"이 간격은 다른 하이퍼바이저 스태트의 업데이트 간격과 동일합니다."

#: ../../<reno.sphinxext ../source/liberty.rst:71 ../source/mitaka.rst:13
#: ../source/mitaka.rst:119 ../source/newton.rst:137 ../source/newton.rst:237
#: ../source/newton.rst:275 origin/stable/ocata>:147 origin/stable/ocata>:403
#: origin/stable/ocata>:430 origin/stable/ocata>:511 stable/2023.2>:173
#: stable/2024.1>:290 stable/2024.2>:216 stable/2025.1>:168 stable/2025.2>:10
#: stable/pike>:216 stable/pike>:580 stable/queens>:581 stable/rocky>:499
#: stable/stein>:401 stable/train>:453 stable/ussuri>:329
#: unmaintained/2023.1>:264 unmaintained/victoria>:315
#: unmaintained/wallaby>:380 unmaintained/xena>:286 unmaintained/yoga>:259
#: unmaintained/zed>:182
msgid "Prelude"
msgstr "전주"

#: ../../<reno.sphinxext stable/2023.2>:150 stable/2024.1>:583
#: unmaintained/2023.1>:218 unmaintained/zed>:56
msgid ""
"Previously ``switchdev`` capabilities should be configured manually by a "
"user with admin privileges using port's binding profile. This blocked "
"regular users from managing ports with Open vSwitch hardware offloading as "
"providing write access to a port's binding profile to non-admin users "
"introduces security risks. For example, a binding profile may contain a "
"``pci_slot`` definition, which denotes the host PCI address of the device "
"attached to the VM. A malicious user can use this parameter to passthrough "
"any host device to a guest, so it is impossible to provide write access to a"
" binding profile to regular users in many scenarios."
msgstr ""
"이전 ``switchdev`` 능력은 사용자가 관리자 권한을 가지고 있는 경우, 포트의 바인딩 프로필을 수동으로 구성해야 합니다. 이로 "
"인해 정상 사용자가 Open vSwitch 하드웨어 오프로딩을 사용할 수 없습니다. 예를 들어, 바인딩 프로필에는 ``pci_slot``"
" 정의가 포함될 수 있으며, 이 정의는 VM에 연결된 장치의 호스트 PCI 주소를 나타냅니다. 위험한 사용자가 이 매개 변수를 사용하여 "
"호스트 장치를 게스트로 전달할 수 있으므로, 정상 사용자가 바인딩 프로필에 쓰기 액세스를 제공할 수 있는 경우가 많습니다."

#: ../../<reno.sphinxext stable/rocky>:1611
msgid ""
"Previously the PowerVM driver would default to 0.5 physical processors per "
"vCPU, which is the default from the pypowervm library. The default will now "
"be 0.1 physical processors per vCPU, from the ``proc_units_factor`` "
"configuration option in the ``powervm`` configuration group."
msgstr ""
"이전에는 PowerVM 드라이버가 기본적으로 0.5 개의 물리적 프로세서가 vCPU에 대한 기본으로, pypowervm 라이브러리에서 "
"기본으로 사용되었습니다.  현재 기본은 ``proc_units_factor`` 구성 옵션의 ``powervm`` 구성 그룹에서 "
"``0.1`` 물리적 프로세서가 vCPU에 대한 기본으로 사용되었습니다."

#: ../../<reno.sphinxext stable/stein>:116 stable/train>:265
#: stable/ussuri>:315 unmaintained/victoria>:792
msgid ""
"Previously, attempting to configure an instance with the ``e1000e`` or "
"legacy ``VirtualE1000e`` VIF types on a host using the QEMU/KVM driver would"
" result in an incorrect ``UnsupportedHardware`` exception. These interfaces "
"are now correctly marked as supported."
msgstr ""
"이전에는 호스트에서 QEMU/KVM 드라이버를 사용하여 ``e1000e`` 또는 전통적인 ``VirtualE1000e`` VIF 타입을 "
"사용하여 인스턴스를 구성하는 시도를 하게 되면 ``UnsupportedHardware`` 예외가 올랐습니다. 이 인터페이스는 현재 적절히"
" 지원되도록 표시되었습니다."

#: ../../<reno.sphinxext stable/2023.2>:424 unmaintained/2023.1>:94
msgid ""
"Previously, deleted rows were archived in batches of max_rows parents + "
"their child rows in a single database transaction. It limited how high a "
"value of max_rows could be specified by the user because of the size of the "
"database transaction it could generate. Symptoms of the behavior were "
"exceeding the maximum configured packet size of the database or timing out "
"due to a deadlock."
msgstr ""
"이전에는, max_rows 부모 rows + 그 child rows를 한 database transaction에서 아카이브하는 "
"batch로 삭제된 row를 archiving했습니다. 이로 인해, max_rows의 사용자 지정 giá치가 database "
"transaction의 크기 때문에 제한되었습니다. 이 behavior의 증상은 database의 최대 구성된 패킷 크기 초과 또는 "
"deadlock으로 인해 시간이 지연되었습니다."

#: ../../<reno.sphinxext stable/train>:945
msgid ""
"Previously, if ``vcpu_pin_set`` was not defined, the libvirt driver would "
"count all available host CPUs when calculating ``VCPU`` inventory, "
"regardless of whether those CPUs were online or not. The driver will now "
"only report the total number of online CPUs. This should result in fewer "
"build failures on hosts with offlined CPUs."
msgstr ""
"이전에는, `vcpu_pin_set`가 정의되지 않았을 때, libvirt 드라이버는 `VCPU` 인 inventory를 계산할 때, "
"모든 사용 가능한 호스트 CPU를 계산했다. 이 CPU가 온라인이 아니더라도. 그러나 드라이버는 현재 온라인 CPU의 총 수를만 "
"báo합니다. 이로 인해, 오프라인 CPU가 있는 호스트에서 빌드 실패가 줄어들 것입니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:845
msgid ""
"Previously, if the libguestfs package was not installed, the nova-compute "
"service would fallback to mounting to the local compute host file system "
"which is a security exposure. This has been discussed for years in several "
"forums:"
msgstr ""
"이전에는, libguestfs 패키지가 설치되지 않았을 때, nova-compute 서비스는 로컬 컴퓨터 호스트 파일 시스템에 마운트하는"
" 것을 fallback로 사용하는 것이 보안 위험이었다. 이 문제는 수년 동안 여러 포럼에서 논의되었다."

#: ../../<reno.sphinxext unmaintained/victoria>:799
msgid ""
"Previously, it was possible to specify values for the "
"``hw:cpu_realtime_mask`` extra spec that were not within the range of valid "
"instances cores. This value is now correctly validated."
msgstr ""
"이전에는 유효한 인스턴스 코어의 범위 이외의 가치로 ``hw:cpu_realtime_mask`` 이외의 추가 spéc이 지정할 수 "
"있었다. 이 값은 현재 적절하게 유효화되었다."

#: ../../<reno.sphinxext stable/pike>:1208
msgid ""
"Previously, setting the ``use_ipv6`` config option to ``False`` prevented "
"the generation of IPv6 network info, even if there were IPv6 networks "
"available. This was fine when using nova-network, where the same config "
"option is used to control generation of these subnets. However, a mismatch "
"between this nova option and equivalent IPv6 options in neutron would have "
"resulted in IPv6 packets being dropped."
msgstr ""
"이전에는 `use_ipv6` 설정 옵션을 `False`로 설정하면, IPv6 네트워크 정보를 생성하지 않게 되었습니다. 이때는 IPv6 "
"네트워크가 sẵn sàng 있더라도 발생했습니다. 그러나 nova-network을 사용할 때는 이 설정 옵션을 사용하여 이 하위 서브넷을"
" 생성하는 것을 제어하는 것이 가능했습니다. 그러나 nova 옵션과 neutron에 equivalent한 IPv6 옵션의 일치가 없으면,"
" IPv6 패킷이 drop되었습니다."

#: ../../<reno.sphinxext stable/pike>:1138
msgid ""
"Previously, setting the ``use_ipv6`` config option to ``False`` prevented "
"the generation of IPv6 rules even when there were IPv6 subnets available. "
"This was fine when using nova-network, where the same config option was used"
" to control generation of these subnets. However, a mismatch between this "
"nova option and equivalent IPv6 options in neutron would have resulted in "
"IPv6 packets being dropped."
msgstr ""
"이전에는 `use_ipv6` 설정 옵션을 `False`로 설정하면, IPv6 규칙의 생성이 IPv6 서브넷이 있는 경우에도 제대로 "
"작동하지 않았습니다. 이 경우 nova-network을 사용할 때는, 이 설정 옵션을 사용하여 이 서브넷의 생성을 제어하는 것이 "
"적절했습니다. 그러나 nova 옵션과 neutron에 equivalent한 IPv6 옵션의 일치가 없으면, IPv6 패킷이 "
"drop되었습니다."

#: ../../<reno.sphinxext stable/pike>:1168
msgid ""
"Previously, setting the `allow_same_net_traffic` config option to `True` "
"allowed for same network traffic when using these port filters. This was the"
" default case and was the only case tested. Setting this to `False` disabled"
" same network traffic *when using the libvirt driver port filtering "
"functionality only*, however, this was neither tested nor documented."
msgstr ""
"이전에는 `allow_same_net_traffic` 설정 옵션을 `True`로 설정하면, 이 포트 필터를 사용할 때는 동일 네트워크 "
"트래픽을 허용할 수 existed. 이 경우는 기본적으로 설정되었으며, 이 경우는 테스트가 된 유일한 경우였다. 이 설정을 "
"`False`로 설정하면, libvirt 드라이버 포트 필터 기능을 사용할 때만 동일 네트워크 트래픽을 비활성화할 수 existed. "
"그러나 이 경우는 neither 테스트 nor 문서화되었다."

#: ../../<reno.sphinxext unmaintained/victoria>:590
msgid ""
"Previously, the number of concurrent snapshots was unlimited, now it is "
"limited via ``[DEFAULT]/max_concurrent_snapshots``, which currently defaults"
" to 5."
msgstr ""
"이전에는 동시.snapshot의 수가 제한이 없었습니다. 현재는 ``[DEFAULT]/max_concurrent_snapshots``를 "
"통해 제한이 있습니다. 현재 기본적으로 5으로 설정되어 있습니다."

#: ../../<reno.sphinxext stable/train>:966
msgid ""
"Previously, the operation would have failed with an obscure error resulting "
"in the instance still running on the source node or ending up in an "
"inoperable state."
msgstr ""
"previously, the operation would have failed with an aggregate error "
"resulting in the instance still running on the source node or ending up in "
"an inoperable state."

#: ../../<reno.sphinxext unmaintained/wallaby>:964
msgid ""
"Previously, when using the libvirt driver on x86 hosts, a USB controller was"
" added by default to all instances even if no guest device actually required"
" this controller. This has been resolved. A USB controller will now only be "
"added if an input or disk device requires a USB bus."
msgstr ""
"이전에는 x86 호스트에서 libvirt 드라이버를 사용할 때, 모든 인스턴스에 기본적으로 USB 컨트롤러가 추가되었으며, 실제로 이 "
"컨트롤러가 필요하지 않은 경우에도 게스트 디바이스가 필요하지 않은 경우에도 추가되었다. 이 문제는 해결되었다. 현재, USB 컨트롤러는 "
"only input 또는 디스크 디바이스가 USB 버스에 필요할 때 추가된다."

#: ../../<reno.sphinxext origin/stable/ocata>:1569
msgid ""
"Prior to Newton, volumes encrypted by the CryptsetupEncryptor and "
"LuksEncryptor encryption providers used a mangled passphrase stripped of "
"leading zeros per hexadecimal. When opening encrypted volumes, LuksEncryptor"
" now attempts to replace these mangled passphrases if detected while "
"CryptsetupEncryptor simply uses the mangled passphrase."
msgstr ""
"Newton 이전에, CryptsetupEncryptor 및 LuksEncryptor 암호화 제공자에 의해 암호화된 볼륨은 가상화된 "
"패스워드를 사용하여 가공된 패스워스를 사용했습니다. 가공된 패스워스는 16진수에서 leading zeros를 제거한 패스워스입니다. "
"가공된 패스워스를 사용하여 볼륨을 열면, LuksEncryptor는 현재 가공된 패스워스를 교체하는 것을 시도합니다. 가공된 패스워스를 "
"사용하는 CryptsetupEncryptor는 단순히 가공된 패스워스를 사용합니다."

#: ../../<reno.sphinxext stable/rocky>:1163
msgid ""
"Prior to microversion 1.8 of the placement API, one could create allocations"
" and not supply a project or user ID for the consumer of the allocated "
"resources. While this is no longer allowed after placement API 1.8, older "
"allocations exist and we now ensure that a consumer record is created for "
"these older allocations. Use the two new CONF options "
"``CONF.placement.incomplete_consumer_project_id`` and "
"``CONF.placement.incomplete_consumer_user_id`` to control the project and "
"user identifiers that are written for these incomplete consumer records."
msgstr ""
"microversion 1.8의 배치 API 이전에, 배치 API를 사용하여 자원 할당을 생성할 수 existed. 이 자원 할당은 "
"프로젝트 ID 또는 사용자 ID를 제공하지 않아도 할당이 가능했다. 그러나 배치 API 1.8 이후에는 이와 같은 할당이 더 이상 "
"허용되지 않는다. 그러나 이전에 할당된 자원은 여전히 존재하고, 현재 이러한 이전 할당에 대한 소비자 레코드가 생성되도록 보장하고 있다."
" 이러한 이전 할당에 대한 소비자 레코드에 대한 프로젝트 ID와 사용자 ID를 제어하려면, 두 가지 새로운 CONF 옵션을 사용할 수 "
"있다. \"CONF.placement.incomplete_consumer_project_id\"와 "
"\"CONF.placement.incomplete_consumer_user_id\"를 사용하여."

#: ../../<reno.sphinxext stable/ussuri>:1175
msgid ""
"Prior to this release Nova determined if ``UEFI`` support should be enable "
"solely by checking host support as reported in `bug 1845628`_."
msgstr ""
"이 릴리스 이전에 노바는 `bug 1845628`_의 보고된 호스트 지원을 확인하여 `UEFI` 지원을 soleliy 활성화해야 하는지 "
"결정했다."

#: ../../<reno.sphinxext stable/queens>:1660
msgid ""
"Privsep transitions. Nova is transitioning from using the older style "
"rootwrap privilege escalation path to the new style Oslo privsep path. This "
"should improve performance and security of Nova in the long term. - | "
"privsep daemons are now started by nova when required. These daemons can be "
"started via rootwrap if required. rootwrap configs therefore need to be "
"updated to include new privsep daemon invocations."
msgstr ""
"Privsep 전환. 노바는 노래 스타일의 루트 wraps privilege escalations 경로에서 오래된 스타일의 privsep"
" 전환 경로로 전환하고 있습니다. 이는 노바의 장기적인 성능과 보안을 개선할 것으로 예상됩니다. - | privsep 데몬은 now "
"nova가 필요할 때 시작됩니다. 이 데몬은 루트 wraps를 통해 필요할 때 시작할 수 있습니다. 루트 wraps의 구성은 새로운 "
"privsep 데몬의 호출을 포함해야 합니다."

#: ../../<reno.sphinxext stable/ussuri>:390
msgid "Python 2 is no longer supported by Nova, Python 3.6 and 3.7 are."
msgstr "Python 2는 Nova에서 더 이상 지원되지 않지만, Python 3.6 및 3.7은 지원됩니다."

#: ../../<reno.sphinxext stable/ussuri>:796
msgid ""
"Python 2.7 support has been dropped. The minimum version of Python now "
"supported by nova is Python 3.6."
msgstr "Python 2.7 지원이 제거되었습니다. nova에서 현재 지원되는 Python의 최소 버전은 Python 3.6입니다."

#: ../../<reno.sphinxext unmaintained/zed>:446
msgid ""
"Python 3.6 & 3.7 support has been dropped. The minimum version of Python now"
" supported by nova is Python 3.8."
msgstr "Python 3.6 & 3.7 지원이 제거되었습니다. nova에서 지원하는 최소 버전은 Python 3.8이었습니다."

#: ../../<reno.sphinxext stable/queens>:952
msgid ""
"QEMU 2.6.0 and Libvirt 2.2.0 allow LUKS encrypted RAW files, block devices "
"and network devices (such as rbd) to be decrypted natively by QEMU. If qemu "
">= 2.6.0 and libvirt >= 2.2.0 are installed and the volume encryption "
"provider is 'luks', the libvirt driver will use native QEMU decryption for "
"encrypted volumes. The libvirt driver will generate a secret to hold the "
"LUKS passphrase for unlocking the volume and the volume driver will use the "
"secret to generate the required encryption XML for the disk. QEMU will then "
"be able to read from and write to the encrypted disk natively, without the "
"need of os-brick encryptors."
msgstr ""
"QEMU 2.6.0과 Libvirt 2.2.0은 LUKS 암호화된 RAW 파일, 블록 장치 및 네트워크 장치(예: rbd)를 "
"natively QEMU에 의해 암호화된 파일을 복원할 수 있습니다. QEMU >= 2.6.0과 Libvirt >= 2.2.0가 설치되어"
" 있고 볼륨 암호화 제공자는 'luks'인 경우, Libvirt 드라이버는 natively QEMU 암호화로 암호화된 볼륨을 사용합니다."
" Libvirt 드라이버는 LUKS 패스워드를 복원할 수 있는 secret를 생성하고 볼륨 드라이버는 secret를 사용하여 "
"required encryption XML를 생성합니다. QEMU는 then 암호화된 디스크에 read와 write를 natively 할"
" 수 있습니다. os-brick 암호화자 without의 필요가 없습니다."

#: ../../<reno.sphinxext unmaintained/zed>:374
msgid ""
"QEMU and the Linux kernel do not currently support transparent live "
"migration of vDPA devices at this time. Hot-plug live migration unplugs the "
"VDPA device on the source host before the VM is live migrated and "
"automatically hot-plugs the device on the destination after the migration. "
"While this can lead to packet loss it enable live migration to be used when "
"needed until transparent live migration can be added in a future release."
msgstr ""
"QEMU와 리눅스 커널은 현재 transparent live migration of vDPA 장치에 대한 지원을 제공하지 "
"않습니다..hot-plug live migration은 소스 호스트에서 VM이 live migrationed 될 때까지 VDPA 장치를 "
"비우고, 마이그레이션 후에 목적지에 hot-plug을 수행합니다. 이 경우 패킷 손실가 가능합니다. 그러나 live migration을 "
"필요할 때까지 사용할 수 있게 합니다. transparent live migration이 future release에서 추가될 때까지."

#: ../../<reno.sphinxext stable/2023.2>:209
msgid ""
"QEMU in its TCG mode (i.e. full system emulation) uses a translation block "
"(TB) cache as an optimization during dynamic code translation. The libvirt "
"driver can now configure the tb-cache size when the virt type is ``qemu``. "
"This helps running VMs with small memory size. In order to use this feature,"
" a configuration option ``[libvirt]/tb_cache_size`` has been introduced."
msgstr ""
"QEMU는 TCG 모드 (i.e. 전체 시스템 simulation)에서 사용하는 번역 블록 (TB) 캐시를 사용하여 동적 코드 번역의 "
"최적화로 동적 번역을 사용합니다. libvirt 드라이버는 virt 타입이 ``qemu`` 인 경우 TB 캐시 크기를 구성할 수 "
"있습니다. 이로 인해 작은 메모리 크기와 함께 실행되는 VMs에 도움이 됩니다. 이 기능을 사용하려면 "
"``[libvirt]/tb_cache_size``라는 구성 옵션을 사용해야 합니다."

#: ../../<reno.sphinxext stable/2023.2>:289
msgid ""
"Qemu>=5.0.0 bumped the default tb-cache size to 1GiB(from 32MiB) and this "
"made it difficult to run multiple guest VMs on systems running with lower "
"memory. With Libvirt>=8.0.0 it's possible to configure lower tb-cache size. "
"A new config option is introduced:"
msgstr ""
"Qemu>=5.0.0은 기본적으로 1GiB(32MiB에서 1GiB)로 tb-cache 크기를 높였고, 이는 낮은 메모리 시스템에서 여러 "
"게스트 VM을 실행하는 것이 어렵게 되었다. Libvirt>=8.0.0과 함께, tb-cache 크기를 낮추는 것은 가능하다. 새로운 "
"구성 옵션을 도입했다."

#: ../../<reno.sphinxext stable/stein>:460
msgid ""
"QoS-enabled ports will have inventories and allocations created on nested "
"resource providers from the start."
msgstr "QoS-허가된 포트는 시작부터 nested 리소스 제공자에서 집합과 할당이 생성됩니다."

#: ../../<reno.sphinxext stable/2025.1>:481
msgid "Quobyte"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1268
msgid ""
"Quota limits and classes are being moved to the API database for Cells v2. "
"In this release, the online data migrations will move any quota limits and "
"classes you have in your main database to the API database, retaining all "
"attributes."
msgstr ""
"집합 제한 및 类는 Cells v2 API 데이터베이스로 이동되고 있습니다. 이 릴리즈에서, 온라인 데이터 이민은 आपक의 chính한 "
"데이터베이스에 있는 집합 제한 및 类을 API 데이터베이스로 이동하고 모든 속성을 유지합니다."

#: ../../<reno.sphinxext stable/pike>:1273
msgid ""
"Quota limits and classes can no longer be soft-deleted as the API database "
"does not replicate the legacy soft-delete functionality from the main "
"database. As such, deleted quota limits and classes are not migrated and the"
" behavior users will experience will be the same as if a purge of deleted "
"records was performed."
msgstr ""
"집합 제한 및 类는 더 이상 소프트 데leted 할 수 없으며 API 데이터베이스는 주요 데이터베이스의 전통적인 소프트 데leted "
"기능을 복제하지 않기 때문에 such이다. 따라서 제거된 집합 제한 및 类은 이식되지 않으며, 사용자들이 경험할 행동은 제거된 레코드의 "
"청소가 수행된 것과 동일하다."

#: ../../<reno.sphinxext stable/train>:1012
msgid ""
"Quota usage counting from placement is opt-in via the "
"``[quota]count_usage_from_placement`` configuration option."
msgstr ""
"집합 사용량 카운팅는 배치에 따라 opt-in 하며, `quota]count_usage_from_placement` 구성 옵션을 통해 "
"선택됩니다."

#: ../../<reno.sphinxext stable/2024.1>:489
msgid ""
"RDP protocol support from remote console API: POST "
"/servers/{server_id}/remote-consoles"
msgstr "가상 터미널 API에서 RDP 프로토콜 지원: POST /servers/{server_id}/remote-consoles"

#: ../../<reno.sphinxext unmaintained/victoria>:514
msgid ""
"Refer to the `Nova Policy Concepts "
"<https://docs.openstack.org/nova/latest/configuration/policy-"
"concepts.html>`_ for details and migration plan."
msgstr ""
"`Nova Policy Concepts "
"<https://docs.openstack.org/nova/latest/configuration/policy-"
"concepts.html>`_에 대한รายละเอียด과 이식 계획을 참조하십시오."

#: ../../<reno.sphinxext unmaintained/wallaby>:455
msgid ""
"Refer to the ``[libvirt]cpu_model_extra_flags`` documentation for more "
"information."
msgstr "``[libvirt]cpu_model_extra_flags`` 문서를 참조하십시오."

#: ../../<reno.sphinxext unmaintained/victoria>:655
msgid "Refer to the `config reference documentation`__ for more information."
msgstr "`config reference documentation`__에 대한 더 많은 정보를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:223 stable/stein>:1305
msgid "Refer to the image API reference for details on image sharing:"
msgstr "이미지 API 참조를 확인하여 이미지 공유에 대한รายละเอียด을 확인하십시오."

#: ../../<reno.sphinxext stable/2023.2>:140 stable/2024.1>:552
#: unmaintained/2023.1>:193
msgid ""
"Relaxed the config option checking of the cpu_power_management feature of "
"the libvirt driver. The nova-compute service will start with "
"[libvirt]cpu_power_management=True and an empty [compute]cpu_dedicated_set "
"configuration. The power management is still only applied to dedicated CPUs."
" So the above configuration only allowed to ensure that cpu_power_management"
" can be enabled independently for configuring cpu_dedicated_set during "
"deployment."
msgstr ""
"집합을 relaxation 한 libvirt driver의 cpu_power_management 기능의 config 옵션 확인을 "
"취소했다. nova-compute 서비스는 [libvirt]cpu_power_management=True와 "
"[compute]cpu_dedicated_set의 비어있는 [compute]cpu_dedicated_set configuration으로 "
"시작한다. power management은 여전히 chỉ dediced CPU에만 적용된다. 따라서 위 configuration은 "
"cpu_power_management를 independently configuring cpu_dedicated_set에 대한 "
"deployment에만 cpu_dedicated_set을 활성화할 수 있도록 하는 것을만으로 guarantee한다."

#: ../../<reno.sphinxext stable/rocky>:1633
msgid ""
"Remember to run ``nova-status upgrade check`` before upgrading to 18.0.0 "
"Rocky to ensure baremetal instances have had their embedded flavor migrated "
"to use the corresponding ironic node custom resource class."
msgstr ""
"``nova-status upgrade check``를 18.0.0 로키로 업그레이드하기 전에 baremetal 인스턴스에 대한 인스턴스"
" 플레어가 corresponding ironic node custom resource class를 사용하기 위해 내장된 플레어를 "
"마이그레이션 한 것을 확인하세요."

#: ../../<reno.sphinxext stable/queens>:1845
msgid "Removal of technical debt from the compute service long-term"
msgstr "compute 서비스에서 기술 부담을 장기적으로 제거하십시오."

#: ../../<reno.sphinxext stable/ussuri>:395
msgid ""
"Removal of the ``nova-dhcpbridge``, ``nova-console`` and ``nova-"
"xvpvncproxy`` services. See the :ref:`Upgrade Notes <21.0.0.0-upgrade-notes-"
"xvncproxy>` section for more details."
msgstr ""
"``nova-dhcpbridge``, ``nova-console`` 및 ``nova-xvpvncproxy`` 서비스의 제거. "
"21.0.0.0 업그레이드 노트 <xvncproxy> 섹션을 참조하십시오."

#: ../../<reno.sphinxext stable/ussuri>:392
msgid ""
"Removal of the ``os-consoles`` and ``os-networks`` REST APIs. See the "
":ref:`Upgrade Notes <21.0.0.0-upgrade-notes-os-consoles>` section for more "
"details."
msgstr ""
"``os-consoles`` 및 ``os-networks`` REST API의 제거. :ref:`21.0.0.0-upgrade-"
"notes-os-consoles` 섹션을 참조하십시오."

#: ../../<reno.sphinxext unmaintained/wallaby>:989
msgid ""
"Remove the old config option ``bindir`` since it was used for *nova-network*"
" which had been removed."
msgstr ""
"old config option `bindir`를 제거하십시오. 이는 *nova-network*이 제거된 후에 사용되었습니다."

#: ../../<reno.sphinxext stable/pike>:740
msgid "Requires OpenStack-API-Version placement 1.5"
msgstr "OpenStack-API-Version 1.5를 사용해야 합니다."

#: ../../<reno.sphinxext stable/queens>:679
msgid ""
"Rescuing an instance having vGPUs will mean that the rescue image won't use "
"the existing vGPUs. When unrescuing, it will use again the existing vGPUs "
"that were allocated to the instance. That said, given Nova looks at all the "
"allocated vGPUs when trying to find unallocated ones, there could be a race "
"condition if an instance is rescued at the moment a new instance asking for "
"vGPUs is created, because both instances could use the same vGPUs. If you "
"want to rescue an instance, make sure to disable the host until we fix that "
"in Nova."
msgstr ""
"인스턴스에 vGPU가 있는 경우, 인스턴스를 구금하는 것은 구금 이미지에서 사용할 수 있는 existing vGPU가 none이면 "
"vGPU를 사용하지 않는다는 것을 의미합니다. 구금을 해제하는 경우, 구금 이미지에서 사용할 수 있는 existing vGPU가 "
"none이면 again existing vGPU가 할당된 인스턴스에서 사용할 수 있습니다. 그러나 Nova가 모든 할당된 vGPU를 "
"확인하여 사용할 수 있는 ones를 찾을 때, 인스턴스를 구금하는 경우에, 새로운 인스턴스가 vGPU를 요청하는 momento에, 두 "
"인스턴스가 same vGPU를 사용할 수 있는 race condition이 발생할 수 있습니다. 인스턴스를 구금하고 싶다면, host를 "
"비활성화하여 Nova가 구금 이미지에서 사용할 수 있는 existing vGPU가 none이면 vGPU를 사용하지 않는다는 것을 "
"확인합니다."

#: ../../<reno.sphinxext stable/queens>:670
msgid ""
"Resizing an instance with a new flavor that has vGPU resources doesn't "
"allocate those vGPUs to the instance (the instance is created without vGPU "
"resources). We propose to work around this problem by rebuilding the "
"instance once it has been resized so then it will have allocated vGPUs."
msgstr ""
"인스턴스를 새로운 플레버와 함께 리사이즈하면 vGPU 자원들이 인스턴스에 할당되지 않으며 (인스턴스는 vGPU 자원들이 없는 상태로 "
"생성된다) vGPU 자원들이 할당되지 않는다. 우리는 이 문제를 해결하기 위해 인스턴스를 리사이즈 한 후에 다시 만들고, 그 때 vGPU"
" 자원들이 할당된다고 제안한다."

#: ../../<reno.sphinxext stable/ussuri>:250 unmaintained/victoria>:820
msgid ""
"Resolve a race condition that may occur during concurrent ``interface "
"detach/attach``, resulting in an interface accidentally unbind after "
"attached. See `bug 1892870`_ for more details."
msgstr ""
"집합에서 동시적으로 인터페이스 detach/attach이 발생할 수 있는 경쟁 조건을 해결하여 인터페이스가.attach() 후에 무의식적으로.unbind되는 경우를 예방합니다. 더 많은 정보는 `bug 1892870`_에서 확인할 수 있습니다.\n"
"\n"
"- 집합 : aggregate\n"
"- 동시적 : concurrent\n"
"- detach/attach : interface detach/attach\n"
"- 무의식적으로 : accidentally\n"
"-.unbind : unbind\n"
"- bug : 버그\n"
"- 1892870 : bug 번호"

#: ../../<reno.sphinxext stable/train>:244 stable/ussuri>:263
#: unmaintained/victoria>:833
msgid ""
"Resolved an issue whereby providing an empty list for the ``policies`` field"
" in the request body of the ``POST /os-server-groups`` API would result in a"
" server error. This only affects the 2.1 to 2.63 microversions, as the 2.64 "
"microversion replaces the ``policies`` list field with a ``policy`` string "
"field. See `bug #1894966`__ for more information."
msgstr ""
"``policies`` 필드의 요청 바디의 ``POST /os-server-groups`` API에서 비어 있는 목록을 제공하는 것은 서버 오류로 이어질 수 있습니다. 이 문제는 2.1에서 2.63 마이크로 버전 사이에만 영향을 미치며, 2.64 마이크로 버전은 ``policies`` 목록 필드를 ``policy`` 문자열 필드와 대체합니다. 더 많은 정보는 `bug #1894966`__에あります.\n"
"\n"
"- aggregate: 집합\n"
"- API endpoint: API endpoint\n"
"- architecture: 아키텍처\n"
"- association: 연결\n"
"- associate IP: IP 연결\n"
"- attribute: 속성\n"
"- authenticate: 인증\n"
"- authorize: 허가\n"
"- availability zone: 가용 구역\n"
"- backend: 백엔드\n"
"- backup: 백업\n"
"- Bare Metal service: Bare Metal 서비스\n"
"- Block Storage service: 블록 스토리지 서비스\n"
"- body: body\n"
"- boot: 부트\n"
"- cache: 캐시\n"
"- CIDR: CIDR\n"
"- client: 클라이언트\n"
"- cluster: 클러스터\n"
"- comma: ,\n"
"- compute: compute\n"
"- Compute service: Compute 서비스\n"
"- config drive: 드라이브 구성\n"
"- configuration: 구성\n"
"- connection: 접속\n"
"- consistency group: 정합성 그룹\n"
"- container: 컨테이너\n"
"- container format: 컨테이너 포맷\n"
"- controller: 컨트롤러\n"
"- credential: 자격 증명서\n"
"- Database service: 데이터베이스 서비스\n"
"- data processing: 데이터 프로세싱\n"
"- Data Processing service: 데이터 프로세싱 서비스\n"
"- destination container: 대상 컨테이너\n"
"- destination object name: 대상 객체 이름\n"
"- device: 장치\n"
"- disassociate IP: IP 해제\n"
"- disk: 디스크\n"
"- disk format: 디스크 포맷\n"
"- disk GB hours: 디스크 GB 사용 시간\n"
"- distribution: 배포\n"
"- domain: 도메인\n"
"- domain group: 도메인 그룹\n"
"- environment: 환경\n"
"- ephemeral disk: ephemeral 디스크\n"
"- ephemeral drive: ephemeral 드라이브\n"
"- existence: 실제\n"
"- export: 내보내기\n"
"- export location: 내보내기 위치\n"
"- fetching: 페칭\n"
"- fingerprint: fingerprint\n"
"- fixed IP: fixed IP\n"
"- flavor: flavor\n"
"- floating IP: floating"

#: ../../<reno.sphinxext unmaintained/victoria>:642
msgid "RetryFilter"
msgstr "RetryFilter"

#: ../../<reno.sphinxext stable/train>:614
msgid ""
"Return ``servers`` field always in the response of GET hypervisors API even "
"there are no servers on hypervisor."
msgstr ""
"`servers` 필드는 항상 GET 하이퍼바이저 API의 응답에서 `servers`를 포함해야 합니다. even if there are"
" no servers on hypervisor."

#: ../../<reno.sphinxext stable/pike>:731
msgid "Return codes"
msgstr "집합"

#: ../../<reno.sphinxext stable/rocky>:1552
msgid ""
"Revert change I766bb5645e3b598468d092fb9e4f18e720617c52 and carry the fork "
"in the scheduler code"
msgstr ""
"Revert change I766bb5645e3b598468d092fb9e4f18e720617c52와 스케줄러 코드에 포크를 "
"유지하십시오."

#: ../../<reno.sphinxext stable/rocky>:1837
msgid ""
"Running API services (nova-osapi_compute or nova-metadata) with eventlet is "
"now deprecated. Deploy with a WSGI server such as uwsgi or mod_wsgi."
msgstr ""
"API 서비스 (nova-osapi_compute 또는 nova-metadata)를 이벤트เล트 (eventlet)와 함께 실행하는 것은"
" 현재 deprecated입니다. WSGI 서버와 같은 우스기 (uwsgi) 또는 모드 유효성 (mod_wsgi)와 함께 배포하는 것을 "
"권장합니다."

#: ../../<reno.sphinxext stable/2025.2>:337
msgid ""
"Running API services (nova-osapi_compute or nova-metadata) with eventlet is "
"removed. Deploy with a WSGI server such as uwsgi or mod_wsgi."
msgstr ""
"API 서비스 (nova-osapi_compute 또는 nova-metadata)를 이벤트리트 (eventlet)와 함께 실행하는 것을 "
"제거했습니다. WSGI 서버와 같은 우스기 (uwsgi) 또는 모드 위스키 (mod_wsgi)와 함께 배포합니다."

#: ../../<reno.sphinxext stable/2025.1>:482
msgid "SMBFS"
msgstr "집합 파일 시스템"

#: ../../<reno.sphinxext stable/pike>:1695 stable/queens>:1595
msgid ""
"Scheduling bare metal (ironic) instances using standard resource classes "
"(VCPU, memory, disk) is deprecated and will no longer be supported in "
"Queens.  Custom resource classes should be used instead. Please refer to the"
" `ironic documentation "
"<https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html#scheduling-based-on-resource-classes>`_ for a detailed "
"explanation."
msgstr ""
"ironic bare metal (ironic) 인스턴스를 표준 리소스 클래스 (VCPU, 메모리, 디스크)로 스케줄링하는 것은 "
"Queens에서 더 이상 지원되지 않으며 deprecated되었습니다.  대신 custom resource classes를 "
"사용해야합니다.  더 자세한 설명은 `ironic documentation "
"<https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html#scheduling-based-on-resource-classes>`_을 참조하십시오."

#: ../../<reno.sphinxext stable/2025.2>:153
msgid ""
"Scope checks and new defaults are enabled by default and it is recommended "
"to use new defaults. However, if your deployment needs more time then you "
"can disable them by switching the below config option in ``nova.conf`` "
"file.:"
msgstr ""
"집합 확인 및 새로운 디폴트가 기본적으로 활성화되어 있으며, 새로운 디폴트를 사용하는 것을 추천합니다. 그러나, 배포가 더 많은 시간이 "
"필요하다면, nova.conf 파일의 아래 config 옵션을 바꾸면 디폴트를 비활성화할 수 있습니다."

#: ../../<reno.sphinxext ../source/liberty.rst:13 ../source/liberty.rst:53
#: ../source/liberty.rst:81 ../source/mitaka.rst:23 ../source/mitaka.rst:104
#: ../source/mitaka.rst:656 ../source/newton.rst:73 ../source/newton.rst:166
#: ../source/newton.rst:1067 origin/stable/ocata>:10 origin/stable/ocata>:58
#: origin/stable/ocata>:94 origin/stable/ocata>:125 origin/stable/ocata>:185
#: origin/stable/ocata>:330 origin/stable/ocata>:413 origin/stable/ocata>:1555
#: stable/2024.1>:72 stable/2024.2>:72 stable/2025.1>:51 stable/2025.2>:407
#: stable/pike>:10 stable/pike>:159 stable/pike>:226 stable/pike>:382
#: stable/pike>:449 stable/pike>:1737 stable/queens>:80 stable/queens>:300
#: stable/queens>:445 stable/queens>:1625 stable/rocky>:172 stable/rocky>:1924
#: stable/stein>:318 stable/train>:10 stable/train>:1264 stable/ussuri>:56
#: unmaintained/victoria>:150 unmaintained/wallaby>:28
#: unmaintained/wallaby>:310 unmaintained/wallaby>:837 unmaintained/xena>:612
msgid "Security Issues"
msgstr "보안 문제"

#: ../../<reno.sphinxext stable/pike>:1130 stable/pike>:1160
msgid "Security groups are disabled"
msgstr "안전 그룹이 비활성화되었습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:565 unmaintained/xena>:141
#: unmaintained/yoga>:218 unmaintained/zed>:169
msgid "See `bug 1982284`_ for more details."
msgstr "`bug 1982284`_에 대한 더 많은 정보는 다음을 참조하십시오."

#: ../../<reno.sphinxext stable/2023.2>:274
msgid "See `spec`__ for more details."
msgstr "`spec`__에 대한 더 많은 정보를 확인하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:932
msgid "See also https://bugs.launchpad.net/nova/+bug/1661258"
msgstr "https://bugs.launchpad.net/nova/+bug/1661258"

#: ../../<reno.sphinxext stable/queens>:433 stable/rocky>:1525
msgid "See https://bugs.launchpad.net/nova/+bug/1759316 for more details."
msgstr "https://bugs.launchpad.net/nova/+bug/1759316"

#: ../../<reno.sphinxext stable/2023.2>:360 unmaintained/2023.1>:251
#: unmaintained/victoria>:34 unmaintained/wallaby>:20 unmaintained/xena>:20
#: unmaintained/yoga>:83 unmaintained/zed>:100
msgid "See https://bugs.launchpad.net/nova/+bug/2004555 for more details."
msgstr "https://bugs.launchpad.net/nova/+bug/2004555"

#: ../../<reno.sphinxext stable/rocky>:1294
msgid ""
"See "
"https://docs.openstack.org/nova/latest/reference/notifications.html#existing-"
"versioned-notifications for notification samples."
msgstr ""
"https://docs.openstack.org/nova/latest/reference/notifications.html#existing-"
"versioned-notifications"

#: ../../<reno.sphinxext stable/rocky>:434
msgid ""
"See related bug https://bugs.launchpad.net/nova/+bug/1796920 for more "
"details."
msgstr "https://bugs.launchpad.net/nova/+bug/1796920에 대한 관련된 버그를 확인하십시오."

#: ../../<reno.sphinxext stable/queens>:1039
msgid ""
"See the `Cinder enable multiattach`_ spec for details on configuring Cinder "
"for multiattach support."
msgstr ""
"`Cinder enable multiattach`_.spec을 참조하여 Cinder를 멀티 액세스 지원을 위해 "
"구성하는รายละเอียด을 확인하십시오."

#: ../../<reno.sphinxext stable/ussuri>:557
msgid ""
"See the `[image_cache]/precache_concurrency` config option for more "
"information about throttling this operation."
msgstr "`[이미지 캐시]/precache_concurrency` 구성 옵션을 확인하여 이 연산의 제한을 더 많은 정보를 얻으려면."

#: ../../<reno.sphinxext stable/queens>:764 unmaintained/yoga>:340
msgid "See the `spec`_ for more details and reasoning."
msgstr "`spec`_에 대한 더 많은รายละเอียด과 reasoning을 확인하십시오."

#: ../../<reno.sphinxext stable/pike>:985
msgid ""
"See the following XenAPI documentation for details: "
"http://xenbits.xen.org/docs/4.2-testing/misc/vbd-interface.txt"
msgstr ""
"XenAPI 문서를 확인하세요: http://xenbits.xen.org/docs/4.2-testing/misc/vbd-"
"interface.txt"

#: ../../<reno.sphinxext stable/pike>:1145
msgid ""
"Seeing as there was no apparent reason for not allowing IPv6 traffic when "
"the network is IPv6-capable, we now ignore this option. Instead, we use the "
"availability of IPv6-capable subnets as an indicator that IPv6 rules should "
"be added."
msgstr ""
"IPv6 트래픽을 허용하지 않기 위한明확한 이유가 없기 때문에, 네트워크가 IPv6- capable 인 경우에만 이 옵션을 무시하고, "
"IPv6- capable Subnet의 Availability를 사용하여 IPv6 규칙을 추가하는 것을 사용합니다."

#: ../../<reno.sphinxext stable/pike>:1215
msgid ""
"Seeing as there was no apparent reason for not including IPv6 network info "
"when IPv6 capable networks are present, we now ignore this option. Instead, "
"we include info for all available networks in the template, be they IPv4 or "
"IPv6."
msgstr ""
"IPv6 네트워크 정보가 존재할 때는 IPv6 수용 가능한 네트워크가 있는 경우에만 IPv6 네트워크 정보를 포함하는 것이 "
"apparent하지 않아, 현재 이 옵션을 무시하고 있습니다. 대신, 모든 उपलब्ध 네트워크에 대한 정보를 템플릿에 포함합니다. "
"이들은 IPv4 또는 IPv6를 포함합니다."

#: ../../<reno.sphinxext stable/2024.1>:486
msgid ""
"Server Action Get RDP Console: POST /servers/{server_id}/action (os-"
"getRDPConsole Action)"
msgstr ""
"서버 액션 GET RDP 콘솔: POST /servers/{server_id}/action (os-getRDPConsole Action)"

#: ../../<reno.sphinxext stable/2025.2>:40
msgid ""
"Service-to-service APIs now use the ``service`` role, reducing unnecessary "
"privileges for cross-service communication."
msgstr ""
"``service`` 역을 사용하여 서비스- 서비스 API가 현재 cross-service communication을 위해 불필요한 "
"권한을 줄이고 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1096
msgid ""
"Setting ``placement_database.connection`` and calling ``nova-manage api_db "
"sync`` will only create tables. No data will be migrated. In an existing "
"OpenStack deployment, if there is existing placement data in the "
"``nova_api`` database this will not be copied. It is up to the deployment to"
" manually replicate that data in a fashion that works best for the "
"environment."
msgstr ""
"``placement_database.connection``를 설정하고 ``nova-manage api_db sync``를 호출하면, "
"테이블만 생성되며, 데이터는 전환되지 않습니다. существ하는 오픈 스텝 배포에서, ``nova_api`` 데이터가 존재하는 경우, "
"이 데이터는 복사되지 않습니다. 배포는 이 데이터를 환경에 적합한 방식으로 수동으로 복사해야 합니다."

#: ../../<reno.sphinxext stable/queens>:1414
msgid ""
"Setting of 'nicira-iface-id' in XenServer VIF's other-config field by XenAPI"
" driver has been removed to avoid VM booting timeout problem when using "
"neutron polling mode. It was previously deprecated in Pike."
msgstr ""
"nicira-iface-id의 XenServer VIF의 other-config 필드에 있는 XenAPI 드라이버의 설정이 "
"XenServer VIF를 사용할 때 polling 모드에서 VM 부팅 시간 오류를 피하기 위해 제거되었다. 이전에는 Pike에서 "
"비공식적으로 비활성화되었다."

#: ../../<reno.sphinxext stable/rocky>:522
msgid ""
"Several REST APIs specific to nova-network were removed and the core "
"functionality of nova-network is planned to be removed in the 19.0.0 Stein "
"release."
msgstr ""
"다음은 REST API의 일부가 nova-network에เฉพาะ으로 적용된もの이 제거되었으며, nova-network의 핵심 기능이 "
"19.0.0 스티恩 릴리즈에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/queens>:1513
msgid "Show & List detail image"
msgstr "Show & List detail image → Show & List detail image"

#: ../../<reno.sphinxext stable/queens>:1498
msgid "Show & List detail server"
msgstr "Show & List detail server → Show & List detail server"

#: ../../<reno.sphinxext stable/stein>:1154
msgid "Show & List image details"
msgstr "Show & List image details"

#: ../../<reno.sphinxext stable/stein>:1139
msgid "Show & List server details"
msgstr "Show & List server details → Show & List server details"

#: ../../<reno.sphinxext stable/2024.1>:494
msgid ""
"Show Console Connection Information: GET /os-console-auth-"
"tokens/{console_token}"
msgstr ""
"Show Console Connection Information: GET /os-console-auth-tokens/{console_token} \n"
"\n"
"- Show Console Connection Information: \n"
"  - Show Console Connection Information: \n"
"    - Show Console Connection Information: \n"
"      - Show Console Connection Information: \n"
"        - Show Console Connection Information: \n"
"          - Show Console Connection Information: \n"
"            - Show Console Connection Information: \n"
"              - Show Console Connection Information: \n"
"                - Show Console Connection Information: \n"
"                  - Show Console Connection Information: \n"
"                    - Show Console Connection Information: \n"
"                      - Show Console Connection Information: \n"
"                        - Show Console Connection Information: \n"
"                          - Show Console Connection Information: \n"
"                            - Show Console Connection Information: \n"
"                              - Show Console Connection Information: \n"
"                                - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
"                              - Show Console Connection Information: \n"
"                                - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information: \n"
" - Show Console Connection Information:"

#: ../../<reno.sphinxext stable/rocky>:1624
msgid ""
"Similarly, the ``ironic_host_manager`` choice for the "
"``[scheduler]/host_manager`` configuration option was deprecated in the "
"17.0.0 Queens release because ``ironic_host_manager`` is only useful when "
"using ``use_baremetal_filters=True`` and ``baremetal_enabled_filters``. Now "
"that those options are gone, the deprecated ``ironic_host_manager`` host "
"manager choice has also been removed. As a result, the "
"``[scheduler]/host_manager`` configuration option has also been removed "
"since there is only one host manager now and no need for an option."
msgstr ""
"ironic_host_manager  선택은 17.0.0 퀸즈 릴리스에서  baremetal_filters=True  및 "
"baremetal_enabled_filters  옵션을 사용할 때만 유용한  ironic_host_manager 만이 useful  하기"
" 때문에 deprecation  이되었습니다.  이 옵션들이 사라진 후,  deprecation  된  "
"ironic_host_manager  호스트 관리자 선택도 사라졌습니다.  결과적으로,  [scheduler]/host_manager  "
"설정 옵션도 사라졌습니다.  하나의 호스트 관리자가 남아 있으며 옵션이 필요하지 않기 때문입니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:374
msgid ""
"Since 26.0.0 (Zed) Nova supports tracking PCI devices in Placement. Now Nova"
" also supports scheduling flavor based PCI device requests via Placement. "
"This support is disable by default. Please read `documentation "
"<https://docs.openstack.org/nova/latest/admin/pci-passthrough.html#pci-"
"tracking-in-placement>`_ for more details on what is supported how this "
"feature can be enabled."
msgstr ""
"26.0.0 (Zed) Nova에서 PCI 장치의 추적을 Placement에서 지원합니다. 현재 Nova는 Placement에서 PCI "
"장치의 추적을 지원하고, 이에 따라 Nova는 Placement에서 flavor 기반의 PCI 장치 요청을 스케줄링할 수 있습니다. 이 "
"기능은 기본적으로 비활성화되어 있습니다. 더 많은 정보와 이 기능을 활성화하는 방법에 대한รายละเอียด은 `documentation"
" <https://docs.openstack.org/nova/latest/admin/pci-passthrough.html#pci-"
"tracking-in-placement>`_을 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:31 stable/stein>:65 stable/train>:216
#: stable/ussuri>:227 unmaintained/victoria>:744
msgid ""
"Since Libvirt v.1.12.0 and the introduction of the `libvirt issue`_ , there "
"is a fact that if we set cache mode whose write semantic is not O_DIRECT "
"(i.e. \"unsafe\", \"writeback\" or \"writethrough\"), there will be a "
"problem with the volume drivers (i.e. LibvirtISCSIVolumeDriver, "
"LibvirtNFSVolumeDriver and so on), which designate native io explicitly."
msgstr ""
"libvirt v.1.12.0 및 `libvirt issue`_의 도입 이후에, cache mode를 설정할 때 write "
"semantic이 O_DIRECT가 아닌 경우 (i.e. \"unsafesafe\", \"writeback\" 또는 "
"\"writethrough\")에는 volume driver에 문제가 발생할 수 있습니다. (i.e. "
"LibvirtISCSIVolumeDriver, LibvirtNFSVolumeDriver와 같은 native io를 명시적으로 지정한 "
"driver)."

#: ../../<reno.sphinxext stable/rocky>:994
msgid ""
"Since `blueprint placement-claims`_ in Pike, the FilterScheduler uses the "
"Placement service to create resource allocations (claims) against a resource"
" provider (i.e. compute node) chosen by the scheduler. That reduces the risk"
" of scheduling collisions when running multiple schedulers."
msgstr ""
"blueprint placement-claims_ in Pike, FilterScheduler는 Placement 서비스를 사용하여 자원"
" 할당(claim)들을 리소스 제공자(예를 들어, 스케줄러가 선택한 컴퓨팅 노드)와 대조하여 자원 할당(claim)을 생성합니다. 이로 "
"인해 여러 스케줄러를 실행할 때 스케줄러 충돌의 위험을 줄여줍니다."

#: ../../<reno.sphinxext stable/rocky>:1000
msgid ""
"Since other scheduler drivers, like the CachingScheduler, do not use "
"Placement, it is recommended to set workers=1 (default) for those other "
"scheduler drivers."
msgstr ""
"다른 스케줄러 드라이버,例如 캐싱 스케줄러(CachingScheduler), Placement을 사용하지 않는다. 따라서 다른 스케줄러 "
"드라이버에 대해 worker=1 (기본값)로 설정하는 것이 권장됩니다."

#: ../../<reno.sphinxext unmaintained/victoria>:843
msgid ""
"Since the 16.0.0 (Pike) release, nova has collected NIC feature flags via "
"libvirt. To look up the NIC feature flags for a whitelisted PCI device the "
"nova libvirt driver computed the libvirt nodedev name by rendering a format "
"string using the netdev name associated with the interface and its current "
"MAC address. In some environments the libvirt nodedev list can become out of"
" sync with the current MAC address assigned to a netdev and as a result the "
"nodedev look up can fail. Nova now uses PCI addresses, rather than MAC "
"addresses, to look up these PCI network devices."
msgstr ""
"16.0.0 (Pike) 릴리스 이후 nova는 libvirt를 통해 NIC 특성 플래그를 수집했습니다. whitelisted PCI "
"장치의 NIC 특성 플래그를 확인하고 싶다면 nova libvirt 드라이버는 인터페이스와 현재 MAC 주소와 관련된 netdev 이름을"
" 사용하여 format string을 rendering하여 libvirt nodedev 이름을 계산합니다. 일부 환경에서 libvirt "
"nodedev 목록은 현재 netdev에 할당된 MAC 주소와 일치하지 않게 되면 nodedev 검색이 실패할 수 있습니다. nova는 "
"이러한 PCI 네트워크 장치를 검색할 때 MAC 주소 대신 PCI 주소를 사용합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1362
msgid ""
"Since the Placement service is now mandatory in Ocata, you need to deploy it"
" and amend your compute node configuration with correct placement "
"instructions before restarting nova-compute or the compute node will refuse "
"to start."
msgstr ""
"Ocata에서 Placement 서비스가zw compulsory라면, Placement 서비스를 배치하고, correct "
"placement instructions를 사용하여 compute node의 configuration을 수정해야 하며, nova-"
"compute 또는 compute node를 재시작할 때 refusal를 받을 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1453
msgid ""
"Since we now use Placement to verify basic CPU/RAM/disk resources when using"
" the FilterScheduler, the ``RamFilter`` and ``DiskFilter`` entries are being"
" removed from the default value for the ``enabled_filters`` config option in"
" the ``[filter_scheduler]`` group.  If you are overriding this option, you "
"probably should remove them from your version.  If you are using "
"CachingScheduler you may wish to enable these filters as we will not use "
"Placement in that case."
msgstr ""
"Placement을 사용하여 FilterScheduler와 함께 기본 CPU/RAM/disk 자원에 대한 cơ bản xác minh을 "
"수행하고 있기 때문에, `RamFilter` 및 `DiskFilter` 항목이 `enabled_filters` 구성 옵션의 기본 값에서 "
"제거되었습니다.  `filter_scheduler` 그룹의 경우.  이 옵션을 ओवर라이드하고 있으신다면, 그 버전에서它们를 제거하는 "
"것이 좋습니다.  CachingScheduler를 사용하고 있으신다면, 이 필터를 활성화하는 것을 고려하십시오.  Placement을 "
"사용하지 않는다는 것은, 그 경우."

#: ../../<reno.sphinxext stable/2023.2>:104 stable/2024.1>:544
#: unmaintained/2023.1>:145
msgid ""
"Some OS platforms don't provide by default cpufreq resources in sysfs, so "
"they don't have CPU scaling governors. That's why we should let the governor"
" strategy to be optional for `CPU power management`_."
msgstr ""
"OS 플랫폼บาง 가지 기본적으로 sysfs에서 cpufreq 자원들을 제공하지 않기 때문에 CPU 스케일링 게버너를 가지고 있지 "
"않습니다. đó는为什么 CPU 파워 관리 _에 대한 게버너 전략을 선택적으로 허용해야 하는 이유입니다."

#: ../../<reno.sphinxext stable/pike>:772
msgid ""
"Some hypervisors add a signature to their guests, e.g. KVM is adding "
"``KVMKVMKVM\\0\\0\\0``, Xen: ``XenVMMXenVMM``. The existence of a hypervisor"
" signature enables some paravirtualization features on the guest as well as "
"disallowing certain drivers which test for the hypervisor to load e.g. "
"Nvidia driver [1]: \"The latest Nvidia driver (337.88) specifically checks "
"for KVM as the hypervisor and reports Code 43 for the driver in a Windows "
"guest when found.  Removing or changing the KVM signature is sufficient for "
"the driver to load and work.\""
msgstr ""
"hypervisors에 의해 손님에게 서명이 추가되는 경우, 예를 들어 KVM는 \"KVMKVMKVM\\0\\0\\0\"를 추가하고 "
"Xen: \"XenVMMXenVMM\"를 추가한다. hypervisor 서명의 존재는 손님에서 일부 파라비irtualization 기능을"
" 활성화하고, 특정 드라이버를 제외하는 데 도움이 된다. 예를 들어, NVIDIA 드라이버 [1]: \" latest NVIDIA "
"드라이버 (337.88)가 KVM를 hypervisor로 확인하고 Windows 손님에서 Code 43를 보고 보고 report한다. "
"KVM 서명을 제거하거나 변경하면 드라이버가 로드되고 작동한다.\""

#: ../../<reno.sphinxext stable/queens>:599
msgid "Some of the `multi-cell cells v2 caveats`_ have been resolved."
msgstr "`multi-cell cells v2 caveats`_의 일부는 해결되었다."

#: ../../<reno.sphinxext stable/pike>:1647
msgid "Some unused policies have been deprecated. These are:"
msgstr "다음은 사용되지 않는 정책입니다."

#: ../../<reno.sphinxext stable/train>:961
msgid ""
"Source or destination compute node does not support libvirt-sriov-live-"
"migration."
msgstr "원본 또는 목적지 컴퓨터 노드는 libvirt-sriov-live-migration을 지원하지 않는다."

#: ../../<reno.sphinxext stable/train>:708
msgid ""
"Specifying an availability zone is only allowed when the server status is "
"``SHELVED_OFFLOADED`` otherwise a 409 HTTPConflict response is returned."
msgstr ""
"가용 구역을 지정하는 것은 only server 상태가 ``SHELVED_OFFLOADED`` 인 경우에만 허용됩니다. 다른 경우에는 "
"409 HTTPConflict 응답이 반환됩니다."

#: ../../<reno.sphinxext stable/ussuri>:587
msgid ""
"Stable device rescue for boot from volume instances is now supported through"
" the use of the 2.87 microversion when the compute hosting the instance also"
" reports the ``COMPUTE_RESCUE_BFV`` trait such as the libvirt driver."
msgstr ""
"안정적인 장치 복원을 볼륨 인스턴스에서 부트로 지원하는 것은 2.87 마이크로 버전을 사용하여 현재 지원됩니다. compute 호스팅하는"
" 인스턴스에서도 trait COMPUTE_RESCUE_BFV와 같은 libvirt 드라이버가 보고 있다면."

#: ../../<reno.sphinxext origin/stable/ocata>:161 stable/pike>:311
#: stable/queens>:1217
msgid ""
"Starting in Ocata, there is a behavior change where aggregate-based "
"overcommit ratios will no longer be honored during scheduling for the "
"FilterScheduler. Instead, overcommit values must be set on a per-compute-"
"node basis in the Nova configuration files."
msgstr ""
"Ocata에서 시작하여, 집합 기반 오버คอม미트 비율은 더 이상 스케줄링 시에 존중되지 않으며, FilterScheduler에서. "
"대신, 각 컴퓨터 노드에 대해 오버คอม미트 값이 Nova 구성 파일에 설정되어야 합니다."

#: ../../<reno.sphinxext stable/rocky>:407
msgid ""
"Starting in the 16.0.0 Pike release, ironic nodes can be scheduled using "
"custom resource classes rather than the standard resource classes VCPU, "
"MEMORY_MB and DISK_GB:"
msgstr ""
"16.0.0 피크 릴리즈부터, ironik 노드는 표준 리소스 클래스 VCPU, MEMORY_MB 및 DISK_GB 대신 사용자定義 "
"리소스 클래스를 사용하여 스케줄할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:813
msgid ""
"Starting in the Ussuri release, compute node resource providers are "
"automatically marked with the ``COMPUTE_NODE`` trait. This allows them to be"
" distinguished easily from other providers, including sharing and nested "
"providers, as well as other non-compute-related providers in a deployment. "
"To make effective use of this trait (e.g. for scheduling purposes), all "
"compute nodes must be upgrade to Ussuri. Alternatively, you can manually add"
" the trait to pre-Ussuri compute node providers via `openstack resource "
"provider trait set <https://docs.openstack.org/osc-"
"placement/train/cli/index.html#resource-provider-trait-set>`_"
msgstr ""
"Ussuri 릴리스부터 시작하여, 컴퓨팅 노드 리소스 제공자는 tự động적으로 `COMPUTE_NODE` trait를 표시합니다. 이"
" trait를 사용하면 다른 제공자, 즉 공유 및 중첩된 제공자, 以及 배포에서 다른 컴퓨팅 관련 제공자와 구별할 수 있습니다. 이 "
"trait를 효과적으로 사용할 수 있도록 (예를 들어 스케줄링 목적으로), 모든 컴퓨팅 노드는 Ussuri 릴리스로 업그레이드해야 "
"합니다. 또는, `openstack resource provider trait set "
"<https://docs.openstack.org/osc-placement/train/cli/index.html#resource-"
"provider-trait-set>`_를 사용하여 이전 Ussuri 릴리스 이전 컴퓨팅 노드 제공자에 trait를 수동으로 추가할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/stein>:707
msgid ""
"Starting with the 2.71 microversion the ``server_groups`` parameter will be "
"in the response body of the following APIs to list the server groups to "
"which the server belongs:"
msgstr ""
"2.71 마이크로 버전부터 시작하여, `server_groups` 매개 변수는 다음 API의 응답 바디에 포함되어서 서버에 속하는 서버 "
"그룹을 liệt합니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:349
msgid ""
"Starting with v2.95 any evacuated instance will be stopped at destination. "
"The required minimum version for Nova computes is 27.0.0 (antelope 2023.1)."
"  Operator can still continue using previous behavior by selecting "
"microversion below v2.95."
msgstr ""
"v2.95 이상의 버전에서 비우어지는 인스턴스는 목적지에서 중단됩니다. Nova 컴퓨터의 최소 버전은 27.0.0 (antelope "
"2023.1)입니다.  운영자들은 이전 행동을 계속 사용할 수 있습니다. 이전 버전 below v2.95을 선택하여 "
"microversion을 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:675
msgid ""
"Such as, for the CPUWeigher, it weighs hosts based on available vCPUs on the"
" compute node, and multiplies it by the cpu weight multiplier. If per-"
"aggregate value (which the key is \"cpu_weight_multiplier\") is found, this "
"value would be chosen as the cpu weight multiplier. Otherwise, it will fall "
"back to the ``[filter_scheduler]/cpu_weight_multiplier``. If more than one "
"value is found for a host in aggregate metadata, the minimum value will be "
"used."
msgstr ""
"다음과 같은 경우, CPUWeigher는 컴퓨터 노드의 사용 가능한 vCPUs에 따라 호스트를 가중치로 계산하고, cpu "
"가중치ulti플라이어를掛기 때문에 가중치가 증가합니다. cpu 가중치ulti플라이어의 경우, key가 "
"\"cpu_weight_multiplier\"인 per-aggregate 가중치가 발견되면, 이 가중치가 cpu 가중치ulti플라이어로 "
"선택됩니다. 그 외의 경우, \"[filter_scheduler]/cpu_weight_multiplier\"를 사용합니다. 호스트에 대한"
" aggregate metadata에서 여러 가중치가 발견되면, 최소 가중치가 사용됩니다."

#: ../../<reno.sphinxext stable/2025.1>:317
msgid ""
"Support creating servers with RBAC shared security groups by using the new "
"``shared`` filter for security groups. See `blueprint shared-security-"
"groups`_ for more details."
msgstr ""
"서버를 RBAC 공유 보안 그룹으로 만들기 위해 new `shared` 필터를 사용하여 보안 그룹을 생성합니다. 더 많은 정보는 "
"`blueprint shared-security-groups`_에서 확인할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1198
msgid ""
"Support for **hw_watchdog_action** as a flavor extra spec has been removed. "
"The valid flavor extra spec is **hw:watchdog_action** and the image "
"property, which takes precedence, is **hw_watchdog_action**."
msgstr ""
"**hw_watchdog_action**의 flavor extra spec으로의 지원이 제거되었다. 유효한 flavor extra "
"spec은 **hw:watchdog_action**이며, 우선권을 가진 image property는 "
"**hw_watchdog_action**이다."

#: ../../<reno.sphinxext stable/2025.1>:452
msgid ""
"Support for Python 3.8 has been removed. Now the minimum python version "
"supported is 3.9 ."
msgstr "Python 3.8의 지원이 제거되었습니다. 현재 최소한의 Python 버전을 지원하는 버전은 3.9입니다."

#: ../../<reno.sphinxext stable/queens>:1604
msgid ""
"Support for Windows / Hyper-V Server 2012 has been deprecated in Queens in "
"nova and will be removed in Rocky. The supported versions are Windows / "
"Hyper-V Server 2012 R2 or newer."
msgstr ""
"Windows / Hyper-V Server 2012의 지원은 Queens에서 nova에서 비활성화되었으며, Rocky에서 "
"제거되었습니다. 지원되는 버전은 Windows / Hyper-V Server 2012 R2 또는 newer입니다."

#: ../../<reno.sphinxext stable/train>:501
msgid ""
"Support for `VPMEM (Virtual Persistent Memory) "
"<https://docs.openstack.org/nova/latest/admin/virtual-persistent-"
"memory.html>`_ when using the libvirt compute driver. This provides data "
"persistence across power cycles at a lower cost and with much larger "
"capacities than DRAM, especially benefitting HPC and memory databases such "
"as redis, rocksdb, oracle, SAP HANA, and Aerospike."
msgstr ""
"`VPMEM (Virtual Persistent Memory) "
"<https://docs.openstack.org/nova/latest/admin/virtual-persistent-"
"memory.html>`_을 사용하는 libvirt 컴퓨터 드라이버에 대한 지원. 이는 전력サイ클을ผ่าน한 데이터 "
"persistence를 저렴한 비용과 DRAM보다 큰 용량으로 제공합니다. 특히 HPC 및 메모리 데이터베이스인 redis, "
"rocksdb, oracle, SAP HANA, 및 Aerospike와 같은 특정 환경에서 이ประโยชน이 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:352
msgid ""
"Support for `cold migrating and resizing servers between Nova cells`__."
msgstr "`cold migrating and resizing servers between Nova cells`__를 지원합니다."

#: ../../<reno.sphinxext stable/ussuri>:373
msgid "Support for `creating servers with accelerator devices via Cyborg`__."
msgstr "`Cyborg`를 통해 가속기 장치를 통해 서버를 생성하는 지원."

#: ../../<reno.sphinxext stable/ussuri>:386
msgid "Support for `heterogenous virtual GPU types per compute node`__."
msgstr "`다형성 가상 그래픽 카드 타입을 각 컴퓨터 노드에 대해 지원합니다`"

#: ../../<reno.sphinxext unmaintained/victoria>:332
msgid ""
"Support for a new ``mixed`` `flavor CPU allocation policy`__ that allows "
"both pinned and floating CPUs within the same instance."
msgstr ""
"``미xing`` `flavor` CPU 할당 정책을 지원합니다. 이는 동일한 인스턴스 내에서 pinned 및 floating CPU를 "
"모두 허용합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:592
msgid ""
"Support for archiving all deleted rows from the database has been added to "
"the ``nova-manage db archive_deleted_rows`` command. The ``--until-"
"complete`` option will continuously run the process until no more rows are "
"available for archiving."
msgstr ""
"nova-manage db archive_deleted_rows 명령어에서 모든 삭제된 행을 아카이브하는 지원이 추가되었다. "
"--until-complete 옵션은 아카이브 프로세스를 계속적으로 실행할 때까지 더 이상 아카이브할 수 있는 행이 없을 때까지 "
"실행된다."

#: ../../<reno.sphinxext stable/train>:619
msgid ""
"Support for archiving deleted rows from the database across all cells has "
"been added to the ``nova-manage db archive_deleted_rows`` command. Specify "
"the ``--all-cells`` option to run the process across all existing cells. It "
"is only possible to archive all DBs from a node where the "
"``[api_database]/connection`` option is configured."
msgstr ""
"``nova-manage db archive_deleted_rows`` 명령에 대한 아카이브를 삭제한 행을 데이터베이스에 걸쳐 모든 "
"세포에서 지원하는 기능이 추가되었습니다. ``--all-cells`` 옵션을 사용하여 모든 존재하는 세포에서 프로세스를 실행합니다. "
"아카이브를 모든 DB에서 실행할 수 있는 것은 노드에서 ``[api_database]/connection`` 옵션을 구성된 경우만 "
"가능합니다."

#: ../../<reno.sphinxext unmaintained/xena>:495
msgid ""
"Support for automatically retrying all database interactions by configuring "
"the ``[database] use_db_reconnect`` config option has been removed. This "
"behavior was only ever supported for interactions with the main database and"
" was generally not necessary as a number of lookups were already explicitly "
"wrapped in retries. The ``[database] use_db_reconnect`` option is provided "
"by oslo.db and will now be ignored by nova."
msgstr ""
"[데이터베이스]의 모든 데이터베이스 상호작용을 tự động 재시도하는 지원이 [데이터베이스]의 `use_db_reconnect` 구성 "
"옵션을 구성하여 제거되었다. 이 behavior는 이중 데이터베이스 상호작용만 지원되었으며, 일반적으로 데이터베이스 상호작용이 이미 "
"재시도된 많은 검색이 이미 wrapping이되었기 때문에 필요하지 않았습니다. `oslo.db`에서 제공되는 `[데이터베이스] "
"use_db_reconnect` 옵션은 nova에 의해 무시되게되었습니다."

#: ../../<reno.sphinxext stable/train>:477
msgid ""
"Support for cold migrating and resizing servers with bandwidth-aware "
"`Quality of Service ports <https://docs.openstack.org/api-"
"guide/compute/port_with_resource_request.html>`_ attached."
msgstr ""
"chilly migrating 및 서버를 크기 조정하는 데-bandwidth-aware `Quality of Service ports "
"<https://docs.openstack.org/api-"
"guide/compute/port_with_resource_request.html>`_을 연결하는 지원."

#: ../../<reno.sphinxext unmaintained/victoria>:102 unmaintained/wallaby>:971
msgid ""
"Support for cold migration and resize between hosts with different network "
"backends was previously incomplete. If the os-vif plugin for all network "
"backends available in the cloud are not installed on all nodes unplugging "
"will fail during confirming the resize. The issue is caused by the VIF "
"unplug that happened during the resize confirm action on the source host "
"when the original backend information of the VIF was not available. The fix "
"moved the unplug to happen during the resize action when such information is"
" still available. See `bug #1895220`_ for more details."
msgstr ""
"cold migration 및 host 간의 네트워크 백엔드가 다르면 resize를 지원하지 wasn't previously "
"complete. os-vif plugin for all network backends available in the cloud가 all"
" nodes에 installed가 not installed on all nodes unplugging will fail during "
"confirming the resize. The issue is caused by the VIF unplug that happened "
"during the resize confirm action on the source host when the original "
"backend information of the VIF was not available. The fix moved the unplug "
"to happen during the resize action when such information is still available."
" See `bug #1895220`_ for more details."

#: ../../<reno.sphinxext unmaintained/wallaby>:708
msgid ""
"Support for custom scheduler drivers, deprecated since the 21.0.0 (Ussuri) "
"release, has been removed. The default ``filter_scheduler`` is now "
"considered performant enough to suit all use cases. Users with specific "
"requirements that they feel are not met by the filter scheduler should "
"contact the nova developers to discuss their issue."
msgstr ""
"사용자 지정 스케줄 드라이버의 지원, 21.0.0 (Ussuri) 릴리스 이후 deprecated 된 것으로, 제거되었다. 기본적으로 "
"``filter_scheduler``가 모든 사용 경우에 적절한 성능을 제공한다고 간주된다. 사용자가 filter_scheduler가 "
"사용자-specific 요구 사항을 충족하지 않는다고 느끼는 사용자, nova 개발자와 문제를 논의하도록 지시한다."

#: ../../<reno.sphinxext stable/rocky>:1265
msgid ""
"Support for filtering out disabled cells during scheduling for server create"
" requests has been added. Firstly the concept of disabled cells has been "
"introduced which means such disabled cells will not be candidates for the "
"scheduler. Secondly changes have been made to the filter scheduler to ensure"
" that it chooses only the enabled cells for scheduling and filters out the "
"disabled ones. Note that operations on existing instances already inside a "
"disabled cell like move operations will not be blocked."
msgstr ""
"집합을 사용하여 서버를 생성하는 요청에 대한 스케줄링에서 비활성화된 세ลล을 제외하는 지원이 추가되었다. 첫째, 비활성화된 세ลล의 "
"개념이 도입되었으며, 이러한 비활성화된 세ลล은 스케줄러의 후보자가 아니라는 것을 의미한다. 둘째, 필터 스케줄러의 변경이 수행되었다. "
"이 변경은 스케줄러가만 비활성화된 세ลล만 필터링하고, 활성화된 세ลล만 스케줄링하는 것을 보장한다. 또한, 이미 비활성화된 세ลล에 "
"있는 существ하는 인스턴스에 대한 연산 (예: 이동 연산)가 차단되지 않는다는 점을 주목한다."

#: ../../<reno.sphinxext unmaintained/victoria>:596
msgid ""
"Support for hooks has been removed. In previous versions of nova, these "
"provided a mechanism to extend nova with custom code through a plugin "
"mechanism. However, they were deprecated in 13.0.0 (Mitaka) as "
"unmaintainable long-term. `Versioned notifications`__ and `vendordata`__ "
"should be used instead. For more information, refer to `this thread`__."
msgstr ""
"집합 지원이 제거되었습니다. 이전 nova 버전에서, 이러한 hook은 custom code를 통해 nova를 확장하는 데 "
"사용되었습니다. 그러나 13.0.0 (Mitaka)에서 비수리 가능하고 장기적으로 유지할 수 없는 것으로 간주되었습니다. "
"`Versioned notifications`__와 `vendordata`__ 대신 사용해야 합니다. 더 많은 정보를 얻으려면 "
"`이-thread`__를 참조하십시오."

#: ../../<reno.sphinxext stable/ussuri>:366
msgid ""
"Support for scope types and additional roles in the default nova policies, "
"allowing for richer access management including the ability to configure "
"*read-only* access to resources. This feature is disabled by default. See "
"the `Policy Concepts`__ documentation for more details."
msgstr ""
"*scope type* 및 기본 nova 정책에 추가 역할을 지원하여 richer access 관리를 가능하게합니다. *read-"
"only* 접근을 리소스에 구성할 수 있습니다. 이 기능은 기본적으로 비활성화됩니다.  *Policy Concepts*__ 문서를 "
"참조하십시오."

#: ../../<reno.sphinxext unmaintained/yoga>:554
msgid ""
"Support for the ``qos-queue`` extension provided by the vmware-nsx neutron "
"plugin for the VMWare NSX Manager has been removed. This extension was "
"removed from the vmware-nsx project when support for NSX-MH was removed in "
"15.0.0."
msgstr ""
"``qos-queue`` 확장 기능을 vmware-nsx 네트워크 플러그인에서 제공하는 vmware-NSX 관리자에 대한 지원이 "
"제거되었습니다. 이 확장 기능은 15.0.0에서 NSX-MH에 대한 지원이 제거되면서 vmware-nsx 프로젝트에서 제거되었습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:352
msgid ""
"Support for the ``xen``, ``uml``, ``lxc`` and ``parallels`` libvirt backends"
" has been deprecated."
msgstr "``xen``, ``uml``, ``lxc`` 및 ``parallels`` libvirt 배경을 지원하는 것은弃용되었습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:723
msgid ""
"Support for the ``xen``, ``uml``, ``lxc`` and ``parallels`` libvirt "
"backends, configured via the ``[libvirt] virt_type`` config option, has been"
" deprecated. None of these drivers have upstream testing and the ``xen`` and"
" ``uml`` backends specifically have never been considered production ready. "
"With this change, only the ``kvm`` and ``qemu`` backends are considered "
"supported when using the libvirt virt driver."
msgstr ""
"``xen``, ``uml``, ``lxc`` 및 ``parallels`` libvirt 백엔드의 지원은 ``[libvirt] "
"virt_type`` 구성 옵션을 통해 구성된 ``xen``, ``uml``, ``lxc`` 및 ``parallels`` libvirt "
"백엔드의 지원이 중단되었습니다. 이 변경은 이러한 드라이버 중 none이 upstream 테스트를 받지 못하고 ``xen`` 및 "
"``uml`` 백엔드가 production-ready로 간주되지 않았음을 의미합니다. 이 변경으로 인해 libvirt virt 드라이버를"
" 사용할 때만 ``kvm`` 및 ``qemu`` 백엔드가 지원되며, ``xen``, ``uml``, ``lxc`` 및 "
"``parallels`` libvirt 백엔드의 지원은 중단되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:597
msgid ""
"Support for the deprecated options will be removed in a future release."
msgstr "Deprecated 옵션에 대한 지원은 향후 릴리스에서 제거될 예정입니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:632
msgid ""
"Support for the libvirt+UML hypervisor model has been removed. This has not "
"been validated in some time and was never intended for production use."
msgstr ""
"libvirt+UML 가상화 모델에 대한 지원이 제거되었다. 이는 오랜 시간에만 유효하지 않았으며 production 용도로 never "
"intended되었다."

#: ../../<reno.sphinxext unmaintained/wallaby>:637
msgid ""
"Support for the libvirt+xen hypervisor model has been removed. This has not "
"been validated in some time and was not supported."
msgstr "libvirt+xen 가상화 모델에 대한 지원이 제거되었다. 이 지원은 오랜 시간 동안 유효하지 않으며 지원되지 않았다."

#: ../../<reno.sphinxext unmaintained/yoga>:292
msgid ""
"Support is added for network backends that leverage SmartNICs to `offload "
"the control plane from the host server`__. Accordingly, Neutron needs to be "
"`configured`__ in order to enable it correctly. Increased security is "
"enabled by removing the control plane from the host server and overhead is "
"reduced by leveraging the cpu and ram resources on modern SmartNIC DPUs."
msgstr ""
"네트워크 백엔드가 스마트 NIC를 사용하여 제어 플레인을 호스트 서버에서 분산하는 것을 지원합니다. 따라서 Neutron은 제대로 "
"작동하기 위해 구성되어야 합니다. 제어 플레인을 호스트 서버에서 분산하면 보안이 증가하고 CPU 및 RAM 자원을 사용하여 오버헤드가 "
"줄어들게 됩니다."

#: ../../<reno.sphinxext stable/rocky>:1820
msgid ""
"Support to monitor performance events for Intel CMT (Cache Monitoring "
"Technology, or \"CQM\" in Linux kernel parlance) -- namely ``cmt``, "
"``mbm_local`` and ``mbm_total`` -- via the config attribute "
"``[libvirt]/enabled_perf_events`` is now *deprecated* from Nova, and will be"
" *removed* in the \"Stein\" release.  Otherwise, if you have enabled those "
"events, and upgraded to Linux kernel 4.14 (or suitable downstream version), "
"it will result in instances failing to boot."
msgstr ""
"인텔 CMT (Cache Monitoring Technology, 또는 \"CQM\"이란 리눅스 커널에서 사용하는 \"cmt\"와 같은)"
" performance 이벤트를 모니터링하는 지원은 Nova에서 *Deprecated*되었으며, \"Stein\" 릴리스에서 *삭제*될 "
"예정입니다.  그럼, \"cmt\" , \"mbm_local\" 및 \"mbm_total\"과 같은 config attribute "
"\"[libvirt]/enabled_perf_events\"를 활성화하고, 리눅스 커널 4.14 (또는 적합한 하위 버전)로 업그레이드할"
" 경우, 인스턴스가 부팅하지 못하는 문제가 발생할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:603
msgid ""
"Support versioned notifications for flavor operations like create, delete, "
"update access and update extra_specs."
msgstr ""
"flavor 운영에 대한 create, delete, update access 및 update extra_specs와 같은 "
"versioned notifications를 지원합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:495
msgid ""
"Support was added to specify a port NUMA affinity policy for SR-IOV ports. "
"This feature allows users to set a NUMA affinity policy between a neutron "
"port and a NUMA guest's CPUs and memory. This feature supports the same "
"policies as the existing VM Scoped PCI NUMA Affinity policy and take "
"precedence over the flavor and image policy. This allows operators to set a "
"default affinity policy in the flavor or image while end users can express a"
" more granular affinity policy. To use this feature operators must enable "
"the ``port-numa-affinity-policy`` neutron extension and configure the "
"service plugin in neutron. By default the extension is listed as available "
"but is not enabled."
msgstr ""
"NUMA 부하 정책을 SR-IOV 포트에 적용하기 위한 지원이 추가되었다. 이 기능은 네트워크 포트와 NUMA 게스트의 CPU와 메모리 "
"사이에 부하 정책을 설정할 수 있는 기능이다. 이 기능은 현재 VM 스코프'de PCI NUMA 부하 정책과 동일한 정책을 지원하며, "
"flavor 및 image 정책보다 우선권을 갖는다. 이로써 운영자는 flavor 또는 image에서 기본 부하 정책을 설정할 수 있지만"
" 사용자들은 더 세부적인 부하 정책을 표현할 수 있다. 이 기능을 사용하려면 운영자는 ``port-numa-affinity-"
"policy`` 네트워크 확장과 네트워크 서비스 플러그인을 활성화해야 한다. 기본적으로 확장은 उपलबnable한 목록에 표시되지만 "
"활성화되지 않는다."

#: ../../<reno.sphinxext stable/pike>:726
msgid ""
"Supports a new method for deleting all inventory for a resource provider"
msgstr "자원 제공자에 대한 모든 인벤토를 삭제하는 새로운 방법을 지원합니다."

#: ../../<reno.sphinxext stable/rocky>:711
msgid ""
"Supports instance rescue and unrescue with ironic virt driver. This feature "
"requires an ironic service supporting API version 1.38 or later, which is "
"present in ironic releases >= 10.1. It also requires python-ironicclient >= "
"2.3.0."
msgstr ""
"집합 인스턴스 구출 및 구출을 지원합니다. 이 기능은 API 버전 1.38 이상을 지원하는 ironic 서비스가 필요합니다. 이ronic"
" 릴리즈 >= 10.1에 있는 API 버전이 필요하며, python-ironicclient >= 2.3.0도 필요합니다."

#: ../../<reno.sphinxext stable/queens>:662
msgid ""
"Suspending a guest having vGPUs doesn't work yet given a libvirt concern (it"
" can't hot-unplug mediated devices from a guest). Workarounds using other "
"instance actions (like snapshotting the instance or shelving it) are "
"recommended until libvirt supports that. If a user asks to suspend the "
"instance, Nova will get an exception that will set the instance state back "
"to ``ACTIVE``, and you can see the suspend action in ``os-instance-action`` "
"API will be Error."
msgstr ""
"가UEST가 vGPU를 사용하고 있는 경우, libvirt의 우려로 인해 현재는 suspend하는 기능이 작동하지 않습니다. "
"(libvirt는 가UEST에서 매개화된 장치들을.hot-unplug할 수 없습니다). libvirt가 support하는 경우까지, 다른"
" 인스턴스 액션을 사용하여 workaround을 시도하는 것이 추천됩니다. (예를 들어, 인스턴스를 snapshoting하거나, "
"shelving하는 것). 가UEST를 suspend하려고 하는 경우, Nova는 예외가 발생하여 인스턴스의 상태를 ``ACTIVE``로"
" 다시 설정하고, ``os-instance-action`` API의 suspend 액션은 오류가 됩니다."

#: ../../<reno.sphinxext stable/stein>:304
msgid ""
"Switch to using the extracted placement. It does not suffer from eventlet."
msgstr "집합을 사용하도록 전환합니다. 이것은 이벤트레트에 의해 영향을 받지 않습니다."

#: ../../<reno.sphinxext stable/pike>:952
msgid ""
"Tagged volume attachment is not supported for shelved-offloaded instances. "
"Tagged device attachment (both volumes and network interfaces) is not "
"supported for Cells V1 deployments."
msgstr ""
"집합에 태그를 부착하는 볼륨의 연결은 보관된 인스턴스에 대한 지원이 없습니다. 태그를 부착하는 장치 연결(보유자와 네트워크 인터페이스 "
"cả를 포함)은 Cells V1 배포에 대한 지원이 없습니다."

#: ../../<reno.sphinxext stable/rocky>:1828
msgid ""
"That is because the Linux kernel has deleted the `perf` framework "
"integration with Intel CMT, as the feature was broken by design -- an "
"incompatibility between Linux's `perf` infrastructure and Intel CMT.  It was"
" removed in upstream Linux version v4.14; but bear in mind that downstream "
"Linux distributions with lower kernel versions than 4.14 have backported the"
" said change."
msgstr ""
"이것은 Linux 커널이 `perf` 프레임워크의 Intel CMT와의統합을 삭제했다. 이는 설계상 기능이 깨져 있었기 때문이다. -- "
"Linux의 `perf`infrastructure와 Intel CMT 사이의 불일치.  이 기능은 upstream Linux 버전 "
"v4.14에서 제거되었지만, 4.14 버전보다 낮은 커널 버전을 가진 하위 Linux 배포판은 said change를 백포트했다."

#: ../../<reno.sphinxext stable/queens>:654
msgid "That said, Nova currently has some caveats for using vGPUs."
msgstr "그럼에도 불구하고, Nova 현재 vGPU를 사용하는 데에는 몇 가지 제한이 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:525 stable/pike>:594
#: stable/queens>:590 stable/rocky>:508
msgid ""
"That said, a few major changes are worth mentioning. This is not an "
"exhaustive list:"
msgstr "그럼에도 불구하고, 몇 가지 주요 변경 사항이 worth mentioning 하기 때문에."

#: ../../<reno.sphinxext origin/stable/ocata>:471
msgid ""
"That will cleanup the ``project_user_quotas``, ``quota_usages`` and "
"``reservations`` tables for the given project in the ``nova`` database and "
"reset the quota limits for the project back to the defaults defined in "
"nova.conf."
msgstr ""
"``project_user_quotas``, ``quota_usages`` 및 ``reservations`` 테이블을 nova "
"데이터베이스에서 주어진 프로젝트에 대해 청소하고 nova.conf에서 정의된 기본 제한치를 프로젝트에 다시 설정합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:479
msgid ""
"The \"API unexpected exception\" message can now be configured by the cloud "
"provider to point to a custom support page. By default it continues to show "
"\"http://bugs.launchpad.net/nova/\". It can be configured using the release "
"file."
msgstr ""
"API 예상 ngoại상 예외 메시지는 현재 클라우드 제공자가 custom support page를 target으로 설정할 수 있습니다."
" 기본적으로는 \"http://bugs.launchpad.net/nova/\"를 표시합니다. release file을 사용하여 구성할 수"
" 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:776
msgid ""
"The \"tunnelled live migration\" has two inherent limitations: (a) it cannot"
" handle live migration of disks in a non-shared storage setup, and (b) it "
"has a huge performance overhead and latency, because it burns more CPU and "
"memory during live migration."
msgstr ""
"\"tunnelled live migration\"는 두 가지 내재적인 제한이 있습니다. (a) 공유 스토리지 설정에서 디스크의 live"
" migration을 처리할 수 없으며, (b) live migration 동안 CPU와 메모리를 더 많이 사용하여 큰 성능 오버헤드와 "
"지연이 발생합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1370
msgid ""
"The ''use_local'' option, which made it possible to perform nova-conductor "
"operations locally, has been removed. This legacy mode was introduced to "
"bridge a gap during the transition to the conductor service. It no longer "
"represents a reasonable alternative for deployers."
msgstr ""
"'local' 옵션, nova-conductor 운영을 cục적으로 수행할 수 있는 것을 possible하게 한 옵션,는 제거되었다. 이"
" 전통적인 모드, 전기 서비스로의 전환을 bridge하는 데 도입되었다. 더 이상 배포자에게 적절한 대안이되지 않는다."

#: ../../<reno.sphinxext origin/stable/ocata>:62 stable/queens>:347
msgid ""
"The 'AMD-SSBD' and 'AMD-NO-SSB' flags have been added to the list of "
"available choices for the ``[libvirt]/cpu_model_extra_flags`` config option."
" These are important for proper mitigation of security issues in AMD CPUs. "
"For more information see https://www.redhat.com/archives/libvir-"
"list/2018-June/msg01111.html"
msgstr ""
"AMD-SSBD 및 AMD-NO-SSB 플래그가 [libvirt]/cpu_model_extra_flags 설정 옵션의 उपलबnable "
"선택 목록에 추가되었다. 이러한 설정은 AMD CPU의 보안 문제의 적절한 대응을 위해 중요하다. 더 많은 정보는 "
"https://www.redhat.com/archives/libvir-list/2018-June/msg01111.html에서 확인할 수 "
"있다."

#: ../../<reno.sphinxext origin/stable/ocata>:129 stable/pike>:183
#: stable/queens>:449
msgid ""
"The 'SSBD' and 'VIRT-SSBD' cpu flags have been added to the list of "
"available choices for the ``[libvirt]/cpu_model_extra_flags`` config option."
" These are important for proper mitigation of the Spectre 3a and 4 CVEs. "
"Note that the use of either of these flags require updated packages below "
"nova, including libvirt, qemu (specifically >=2.9.0 for virt-ssbd), linux, "
"and system firmware. For more information see https://www.us-"
"cert.gov/ncas/alerts/TA18-141A"
msgstr ""
"SSBD 및 VIRT-SSBD CPU 플래그가 [libvirt]/cpu_model_extra_flags 설정 옵션의 사용 가능한 선택 "
"목록에 추가되었다. 이들은 Spectre 3a 및 4 CVE의 적절한 대비에 중요하다. 사용하는 플래그 중 하나를 사용하는 것은 nova"
" 아래의 패키지, libvirt, qemu (특히 virt-ssbd >=2.9.0), linux 및 시스템 फर미어를 포함하여 업데이트된"
" 패키지가 필요하다. 더 많은 정보는 https://www.us-cert.gov/ncas/alerts/TA18-141A에서 확인할 수 "
"있다."

#: ../../<reno.sphinxext stable/queens>:1457
msgid ""
"The *TrustedFilter* along with its related ``[trusted_computing]`` "
"configuration options were deprecated in the 16.0.0 Pike release and have "
"been removed in the 17.0.0 Queens release. The *TrustedFilter* was always "
"experimental, had no continuous integration testing to prove it still "
"worked, and no reported users."
msgstr ""
"*TrustedFilter*은 *trusted_computing* 관련된 구성 옵션과 함께 16.0.0 피크 릴리즈에서弃용되었으며 "
"17.0.0 퀸즈 릴리즈에서 제거되었다. *TrustedFilter*는 항상 실험적이었고, 연속적 인 통합 테스트를 통해 여전히 "
"작동하는지 확인할 수 없으며, 사용자 báo cáo가 없었다."

#: ../../<reno.sphinxext stable/queens>:1832
msgid "The *block-storage* 3.44 API microversion is available"
msgstr "*블록 스토리지* 3.44 API 마이크로 버전이 사용 가능합니다"

#: ../../<reno.sphinxext stable/train>:1065
msgid ""
"The *cells v1* feature has been deprecated since the 16.0.0 Pike release and"
" has now been removed. The ``nova-cells`` service and ``nova-manage cells`` "
"commands have been removed, while the ``nova-manage cell_v2 "
"simple_cell_setup`` command will no longer check if cells v1 is enabled and "
"therefore can no longer exit with ``2``."
msgstr ""
"*cells v1* 기능은 16.0.0 피크 릴리즈 이후 deprecated되었으며 현재 제거되었습니다. `nova-cells` 서비스와"
" `nova-manage cells` 명령이 제거되었으며, `nova-manage cell_v2 simple_cell_setup` 명령은"
" `cells v1`가 활성화된지 확인하지 않게 되므로 더 이상 `2`로 종료할 수 없습니다."

#: ../../<reno.sphinxext stable/train>:1071
msgid ""
"The *cells v1* specific REST APIs have been removed along with their related"
" policy rules. Calling these APIs will now result in a ``410 (Gone)`` error "
"response."
msgstr ""
"*cells v1* API는 관련 정책 규칙과 함께 제거되었습니다. 이 API를 호출하면 현재 ``410 (Gone)`` 오류 응답을 "
"반환합니다."

#: ../../<reno.sphinxext stable/train>:1091
msgid ""
"The *cells v1* specific configuration options, previously found in "
"``cells``, have been removed."
msgstr "*cells v1*의 특정 구성 옵션, 이전에 ``cells``에서 찾은 것들은 제거되었다."

#: ../../<reno.sphinxext stable/train>:1086
msgid "The *cells v1* specific policies have been removed."
msgstr "*cells v1* 특정 정책은 제거되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:856
msgid ""
"The *nova-network* feature has been deprecated since the 14.0.0 (Newton) "
"release and has now been removed. The remaining *nova-network* specific REST"
" APIs have been removed along with their related policy rules. Calling these"
" APIs will now result in a ``410 (Gone)`` error response."
msgstr ""
"*nova-network* 기능은 14.0.0 (Newton) 릴리스 이후 deprecated되었으며 현재 제거되었습니다. 남은 "
"*nova-network* 특정 REST API는 관련 정책 규칙과 함께 제거되었습니다. 이러한 API를 호출하면 현재 ``410 "
"(Gone)`` 오류 응답을 반환합니다."

#: ../../<reno.sphinxext stable/rocky>:1466
msgid ""
"The *nova-network* service has been deprecated since the 14.0.0 Newton "
"release and now the following *nova-network* specific REST APIs have been "
"removed along with their related policy rules. Calling these APIs will now "
"result in a ``410 HTTPGone`` error response."
msgstr ""
"*nova-network* 서비스는 14.0.0 뉴턴 릴리스 이후 deprecated되었으며, 이와 관련된 *nova-network* "
"특정 REST API는 함께 제거되었습니다. 이 API를 호출하면 현재 ``410 HTTPGone`` 오류 응답이 발생합니다."

#: ../../<reno.sphinxext stable/queens>:1125
msgid ""
"The *nova-novncproxy* server can now be configured to do a security "
"negotiation with the compute node VNC server. If the VeNCrypt auth scheme is"
" enabled, this establishes a TLS session to provide encryption of all data. "
"The proxy will validate the x509 certs issued by the remote server to ensure"
" it is connecting to a valid compute node. The proxy can also send its own "
"x509 cert to allow the compute node to validate that the connection comes "
"from the official proxy server."
msgstr ""
"*nova-novncproxy* 서버는 현재 컴퓨터 노드 VNC 서버와 보안 협상에 참여할 수 있습니다. VeNCrypt 인증 스킵이 "
"활성화된 경우, 이는 모든 데이터를 암호화하는 TLS 세션을 제공합니다. 프록시는 원격 서버에서 발급된 x509 인증서를 확인하여 컴퓨터"
" 노드에 연결하는지 확인합니다. 프록시는 공식 프록시 서버에서 연결하는지 확인하기 위해 자신의 x509 인증서를 보냅니다."

#: ../../<reno.sphinxext stable/queens>:797
msgid ""
"The 1.12 version of the placement API changes handling of the `PUT "
"/allocations/{consumer_uuid}` request to use a dict-based structure for the "
"JSON request body to make it more aligned with the response body of `GET "
"/allocations/{consumer_uuid}`. Because `PUT` requires `user_id` and "
"`project_id` in the request body, these fields are added to the `GET` "
"response. In addition, the response body for ``GET /allocation_candidates`` "
"is updated so the allocations in the ``allocation_requests`` object work "
"with the new `PUT` format."
msgstr ""
"1.12 버전의 배치 API는 `PUT /allocations/{consumer_uuid}` 요청을 dict-기반 구조를 사용하여 "
"JSON 요청 바디를 사용하여 응답 바디와 일치시키기 위해 사용합니다. `PUT`는 요청 바디에 `user_id`와 "
"`project_id`를 필요로 하므로, 이 필드는 `GET` 응답에 추가됩니다. 또한, `GET "
"/allocation_candidates` 응답 바디는 `allocation_requests` 객체와 함께 new `PUT` 형식과 "
"일치하는.allocations을 사용하도록 업데이트됩니다."

#: ../../<reno.sphinxext stable/pike>:804
msgid ""
"The 1.7 version of the placement API changes handling of `PUT "
"/resource_classes/{name}` to be a create or verification of the resource "
"class with `{name}`. If the resource class is a custom resource class and "
"does not already exist it will be created and a ``201`` response code "
"returned. If the class already exists the response code will be ``204``. "
"This makes it possible to check or create a resource class in one request."
msgstr ""
"1.7 버전의 위치 API는 `PUT /resource_classes/{name}`를 `{name}`의 리소스 클래스를 생성하거나 "
"확인하는 create 또는 확인을 의미하는 것으로 바꾸었다. 리소스 클래스가 custom 리소스 클래스이면 이미 존재하지 않으면 생성되어"
" `201` response code가 반환된다. 이미 존재하는 경우 `204` response code가 반환된다. 이로 인해 리소스 "
"클래스를 하나의 요청에서 확인하거나 생성할 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:520
msgid ""
"The 15.0.0 release includes many new features and bug fixes. It is difficult"
" to cover all the changes that have been introduced. Please at least read "
"the upgrade section which describes the required actions to upgrade your "
"cloud from 14.0.0 (Newton) to 15.0.0 (Ocata)."
msgstr ""
"15.0.0 버전은 많은 새로운 기능과 버그修复을 포함합니다. 모든 변경 사항을ครอบ giữ하는 것은 어렵습니다. 따라서 14.0.0 "
"(Newton)에서 15.0.0 (Ocata)로 클라우드를 업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을至少 "
"읽으십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:434
msgid ""
"The 15.0.1 Ocata release contains fixes for several high severity, high "
"impact bugs. If you have not yet upgraded to 15.0.0, it is recommended to "
"upgrade directly to 15.0.1."
msgstr ""
"15.0.1 오카타 릴리스는 여러 높은 심각도, 높은 영향을 가진 버그를修复하는 것을 포함합니다. 15.0.0으로 업그레이드하지 않은 "
"경우, 15.0.1으로 직접 업그레이드하는 것을 권장합니다."

#: ../../<reno.sphinxext stable/pike>:589
msgid ""
"The 16.0.0 release includes many new features and bug fixes. It is difficult"
" to cover all the changes that have been introduced. Please at least read "
"the upgrade section which describes the required actions to upgrade your "
"cloud from 15.0.0 (Ocata) to 16.0.0 (Pike)."
msgstr ""
"16.0.0 버전은 많은 새로운 기능과 버그修复을 포함합니다. 모든 변경 사항을ครอบ giữ하는 것은 어렵습니다. upgrade "
"섹션을至少 읽으십시오. upgrade를 위해 15.0.0 (Ocata)에서 16.0.0 (Pike)로 클라우드를 업그레이드하는 आवश한 "
"hành động을 설명합니다."

#: ../../<reno.sphinxext stable/queens>:585
msgid ""
"The 17.0.0 release includes many new features and bug fixes. It is difficult"
" to cover all the changes that have been introduced. Please at least read "
"the upgrade section which describes the required actions to upgrade your "
"cloud from 16.0.0 (Pike) to 17.0.0 (Queens)."
msgstr ""
"17.0.0 버전은 많은 새로운 기능과 버그修复이 포함되어 있습니다. 모든 변경 사항을ครอบ giữ하는 것은 어렵습니다. 따라서 "
"16.0.0 (Pike)에서 17.0.0 (Queens)로 클라우드를 업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 "
"섹션을至少 읽으십시오."

#: ../../<reno.sphinxext stable/rocky>:503
msgid ""
"The 18.0.0 release includes many new features and bug fixes. It is difficult"
" to cover all the changes that have been introduced. Please at least read "
"the upgrade section which describes the required actions to upgrade your "
"cloud from 17.0.0 (Queens) to 18.0.0 (Rocky)."
msgstr ""
"18.0.0 버전은 많은 새로운 기능과 버그修复이 포함되어 있습니다. 모든 변경 사항을ครอบ giữ하는 것은 어렵습니다. 따라서 "
"17.0.0 (Queens)에서 18.0.0 (Rocky)로 클라우드를 업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 "
"섹션을至少 읽어보세요."

#: ../../<reno.sphinxext stable/stein>:405
msgid ""
"The 19.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 18.0.0 (Rocky) to 19.0.0 (Stein)."
msgstr ""
"19.0.0 버전은 많은 새로운 기능과 버그修复이 포함되어 있습니다. 18.0.0 (Rocky)에서 19.0.0 (Stein)으로 "
"클라우드를 업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어보세요."

#: ../../<reno.sphinxext stable/pike>:1895
msgid ""
"The 2.45 microversion is introduced which changes the response for the "
"``createImage`` and ``createBackup`` server action APIs to no longer return "
"a ``Location`` response header. With microversion 2.45 those APIs now return"
" a json dict in the response body with a single ``image_id`` key whose value"
" is the snapshot image ID (a uuid). The old ``Location`` header in the "
"response before microversion 2.45 is most likely broken and inaccessible by "
"end users since it relies on the internal Glance API server configuration "
"and does not take into account Glance API versions."
msgstr ""
"2.45 마이크로 버전이 도입되며, `createImage` 및 `createBackup` 서버 액션 API의 응답을 더 이상 "
"`Location` 헤더를 반환하지 않게 하는 changes가 발생했다. 2.45 마이크로 버전과 같은 경우, 이 API는 응답 본체에 "
"JSON dict를 반환하고, 단일 `image_id` 키가 있으며, UUID의 스냅샘 image ID의值이다. 이전 `Location`"
" 헤더는 2.45 마이크로 버전 이전에 응답을 반환하는 것과는 달리, 내부 Glance API 서버 구성에 의존하여 내부 Glance "
"API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 "
"Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 "
"내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 "
"의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 "
"구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API "
"서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance "
"API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 "
"Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 "
"내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 "
"의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 "
"구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API "
"서버 구성에 의존하여 내부 Glance API 서버 구성에 의존하여 내부 Glance API 서버 구성에 의"

#: ../../<reno.sphinxext stable/pike>:833
msgid ""
"The 2.51 microversion exposes the ``events`` field in the response body for "
"the ``GET /servers/{server_id}/os-instance-actions/{request_id}`` API. This "
"is useful for API users to monitor when a volume extend operation completes "
"for the given server instance. By default only users with the administrator "
"role will be able to see event ``traceback`` details."
msgstr ""
"``events`` 필드는 API 응답 바디에서 ``GET /servers/{server_id}/os-instance-"
"actions/{request_id}`` API에 exposure되며, 이 microversion은 2.51입니다. 이 "
"microversion은 사용자에게 volume extend operation이 완료되기 위해 server instance에 대한 "
"monitor를 제공합니다. 기본적으로 only administrator role을 가진 사용자만 event tracebaack의 "
"details를 볼 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:753
msgid "The 2.57 microversion makes the following changes:"
msgstr "2.57 마이크로 버전은 다음 변경 사항을 적용합니다."

#: ../../<reno.sphinxext stable/rocky>:1299
msgid ""
"The 2.63 compute REST API microversion adds support for the "
"``trusted_image_certificates`` parameter, which is used to define a list of "
"trusted certificate IDs that can be used during image signature verification"
" and certificate validation. The list is restricted to a maximum of 50 IDs. "
"Note that there is not support with volume-backed servers."
msgstr ""
"2.63 compute REST API microversion에서 trusted_image_certificates 매개 변수가 "
"추가되었습니다. 이 매개 변수는 이미지 서명 xác minh 및 서명 유효성 확인에서 사용할 수 있는 신뢰할 수 있는 서명 ID의 목록을"
" 정의합니다. 이 목록은 50개 ID의 제한을 받습니다. volume-backed servers에 대한 지원은 없습니다."

#: ../../<reno.sphinxext stable/stein>:431
msgid ""
"The 2.69 compute API microversion adds handling of server details in the "
"presence of down or poor-performing cells in a multi-cell environment for "
"the ``GET /servers``, ``GET /servers/detail``, ``GET /servers/{server_id}``,"
" ``GET /os-services`` REST APIs. See the `handling down cells`_ "
"documentation for more details."
msgstr ""
"2.69 compute API microversion은 down 또는 poor-performing cells가 있는 다중 세포 환경에서 "
"server details를 처리하는 것을 추가합니다. \"GET /servers\", \"GET /servers/detail\", "
"\"GET /servers/{server_id}\", \"GET /os-services\" REST API에 대해. down cells "
"handling에 대한 더 많은 정보는 handling down cells_ documentation을 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:652
msgid ""
"The 2.70 compute API microversion exposes virtual device tags for volume "
"attachments and virtual interfaces (ports). A ``tag`` parameter is added to "
"the response body for the following APIs:"
msgstr ""
"집합 2.70 컴퓨터 API 마이크로 버전은 볼륨 앱과 가상 인터페이스 (포트) 에 대한 가상 장치 태그를 노출합니다. 태그 "
"parameter은 다음 API의 응답 바디에 추가됩니다."

#: ../../<reno.sphinxext stable/ussuri>:498
msgid ""
"The 2.86 microversion adds support for flavor extra spec validation when "
"creating or updating flavor extra specs. Use of an unrecognized or invalid "
"flavor extra spec in the following namespaces will result in a HTTP 400 "
"response."
msgstr ""
"2.86 마이크로 버전이 맛집 추가 spéc이 유효성 확인을 지원하는 것을 추가합니다. 맛집 추가 spéc을 생성하거나 업데이트할 때 "
"사용할 수 있습니다. 비정상적인 또는 유효하지 않은 맛집 추가 spéc을 다음 네임스페이스에서 사용하면 HTTP 400 응답이 "
"발생합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:802
msgid ""
"The 2.88 API microversion has been added. This microversion removes a number"
" of fields have been removed from the ``GET /os-hypervisors/detail`` "
"(detailed list) and ``GET /os-hypervisors/{hypervisor_id}`` (show) APIs::"
msgstr ""
"2.88 API 마이크로 버전이 추가되었습니다. 이 마이크로 버전은 GET /os-hypervisors/detail (상세 목록) 및 "
"GET /os-hypervisors/{hypervisor_id} (보여주기) API에서 여러 필드를 제거했습니다."

#: ../../<reno.sphinxext unmaintained/xena>:421
msgid ""
"The 2.90 microversion has been added. This microversion allows users to "
"specify a requested hostname to be configured for the instance metadata when"
" creating an instance (``POST /servers``), updating an instance (``PUT "
"/servers/{id}``), or rebuilding an instance (``POST "
"/servers/{server_id}/action (rebuild)``). When specified, this hostname "
"replaces the hostname that nova auto-generates from the instance display "
"name. As with the auto-generated hostnames, a service such as ``cloud-init``"
" can automatically configure the hostname in the guest OS using this "
"information retrieved from the metadata service."
msgstr ""
"2.90 마이크로 버전이 추가되었습니다. 이 마이크로 버전은 인스턴스 메타데이터를 생성할 때, 인스턴스를 생성할 때, 인스턴스를 "
"업데이트할 때 또는 인스턴스를 재건할 때, 사용자에게 요청한 호스트 이름을 구성할 수 있도록 허용합니다. (``POST "
"/servers``), (``PUT /servers/{id}``), 또는 (``POST /servers/{server_id}/action"
" (rebuild)``). 요청된 호스트 이름이 지정되면, nova가 인스턴스 디스플레이 이름을 기반으로 tự động 생성하는 호스트 "
"이름을 대체합니다. auto-generatd 호스트 이름과 마찬가지로, cloud-init과 같은 서비스는 이 정보를 메타데이터 "
"서비스에서 retrieves한 정보를 사용하여 guest OS에서 호스트 이름을 자동으로 구성할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/zed>:249
msgid "The 2.92 microversion makes the following changes:"
msgstr "2.92 마이크로 버전은 다음 변경 사항을 적용합니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:382
msgid ""
"The 2.94 microversion has been added. This microversion extends microversion"
" 2.90 by allowing Fully Qualified Domain Names (FQDN) wherever the "
"``hostname`` is able to be specified. This consists of creating an instance "
"(``POST /servers``), updating an instance (``PUT /servers/{id}``), or "
"rebuilding an instance (``POST /servers/{server_id}/action (rebuild)``). "
"When using an FQDN as the instance hostname, the ``[api]dhcp_domain`` "
"configuration option must be set to the empty string in order for the "
"correct FQDN to appear in the ``hostname`` field in the metadata API."
msgstr ""
"2.94 마이크로 버전이 추가되었습니다. 이 마이크로 버전은 `hostname`이 명시할 수 있는 곳에서 Fully Qualified "
"Domain Names (FQDN)을 허용하여 마이크로 버전 2.90을 확장합니다. 이 consists of 인스턴스 (``POST "
"/servers``), 인스턴스를 업데이트하는 (``PUT /servers/{id}``), 또는 인스턴스를 재건하는 (``POST "
"/servers/{server_id}/action (rebuild)``) thing을 만드는 것입니다. FQDN을 인스턴스 호스트 "
"이름으로 사용할 때, `api]dhcp_domain` 구성 옵션은 비어 있는 문자열로 설정되어야 correct FQDN이 metadata"
" API의 `hostname` 필드에 나타날 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:446
msgid ""
"The 2.96 microversion has been added. This microversion adds "
"pinned_availability_zone in `server show` and `server list --long` "
"responses."
msgstr ""
"2.96 마이크로 버전이 추가되었습니다. 이 마이크로 버전은 `서버 show` 및 `서버 list --long` 응답에서 "
"pinned_availability_zone를 추가합니다."

#: ../../<reno.sphinxext stable/2025.1>:277
msgid ""
"The 2.98 microversion has been added. This microversion adds support for "
"including image properties as new ``properties`` subkey under the struct at "
"the existing ``image`` key in the response for ``GET /servers/{server_id}`` "
"(server show), ``PUT /servers/{server_id}`` (server update), and ``GET "
"/servers/detail`` (list server --long) APIs. Also the same is included in "
"rebuild case of ``POST /server/{server_id}/action`` (server rebuild) API "
"response."
msgstr ""
"2.98 마이크로 버전이 추가되었습니다. 이 마이크로 버전은 existing image key의 response에서 image properties를 new properties subkey로 추가하여 struct의 properties subkey에 포함되도록 support합니다. \n"
"\n"
"*   GET /servers/{server_id}` (서버 표시) API\n"
"*   PUT /servers/{server_id}` (서버 업데이트하기) API\n"
"*   GET /servers/detail` (서버 목록 --long) API\n"
"\n"
"이 마이크로 버전은 server rebuild API response의 rebuild case에도 포함됩니다. \n"
"\n"
"*   POST /server/{server_id}/action` (서버 재건하기) API"

#: ../../<reno.sphinxext stable/train>:457
msgid ""
"The 20.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 19.0.0 (Stein) to 20.0.0 (Train)."
msgstr ""
"20.0.0 버전은 많은 새로운 기능과 버그修复을 포함합니다. 19.0.0 (Stein)에서 20.0.0 (Train)으로 클라우드를 "
"업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어보세요."

#: ../../<reno.sphinxext stable/ussuri>:333
msgid ""
"The 21.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 20.0.0 (Train) to 21.0.0 (Ussuri)."
msgstr ""
"21.0.0 버전은 많은 새로운 기능과 버그修复을 포함합니다. 20.0.0 (트레인)에서 21.0.0 (우수리)로 클라우드를 "
"업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어 주세요."

#: ../../<reno.sphinxext unmaintained/victoria>:319
msgid ""
"The 22.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 21.0.0 (Ussuri) to 22.0.0 (Victoria)."
msgstr ""
"22.0.0 버전은 많은 새로운 기능과 버그修复을 포함합니다. 21.0.0 (Ussuri)에서 22.0.0 (Victoria)로 "
"클라우드를 업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어보세요."

#: ../../<reno.sphinxext unmaintained/wallaby>:384
msgid ""
"The 23.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 22.0.0 (Victoria) to 23.0.0 (Wallaby)."
msgstr ""
"23.0.0 버전은 많은 새로운 기능과 버그fix가 포함되어 있습니다. 22.0.0 (Victoria)에서 23.0.0 "
"(Wallaby)로 클라우드를 업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어 주세요."

#: ../../<reno.sphinxext unmaintained/xena>:290
msgid ""
"The 24.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 23.0.0 (Wallaby) to 24.0.0 (Xena)."
msgstr ""
"24.0.0 버전은 많은 새로운 기능과 버그修复이 포함되어 있습니다. 23.0.0 (Wallaby)에서 24.0.0 (Xena)로 "
"클라우드를 업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어 주세요."

#: ../../<reno.sphinxext unmaintained/yoga>:263
msgid ""
"The 25.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 24.0.0 (Xena) to 25.0.0 (Yoga)."
msgstr ""
"25.0.0 버전은 많은 새로운 기능과 버그修复을 포함합니다. 24.0.0 (Xena)에서 25.0.0 (Yoga)로 클라우드를 "
"업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어보세요."

#: ../../<reno.sphinxext unmaintained/zed>:186
msgid ""
"The 26.0.0 release includes many new features and bug fixes. Please be sure "
"to read the upgrade section which describes the required actions to upgrade "
"your cloud from 25.0.0 (Yoga) to 26.0.0 (Zed)."
msgstr ""
"26.0.0 버전은 많은 새로운 기능과 버그修复을 포함합니다. 25.0.0 (Yoga)에서 26.0.0 (Zed)로 클라우드를 "
"업그레이드하는 데 필요한 hành động을 설명하는 업그레이드 섹션을 읽어보십시오."

#: ../../<reno.sphinxext stable/pike>:1726
msgid ""
"The CachingScheduler and ChanceScheduler drivers are deprecated in Pike. "
"These are not integrated with the placement service, and their primary "
"purpose (speed over correctness) should be addressed by the default "
"FilterScheduler going forward. If ChanceScheduler behavior is desired (i.e. "
"speed trumps correctness) then configuring the FilterScheduler with no "
"enabled filters should approximate that behavior."
msgstr ""
"CachingScheduler 및 ChanceScheduler 드라이버는 Pike에서弃기되었습니다. 이 드라이버는 배치 서비스와 통합되지"
" 않으며, chính한 목적(속도보다正確성)가 주어지지 않아야 합니다. CachingScheduler의 동작이 원하는 경우(속도는 "
"정確성보다 우선시) FilterScheduler를 구성하여 필터가 활성화되지 않은 경우에 대해 그 behavior를 근사할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:640
msgid ""
"The Compute API will return a 400 (Bad Request) response to a request to "
"directly boot an image created from an encrypted volume."
msgstr ""
"Compute API는 암호화된 볼륨에서 생성된 이미지에 직접 부트를 요청할 때 400 (Bad Request) 응답을 반환합니다."

#: ../../<reno.sphinxext stable/queens>:36 stable/rocky>:57 stable/stein>:134
#: stable/train>:323 stable/ussuri>:1237
msgid ""
"The Compute service has never supported direct booting of an instance from "
"an image that was created by the Block Storage service from an encrypted "
"volume.  Previously, this operation would result in an ACTIVE instance that "
"was unusable.  Beginning with this release, an attempt to boot from such an "
"image will result in the Compute API returning a 400 (Bad Request) response."
msgstr ""
"Compute 서비스는 Block Storage 서비스가 암호화된 볼륨에서 생성된 이미지에서 인스턴스를 직접 부트로 하는 것을 지원하지 "
"않는다.  이전에, 이러한 연산은 활성화된 인스턴스를 사용할 수 없는 ACTIVE 상태를 일으켰다.  이 릴리스부터, 이러한 이미지에서 "
"부트를 시도하면 Compute API가 400 (Bad Request) 응답을 반환한다."

#: ../../<reno.sphinxext stable/pike>:600
msgid ""
"The FilterScheduler driver now provides allocations to the Placement API, "
"which helps concurrent schedulers to verify resource consumptions directly "
"without waiting for compute services to ask for a reschedule in case of a "
"race condition. That is an important performance improvement that includes "
"allowing one to use more than one scheduler worker if there are capacity "
"concerns. For more details, see the `Pike Upgrade Notes for Placement`_."
msgstr ""
"FilterScheduler 드라이버는 현재 Placement API에 할당을 제공하고, 이로써 동시 스케줄러가 자원 소비를 직접 확인할"
" 수 있게 되고, compute 서비스가 경쟁 조건의 경우 다시 스케줄을 요청할 필요가 없게 됩니다. 이것은 경쟁 조건의 경우 스케줄러 "
"워커를 하나 이상 사용할 수 있는 것을 허용하는 중요한 성능 개선입니다. 더 많은 세부 사항은 `Pike Upgrade Notes for"
" Placement`를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:992 stable/rocky>:860
msgid ""
"The FilterScheduler is currently the only scheduler driver that supports "
"this feature."
msgstr "FilterScheduler는 현재 이 기능을 지원하는 유일한 스케줄러 드라이버입니다."

#: ../../<reno.sphinxext stable/queens>:1790
msgid ""
"The FilterScheduler now limits the number of results in the query it makes "
"to placement to avoid situations where every compute node in a large "
"deployment is returned. This is configurable with the new "
"``[scheduler]/max_placement_results`` configuration option, which defaults "
"to 1000, a sane starting value for any size deployment."
msgstr ""
"FilterScheduler는 쿼리를 사용하여 배치에 대한 결과의 수를 제한하여, 큰 배포에서 모든 컴퓨팅 노드가 반환되는 상황을 피하기"
" 위해 now limits the number of results in the query it makes to placement. 이 "
"설정은 new ``[scheduler]/max_placement_results`` 설정 옵션으로 configurable합니다. 이 설정은"
" 1000으로 기본적으로 설정되어, 모든 배포 크기에서 적절한 시작 값입니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:510
msgid ""
"The Hyper-V driver can now attach Cinder RBD volumes. The minimum "
"requirements are Ceph 16 (Pacific) and Windows Server 2016."
msgstr ""
"Hyper-V 드라이버는 현재 Cinder RBD 볼륨을 연결할 수 있습니다. 최소한의 요구 사항은 Ceph 16 (Pacific) 및 "
"Windows Server 2016입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1583
msgid ""
"The Hyper-V driver no longer accepts cold migrating instances to the same "
"host. Note that this does not apply to resizes, in which case this is still "
"allowed."
msgstr ""
"Hyper-V 드라이버는 더 이상 같은 호스트에 冷동 이민을 허용하지 않습니다. 이에 대한 주의가 필요합니다. 그러나 확장은 여전히 "
"허용됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:607
msgid ""
"The Hyper-V driver now supports the following quota flavor extra specs, "
"allowing to specify IO limits applied for each of the instance local disks, "
"individually."
msgstr ""
"Hyper-V 드라이버는 다음_quota 플레어의 추가 속성으로 인해 각 인스턴스 로컬 디스크에 적용되는 I/O 제한을 "
"individually 지정할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:624
msgid ""
"The Hyper-V driver now uses os-brick for volume related operations, "
"introducing the following new features:"
msgstr "Hyper-V 드라이버는 현재 os-brick를 사용하여 볼륨 관련 연산을 수행하고, 다음 새로운 기능을 도입합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:413
msgid "The Hyper-V virt driver can now attach Cinder RBD volumes."
msgstr "Hyper-V virt 드라이버는 현재 Cinder RBD 볼륨을 연결할 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:360
msgid ""
"The Hyper-V virt driver has been removed. It was deprecated in the Nova "
"27.2.0 (Antelope) release. This driver was untested and has no maintainers. "
"In addition, it had a dependency on the OpenStack Winstacker project that "
"also has been retired."
msgstr ""
"하이퍼-V virt 드라이버가 제거되었다. 노바 27.2.0 (Antelope) 릴리스에서弃기되었다. 이 드라이버는 테스트되지 않았고 "
"유지 관리자가 없으며, 오픈 스텝 Winstacker 프로젝트와도 연관성이 있다. 이 프로젝트도 폐기되었다."

#: ../../<reno.sphinxext stable/2023.2>:195
msgid ""
"The Ironic driver ``[ironic]/peer_list`` configuration option has been "
"deprecated. The Ironic driver now more closely models other Nova drivers by "
"having a single compute have exclusive control over assigned nodes. If high "
"availability of a single compute service is required, operators should use "
"active/passive failover."
msgstr ""
"ironic 드라이버의 `[ironic]/peer_list` 구성 옵션은弃용되었습니다. ironic 드라이버는 다른 Nova 드라이버와 "
"더ใกล게 모델링되어 하나의 컴퓨팅이 할당된 노드에 대한 독점적 제어를 가지고 있습니다. 단일 컴퓨팅 서비스의 고가용性이 필요할 경우, "
"운영자에는 활성/비활성 failover를 사용해야 합니다."

#: ../../<reno.sphinxext stable/2024.1>:311
msgid ""
"The Ironic driver ``[ironic]/peer_list`` configuration option has been "
"deprecated. The Ironic driver now more closely models other Nova drivers "
"where compute nodes do not move between compute service instances. If high "
"availability of a single compute service is required, operators should use "
"active/passive failover between 2 compute service agents configured to share"
" the same compute service host value``[DEFAULT]/host``. Ironic nova-compute "
"services can now be configured to target a specific shard of ironic nodes by"
" setting the ``[ironic]/shard`` configuration option and a new ``nova-manage"
" db ironic_compute_node_move`` command can help the operators when upgrading"
" their computes to specify which shard they should manage."
msgstr ""
"ironic 드라이버의 ``[ironic]/peer_list`` 구성 옵션은弃용되었습니다. ironic 드라이버는 다른 Nova "
"드라이버와 더ใกล게 모델링되어 compute 노드는 compute 서비스 인스턴스 사이에 이동하지 않습니다. 단일 compute "
"서비스의 고용성이 필요하다면, 2개의 compute 서비스 एजент이 동일한 compute 서비스 호스트 값으로 구성된 경우에 "
"failover를 사용하여 활성/비활성 failover를 사용해야합니다. ironic nova-compute 서비스는 "
"``[ironic]/shard`` 구성 옵션을 설정하여 특정 shard의 ironic 노드를 표적할 수 있습니다. 새로운 ``nova-"
"manage db ironic_compute_node_move`` 명령은 업그레이드를 위해 compute를 spécifique한 "
"shard를 관리할 수 있도록 도와줍니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1204
msgid ""
"The Ironic driver now requires python-ironicclient>=1.9.0, and requires "
"Ironic service to support API version 1.28 or higher. As usual, Ironic "
"should be upgraded before Nova for a smooth upgrade process."
msgstr ""
"Ironic 드라이버는 python-ironicclient>=1.9.0을 사용해야 하며, API 버전 1.28 이상을 지원하기 위해 "
"Ironic 서비스를 사용해야 합니다. 일반적으로, Ironic은 Nova와 함께 업그레이드하는 것이 보통의 업그레이드 프로세스입니다."

#: ../../<reno.sphinxext stable/train>:1176
msgid ""
"The Libvirt SR-IOV migration feature intoduced in this release requires both"
" the source and destination node to support the feature. As a result it will"
" be automatically disabled until the conductor and compute nodes have been "
"upgraded."
msgstr ""
"이 릴리즈에서 도입된 Libvirt SR-IOV 이민화 기능은 소스 노드와 목적 노드가 기능을 지원하는 것을 필요로 합니다. 따라서 이 "
"기능은 conductor 및 compute 노드가 업그레이드 될 때까지 tự động 비활성화됩니다."

#: ../../<reno.sphinxext unmaintained/zed>:278
msgid ""
"The Libvirt driver can now add a virtual IOMMU device to all created guests,"
" when running on an x86 host and using the Q35 machine type or on AArch64."
msgstr ""
"libvirt 드라이버는 x86 호스트에서 Q35 मशीन 타입 또는 AArch64를 사용할 때만 생성된 게스트에 모든 가상 IOMMU "
"장치를 추가할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:531
msgid ""
"The Nova FilterScheduler driver is now able to make scheduling decisions "
"based on the new Placement RESTful API endpoint that becomes mandatory in "
"Ocata. Accordingly, the compute nodes will refuse to start if you do not "
"amend the configuration to add the ``[placement]`` section so they can "
"provide their resource usage. For the moment, only CPU, RAM and disk "
"resource usage are verified by the Placement API, but we plan to add more "
"resource classes in the next release. You will find further details in the "
"features and upgrade sections below, and the `Placement API`_ page."
msgstr ""
"Nova FilterScheduler 드라이버는 now Ocata에서 강제적으로 사용해야 하는 새로운 Placement RESTful "
"API endpoint에 따라 스케줄링 결정을 내릴 수 있습니다. 따라서, 컴퓨터 노드는 `[placement]` 섹션을 추가하여 자원 "
"사용을 제공할 수 있도록 구성이 변경되지 않으면 시작하지 않습니다. 현재, Placement API는 CPU, RAM 및 디스크 자원 "
"사용만 확인하고, 다음 릴리스에서 더 많은 자원 클래스를 추가할 계획입니다. 더 많은 정보는 아래의 기능 및 업그레이드 섹션, 및 "
"`Placement API`_ 페이지에서 찾을 수 있습니다."

#: ../../<reno.sphinxext unmaintained/zed>:312
msgid ""
"The Nova policies have been modified to drop the system scope. Every API "
"policy is scoped to project. This means that system scoped users will get "
"403 permission denied error."
msgstr ""
"노바 정책이 시스템 스코프를 drop 하여서 시스템 범위가 제외되었다. 모든 API 정책은 프로젝트 범위로 scoped된다. 이것은 "
"시스템 스코프의 사용자에게 403 permission denied error가 발생한다."

#: ../../<reno.sphinxext unmaintained/yoga>:346
msgid ""
"The Nova policies have been modified to isolate the system and project level"
" APIs policy. This means system users will be allowed to perform the "
"operation on system level resources and will not to allowed any operation on"
" project level resources. Project Level APIs operation will be performed by "
"the project scoped users. Currently, nova supports:"
msgstr ""
"nova 정책은 시스템 및 프로젝트 수준 API 정책을 분리하기 위해 수정되었다. 이 nghĩa는 시스템 사용자는 시스템 수준 리소스를 "
"수행하는 연산을 허용하고 프로젝트 수준 리소스에 대한 연산을 허용하지 않는다. 프로젝트 수준 API 연산은 프로젝트 스코프 사용자에 의해"
" 수행된다. 현재, nova는 다음과 같은 것을 지원한다."

#: ../../<reno.sphinxext stable/ussuri>:482
msgid ""
"The Nova policies implemented the scope concept and new default roles "
"(``admin``, ``member``, and ``reader``) provided by keystone."
msgstr ""
"Nova 정책은 keystone에서 제공하는 admin, member, reader 등 새로운 기본 역할을 implementation하고"
" scope concept을 구현했습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:425
msgid ""
"The Nova service enable the API policies (RBAC) new defaults and scope by "
"default. The Default value of config options ``[oslo_policy] enforce_scope``"
" and ``[oslo_policy] oslo_policy.enforce_new_defaults`` have been changed to"
" ``True``."
msgstr ""
"nova 서비스는 API 정책(RBAC)을 위한 기본값을 설정하여 새로운 scope를 기본적으로 사용합니다. \n"
"config 옵션의 기본값인 `[oslo_policy] enforce_scope` 및 `[oslo_policy] oslo_policy.enforce_new_defaults`의 기본값이 `True`로 변경되었습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:268
msgid ""
"The OpenStack 2023.1 (Nova 27.0.0) release includes many new features and "
"bug fixes. Please be sure to read the upgrade section which describes the "
"required actions to upgrade your cloud from 26.0.0 (Zed) to 27.0.0 (2023.1)."
" As a reminder, OpenStack 2023.1 is our first `Skip-Level-Upgrade Release`__"
" (starting from now, we name it a `SLURP release`) where you can rolling-"
"upgrade your compute services from OpenStack Yoga as an experimental "
"feature. Next SLURP release will be 2024.1."
msgstr ""
"OpenStack 2023.1 (Nova 27.0.0) 버전은 많은 새로운 기능과 버그修复을 포함합니다. upgrade 섹션을 읽어야 "
"합니다. 이 섹션은 26.0.0 (Zed)에서 27.0.0 (2023.1)으로 클라우드를 업그레이드하는 데 필요한 hành động을 "
"설명합니다. reminder로 OpenStack 2023.1은 우리가 현재 `Skip-Level-Upgrade Release`__ "
"(현재부터 SLURP 릴리스라고 이름을 붙인)로 시작하는 첫 번째 릴리스입니다. OpenStack Yoga에서 compute 서비스를 "
"rolling-upgrade 할 수 있는 experimental 기능으로, 다음 SLURP 릴리스는 2024.1입니다."

#: ../../<reno.sphinxext stable/2023.2>:177
msgid ""
"The OpenStack 2023.2 (Nova 28.0.0) release includes many new features and "
"bug fixes. Please be sure to read the upgrade section which describes the "
"required actions to upgrade your cloud from 27.0.0 (2023.1) to 28.0.0 "
"(2023.2). As a reminder, OpenStack 2023.2 is a non-`Skip-Level-Upgrade "
"Release`__ (starting from now, we name it a `non-SLURP release`) meaning "
"that you can only do rolling-upgrade from 2023.1. Next SLURP release will be"
" 2024.1 where you will be able to upgrade from 2023.1 directly by skipping "
"this release."
msgstr ""
"OpenStack 2023.2 (Nova 28.0.0) 릴리스는 많은 새로운 기능과 버그修复을 포함합니다._upgrade section을"
" 읽어야 합니다. 이 section은 27.0.0 (2023.1)에서 28.0.0 (2023.2)로 클라우드를 업그레이드하는 आवश정한 "
"행동을 설명합니다. _`Skip-Level-Upgrade Release`__ (현재부터는 `non-SLURP 릴리스`라고 이름을 붙인다는"
" 것을 기억하십시오)라는 것을 기억하세요. OpenStack 2023.2은 2023.1에서 only rolling-upgrade를 할 수"
" 있습니다. 다음 SLURP 릴리스는 2024.1이며, 2023.1에서 직접 업그레이드할 수 있습니다. 이 릴리스를 skips할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/2024.1>:294
msgid ""
"The OpenStack 2024.1 (Nova 29.0.0) release includes many new features and "
"bug fixes. Please be sure to read the upgrade section which describes the "
"required actions to upgrade your cloud from 28.0.0 (2023.2) to 29.0.0 "
"(2024.1). As a reminder, OpenStack 2024.1 is a `Skip-Level-Upgrade "
"Release`__ (starting from now, we name it a `SLURP release`) meaning that "
"you can do rolling-upgrade from 2023.1 and skip 2023.2."
msgstr ""
"OpenStack 2024.1 (Nova 29.0.0) 릴리스는 많은 새로운 기능과 버그修复을 포함합니다._upgrade section을"
" 읽어야 합니다. 이 section은 28.0.0 (2023.2)에서 29.0.0 (2024.1)으로 클라우드를 업그레이드하는必要한 "
"hành động을 설명합니다. 2023.1부터 업그레이드할 수 있으며 2023.2를 skipping할 수 있습니다. OpenStack "
"2024.1은 `Skip-Level-Upgrade Release`__ (현재부터는 `SLURP 릴리스`라고 이름을 변경합니다.)로 "
"정의되어 있습니다."

#: ../../<reno.sphinxext stable/2024.2>:220
msgid ""
"The OpenStack 2024.2 (Nova 30.0.0) release includes many new features and "
"bug fixes. Please be sure to read the upgrade section which describes the "
"required actions to upgrade your cloud from 29.0.0 (2024.1) to 30.0.0 "
"(2024.2). As a reminder, OpenStack 2024.2 is not a `Skip-Level-Upgrade "
"Release`__ (starting from now, we name it a `SLURP release`) meaning that "
"you can't do rolling-upgrade from 2023.2, you first need to upgrade to "
"2024.1."
msgstr ""
"OpenStack 2024.2 (Nova 30.0.0) 버전은 많은 새로운 기능과 버그修复을 포함합니다._upgrade section을 "
"읽어야 합니다. 이 section은 29.0.0 (2024.1)에서 30.0.0 (2024.2)로 클라우드를 업그레이드하는 데 필요한 "
"hành động을 설명합니다. 2023.2부터는 `Skip-Level-Upgrade Release`__ (현재는 `SLURP "
"release`라고 이름이 변경되었습니다)가 아닌으로, 2023.2에서 2024.1으로 업그레이드 후에만 2024.2로 업그레이드할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/2025.1>:172
msgid ""
"The OpenStack 2025.1 Epoxy (Nova 31.0.0) release includes many new features "
"and bug fixes. Please be sure to read the upgrade section which describes "
"the required actions to upgrade your cloud from 30.0.0 (2024.2) to 31.0.0 "
"(2025.1). As a reminder, OpenStack 2025.1 is a `Skip-Level-Upgrade "
"Release`__ (starting from now, we name it a `SLURP release`) meaning that "
"you can do rolling-upgrades from 2024.1 Caracal directly by skipping to "
"upgrade to 2024.2 Dalmatian release."
msgstr ""
"OpenStack 2025.1 Epoxy (Nova 31.0.0) 릴리스는 많은 새로운 기능과 버그修复을 포함합니다. 이 릴리스를 "
"업그레이드하는 것을 원활하게 할 수 있도록_upgrade_section을 읽어야 합니다. 이 upgrading을 위해 30.0.0 "
"(2024.2)에서 31.0.0 (2025.1)으로 업그레이드하는 것을 목표로 합니다. OpenStack 2025.1은 `Skip-"
"Level-Upgrade Release`__ (현재부터는 `SLURP 릴리스`라고 이름을 붙입니다.)로, 2024.1 Caracal에서 "
"2024.2 Dalmatian 릴리스로 업그레이드를 skipping하여 2024.1 Caracal에서 2025.1 Epoxy로 "
"rolling-upgrade를 할 수 있습니다."

#: ../../<reno.sphinxext stable/2025.2>:14
msgid ""
"The OpenStack 2025.2 (Nova 32.0.0) release includes many new features and "
"bug fixes. Please be sure to read the upgrade section which describes the "
"required actions to upgrade your cloud from 31.0.0 (2025.1) to 32.0.0 "
"(2025.2). As a reminder, OpenStack 2025.2 is not a `Skip-Level-Upgrade "
"Release <https://governance.openstack.org/tc/resolutions/20220210-release-"
"cadence-adjustment.html>`_ (starting from now, we name it a `SLURP release`)"
" meaning that you can't do rolling-upgrade from 2024.2, you first need to "
"upgrade to 2025.1."
msgstr ""
"OpenStack 2025.2 (Nova 32.0.0) 버전은 많은 새로운 기능과 버그修复을 포함합니다._upgrade_ section을"
" 읽어야 합니다. 이 section은 31.0.0 (2025.1)에서 32.0.0 (2025.2)로 클라우드를 업그레이드하는 데 필요한 "
"hành động을 설명합니다. OpenStack 2025.2은 `Skip-Level-Upgrade Release "
"<https://governance.openstack.org/tc/resolutions/20220210-release-cadence-"
"adjustment.html>`_ (현재부터는 SLURP 릴리스라고 이름을 붙인다는 의미는)가 아니며, 2024.2에서 롤링 업그레이드를"
" 할 수 없습니다. 따라서 2025.1으로 업그레이드해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1632 stable/pike>:1906
msgid ""
"The Placement API can be set to connect to a specific keystone endpoint "
"interface using the ``os_interface`` option in the ``[placement]`` section "
"inside ``nova.conf``. This value is not required but can be used if a non-"
"default endpoint interface is desired for connecting to the Placement "
"service. By default, keystoneauth will connect to the \"public\" endpoint."
msgstr ""
"Placement API는 `os_interface` 옵션을 사용하여 `nova.conf` 내의 `[placement]` 섹션에 있는 "
"`os_interface` 옵션을 사용하여 특정 keystone endpoint 인터페이스를 연결할 수 있습니다. 이 값은 필요하지 "
"않지만, Placement 서비스에 연결할 때 비default endpoint 인터페이스를 원하는 경우 사용할 수 있습니다. 기본적으로 "
"keystoneauth는 \"public\" endpoint에 연결합니다."

#: ../../<reno.sphinxext stable/rocky>:1195
msgid ""
"The PowerVM driver now supports hot plugging/unplugging of network "
"interfaces."
msgstr "PowerVM 드라이버는 현재 네트워크 인터페이스를.hot 플ugging/unplugging을 지원합니다."

#: ../../<reno.sphinxext stable/rocky>:1200
msgid ""
"The PowerVM virt driver now supports booting from local ephemeral disk. Two "
"new configuration options have been introduced to the ``powervm`` "
"configuration group, ``disk_driver`` and ``volume_group_name``. The former "
"allows the selection of either ssp or localdisk for the PowerVM disk driver."
" The latter specifies the name of the volume group when using the localdisk "
"disk driver."
msgstr ""
"PowerVM virt 드라이버는 현재 로컬 임시 디스크에서 부트를 지원합니다. 2개의 새로운 구성 옵션들이 ``powervm`` 구성 "
"그룹에 도입되었습니다. ``disk_driver``과 ``volume_group_name``입니다.  前者の 경우 PowerVM 디스크 "
"드라이버에 ssp 또는 localdisk를 선택할 수 있습니다.  후者の 경우 localdisk 디스크 드라이버를 사용할 때는 "
"volume group의 이름을 지정합니다."

#: ../../<reno.sphinxext stable/rocky>:1209
msgid "The PowerVM virt driver now supports instance snapshot."
msgstr "PowerVM virt 드라이버는 현재 인스턴스 스냅샷을 지원합니다."

#: ../../<reno.sphinxext stable/rocky>:1213
msgid ""
"The PowerVM virt driver now supports vSCSI Fibre Channel cinder volumes. "
"PowerVM now supports attaching, detaching, and extending the size of vSCSI "
"FC cinder volumes."
msgstr ""
"PowerVM virt 드라이버는 현재 vSCSI Fibre Channel Cinder 볼륨을 지원합니다. PowerVM은 현재 "
"vSCSI FC Cinder 볼륨을 연결, 해제 및 크기를 확장할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1599
msgid ""
"The PowerVM virt driver previously used the PowerVM Shared Storage Pool disk"
" driver by default. The default disk driver for PowerVM is now localdisk. "
"See configuration option ``[powervm]/disk_driver`` for usage details."
msgstr ""
"PowerVM virt 드라이버는 이전에 PowerVM 공유 스토리지 풀 디스크 드라이버를 기본적으로 사용했습니다. PowerVM의 기본"
" 디스크 드라이버는 now localdisk입니다. PowerVM의 구성 옵션 ``[powervm]/disk_driver``를 "
"참조하십시오."

#: ../../<reno.sphinxext stable/train>:841
msgid ""
"The Quobyte Nova volume driver now supports identifying Quobyte mounts via "
"the mounts fstype field, which is used by Quobyte 2.x clients. The previous "
"behaviour is deprecated and may be removed from the Quobyte clients in the "
"future."
msgstr ""
"Quobyte Nova 볼륨 드라이버는 Quobyte 2.x 클라이언트에 의해 사용되는 mounts fstype 필드를 통해 "
"Quobyte 마운트를 식별할 수 있습니다. 이전의 행동은 deprecated되어 향후 Quobyte 클라이언트에서 제거될 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:481
msgid ""
"The RDP console was only available for the HyperV driver, therefore the RDP "
"console related APIs below will return HTTP ``400 (BadRequest)`` error:"
msgstr ""
"RDP 콘솔은 하이퍼바이저 드라이버만이 사용할 수 existed therefore RDP 콘솔 관련 API below will "
"return HTTP ``400 (BadRequest)`` error:"

#: ../../<reno.sphinxext stable/queens>:557 stable/rocky>:2009
msgid ""
"The SchedulerReportClient "
"(``nova.scheduler.client.report.SchedulerReportClient``) sends requests with"
" the global request ID in the ``X-Openstack-Request-Id`` header to the "
"placement service. `Bug 1734625`_"
msgstr ""
"SchedulerReportClient "
"(``nova.scheduler.client.report.SchedulerReportClient``)는 전역 요청 ID를 포함하는 "
"X-Openstack-Request-Id 헤더를 통해 배치 서비스에 요청을 보냅니다.  `_Bug 1734625_`"

#: ../../<reno.sphinxext origin/stable/ocata>:722
msgid ""
"The UEFI Secure Boot feature can be requested through the image property "
"\"os_secure_boot\" (acceptable values: \"disabled\", \"optional\", "
"\"required\") or flavor extra spec \"os:secure_boot\" (acceptable values: "
"\"disabled\", \"required\"). The flavor extra spec will take precedence. If "
"the image property and the flavor extra spec values are conflicting, then an"
" exception is raised."
msgstr ""
"UEFI 보안 부트 기능은 \"os_secure_boot\" 속성(\"disabled\", \"optional\", "
"\"required\"을 사용할 수 있는 가치) 또는 \"os:secure_boot\" flavor extra spec ( "
"\"disabled\", \"required\"을 사용할 수 있는 가치)로 요청할 수 있습니다. flavor extra spec은 "
"precedence를 지닙니다. image 속성과 flavor extra spec의 가치가 충돌할 경우 예외가 발생합니다."

#: ../../<reno.sphinxext stable/rocky>:741
msgid ""
"The URLs in cell mapping records may now include variables that are filled "
"from the corresponding default URL specified in the host's configuration "
"file. This allows per-host credentials, as well as other values to be set in"
" the config file which will affect the URL of a cell, as calculated when "
"loading the record. For ``database_connection``, the "
"``[database]/connection`` URL is used as the base. For ``transport_url``, "
"the ``[DEFAULT]/transport_url`` is used. For more information, see the cells"
" configuration docs: https://docs.openstack.org/nova/latest/user/cells.html"
msgstr ""
"세ลล์ 매핑 레코드에 있는 URL은 현재 기본 URL을 사용하여 fill 된 변수가 포함될 수 있습니다. 이 기본 URL은 호스트의 "
"구성 파일에 지정된 corresponding URL입니다. 이 allows per-host credentials, as well as "
"other values to be set in the config file which will affect the URL of a "
"cell, as calculated when loading the record. For ``database_connection``, "
"the ``[database]/connection`` URL is used as the base. For "
"``transport_url``, the ``[DEFAULT]/transport_url`` is used. For more "
"information, see the cells configuration docs: "
"https://docs.openstack.org/nova/latest/user/cells.html"

#: ../../<reno.sphinxext stable/stein>:796
msgid ""
"The VMware compute driver now supports live migration. Each compute node "
"must be managing a cluster in the same vCenter and ESX hosts must have "
"vMotion enabled."
msgstr ""
"VMware compute 드라이버는 현재 live migration을 지원합니다. 각 compute 노드는 동일한 vCenter에서 "
"cluster를 관리해야 하며 ESX 호스트는 vMotion을 활성화해야 합니다."

#: ../../<reno.sphinxext stable/queens>:1096
msgid ""
"The VMware vCenter compute driver now supports booting from images which "
"specify they require UEFI or BIOS firmware, using the ``hw_firmware_type`` "
"image metadata."
msgstr ""
"VMware vCenter compute 드라이버는 UEFI 또는 BIOS 패밀리 firmware를 요구하는 이미지를 부트로 사용할 수 "
"있는 `hw_firmware_type` 이미지 메타데이터를 사용하여 now supports booting from images."

#: ../../<reno.sphinxext stable/stein>:454
msgid ""
"The VMwareVCDriver now supports live migration. See the `live migration "
"configuration`_ documentation for information on how to enable it."
msgstr ""
"VMware VCDriver 현재 live migration을 지원합니다. live migration 구성에 대한 정보는 [live "
"migration configuration]_ 문서를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:1847
msgid ""
"The VNC option affects the libvirt and VMWare virt drivers, while the SPICE "
"option only affects libvirt. For the libvirt driver, configuring these "
"options resulted in lossy keymap conversions for the given graphics method. "
"It is recommended that users should unset these options and configure their "
"guests as necessary instead. In the case of noVNC, noVNC 1.0.0 should be "
"used as this provides support for QEMU's Extended Key Event messages. Refer "
"to `bug #1682020`__ and the `QEMU RFB pull request`__ for more information."
msgstr ""
"VNC 옵션은 libvirt 및 VMWare virt 드라이버에 영향을 미치며, SPICE 옵션은 libvirt에만 영향을 미치고 "
"있습니다. libvirt 드라이버의 경우, 이러한 옵션을 구성하면 graphics 메서드에 대한 lossy keymap "
"conversion이 발생합니다. 이러한 옵션을 비활성화하고, 필요한 경우 게스트를 구성하는 것이 좋습니다. noVNC의 경우, "
"noVNC 1.0.0을 사용하는 것이 QEMU의 Extended Key Event 메시지에 대한 지원을 제공합니다. 더 많은 정보는 "
"`bug #1682020`__ 및 `QEMU RFB pull request`__에 참조하십시오."

#: ../../<reno.sphinxext stable/train>:517
msgid ""
"The XenAPI virt driver is now deprecated and may be removed in a future "
"release as its quality can not be ensured due to lack of maintainers."
msgstr ""
"XenAPI virt 드라이버는 현재 deprecated되어 향후 릴리스에서 제거될 수 있으며, 유지 관리자가 부족하여品질을 보장할 수 "
"없기 때문이다."

#: ../../<reno.sphinxext origin/stable/ocata>:651
msgid ""
"The XenServer compute driver now supports hot-plugging virtual network "
"interfaces."
msgstr "XenServer 컴퓨터 드라이버는 현재 가상 네트워크 인터페이스를.hot 플러그로 지원합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1525
msgid ""
"The XenServer driver provides support for downloading images via torrents. "
"This feature has not been tested, and it's not clear whether there's a clear"
" use case for such a feature. As a result, this feature is now deprecated as"
" are the following config options."
msgstr ""
"XenServer 드라이버는 이미지를 torrent를 통해 다운로드하는 지원을 제공합니다. 이 기능은 테스트되지 않았으며, 이러한 기능을 사용하는 경우가 명확하지 않다. 따라서, 이 기능은 다음 구성 옵션과 함께弃기되었습니다.\n"
"\n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집합 \n"
"- 집"

#: ../../<reno.sphinxext unmaintained/zed>:485
msgid ""
"The [pci]passthrough_whitelist config option is renamed to [pci]device_spec."
" The old name is deprecated and aliased to the new one. The old name will be"
" removed in a future release."
msgstr ""
"pci]passthrough_whitelist 구성 옵션은 [pci]device_spec로 이름이 변경되었으며,舊 이름은 "
"비활성화되었으며, 새로운 이름으로 대체되었다.舊 이름은 향후 릴리스에서 제거될 예정이다."

#: ../../<reno.sphinxext stable/2023.2>:414 unmaintained/2023.1>:138
msgid ""
"The `CPU power management`_ feature has been fixed to use privsep to avoid a"
" FileNotFound error when offlining CPUs."
msgstr ""
"`CPU power management`_ 기능은 CPU를 비활성화할 때 FileNotFound 오류를 피하기 위해 privsep를 "
"사용하여 고정되었습니다."

#: ../../<reno.sphinxext stable/queens>:1557
msgid ""
"The `IronicHostManager` is now deprecated along with the "
"``[scheduler]/host_manager`` option of ``ironic_host_manager``."
msgstr ""
"`IronicHostManager`은 `ironic_host_manager`의 `[scheduler]/host_manager` 옵션과 "
"함께弃용되었습니다."

#: ../../<reno.sphinxext stable/rocky>:2048
msgid ""
"The `[api]/instance_list_per_project_cells` configuration option was added, "
"which controls whether or not an instance list for non-admin users checks "
"all cell databases for results.  If disabled (the default), then a list will"
" always contact each cell database looking for instances. This is "
"appropriate if you have a small number of cells, and/or if you spread "
"instances from tenants evenly across cells. If you confine tenants to a "
"subset of cells, then enabling this will result in fewer cell database "
"calls, as nova will only query the cells for which the tenant has instances "
"mapped. Doing this requires one more (fast) call to the API database to get "
"the relevant subset of cells, so if that is likely to always be the same, "
"disabling this feature will provide better performance."
msgstr ""
"`[api]/instance_list_per_project_cells` 구성 옵션은 사용자 목록에 대한 인스턴스 목록을 확인할 때, "
"비-admin 사용자에 대한 인스턴스 목록이 모든 세포 데이터베이스에 결과를 확인할지 여부를 제어하는 옵션입니다.  비활성화된 경우 "
"(기본값), 인스턴스 목록은 항상 각 세포 데이터베이스를 확인하여 인스턴스를 찾습니다. 이 경우, 작은 세포 수가 있거나/또는 "
"tenant를 evenly 분산하여 인스턴스를 세포에 spread 한 경우 적합합니다.  tenant를 특정 세포에 제한하면, 이 옵션을"
" 활성화하면 세포 데이터베이스를 적게 호출할 수 있습니다. nova가 tenant에 인스턴스를 mapping한 세포를 only "
"query할 수 있습니다. 이 기능을 활성화하는 것은, API 데이터베이스에 relevnt한 subset of 세포를 얻기 위해 더 많은"
" (fast) 호출을 필요로합니다.  이 경우, 항상 동일한 subset of 세포가 있을 경우, 이 기능을 비활성화하면 성능이 "
"개선됩니다."

#: ../../<reno.sphinxext stable/train>:1057
msgid ""
"The ``--version`` argument has been removed in the following commands. Use "
"the ``VERSION`` positional argument instead."
msgstr "``--version`` 매개 변수는 다음 명령어에서 제거되었다. 대신 ``VERSION`` 매개 변수를 사용하십시오."

#: ../../<reno.sphinxext stable/pike>:1720
msgid ""
"The ``--version`` parameters of the ``nova-manage api_db sync`` and ``nova-"
"manage db sync`` commands has been deprecated in favor of positional "
"arguments."
msgstr ""
"``--version`` 매개 변수의 ``nova-manage api_db sync`` 및 ``nova-manage db sync`` "
"명령어의 사용은.positional arguments에 대체되어서 deprecated되었습니다."

#: ../../<reno.sphinxext stable/train>:1213
msgid ""
"The ``AggregateCoreFilter``, ``AggregateRamFilter`` and "
"``AggregateDiskFilter`` are now deprecated. They will be removed in a future"
" release and should no longer be used. Their functionality has been replaced"
" with a placement native approach by combining host aggregate mirroring "
"added in Rocky and initial allocation ratios added in Stein. See the "
"`scheduler documentation`_ for details."
msgstr ""
"``집합CoreFilter``, ``집합RAMFilter`` 및 ``집합디스크Filter``은 현재 deprecated입니다. 이들은 "
"향후 릴리스에서 제거되어 더 이상 사용되지 않도록 할 수 있습니다. 그들의 기능은 로키에서 추가된 호스트 집합 반영 및 스티恩에서 추가된"
" 초기 할당 비율을 결합하여 원산지 기반 접근 방식을 사용하여 대체되었습니다.  자세한 내용은 `스케줄 문서`에서 확인하십시오."

#: ../../<reno.sphinxext unmaintained/xena>:579
msgid ""
"The ``AvailabilityZoneFilter`` scheduler filters is now deprecated for "
"removal in a future release. The functionality of the "
"``AvailabilityZoneFilter`` has been replaced by the "
"``map_az_to_placement_aggregate`` pre-filter which was introduced in 18.0.0 "
"(Rocky). This pre-filter is now enabled by default and will be mandatory in "
"a future release."
msgstr ""
"``AvailabilityZoneFilter`` 스케줄러는 현재 future release에서 제거되기 위해 "
"deprecated되었습니다. ``AvailabilityZoneFilter``의 기능은 18.0.0 (Rocky)에서 도입된 "
"``map_az_to_placement_aggregate`` pre-filter에 의해 대체되었습니다. 이 pre-filter은 현재 "
"기본적으로 활성화되어 있으며 future release에서 필수적이게 됩니다."

#: ../../<reno.sphinxext stable/2023.2>:315
msgid ""
"The ``AvailabilityZoneFilter`` was deprecated for removal in 24.0.0 (Xena) "
"and has now been removed. The functionality of the``AvailabilityZoneFilter``"
" has been replaced by the``map_az_to_placement_aggregate`` pre-filter. The  "
"pre-filter was introduced in 18.0.0 (Rocky) and enabled by default in 24.0.0"
" (Xena). This pre-filter is now always enabled and the ``[scheduler] "
"query_placement_for_availability_zone`` config option has been removed."
msgstr ""
"``AvailabilityZoneFilter``은 24.0.0 (Xena)에서 제거되기 위해弃용되었으며 현재 제거되었습니다. "
"``AvailabilityZoneFilter``의 기능은 ``map_az_to_placement_aggregate`` 전처리 함수에 "
"대체되었습니다. 전처리 함수는 18.0.0 (Rocky)에서 도입되었으며 24.0.0 (Xena)에서 기본적으로 활성화되었습니다. 현재 "
"전처리 함수는 항상 활성화되어且 ``[scheduler] query_placement_for_availability_zone`` 구성 "
"옵션은 제거되었습니다."

#: ../../<reno.sphinxext stable/stein>:1236
msgid ""
"The ``CoreFilter``, ``DiskFilter`` and ``RamFilter`` are now deprecated. "
"VCPU, DISK_GB and MEMORY_MB filtering is performed natively using the "
"Placement service when using the ``filter_scheduler`` driver. Users of the "
"``caching_scheduler`` driver may still rely on these filters but the "
"``caching_scheduler`` driver is itself deprecated. Furthermore, enabling "
"these filters may incorrectly filter out baremetal nodes which must be "
"`scheduled using custom resource classes "
"<https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html>`_."
msgstr ""
"``CoreFilter``, ``DiskFilter`` 및 ``RamFilter``는 현재 비활성화되었습니다. VCPU, DISK_GB "
"및 MEMORY_MB 필터링은 ``filter_scheduler`` 드라이버를 사용할 때 ``Placement`` 서비스를 사용하여 "
"nativamente 수행됩니다. ``caching_scheduler`` 드라이버를 사용하는 사용자들은 여전히 이러한 필터를 사용할 수 "
"있지만 ``caching_scheduler`` 드라이버 자체는 비활성화되었습니다. 또한, 이러한 필터를 활성화하면 Bare Metal "
"노드는 `custom resource classes "
"<https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html>`_을 사용하여 스케줄링해야 하는 `scheduled using` 명령을 사용하여 Bare Metal 노드를 "
"필터링할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:1126
msgid ""
"The ``CoreFilter``, ``DiskFilter`` and ``RamFilter``, which were deprecated "
"in Stein (19.0.0), are now removed. ``VCPU``, ``DISK_GB`` and ``MEMORY_MB`` "
"filtering is performed natively using the Placement service. These filters "
"have been warning operators at startup that they conflict with proper "
"operation of placement and should have been disabled since approximately "
"Pike. If you did still have these filters enabled and were relying on them "
"to account for virt driver overhead (at the expense of scheduler races and "
"retries), see the `scheduler`_ documentation about the topic."
msgstr ""
"``CoreFilter``, ``DiskFilter`` 및 ``RamFilter``, Stein (19.0.0)에서弃용된 것은 현재 "
"제거되었다. ``VCPU``, ``DISK_GB`` 및 ``MEMORY_MB`` 필터는 nativelly Placement "
"service를 사용하여 수행된다. 이러한 필터는 시작 시에 운영자가 배치와 적절한 작동을 conflict하는 것을 경고하고, "
"Pike에서부터 disable되어야 했다. 만약 여전히 이러한 필터를 활성화하고 virt driver overhead (scheduler"
" races와 retry의 비용으로) 계정에 의존하고 있다면, `scheduler`_ documentation에 대해 주제에 대해 더 "
"자세히 erfahren할 수 있다."

#: ../../<reno.sphinxext stable/queens>:14 stable/rocky>:119 stable/stein>:166
#: stable/train>:406 stable/ussuri>:1213
msgid ""
"The ``DELETE /os-services/{service_id}`` compute API will now return a ``409"
" HTTPConflict`` response when trying to delete a ``nova-compute`` service "
"which is involved in in-progress migrations. This is because doing so would "
"not only orphan the compute node resource provider in the placement service "
"on which those instances have resource allocations but can also break the "
"ability to confirm/revert a pending resize properly. See "
"https://bugs.launchpad.net/nova/+bug/1852610 for more details."
msgstr ""
"``DELETE /os-services/{service_id}`` 컴퓨터 API는 현재 ``nova-compute`` 서비스가 진행 중인"
" 이중 마이그레이션에 참여하는 경우, ``DELETE`` 요청을 시도할 때는 ``409 HTTPConflict``응답을 반환합니다. 이 "
"이유는 compute 노드 리소스 제공자를 placement 서비스에 있는 resource allocation에 orphaning 할 "
"뿐만 아니라, pending resize를 확인/반려할 수 있는 능력을 깨뜨리는 것입니다. 더 많은 정보는 "
"https://bugs.launchpad.net/nova/+bug/1852610 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:200 stable/queens>:466
#: stable/rocky>:2018
msgid ""
"The ``DELETE /os-services/{service_id}`` compute API will now return a ``409"
" HTTPConflict`` response when trying to delete a ``nova-compute`` service "
"which is still hosting instances. This is because doing so would orphan the "
"compute node resource provider in the placement service on which those "
"instances have resource allocations, which affects scheduling. See "
"https://bugs.launchpad.net/nova/+bug/1763183 for more details."
msgstr ""
"``DELETE /os-services/{service_id}`` 컴퓨터 API는 현재 `nova-compute` 서비스가 인스턴스를 "
"호스팅하는 경우, `DELETE`요청을 시도하면 `409 HTTPConflict`응답을 반환합니다. 이는 인스턴스에 리소스 할당이 있는 "
"`placement` 서비스의 `compute node` 리소스 프로바이더를 비우게 되면, 스케줄링에 영향을 미치게 됩니다. 더 많은 "
"정보는 https://bugs.launchpad.net/nova/+bug/1763183에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:464
msgid ""
"The ``GET /os-migrations`` API will also have optional ``user_id`` and "
"``project_id`` query parameters for filtering migrations by user and/or "
"project, for example:"
msgstr ""
"GET /os-migrations API는 사용자와/or 프로젝트에 따라 이ми그레이션을 필터링하기 위해 옵셔널 user_id 및 "
"project_id query parameter를 가지고 있습니다. 예를 들어:"

#: ../../<reno.sphinxext stable/2024.1>:476
msgid ""
"The ``HyperV`` virt driver has been removed. It was deprecated in the Nova "
"27.2.0 (Antelope) release. This driver was untested and has no maintainers. "
"In addition, it has a dependency on the OpenStack Winstacker project that "
"also has been retired."
msgstr ""
"``HyperV`` virt 드라이버가 제거되었다. Nova 27.2.0 (Antelope) 릴리스에서弃기되었다. 이 드라이버는 "
"테스트되지 않았으며 유지자가 없으며, 오픈 스텝 Winstacker 프로젝트와도 연관성이 있다. 또한, 이 드라이버는 오픈 스텝 "
"Winstacker 프로젝트가 폐지되었다."

#: ../../<reno.sphinxext unmaintained/wallaby>:743
msgid ""
"The ``Ironic Flavor Migration`` upgrade check has been removed. It is no "
"longer necessary."
msgstr "``Ironic Flavor Migration`` 업그레이드 확인은 제거되었다. 더 이상 필요하지 않다."

#: ../../<reno.sphinxext unmaintained/wallaby>:539
msgid ""
"The ``POST /servers/{server_id}/os-interface`` API now supports attaching "
"neutron ports with QoS minimum bandwidth rules attached."
msgstr ""
"``POST /servers/{server_id}/os-interface`` API는 현재 네트론 포트를 연결하여 QoS 최소帯width"
" 규칙을附加하는 것을 지원합니다."

#: ../../<reno.sphinxext stable/stein>:1454
msgid ""
"The ``POST /servers/{server_id}/os-interface`` request and the ``POST "
"/servers`` request will be rejected with HTTP 400 if the Neutron network "
"referenced in the request body has `QoS minimum bandwidth rule`_ attached as"
" Nova currently cannot support such operations."
msgstr ""
"``POST /servers/{server_id}/os-interface`` 요청과 ``POST /servers`` 요청은 HTTP "
"400로 거부되며, 요청 본문에 참조된 Neutron 네트워크에 `QoS minimum bandwidth rule`_이 부착된 경우 "
"Nova는 현재 이러한 연산을 지원하지 않기 때문이다."

#: ../../<reno.sphinxext stable/stein>:1444
msgid ""
"The ``POST /servers/{server_id}/os-interface`` request will be rejected with"
" HTTP 400 if the Neutron port referenced in the request body has resource "
"request as Nova currently cannot support such operation. For example a "
"Neutron port has resource request if a `QoS minimum bandwidth rule`_ is "
"attached to that port in Neutron."
msgstr ""
"``POST /servers/{server_id}/os-interface`` 요청은 요청 본문에 참조된 Neutron 포트가 리소스 "
"요청을 받는 경우 HTTP 400로 거부됩니다. 현재 Nova는 이러한 연산을 지원하지 않기 때문입니다. 예를 들어, Neutron "
"포트가 리소스 요청을 받는 경우 `QoS minimum bandwidth rule`_을 Neutron에 연결한 경우가 있습니다."

#: ../../<reno.sphinxext unmaintained/xena>:248 unmaintained/yoga>:579
msgid ""
"The ``POST /servers`` (create server) API will now reject attempts to create"
" a server with the same port specified multiple times. This was previously "
"accepted by the API but the instance would fail to spawn and would instead "
"transition to the error state."
msgstr ""
"``POST /servers`` (서버 생성) API는 동일한 포트를 여러 번 지정하여 서버를 생성하려고 시도하는 시도를 "
"now拒绝합니다. 이 API는 이전에 동일한 포트를 지정하여 서버를 생성하려고 시도하는 시도는 이전에 수용되었습니다. 그러나 인스턴스는 "
"성공하지 못하고 오류 상태로 전환했습니다."

#: ../../<reno.sphinxext stable/pike>:1771
msgid ""
"The ``POST`` and ``DELETE`` operations on the ``os-assisted-volume-"
"snapshots`` API will now fail with a 400 error if the related instance is "
"undergoing a task state transition or does not have a host, i.e. is shelved "
"offloaded."
msgstr ""
"``POST`` 및 ``DELETE`` 연산은 ``os-assisted-volume-snapshots`` API에 대해 현재 ``task"
" state transition``이 발생하거나 호스트가 없는 경우 (즉, ``shelved offloaded`` )인 경우 400 "
"오류로 실패합니다."

#: ../../<reno.sphinxext stable/pike>:1871
msgid ""
"The ``PUT /os-services/disable``, ``PUT /os-services/enable`` and ``PUT /os-"
"services/force-down`` APIs to enable, disable, or force-down a service will "
"now only work with *nova-compute* services. If you are using those APIs to "
"try and disable a non-compute service, like nova-scheduler or nova-"
"conductor, those APIs will result in a 404 response."
msgstr ""
"``PUT /os-services/disable``, ``PUT /os-services/enable`` 및 ``PUT /os-"
"services/force-down`` API를 사용하여 서비스를 활성화, 비활성화 또는 강제로 down 시킬 수 있는 것은 이제 "
"*nova-compute* 서비스만 작동합니다. nova-scheduler 또는 nova-conductor와 같은 비 컴퓨터 서비스를 "
"비활성화 시도하는 경우, API는 404 응답을 반환합니다."

#: ../../<reno.sphinxext stable/train>:1224
msgid ""
"The ``RetryFilter`` is deprecated and will be removed in an upcoming "
"release. Since the 17.0.0 (Queens) release, the scheduler has provided "
"alternate hosts for rescheduling so the scheduler does not need to be called"
" during a reschedule which makes the ``RetryFilter`` useless. See the "
"`Return Alternate Hosts`_ spec for details."
msgstr ""
"``RetryFilter``은 향후 출시된 버전에서弃용되며 제거될 예정입니다. 17.0.0 (Queens) 버전부터 스케줄러는 "
"재排cheduling에 대체 호스트를 제공하기 때문에 재排cheduling 시 스케줄러를 호출할 필요가 없게되었습니다. 따라서 "
"``RetryFilter``는 사용할 수 없습니다. 더 자세한 정보는 `Return Alternate Hosts`_ spec을 "
"참조하십시오."

#: ../../<reno.sphinxext stable/pike>:1641
msgid ""
"The ``TrustedFilter`` scheduler filter has been experimental since its "
"existence on May 18, 2012. Due to the lack of tests and activity with it, "
"it's now deprecated and set for removal in the 17.0.0 Queens release."
msgstr ""
"``TrustedFilter`` 스케줄러 필터는 2012년 5월 18일부터 존재가 시작된 이후 실험적이었습니다. 테스트와 활동이 부족하여"
" 현재 deprecated되어 17.0.0 퀸즈 릴리스에서 제거되도록 설정되었습니다."

#: ../../<reno.sphinxext stable/queens>:1279
msgid ""
"The ``TypeAffinityFilter``, which was deprecated in the 16.0.0 Pike release,"
" has been removed. The filter was flawed in that it relied on the flavor "
"``id`` primary key which cannot be relied upon since you cannot \"edit\" a "
"flavor to change its disk, vcpu, etc values. Therefore to change a given "
"flavor, it must be deleted and re-created, which means a new ``id`` value, "
"thus potentially breaking the usefulness of the filter. Also, the flavor "
"migration from the ``nova`` database to the ``nova_api`` database would also"
" have resulted in different ``id`` values."
msgstr ""
"``TypeAffinityFilter``은 16.0.0 Pike 릴리스에서弃기되었으며, 이 필터는 ``id`` flavor primary"
" key에 의존하여 비trustworthy했습니다. 이 flavor의 디스크, vcpu, etc. 값은 변경할 수 없기 때문에 "
"``id`` primary key에 의존하여 필터가 비trustworthy했습니다. 따라서 특정 flavor를 변경하려면 필터가 "
"usefulness를 깨트릴 수 있는 ``id`` 값이 변경되는 것을 피하기 위해 필터를 삭제하고 재생성해야 합니다. 또한 "
"``nova`` 데이터베이스에서 ``nova_api`` 데이터베이스로 flavor migration이 발생했을 때도 ``id`` 값이 "
"다르게 변경되었습니다."

#: ../../<reno.sphinxext stable/pike>:969
msgid ""
"The ``XenAPI`` compute driver now supports creating servers with virtual "
"interface and block device tags which was introduced in the ``2.32`` "
"microversion."
msgstr ""
"``XenAPI`` 컴퓨터 드라이버는 2.32 마이크로 버전에서 도입된 가상 인터페이스 및 블록 디스크 태그를 사용하여 서버를 생성하는 "
"것을 지원합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:661
msgid ""
"The ``XenAPI`` driver, which was deprecated in the 20.0.0 (Train), has now "
"been removed."
msgstr "``XenAPI`` 드라이버, 20.0.0 (트레인) 버전에서弃기되었지만 현재는 제거되었습니다."

#: ../../<reno.sphinxext stable/queens>:1443
msgid ""
"The ``[DEFAULT] vendordata_driver`` option was deprecated in Mitaka and has "
"now been removed. Configuration of vendordata drivers should now be done by "
"using the ``[api] vendordata_providers`` option. For more information, refer"
" to the `vendordata documentation`__."
msgstr ""
"``[DEFAULT] vendordata_driver`` 옵션은 미타카에서弃용되었으며 현재 제거되었습니다. `vendordata` "
"드라이버의 구성은 `vendordata_providers` 옵션을 사용하여 now를 사용하여 구성해야 합니다. 더 많은 정보를 얻으려면 "
"`vendordata documentation`를 참조하십시오."

#: ../../<reno.sphinxext stable/train>:909
msgid ""
"The ``[DEFAULT]/block_device_allocate_retries`` configuration option now has"
" a minimum required value of 0. Any previous configuration with a value less"
" than zero will now result in an error."
msgstr ""
"``[DEFAULT]/block_device_allocate_retries`` 설정 옵션은 현재 0으로 최소한의 필요 giá치가 설정되어"
" 있습니다. 이전에 0 이하의 giá치가 설정된 모든 설정은 현재 오류를 일으킬 것입니다."

#: ../../<reno.sphinxext stable/train>:1141
msgid ""
"The ``[DEFAULT]/default_flavor`` option deprecated in 14.0.0 (Newton) has "
"been removed."
msgstr "``[DEFAULT]/default_flavor`` 옵션은 Newton(14.0.0) 버전에서弃용되었으며 제거되었습니다."

#: ../../<reno.sphinxext stable/pike>:1886
msgid ""
"The ``[DEFAULT]/enable_new_services`` configuration option will now only be "
"used to auto-disable new nova-compute services. Other services like nova-"
"conductor, nova-scheduler and nova-osapi_compute will not be auto-disabled "
"since disabling them does nothing functionally, and starting in Pike the "
"``PUT /os-services/enable`` REST API will not be able to find non-compute "
"services to enable them."
msgstr ""
"``[DEFAULT]/enable_new_services`` 구성 옵션은 이제 nova-compute 서비스에만 자동으로 비활성화되도록 "
"사용되며, nova-conductor, nova-scheduler 및 nova-osapi_compute와 같은 다른 서비스는 비활성화가 "
"기능적으로 아무런 영향을 미치지 않기 때문에 비활성화되지 않습니다. Pike에서 시작하여, ``PUT /os-"
"services/enable`` REST API는 nova-compute 서비스를 비활성화할 수 있는 비 컴퓨터 서비스를 찾을 수 없기 "
"때문에 비활성화할 수 없습니다."

#: ../../<reno.sphinxext stable/queens>:1734
msgid ""
"The ``[DEFAULT]/log_options`` configuration option can be used to log "
"configuration options at DEBUG level when the `placement-api` and/or `nova-"
"api` services are started under WSGI. The default behavior is to log options"
" on startup."
msgstr ""
"``[DEFAULT]/log_options`` 구성 옵션은 WSGI에서 `placement-api` 및/or `nova-api` 서비스가"
" 시작될 때 DEBUG 수준의 구성 옵션을 로그할 수 있습니다. 기본 behavior는 시작 시 로그 옵션을 로그합니다."

#: ../../<reno.sphinxext stable/rocky>:1712
msgid ""
"The ``[DEFAULT]/scheduler_driver_task_period`` configuration option, which "
"was deprecated in the 15.0.0 Ocata release, has now been removed. Use the "
"``[scheduler]/periodic_task_interval`` option instead."
msgstr ""
"``[DEFAULT]/scheduler_driver_task_period`` 설정 옵션, 15.0.0 오카타 릴리스에서弃기되었으며 현재 "
"제거되었습니다. 대신 ``[scheduler]/periodic_task_interval`` 옵션을 사용하십시오."

#: ../../<reno.sphinxext stable/stein>:1328
msgid ""
"The ``[DEFAULT]/shutdown_timeout`` configuration option minimum value has "
"been fixed to be 0 rather than 1 to align with the corresponding "
"``os_shutdown_timeout`` image property. See bug "
"https://launchpad.net/bugs/1799707 for details."
msgstr ""
"``[DEFAULT]/shutdown_timeout`` 구성 옵션의 최소 giá치가 0으로 고정되어 1보다 1을 맞추기 위해 "
"``os_shutdown_timeout`` 이미지 속성과 일치합니다. 더 많은 정보는 "
"https://launchpad.net/bugs/1799707 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/2025.2>:342
msgid ""
"The ``[api] neutron_default_tenant_id`` option has been renamed to ``[api] "
"neutron_default_project_id`` and the legacy ``[DEFAULT] "
"neutron_default_tenant_id`` alias removed."
msgstr ""
"``[API] neutron_default_tenant_id`` 옵션은 ``[API] neutron_default_project_id``"
" 로 이름이 변경되었으며, 전통적인 ``[DEFAULT] neutron_default_tenant_id`` 가lias가 제거되었다."

#: ../../<reno.sphinxext unmaintained/zed>:491
msgid ""
"The ``[api] use_forwarded_for`` parameter has been deprecated. Instead of "
"using this parameter, add the ``HTTPProxyToWSGI`` middleware to api "
"pipelines, and ``[oslo_middleware] enable_proxy_headers_parsing = True`` to "
"nova.conf."
msgstr ""
"``[api] use_forwarded_for`` 파라미터는弃용되었습니다. 대신 이 파라미터를 사용하는 대신, API 파이프라인에 "
"HTTPProxyToWSGI 미들웨어를 추가하고, nova.conf에 enable_proxy_headers_parsing = True를 "
"설정합니다."

#: ../../<reno.sphinxext stable/queens>:1452
msgid ""
"The ``[api] vendordata_providers`` option now defaults to ``[StaticJSON]``. "
"This ensures existing behavior of the vendordata v1 driver is preserved."
msgstr ""
"``[api] vendordata_providers`` 옵션은 현재 ``[StaticJSON]``으로 기본적으로 설정되었습니다. 이것은 "
"existing behavior of the vendordata v1 driver가 유지되도록 보장합니다."

#: ../../<reno.sphinxext stable/pike>:1487
msgid ""
"The ``[api]/allow_instance_snapshots`` configuration option is now "
"deprecated for removal. To disable snapshots in the ``createImage`` server "
"action API, change the ``os_compute_api:servers:create_image`` and "
"``os_compute_api:servers:create_image:allow_volume_backed`` policies."
msgstr ""
"``[api]/allow_instance_snapshots`` 구성 옵션은 now deprecated for removal으로 "
"변경되었습니다. snapshots을 disable करन을 위해 ``createImage`` server action API를 변경하려면"
" ``os_compute_api:servers:create_image`` 및 "
"``os_compute_api:servers:create_image:allow_volume_backed`` 정책을 변경해야 합니다."

#: ../../<reno.sphinxext stable/stein>:1215
msgid ""
"The ``[api]/hide_server_address_states`` configuration option and "
"``os_compute_api:os-hide-server-addresses`` policy rule were deprecated in "
"the 17.0.0 Queens release. They have now been removed. If you never changed "
"these values, the API behavior remains unchanged."
msgstr ""
"``[api]/hide_server_address_states`` 구성 옵션과 ``os_compute_api:os-hide-server-"
"addresses`` 정책 규칙은 17.0.0 퀸즈 릴리스에서弃기되었으며 현재 제거되었습니다. 이 값이 ever 바뀐 경우 API 행동은"
" unchanged remain합니다."

#: ../../<reno.sphinxext stable/ussuri>:1056
msgid ""
"The ``[api]auth_strategy`` conf option and the corresponding test-only "
"``noauth2`` pipeline in ``api-paste.ini`` are deprecated and will be removed"
" in a future release. The only supported ``auth_strategy`` is ``keystone``, "
"the default."
msgstr ""
"``[api]auth_strategy`` conf 옵션과 corresponding test-only "
"``noauth2``.pipeline은 ``api-paste.ini``에 있는 ``[api]auth_strategy`` conf 옵션과 "
"corresponding test-only ``noauth2`` pipeline은 deprecated되어 향후 릴리스에서 제거될 "
"예정입니다.  ``auth_strategy`` 옵션은 ``keystone``만 지원하며, 기본값입니다."

#: ../../<reno.sphinxext stable/stein>:1417
msgid ""
"The ``[cinder]/catalog_info`` default value is changed such that the "
"``service_name`` portion of the value is no longer set and is also no longer"
" required. Since looking up the cinder endpoint in the service catalog "
"should only need the endpoint type (``volumev3`` by default) and interface "
"(``publicURL`` by default), the service name is dropped and only provided "
"during endpoint lookup if configured. See `bug 1803627 "
"<https://bugs.launchpad.net/nova/+bug/1803627>`_ for details."
msgstr ""
"``[cinder]/catalog_info`` 기본값이 변경되어 ``service_name`` 부분이 더 이상 설정되지 않으며 더 이상 "
"필요하지 않습니다. 이로 인해 cinder 엔드포인트를 서비스 카탈로그에서查색할 때는 엔드포인트 타입 (기본적으로 ``volumev3``"
" )과 인터페이스 (기본적으로 ``publicURL`` )만 필요하므로 서비스 이름이 drop되며,만약 구성된 경우에만 제공됩니다.  더"
" 자세한 정보는 `bug 1803627 <https://bugs.launchpad.net/nova/+bug/1803627>`_에서 확인할"
" 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1718
msgid ""
"The ``[conductor] topic`` configuration option was previously deprecated and"
" is now removed from nova.  There was no need to let users choose the RPC "
"topics for all services. There was little benefit from this and it made it "
"really easy to break nova by changing the value of topic options."
msgstr ""
"``[conductor] topic`` 설정 옵션은 이전에 비활성화되었으며 현재 nova에서 제거되었다. 사용자들이 모든 서비스의 RPC"
" 토픽을 선택할 필요가 없다는 것이 사실이었다. 이 설정이 사용되면 이에 대한 이익이 거의 있었고, 이 설정을 변경하면 nova를 "
"REALLY 쉽게 깨트릴 수 있었다."

#: ../../<reno.sphinxext stable/stein>:1207
msgid ""
"The ``[filter_scheduler]/soft_affinity_weight_multiplier`` and "
"``[filter_scheduler]/soft_anti_affinity_weight_multiplier`` configuration "
"options now have a hard minimum value of 0.0. Also, the deprecated alias to "
"the ``[DEFAULT]`` group has been removed so the options must appear in the "
"``[filter_scheduler]`` group."
msgstr ""
"``[filter_scheduler]/soft_affinity_weight_multiplier`` 및 "
"``[filter_scheduler]/soft_anti_affinity_weight_multiplier`` 구성 옵션은 현재 0.0의 硬"
" 최소 giá치가 있습니다. 또한, deprecated alias를 ``[DEFAULT]`` 그룹으로 사용하는_alias가 "
"제거되었습니다. 따라서 옵션은 ``[filter_scheduler]`` 그룹에 나타나야 합니다."

#: ../../<reno.sphinxext stable/rocky>:1618
msgid ""
"The ``[filter_scheduler]/use_baremetal_filters`` and "
"``[filter_scheduler]/baremetal_enabled_filters`` configuration options were "
"deprecated in the 16.0.0 Pike release since deployments serving baremetal "
"instances should be `scheduling based on resource classes`_. Those options "
"have now been removed."
msgstr ""
"``[필터 스케줄러]/use_baremetal_filters`` 및 ``[필터 "
"스케줄러]/baremetal_enabled_filters`` 구성 옵션은 16.0.0 피크 릴리즈에서 배포가 바레메탈 인스턴스를 사용할 "
"때는 리소스 클래스에 따라 스케줄링을 기반으로해야 하는데서부터 비어메탈 필터를 사용하도록 구성 옵션을 비활성화했다.  이 옵션은 이제 "
"제거되었다."

#: ../../<reno.sphinxext unmaintained/wallaby>:732
msgid ""
"The ``[glance]/allowed_direct_url_schemes`` config option, which was first "
"deprecated in the 17.0.0 (Queens) release has now been removed."
msgstr ""
"``[glance]/allowed_direct_url_schemes`` 설정 옵션, 17.0.0 (_queens) 릴리스에서 처음 "
"비활성화되었으며 현재 제거되었습니다."

#: ../../<reno.sphinxext stable/queens>:1545
msgid ""
"The ``[glance]/allowed_direct_url_schemes`` configuration option and "
"``nova.image.download.modules`` extension point have been deprecated for "
"removal. These were originally added for the *nova.image.download.file* "
"FileTransfer extension which was removed in the 16.0.0 Pike release. The "
"``nova.image.download.modules`` extension point is not maintained and there "
"is no indication of its use in production clouds. If you are using this "
"extension point, please make the nova development team aware by contacting "
"us in the #openstack-nova freenode IRC channel or on the openstack-dev "
"mailing list."
msgstr ""
"``[glance]/allowed_direct_url_schemes`` 구성 옵션과 "
"``nova.image.download.modules`` 확장점은 제거를 위해 비활성화되었습니다. 이들은 원래 "
"*nova.image.download.file* FileTransfer 확장점을 제거한 16.0.0 피크 릴리스에 추가되었습니다. "
"``nova.image.download.modules`` 확장점은 유지 관리되지 않으며 프로덕션 클라우드에서 사용하는 지시가 없으며, 이"
" 확장점을 사용하고 있으시면, nova 개발 팀에 알려 주시면 #openstack-nova freenode IRC 채널 또는 "
"openstack-dev 이메일 목록을 통해 chúng과 liên hệ할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:1063
msgid ""
"The ``[glance]api_servers`` configuration option is deprecated and will be "
"removed in a future release. Deployments should use standard keystoneauth1 "
"options to configure communication with a single image service endpoint. Any"
" load balancing or high availability requirements should be satisfied "
"outside of nova."
msgstr ""
"``[glance]api_servers`` 구성 옵션은 향후 릴리스에서弃소화되며 제거될 예정입니다. 배포는 표준 keystoneauth1"
" 옵션을 사용하여 단일 이미지 서비스 엔드포인트와의 통신을 구성해야 합니다. 로드 밸런싱 또는 고가용성 요구 사항은 nova "
"outside에서 충족해야 합니다."

#: ../../<reno.sphinxext stable/2025.2>:396
msgid ""
"The ``[libvirt] num_memory_encrypted_guests`` option has been deprecated and"
" will be removed in a future release. The option will be completely replaced"
" by the number of SEV-encrypted guests presented by domain capabilities API "
"in libvirt, which is available since version 8.0.0 . The libvirt's API is "
"more feature complete and supports detecting the limit for SEV-ES-encrypted "
"guests."
msgstr ""
"``[libvirt] num_memory_encrypted_guests`` 옵션은 현재 deprecated되어 향후 릴리스에서 제거될 "
"예정입니다. 이 옵션은 도메인 능력 API에서 제공하는 SEV-암호화된 게스트의 수를 나타내는 libvirt의 옵션으로 대체될 "
"예정입니다. 이 옵션은 버전 8.0.0부터 사용할 수 있습니다. libvirt의 API는 더 많은 기능을 지원하고 SEV-ES-암호화된 "
"게스트의 제한을 감지할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:726
msgid ""
"The ``[libvirt] use_usb_tablet`` config option, which was first deprecated "
"in the 14.0.0 (Newton) release, has now been removed. It has been replaced "
"by the ``[DEFAULT] pointer_model`` config option."
msgstr ""
"``[libvirt] use_usb_tablet`` 설정 옵션, Newton 릴리스(14.0.0)에서 처음 비활성화되었으며 현재는 "
"제거되었다. 이 설정 옵션은 ``[DEFAULT] pointer_model`` 설정 옵션에 대체되었다."

#: ../../<reno.sphinxext unmaintained/wallaby>:642
msgid ""
"The ``[libvirt] xen_hvmloader_path`` config option has been removed. This "
"was only used with the libvirt+xen hypervisor, which is no longer supported."
msgstr ""
"``[libvirt] xen_hvmloader_path`` 설정 옵션은 제거되었습니다. 이 옵션은 libvirt+xen 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 가상화 "
"가상화 가상화 가상화 가상화 가상화 가상화 가"

#: ../../<reno.sphinxext stable/stein>:642
msgid ""
"The ``[libvirt]/live_migration_completion_timeout`` is restricted by minimum"
" 0 and will now raise a ValueError if the configuration option value is less"
" than minimum value."
msgstr ""
"``[libvirt]/live_migration_completion_timeout``은 최소 0으로 제한되고, 현재 구성 옵션의 giá "
"trị가 최소 giá trị보다 낮을 경우 ValueError를 발생시킵니다."

#: ../../<reno.sphinxext stable/rocky>:1894
msgid ""
"The ``[libvirt]/sparse_logical_volumes`` configuration option is now "
"deprecated. Sparse logical volumes were never verified by tests in Nova and "
"some bugs were found without having fixes so we prefer to deprecate that "
"feature. By default, the LVM image backend allocates all the disk size to a "
"logical volume. If you want to have the volume group having thin-provisioned"
" logical volumes, use Cinder with volume-backed instances."
msgstr ""
"``[libvirt]/sparse_logical_volumes`` 구성 옵션은 현재 비활성화되었습니다._sparse logical "
"volumes는 Nova에서 테스트를 통해 never verified되었으며, 일부 버그가fix가 없기 때문에 이 기능을 비활성화하는 "
"것을 선호합니다. 기본적으로 LVM image backend는 모든 디스크 크기를 논리 볼륨에 할당합니다. thin-provisioned"
" logical volumes를 가진 volume group를 원하는 경우, Cinder와 volume-backed 인스턴스를 사용하는 "
"것을 사용하십시오."

#: ../../<reno.sphinxext unmaintained/wallaby>:773
msgid ""
"The ``[libvirt]live_migration_tunnelled`` option is deprecated as of Wallaby"
" (23.0.0) release."
msgstr "``[libvirt]live_migration_tunnelled`` 옵션은 월라비 (23.0.0) 릴리스부터弃기되었습니다."

#: ../../<reno.sphinxext stable/train>:1170
msgid ""
"The ``[neutron]/url`` configuration option, which was deprecated in the "
"17.0.0 Queens release, has now been removed. The same functionality is "
"available via the ``[neutron]/endpoint_override`` option."
msgstr ""
"``[neutron]/url`` 구성 옵션, 17.0.0 퀸즈 릴리스에서弃기되었으며 현재 제거되었습니다. 동일한 기능은 "
"``[neutron]/endpoint_override`` 옵션을 통해 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1258
msgid ""
"The ``[notifications]/default_publisher_id`` configuration option now "
"defaults to ``[DEFAULT]/host`` rather than ``[DEFAULT]/my_ip``."
msgstr ""
"``[notifications]/default_publisher_id`` 설정 옵션은 현재 ``[DEFAULT]/host`` 보다는 "
"``[DEFAULT]/my_ip``와 같은 ``[DEFAULT]/my_ip``를 사용하지 않습니다."

#: ../../<reno.sphinxext stable/ussuri>:825
msgid ""
"The ``[osapi_v21]/project_id_regex`` configuration option which has been "
"deprecated since the Mitaka 13.0.0 release has now been removed."
msgstr ""
"``[osapi_v21]/project_id_regex`` 설정 옵션은 미타카 13.0.0 릴리스 이후에弃기되었으며 현재 제거되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1755
msgid ""
"The ``[quota]/driver`` configuration option is no longer deprecated but now "
"only allows one of two possible values:"
msgstr "``[quota]/driver`` 설정 옵션은 더 이상 비활성화되었지만 현재 두 가지 가능성 중 하나만 허용합니다."

#: ../../<reno.sphinxext stable/ussuri>:1071
msgid ""
"The ``[scheduler] driver`` config option has been deprecated. This was "
"previously used to switch between different scheduler drivers including "
"custom, out-of-tree ones. However, only the ``FilterScheduler`` has been "
"supported in-tree since 19.0.0 (Stein) and nova increasingly relies on "
"placement for basic functionality, meaning developing and maintaining out-"
"of-tree drivers is increasingly difficult. Users who still rely on a custom "
"scheduler driver should migrate to the filter scheduler, using custom "
"filters and weighters where necessary."
msgstr ""
"``[scheduler] 드라이버`` 설정 옵션은弃용되었습니다. 이 옵션은 이전에 custom, out-of-tree 드라이버를 포함한 "
"다양한 scheduler 드라이버를chsel할 수있었습니다. 그러나 19.0.0 (Stein) 이후에 only "
"FilterScheduler가 in-tree에 지원되었으며, nova는 기본 기능을위한 배치에 increasingly 의존하고 있으므로,"
" out-of-tree 드라이버를 개발하고 유지 관리하는 것은 increasingly increasingly 어렵습니다. 사용자들은 "
"여전히 custom scheduler 드라이버를 사용하고 있다면, filter scheduler로 mig리ят해야하며, custom "
"filters와 weighters가 필요할 때 custom filters와 weighters를 사용해야합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:721
msgid ""
"The ``[scheduler] periodic_task_interval`` config option has been removed. "
"It was no longer used by any supported scheduler drivers."
msgstr ""
"``[scheduler] periodical_task_interval`` 설정 옵션은 더 이상 사용되지 않는다. 지원되는 모든 스케줄러 "
"드라이버에 의해 사용되지 않는다."

#: ../../<reno.sphinxext unmaintained/wallaby>:716
msgid ""
"The ``[scheduler] scheduler_driver`` config option has been removed, along "
"with the ``nova.scheduler.driver`` setuptools entrypoint."
msgstr ""
"``[scheduler] scheduler_driver`` 설정 옵션은 `nova.scheduler.driver` setuptools "
"엔트리 포인트와 함께 제거되었습니다."

#: ../../<reno.sphinxext stable/train>:986
msgid ""
"The ``[upgrade_levels]/compute`` RPC API pin is removed (or set to \"auto\")"
" and services are restarted."
msgstr "``[업그레이드 수준]/compute`` RPC API pin이 제거되거나 (\"auto\")로 설정되며 서비스가 재시작됩니다."

#: ../../<reno.sphinxext stable/pike>:1376
msgid ""
"The ``[vmware] wsdl_location`` configuration option has been removed after "
"being deprecated in 15.0.0. It was unused and should have no impact."
msgstr ""
"``[vmware] wsdl_location`` 설정 옵션은 15.0.0에서 비활성화된 후 제거되었다. 비활성화된 후에는 사용되지 "
"않았으며 영향을 미치지 않는다."

#: ../../<reno.sphinxext unmaintained/victoria>:621
msgid ""
"The ``[vnc] keymap`` and ``[spice] keymap`` configuration options, first "
"deprecated in 18.0.0 (Rocky), have now been removed.  The VNC option "
"affected the libvirt and VMWare virt drivers, while the SPICE option only "
"affected libvirt. For the libvirt driver, configuring these options resulted"
" in lossy keymap conversions for the given graphics method.  Users can "
"replace this host-level configuration with guest-level configuration. This "
"requires noVNC 1.0.0 or greater, which provides support for QEMU's Extended "
"Key Event messages. Refer to `bug #1682020`__ and the `QEMU RFB pull "
"request`__ for more information."
msgstr ""
"``[vnc] keymap`` 및 ``[spice] keymap`` 설정 옵션, 18.0.0 (Rocky)에서 처음 비활성화되었으며 현재"
" 제거되었습니다. VNC 옵션은 libvirt 및 VMWare virt 드라이버에 영향을 미쳤으며, SPICE 옵션은 libvirt에만 "
"영향을 미쳤습니다. libvirt 드라이버에서 이러한 옵션을 구성하면, 사용자에게는 graphics method에 따라 lossy "
"keymap conversion이 발생합니다. 이 host-level 설정은 guest-level 설정으로 대체할 수 있습니다. 이에 "
"대해 noVNC 1.0.0 이상이 필요하며, QEMU의 Extended Key Event messages에 대한 지원을 제공합니다. 더 "
"많은 정보는 `bug #1682020`__ 및 `QEMU RFB pull request`__에 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:1861
msgid ""
"The ``[vnc] keymap`` and ``[spice] keymap`` options will be removed in a "
"future release."
msgstr "``[vnc] keymap`` 및 ``[spice] keymap`` 옵션은 향후 릴리스에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/stein>:1246
msgid ""
"The ``[workarounds] disable_libvirt_livesnapshot`` config option has been "
"deprecated. This was necessary to work around an issue with libvirt v1.2.2, "
"which we no longer support. For more information refer to `bug #1334398`__."
msgstr ""
"``[workarounds] disable_libvirt_livesnapshot`` 설정 옵션은 더 이상 사용되지 않습니다. 이 설정 "
"옵션은 libvirt v1.2.2과 관련된 문제를 해결하기 위해 필요했습니다. 그러나 이 문제는 더 이상 지원되지 않습니다. 더 많은 "
"정보를 얻으려면 `bug #1334398`를 참조하십시오."

#: ../../<reno.sphinxext stable/ussuri>:1394
msgid ""
"The ``[workarounds]/disable_native_luksv1`` configuration option has been "
"introduced. This can be used by operators to workaround recently discovered "
"performance issues found within the `libgcrypt library`__ used by QEMU when "
"natively decrypting LUKSv1 encrypted disks. Enabling this option will result"
" in the use of the legacy ``dm-crypt`` based os-brick provided encryptors."
msgstr ""
"``[workarounds]/disable_native_luksv1`` 설정 옵션은 도입되었다. 이 설정 옵션은 QEMU가 "
"natively LUKSv1 암호화된 디스크를 암호화하는 `libgcrypt library`__을 사용하여 최근에 발견된 성능 문제를 "
"해결하기 위해 운영자에 의해 사용할 수 있다. 이 설정을 활성화하면 legacy ``dm-crypt`` 기반의 os-brick 제공 "
"암호화자를 사용하게 된다."

#: ../../<reno.sphinxext stable/queens>:198 stable/rocky>:343
#: stable/stein>:1393
msgid ""
"The ``[workarounds]/ensure_libvirt_rbd_instance_dir_cleanup`` configuration "
"option has been introduced. This can be used by operators to ensure that "
"instance directories are always removed during cleanup within the Libvirt "
"driver while using ``[libvirt]/images_type = rbd``. This works around known "
"issues such as `bug 1414895`_ when cleaning up after an evacuation and `bug "
"1761062`_ when reverting from an instance resize."
msgstr ""
"``[workarounds]/ensure_libvirt_rbd_instance_dir_cleanup`` 설정 옵션은 도입되었다. 이 설정"
" 옵션은 Libvirt 드라이버를 사용하여 `libvirt/images_type = rbd`를 사용할 때, 인스턴스 디렉터리를 항상 청소"
" 중에 제거하는 것을 보장하는 데 사용할 수 있다. 이 설정은 evacuation 후 청소와 인스턴스 리サイズ 후 복원하는 동안 알려진 "
"문제인 `bug 1414895`_와 `bug 1761062`_를 해결한다."

#: ../../<reno.sphinxext stable/ussuri>:1420
msgid ""
"The ``[workarounds]/rbd_volume_local_attach`` configuration option has been "
"introduced. This can be used by operators to ensure RBD volumes are "
"connected to compute hosts as block devices. This can be used with the "
"``[worarounds]/disable_native_luksv1`` configuration option to workaround "
"recently discovered performance issues found within the `libgcrypt "
"library`__ used by QEMU when natively decrypting LUKSv1 encrypted disks."
msgstr ""
"``[workarounds]/rbd_volume_local_attach`` 설정 옵션은 도입되었다. 이 설정 옵션은 연산자들이 RBD "
"볼륨을 컴퓨터 호스트에 블록 디스크로 연결하는 것을 보장할 수 있다. 이 설정 옵션은 `libgcrypt library`__에 의해 "
"사용되는 QEMU가 natively LUKSv1 암호화된 디스크를 암호화하는 동안 발견된 최근의 성능 문제를 해결하기 위해 "
"`disable_native_luksv1` 설정 옵션과 함께 사용할 수 있다."

#: ../../<reno.sphinxext unmaintained/xena>:512
msgid ""
"The ``[workarounds]disable_native_luksv1`` workaround configurable has been "
"removed after previously being deprecated during the Wallaby (23.0.0) "
"release."
msgstr ""
"``[workarounds]disable_native_luksv1`` 설정이 Wallaby (23.0.0) 릴리스 이후에 비공식적으로 "
"비활성화되었습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:789
msgid ""
"The ``[workarounds]rbd_volume_local_attach`` and "
"``[workarounds]disable_native_luksv1`` options have been deprecated as of "
"the 23.0.0 release ahead of removal in the future as the underlying "
"``libgcrypt`` performance regressions that prompted their introduction have "
"been resolved."
msgstr ""
"``[workarounds]rbd_volume_local_attach`` 및 "
"``[workarounds]disable_native_luksv1`` 옵션은 23.0.0 버전부터 제거를 앞두고 제거되기 전에 하위 "
"``libgcrypt`` 성능 저하가 인도된 옵션으로 인해 제거되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:518
msgid ""
"The ``[workarounds]rbd_volume_local_attach`` workaround configurable has "
"been removed after previously being deprecated in the Wallaby (23.0.0) "
"release."
msgstr ""
"``[workarounds]rbd_volume_local_attach`` 설정이 Wallaby (23.0.0) 릴리스에서 이전에 "
"비활성화된 후에 제거되었습니다."

#: ../../<reno.sphinxext stable/2025.1>:472
msgid ""
"The ``[wsgi] secure_proxy_ssl_header`` parameter has been deprecated. Use "
"the ``http_proxy_to_wsgi`` middleware from ``oslo.middleware`` instead."
msgstr ""
"``[wsgi] secure_proxy_ssl_header`` 매개 변수는弃용되었습니다. 대신 "
"``oslo.middleware.http_proxy_to_wsgi`` 미드웨어를 사용하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:1509 stable/pike>:1633
msgid ""
"The ``[xenserver]/vif_driver`` configuration option is deprecated for "
"removal. The ``XenAPIOpenVswitchDriver`` vif driver is used for Neutron and "
"the ``XenAPIBridgeDriver`` vif driver is used for nova-network, which itself"
" is deprecated. In the future, the ``use_neutron`` configuration option will"
" be used to determine which vif driver to load."
msgstr ""
"``[xenserver]/vif_driver`` 설정 옵션은 제거를 위해弃용되었습니다. XenAPIOpenVswitchDriver vif"
" 드라이버는 Neutron을 사용하고 XenAPIBridgeDriver vif 드라이버는 nova-network를 사용합니다. 이는 "
"itself가弃용되었습니다. 미래에, use_neutron 설정 옵션을 통해 vif 드라이버를 로드할 수 있는지 결정할 것입니다."

#: ../../<reno.sphinxext stable/rocky>:1725
msgid ""
"The ``[xenserver]/vif_driver`` configuration option was deprecated in the "
"15.0.0 Ocata release and has now been removed. The only supported vif driver"
" is now ``XenAPIOpenVswitchDriver`` used with Neutron as the backend "
"networking service configured to run the ``neutron-openvswitch-agent`` "
"service. See the `XenServer configuration guide`_ for more details on "
"networking setup."
msgstr ""
"``[xenserver]/vif_driver`` 설정 옵션은 15.0.0 Ocata 릴리스에서弃용되었으며 현재 제거되었습니다. 현재 "
"지원되는 vif 드라이버는 ``XenAPIOpenVswitchDriver`` 이며 Neutron을 백엔드 네트워킹 서비스로 사용하여 "
"``neutron-openvswitch-agent`` 서비스를 실행하는 것을 지원합니다. 더 많은 네트워킹 설정에 대한 정보는 "
"`XenServer 설정 가이드`_를 참조하십시오."

#: ../../<reno.sphinxext unmaintained/yoga>:535
msgid ""
"The ``bandwidth`` field has been removed from the ``instance.exists`` and "
"``instance.update`` versioned notifications and the version for both "
"notifications has been bumped to 2.0. The ``bandwidth`` field was only "
"relevant when the XenAPI virt driver was in use, but this driver was removed"
" in the Victoria (22.0.0) release and the field has been a no-op since."
msgstr ""
"``bandwidth`` 필드는 ``instance.exists`` 및 ``instance.update`` 버전화된 알림에서 "
"제거되었으며, 두 가지 알림의 버전은 2.0으로 업그레이드되었습니다. ``bandwidth`` 필드는 XenAPI virt 드라이버가 "
"사용되는 경우만 의미가 있었지만, 이 드라이버는 Victoria (22.0.0) 릴리스에서 제거되었으며, 필드는 이미 비용이 없는 "
"경우가되었습니다."

#: ../../<reno.sphinxext stable/stein>:1091
msgid ""
"The ``caching_scheduler`` scheduler driver, which was deprecated in the "
"16.0.0 Pike release, has now been removed. Unlike the default "
"``filter_scheduler`` scheduler driver which creates resource allocations in "
"the placement service during scheduling, the ``caching_scheduler`` driver "
"did not interface with the placement service. As more and more functionality"
" within nova relies on managing (sometimes complex) resource allocations in "
"the placement service, compatibility with the ``caching_scheduler`` driver "
"is difficult to maintain, and seldom tested. The original reasons behind the"
" need for the CachingScheduler should now be resolved with the "
"FilterScheduler and the placement service, notably:"
msgstr ""
"``caching_scheduler`` 스케줄러 드라이버, 16.0.0 피크 릴리즈에서弃기된 후에 제거되었다. 기본 "
"``filter_scheduler`` 스케줄러 드라이버와는 달리, ``caching_scheduler`` 드라이버는 배치 서비스와 "
"인터페이스를 맺지 않았다. 배치 서비스에서 자원 할당을 관리하는 nova의 기능은 점점 더 복잡해지고, "
"``caching_scheduler`` 드라이버와의 호환성은 유지하기가 어렵고 seldom 테스트된다. "
"``caching_scheduler``의 원래 이유는 now ``filter_scheduler``와 배치 서비스를 통해 해결된다."

#: ../../<reno.sphinxext stable/stein>:1197
msgid ""
"The ``chance_scheduler`` scheduler driver was deprecated in Pike and has now"
" been removed. You should enable the ``filter_scheduler`` driver instead. If"
" ``chance_scheduler`` behavior is desired (i.e. speed is valued over "
"correctness) then configuring the ``filter_scheduler`` with only the "
"``AllHostsFilter`` enabled and adjusting "
"``[filter_scheduler]/host_subset_size`` will provide similar performance."
msgstr ""
"``chance_scheduler`` 스케줄러 드라이버는 피크에서弃용되었으며 현재 제거되었습니다. 따라서 "
"``filter_scheduler`` 드라이버를 활성화해야 합니다. ``chance_scheduler`` 동작이 원하는 경우 (즉, "
"속도가 정確성보다 더 중요하다면) ``filter_scheduler``를 구성할 때만 ``AllHostsFilter``를 활성화하고 "
"``[filter_scheduler]/host_subset_size``를 조정하면 유사한 성능을 제공할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:765
msgid ""
"The ``changes-before`` request parameter can be passed to the servers, os-"
"instance-action and os-migrations APIs:"
msgstr ""
"``changes-before`` 요청 매개 변수는 서버에 전달할 수 있으며, os-instance-action API와 os-"
"migrations API에서 사용할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1284
msgid ""
"The ``compute_stats_class`` configuration option was deprecated since the "
"13.0.0 Mitaka release and has been removed. Compute statistics are now "
"always generated from the ``nova.compute.stats.Stats`` class within Nova."
msgstr ""
"``compute_stats_class`` 구성 옵션은 13.0.0 미타카 릴리스 이후부터弃용되었으며 제거되었다. 컴퓨터 통계는 이제 "
"항상 Nova에서 Nova.compute.stats.Stats 클래스에서 생성된다."

#: ../../<reno.sphinxext stable/stein>:1228
msgid ""
"The ``config_drive_format`` config option has been deprecated. This was "
"necessary to workaround an issue with libvirt that was later resolved in "
"libvirt v1.2.17. For more information refer to `bug #1246201`__."
msgstr ""
"``config_drive_format`` 설정 옵션은弃용되었습니다. 이 것은 libvirt에 대한 문제를 해결하기 위해 필요했으며 나중에 libvirt v1.2.17에서 해결되었습니다. 더 많은 정보는 `bug #1246201`__를 참조하십시오.\n"
"\n"
"* `__ : bug #1246201의 원문입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1432
msgid ""
"The ``console_driver`` config opt in the ``DEFAULT`` group has been "
"deprecated and will be removed in a future release. This option no longer "
"does anything. Previously this option had only two valid, in-tree values: "
"``nova.console.xvp.XVPConsoleProxy`` and "
"``nova.console.fake.FakeConsoleProxy``. The latter of these was only used in"
" tests and has since been replaced."
msgstr ""
"``console_driver`` config opt ``DEFAULT`` 그룹에 있는 ``console_driver`` "
"옵션은弃용되었으며 향후 릴리스에서 제거될 예정입니다. 이 옵션은 더 이상อะไร도 할 수 없습니다. 이전에 이 옵션은 두 가지 유효한, "
"내부에 있는 값만 existed: ``nova.console.xvp.XVPConsoleProxy`` 및 "
"``nova.console.fake.FakeConsoleProxy``. 이 중에서 나중에 테스트에서만 사용되었으며 이후 대체되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1015
msgid ""
"The ``console_token_ttl`` configuration option has been moved to the "
"``consoleauth`` group and renamed ``token_ttl``. It should no longer be "
"included in the ``DEFAULT`` group."
msgstr ""
"``console_token_ttl`` 구성 옵션은 ``consoleauth`` 그룹에 이동되었으며 ``token_ttl``로 이름이 "
"변경되었다. 더 이상 ``DEFAULT`` 그룹에 포함되지 않도록 할 수 있다."

#: ../../<reno.sphinxext stable/rocky>:1657
msgid ""
"The ``db_driver`` configuration option was deprecated in a previous release "
"and has now been removed. This option allowed you to replace the SQLAlchemy "
"database layer with one of your own. The approach was deprecated and "
"unsupported, and it is now time to remove it completely."
msgstr ""
"``db_driver`` 설정 옵션은 이전 버전에서弃용되었으며 현재 완전히 제거되었습니다. 이 옵션은 SQLAlchemy 데이터베이스 "
"계층을 자신의 것으로 대체할 수 있는 것을 허용했습니다. 이 접근 방식은弃용되었으며 지원되지 않았으며, 지금은 완전히 제거해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:202 stable/pike>:350
#: stable/queens>:815
msgid ""
"The ``delete_host`` command has been added in ``nova-manage cell_v2`` to "
"delete a host from a cell (host mappings). The ``force`` option has been "
"added in ``nova-manage cell_v2 delete_cell``. If the ``force`` option is "
"specified, a cell can be deleted even if the cell has hosts."
msgstr ""
"``delete_host`` 명령은 ``nova-manage cell_v2``에서 세ลล에 호스트를 삭제하는 것을 추가했습니다. 호스트 "
"매핑에 대한 세ลล에서 호스트를 삭제하는 데 사용됩니다. ``force`` 옵션은 ``nova-manage cell_v2 "
"delete_cell``에서 추가되었습니다. ``force`` 옵션을 지정하면 세ลล에 호스트가 있는 경우에도 세ลล을 삭제할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/train>:1472
msgid ""
"The ``dhcp_domain`` option has been undeprecated and moved to the ``[api]`` "
"group. It is used by the metadata service to configure fully-qualified "
"domain names for instances, in addition to its role configuring DHCP "
"services for *nova-network*. This use case was missed when deprecating the "
"option initially."
msgstr ""
"``dhcp_domain`` 옵션은 deprecated가되었으며 ``[api]`` 그룹으로 이동되었습니다. 이 옵션은 메타데이터 서비스가"
" 인스턴스에 fully-qualified domain names을 구성하는 데 사용되며, *nova-network*에 대한 DHCP "
"서비스를 구성하는 데도 역할을합니다. 이 사용 경우는 처음에 옵션을 deprecated 할 때 misses되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1888
msgid ""
"The ``fping_path`` configuration option has been deprecated. /os-fping is "
"used by nova-network and nova-network itself is deprecated and will be "
"removed in the future."
msgstr ""
"``fping_path`` 설정 옵션은弃용되었습니다. /os-fping은 nova-network에 의해 사용되고, nova-network"
" 자체가弃용되며 미래에 제거될 예정입니다."

#: ../../<reno.sphinxext stable/queens>:1535
msgid ""
"The ``hide_server_address_states`` configuration option is now deprecated "
"for removal. In future, there will be hard coded server state ``building`` "
"for which server address will be hidden. The policy 'os_compute_api:os-hide-"
"server-addresses' also is deprecated for removal. More details `here`_"
msgstr ""
"``hide_server_address_states`` 구성 옵션은 현재 제거를 위해 비활성화되었습니다. 미래에는 server state"
" 'building'이 하드 코딩되어 server address가-hidden 될 것입니다. policy "
"'os_compute_api:os-hide-server-addresses'도 제거를 위해 비활성화되었습니다. 더 많은 정보는 `여기`"

#: ../../<reno.sphinxext stable/2025.2>:225
msgid ""
"The ``hw:sound_model`` flavor extra spec and the matching ``hw_sound_model``"
" image property were added to allow the configuration of a sound device "
"within an instance. This is useful with the new spice-direct console type. "
"The default remains no sound device, but when using the libvirt hypervisor "
"driver you can select from ``sb16``, ``es1370``, ``pcspk``, ``ac97``, "
"``ich6``, ``ich9``, ``usb``, and ``virtio``. For most use-cases ``usb`` is "
"likely to be the best choice unless you have at least libvirt 8.2.0 and "
"libvirt 10.4.0."
msgstr ""
"``hw:sound_model`` 플레버의 추가 스펙과 일치하는 ``hw:sound_model`` 이미지 속성은 사운드 장치를 인스턴스 "
"내에서 구성할 수 있도록 추가되었다. 이 기능은 새로운 스피시-디렉트 콘솔 타입과 함께 유용하다. 기본적으로 사운드 장치가 none으로 "
"유지되지만 libvirt 하이퍼바이저 드라이버를 사용할 때는 ``sb16``, ``es1370``, ``pcspk``, ``ac97``,"
" ``ich6``, ``ich9``, ``usb``, 및 ``virtio``를 선택할 수 있다. 대부분의 사용 경우에 ``usb``가 "
"가장 적합한 선택이지만至少 libvirt 8.2.0 및 libvirt 10.4.0를 사용하는 경우는 ngoại lệ이다."

#: ../../<reno.sphinxext stable/2025.2>:258
msgid ""
"The ``hw:usb_model`` flavor extra spec and the matching ``hw_usb_model`` "
"image property were added to allow the configuration of a USB controller "
"within an instance. This is useful with the new spice-direct console type "
"which supports passing through USB devices from the client to the instance, "
"such as a smart card reader. There is also an additional "
"``hw:redirected_usb_ports`` / ``hw_redirected_usb_ports`` pair which "
"controls how many ports the USB controller has. This number will vary based "
"on the USB controller selected. The default remains no USB controller, but "
"when using the libvirt hypervisor driver you can now also select from "
"``qemu_xhci`` and ``nec_xhci``."
msgstr ""
"``hw:usb_model`` 플레버의 추가 스펙과 일치하는 ``hw_usb_model`` 이미지 속성은 인스턴스 내에서 USB "
"컨트롤러의 구성이 가능하도록 추가되었다. 이 기능은 새로운 스피시-디렉트 콘솔 타입이 USB 장치가 클라이언트에서 인스턴스에 전달되는 "
"것을 지원하는 새로운 기능이다. 예를 들어 스마트 카드 리더와 같은 USB 장치를 통과할 수 있다. 또한 추가 "
"``hw:redirected_usb_ports`` / ``hw_redirected_usb_ports`` 쌍이 USB 컨트롤러가 선택된 "
"수를 제어한다. 이 수는 USB 컨트롤러에 따라 달라진다. 기본적으로는 USB 컨트롤러가 none인 경우지만 libvirt 하이퍼바이저 "
"드라이버를 사용할 때는 ``qemu_xhci`` 및 ``nec_xhci``를 선택할 수 있다."

#: ../../<reno.sphinxext unmaintained/yoga>:392
msgid ""
"The ``hw:vif_multiqueue_enabled`` flavor extra spec has been added. This is "
"a boolean option that, when set, can be used to enable or disable multiqueue"
" for virtio-net VIFs. It complements the equivalent image metadata property,"
" ``hw_vif_multiqueue_enabled``. If both values are set, they must be "
"identical or an error will be raised."
msgstr ""
"``hw:vif_multiqueue_enabled`` 플레버의 추가 속성은 ``hw:vif_multiqueue_enabled``와 동일한"
" 속성으로, virtio-net VIFs의 multi-queue를 활성화하거나 비활성화할 수 있는 보oleans 옵션입니다. 이 속성은 "
"equivalent image metadata property와 동일한 ``hw_vif_multiqueue_enabled``를 "
"보완합니다. 두 속성의值가 모두 설정되면, 동일한 값이 아니면 오류가 발생합니다."

#: ../../<reno.sphinxext stable/queens>:1488
msgid ""
"The ``idle_timeout`` option in the ``api_database`` group has been renamed "
"to ``connection_recycle_time``."
msgstr ""
"``api_database`` 그룹의 ``idle_timeout`` 옵션은 ``connection_recycle_time`` 로 이름이 "
"변경되었습니다."

#: ../../<reno.sphinxext stable/train>:1145
msgid ""
"The ``image_info_filename_pattern``, ``checksum_base_images``, and "
"``checksum_interval_seconds`` options have been removed in the ``[libvirt]``"
" config section."
msgstr ""
"``image_info_filename_pattern``, ``checksum_base_images``, 및 "
"``checksum_interval_seconds`` 옵션은 ``[libvirt]`` 구성 섹션에서 제거되었습니다."

#: ../../<reno.sphinxext stable/queens>:542 stable/rocky>:1507
msgid ""
"The ``image_ref_url`` entry in legacy instance notification payloads will be"
" just the instance image id if ``[glance]/api_servers`` is not set and the "
"notification is being sent from a periodic task. In this case the periodic "
"task does not have a token to get the image service endpoint URL from the "
"identity service so only the image id is in the payload. This does not "
"affect versioned notifications."
msgstr ""
"``image_ref_url`` 항목은 전통적인 인스턴스 알림 패드에서 ``[glance]/api_servers``가 설정되지 않은 "
"경우, 단기 일정 task에서 알림을 보낸다면, 단순히 인스턴스 이미지 ID가 들어가게 됩니다. 이 경우, 단기 일정 task는 ID "
"서비스에서 이미지 서비스 엔드포인트 URL을 얻기 위해 토큰을 가지고 있지 않기 때문에, 단지 이미지 ID가 패드에 들어가게 됩니다. 이"
" 경우는 버전화된 알림에 영향을 미치지 않습니다."

#: ../../<reno.sphinxext stable/rocky>:1916
msgid ""
"The ``image_upload_handler`` option in the ``xenserver`` conf section has "
"been deprecated. Please use the new option of ``image_handler`` to configure"
" the image handler which is used to download or upload images."
msgstr ""
"``image_upload_handler`` 옵션은 ``xenserver`` conf 섹션에 있는 "
"``image_upload_handler`` 옵션은弃용되었습니다. 새로운 ``image_handler`` 옵션을 사용하여 이미지 핸들러를"
" 구성하여 이미지 tải-down 또는 tải-up을 사용합니다."

#: ../../<reno.sphinxext stable/stein>:895
msgid ""
"The ``initial_cpu_allocation_ratio``, ``initial_ram_allocation_ratio`` and "
"``initial_disk_allocation_ratio`` configuration options have been added to "
"the ``DEFAULT`` group:"
msgstr ""
"``기본 CPU 할당 비율``, ``기본 RAM 할당 비율`` 및 ``기본 디스크 할당 비율`` 설정 옵션은 ``DEFAULT`` 그룹에"
" 추가되었습니다."

#: ../../<reno.sphinxext stable/queens>:760
msgid ""
"The ``injected_files``, ``injected_file_content_bytes`` and "
"``injected_file_path_bytes`` quotas are removed from the ``os-quota-sets`` "
"and ``os-quota-class-sets`` APIs."
msgstr ""
"``injected_files``, ``injected_file_content_bytes`` 및 "
"``injected_file_path_bytes`` quotas가 ``os-quota-sets`` 및 ``os-quota-class-"
"sets`` API에서 제거됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1517
msgid ""
"The ``live_migration_uri`` option in the [libvirt] configuration section is "
"deprecated, and will be removed in a future release. The "
"``live_migration_scheme`` should be used to change scheme used for live "
"migration, and ``live_migration_inbound_addr`` should be used to change "
"target URI."
msgstr ""
"``live_migration_uri`` 옵션은 [libvirt] 구성 섹션에 있는 것으로, 향후 릴리스에서 제거될 예정입니다. "
"``live_migration_scheme``를 사용하여 live migration scheme을 변경하고, "
"``live_migration_inbound_addr``를 사용하여 목표 URI를 변경해야 합니다."

#: ../../<reno.sphinxext stable/rocky>:397 stable/stein>:1321
msgid ""
"The ``long_rpc_timeout`` configuration option is now used for the RPC call "
"to the scheduler to select a host. This is in order to avoid a timeout when "
"scheduling multiple servers in a single request and/or when the scheduler "
"needs to process a large number of hosts."
msgstr ""
"``long_rpc_timeout`` 설정 옵션은 현재 스케줄러에 RPC 호출을 위해 호스트를 선택하는 데 사용됩니다. 이로 인해 여러 "
"서버를 한 요청에서 scheduling 할 때 또는 스케줄러가 많은 호스트를 처리해야 할 때 타임아웃을 피할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:758
msgid ""
"The ``maxPersonality`` and ``maxPersonalitySize`` limits are excluded from "
"the ``GET /limits`` API response."
msgstr ""
"``maxPersonality`` 및 ``maxPersonalitySize`` 제한은 ``GET /limits`` API 응답에서 "
"제외됩니다."

#: ../../<reno.sphinxext stable/train>:972
msgid ""
"The ``max_concurrent_live_migrations`` configuration option has been "
"restricted by the minimum value and now raises a ValueError if the value is "
"less than 0."
msgstr ""
"``max_concurrent_live_migrations`` 속성은 최소한의 giá치로 제한되어 있으며 현재 0보다 작은 giá치가면 "
"ValueError가 발생합니다."

#: ../../<reno.sphinxext stable/stein>:1040
msgid ""
"The ``maximum_instance_delete_attempts`` configuration option has been "
"restricted by the minimum value and now raises a ValueError if the value is "
"less than 1."
msgstr ""
"``maximum_instance_delete_attempts`` 설정 옵션은 최소한의 giá치로 제한되어 현재 최소한의 giá치보다 "
"작을 경우 ValueError가 발생합니다."

#: ../../<reno.sphinxext stable/ussuri>:883
msgid ""
"The ``networks`` quota, which was only enabled if the "
"``enabled_network_quota`` config option was enabled and only useful with "
"*nova-network*, is removed. It will not longer be present in the responses "
"for the APIs while attempts to update the quota will be rejected."
msgstr ""
"``네트워크`` 가용량, *nova-network*만 사용할 수 있는 경우에만 활성화되며, *enabled_network_quota* "
"config 옵션이 활성화된 경우에만 활성화되며, 제거되었습니다. API 응답에서 더 이상 나타나지 않으며, 가용량을 업데이트하려고 "
"시도할 경우 거부됩니다."

#: ../../<reno.sphinxext stable/ussuri>:654
msgid ""
"The ``non_inheritable_image_properties`` configuration option inhibits the "
"transfer of image properties from the image an instance was created from to "
"images created from that instance.  There are, however, image properties "
"(for example, the properties used for image signature validation) that "
"should *never* be transferred to an instance snapshot. Prior to this "
"release, such properties were included in the default setting for this "
"configuration option, but this allowed the possibility that they might be "
"removed by mistake, thereby resulting in a poor user experience.  To prevent"
" that from happening, nova now maintains an internal list of image "
"properties that are absolutely non-inheritable regardless of the setting of "
"the configuration option.  See the help text for "
"``non_inheritable_image_properties`` in the sample nova configuration file "
"for details."
msgstr ""
"``non_inheritable_image_properties`` 설정 옵션은 인스턴스에서 이미지로 생성된 이미지의 속성들을 이미지에서 "
"이미지로 생성된 속성들을 전달하지 않도록 방지한다.  그러나, 이미지 속성 (예를 들어, 이미지 서명 유효성 확인을 위한 속성)가 "
"인스턴스 스냅샘을 위해 전달되지 shouldn't* ever* 전달되어야 하는 속성들이 있다.  이전 버전에서는 이러한 속성들이 설정 "
"옵션의 디폴트 설정에 포함되어 있었지만, 이는 오류로 인해 poor user experience를 유발할 수 있는 가능성이 있었다.  "
"이러한 문제를 방지하기 위해 nova는 설정 옵션의 설정에 관계없이 absolutelly non-inheritable 속성들을 "
"internal list에 유지한다.  ``non_inheritable_image_properties`` 설정 옵션에 대한 도움말을 "
"확인하려면 샘플 nova 설정 파일을 확인하십시오."

#: ../../<reno.sphinxext stable/queens>:1239
msgid ""
"The ``notify_on_api_faults`` config option and the ``api.fault`` "
"notification it enabled have been removed. As noted in `bug 1699115`_, the "
"``api.fault`` notification has not worked since the v2.1 API was introduced."
" As the v2.0 API is supported with the v2.1 codebase since Newton, this "
"notification has not been emitted since Newton. Given that no one has "
"reported an issue with this in that time, it is simply removed."
msgstr ""
"``notify_on_api_faults`` 설정 옵션과 ``api.fault``通知이 활성화된 것을 제거했습니다. `bug "
"1699115`_에 noted 된 것과 같이 v2.1 API가 도입된 이후 ``api.fault``通知은 작동하지 않습니다. v2.0 "
"API는 Newton 이후 v2.1 코드베이스와 함께 지원되기 때문에 Newton 이후 이通知은 발신되지 않았습니다. 이 동안 이 문제에"
" 대한 신고가 없기 때문에 단순히 제거되었습니다."

#: ../../<reno.sphinxext stable/pike>:125 stable/queens>:412
#: stable/rocky>:1495
msgid ""
"The ``nova-api`` service now requires the ``[placement]`` section to be "
"configured in nova.conf if you are using a separate config file just for "
"that service. This is because the ``nova-api`` service now needs to talk to "
"the placement service in order to delete resource provider allocations when "
"deleting an instance and the ``nova-compute`` service on which that instance"
" is running is down. This change is idempotent if ``[placement]`` is not "
"configured in ``nova-api`` but it will result in new warnings in the logs "
"until configured. See bug https://bugs.launchpad.net/nova/+bug/1679750 for "
"more details."
msgstr ""
"``nova-api`` 서비스는 separate config file로만 사용되는 경우 nova.conf의 ``[placement]`` "
"섹션을 구성해야 합니다. 이는 ``nova-api`` 서비스가 인스턴스를 삭제할 때 자원 제공자 할당을 삭제하고 ``nova-"
"compute`` 서비스가-running 인스턴스에 대해 삭제할 때 ``placement`` 서비스와 대화해야 하는데, ``nova-"
"compute`` 서비스가 down이 된 경우입니다. 이 변경은 ``[placement]``이 구성되지 않은 경우 "
"idempotent이지만 로그에 새로운 경고가 발생할 때까지 구성되지 않은 경우입니다. 더 많은 정보는 "
"https://bugs.launchpad.net/nova/+bug/1679750 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1382
msgid ""
"The ``nova-compute`` service requires Placement API version 1.14 at a "
"minimum to support `nested resource providers`_."
msgstr ""
"``nova-compute`` 서비스는 최소한으로 `nested resource providers`_을 지원하기 위해 Placement "
"API 버전 1.14을 필요로합니다."

#: ../../<reno.sphinxext stable/pike>:530 stable/queens>:1251
msgid ""
"The ``nova-conductor`` service now needs access to the Placement service in "
"the case of forcing a destination host during a live migration. Ensure the "
"``[placement]`` section of nova.conf for the ``nova-conductor`` service is "
"filled in."
msgstr ""
"nova-conductor 서비스는 live migration의 경우, 목적지 호스트를 강제로 설정할 때 Placement 서비스에 "
"접근해야합니다. nova.conf의 [placement] 섹션에서 nova-conductor 서비스에 "
"Platzacementservice에 접근해야합니다. nova-conductor 서비스의 [placement] 섹션을 완성해야합니다."

#: ../../<reno.sphinxext stable/ussuri>:832
msgid ""
"The ``nova-console`` service has been deprecated since the 19.0.0 Stein "
"release and has now been removed. The following configuration options are "
"therefore removed."
msgstr ""
"``nova-console`` 서비스는 19.0.0 스티恩 릴리즈 이후 deprecated되었으며 현재 제거되었습니다. 따라서 다음 구성"
" 옵션은 제거되었습니다."

#: ../../<reno.sphinxext stable/stein>:1255
msgid ""
"The ``nova-console`` service is deprecated as it is XenAPI specific, does "
"not function properly in a multi-cell environment, and has effectively been "
"replaced by noVNC and the ``nova-novncproxy`` service. noVNC should "
"therefore be configured instead."
msgstr ""
"``nova-console`` 서비스는 XenAPI 특이적이며, 다세포 환경에서 제대로 작동하지 않으며, noVNC와 ``nova-"
"novncproxy`` 서비스에 의해 효과적으로 대체되어 있기 때문에弃기되었다. 따라서 noVNC를 대신으로 구성해야 한다."

#: ../../<reno.sphinxext stable/rocky>:1778
msgid ""
"The ``nova-consoleauth`` service has been deprecated and new consoles will "
"have their token authorizations stored in cell databases. With this, console"
" proxies are required to be deployed per cell. All existing consoles will be"
" reset. For most operators, this should be a minimal disruption as the "
"default TTL of a console token is 10 minutes."
msgstr ""
"``nova-consoleauth`` 서비스는弃용되었으며, 새로운 콘솔은 세ลล 데이터베이스에 토큰 권한을 저장할 것이다. 이에 따라, "
"세ลล당 콘솔 프록시가 배포되어야 한다. 모든 현재 콘솔은 재설정될 것이다. 대부분의 운영자에게는 이가 최소한의 방해를 일으킬 것으로 "
"예상된다. 콘솔 토큰의 디폴트 TTL은 10분이다."

#: ../../<reno.sphinxext stable/train>:1156
msgid ""
"The ``nova-consoleauth`` service has been deprecated since the 18.0.0 Rocky "
"release and has now been removed. The following configuration options have "
"been removed:"
msgstr ""
"``nova-consoleauth`` 서비스는 18.0.0 로키 릴리스 이후부터弃용되었으며 현재 제거되었습니다. 다음 구성 옵션들이 "
"제거되었습니다:"

#: ../../<reno.sphinxext stable/rocky>:1876
msgid ""
"The ``nova-consoleauth`` service has been deprecated. Console token "
"authorization storage is moving from the ``nova-consoleauth`` service "
"backend to the database backend, with storage happening in both, in Rocky. "
"In Stein, only the database backend will be used for console token "
"authorization storage."
msgstr ""
"``nova-consoleauth`` 서비스는弃용되었습니다. 콘솔 토큰 인증 스토리지는 ``nova-consoleauth`` 서비스 "
"배경에서 데이터베이스 배경으로 이동하고, 스토리지는 로키에서 ambos에서 발생합니다. 스티恩에서는 only 데이터베이스 배경이 콘솔 "
"토큰 인증 스토리지를 사용할 것입니다."

#: ../../<reno.sphinxext stable/train>:520
msgid ""
"The ``nova-consoleauth`` service has been removed as it was deprecated since"
" the 18.0.0 (Rocky) release."
msgstr "``nova-consoleauth`` 서비스는 18.0.0 (Rocky) 릴리스 이후부터弃기되었기 때문에 제거되었습니다."

#: ../../<reno.sphinxext stable/rocky>:464 stable/stein>:1069
msgid ""
"The ``nova-consoleauth`` service is deprecated and should no longer be "
"deployed, however, if there is a requirement to maintain support for "
"existing console sessions through a live/rolling upgrade, operators should "
"set ``[workarounds]enable_consoleauth = True`` in their configuration and "
"continue running ``nova-consoleauth`` for the duration of the live/rolling "
"upgrade. A new check has been added to the ``nova-status upgrade check`` CLI"
" to help with this and it will emit a warning and provide additional "
"instructions to set ``[workarounds]enable_consoleauth = True`` while "
"performing a live/rolling upgrade."
msgstr ""
"``nova-consoleauth`` 서비스는 더 이상 배포되지 않으며, live/rolling 업그레이드를 통해 기존 콘솔 세션의 "
"지원을 유지하고자 하는 경우, 운영자들은 ``[workarounds]enable_consoleauth = True``를 설정하고 "
"live/rolling 업그레이드 duration 동안 ``nova-consoleauth``를 계속 실행해야 합니다. 새로운 체크가 "
"``nova-status upgrade check`` CLI에 추가되었으며, live/rolling 업그레이드에 대한 이에 대한 추가 "
"지침을 제공하고,警告를 발신합니다. ``[workarounds]enable_consoleauth = True``를 설정하는 동안."

#: ../../<reno.sphinxext stable/ussuri>:913
msgid ""
"The ``nova-dhcpbridge`` service has been removed. This was only used with "
"the now-removed *nova-network* service."
msgstr ""
"``nova-dhcpbridge`` 서비스는 제거되었습니다. 이 서비스는 현재 제거된 *nova-network* 서비스와 함께 "
"사용되었습니다."

#: ../../<reno.sphinxext stable/pike>:1409
msgid ""
"The ``nova-manage api_db sync`` and ``nova-manage db sync`` commands "
"previously took an optional ``--version`` parameter to determine which "
"version to sync to. For example::"
msgstr ""
"``nova-manage api_db sync`` 및 ``nova-manage db sync`` 명령은 이전에 옵션 "
"``--version`` parameter를 사용하여.sync할 버전을 결정할 수 existed. 예를 들어:"

#: ../../<reno.sphinxext stable/queens>:823
msgid ""
"The ``nova-manage cell_v2 delete_cell`` command returns an exit code 4 when "
"there are instance mappings to a cell to delete but all instances have been "
"deleted in the cell."
msgstr ""
"``nova-manage cell_v2 delete_cell`` 명령은 세ลล을 삭제할 때 인스턴스 매핑이 존재하고 모든 인스턴스들이 "
"세ลล에서 삭제된 경우에 exit code 4를 반환합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1587
msgid ""
"The ``nova-manage cell_v2 simple_cell_setup`` command now creates the "
"default cell0 database connection using the ``[database]`` connection "
"configuration option rather than the ``[api_database]`` connection. The "
"cell0 database schema is the `main` database, i.e. the `instances` table, "
"rather than the `api` database schema. In other words, the cell0 database "
"would be called something like ``nova_cell0`` rather than "
"``nova_api_cell0``."
msgstr ""
"nova-manage cell_v2 simple_cell_setup 명령은 현재 [database] 연결 설정 옵션을 사용하여 "
"[api_database] 연결 대신 default cell0 데이터베이스 연결을 생성합니다. cell0 데이터베이스 스키마는 main "
"데이터베이스, 즉 instances 테이블이 아니라 api 데이터베이스 스키마입니다. 즉, cell0 데이터베이스는 nova_cell0과"
" 같은 이름으로 불리우기보다는 nova_api_cell0과 같은 이름으로 불리우기 때문입니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:737
msgid ""
"The ``nova-manage db ironic_flavor_migration`` command has been removed. "
"This command could be used to assist users skipping the 16.0.0 (Pike) "
"release, which is now in the distant past."
msgstr ""
"``nova-manage db ironic_flavor_migration`` 명령은 제거되었습니다. 이 명령은 사용자가 16.0.0 "
"(Pike) 릴리스를 skips하기 위해 도움을 줄 수 있는 것을 목표로 설계되었습니다. 현재는 거리에서 떨어진 과거에 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:748
msgid ""
"The ``nova-manage db null_instance_uuid_scan`` command has been removed. A "
"blocking migration has been in place since the 12.0.0 (Liberty) release "
"making this check unnecessary."
msgstr ""
"``nova-manage db null_instance_uuid_scan`` 명령은 12.0.0 (Liberty) 릴리스 이후 블록링 "
"이민이 진행되어 이 확인이 불필요한 상황이되었습니다."

#: ../../<reno.sphinxext stable/rocky>:378 stable/stein>:1056
msgid ""
"The ``nova-manage db online_data_migrations`` command now returns exit "
"status 2 in the case where some migrations failed (raised exceptions) and no"
" others were completed successfully from the last batch attempted. This "
"should be considered a fatal condition that requires intervention. Exit "
"status 1 will be returned in the case where the ``--max-count`` option was "
"used and some migrations failed but others succeeded (updated at least one "
"row), because more work may remain for the non-failing migrations, and their"
" completion may be a dependency for the failing ones. The command should be "
"reiterated while it returns exit status 1, and considered completed "
"successfully only when it returns exit status 0."
msgstr ""
"``nova-manage db online_data_migrations`` 명령은 일부 마이그레이션이 실패하고 (예외가 발생했다) 마지막"
" 배치에서 성공적으로 마이그레이션을 완료하지 못한 경우에 2를 리턴하는 경우가 있습니다. 이 경우는 심각한 상태로 개입이 필요합니다. "
"``--max-count`` 옵션을 사용한 경우, 일부 마이그레이션이 실패했지만 다른 마이그레이션들이至少 한 행을 업데이트한 경우에 1을"
" 리턴합니다. 이 경우, 비 실패한 마이그레이션에 대한 추가 작업이 더 필요할 수 있으며, 이 마이그레이션의 성공은 실패한 마이그레이션의"
" 성공에 의존할 수 있습니다. 이 명령은 1을 리턴할 때 반복되어야하며, 0을 리턴할 때만 성공적으로 완료된 것으로 간주됩니다."

#: ../../<reno.sphinxext stable/stein>:979
msgid ""
"The ``nova-manage db online_data_migrations`` command will now fill missing "
"``virtual_interfaces`` records for instances created before the Newton "
"release. This is related to a fix for https://launchpad.net/bugs/1751923 "
"which makes the _heal_instance_info_cache periodic task in the ``nova-"
"compute`` service regenerate an instance network info cache from the current"
" neutron port list, and the VIFs from the database are needed to maintain "
"the port order for the instance."
msgstr ""
"nova-manage db online_data_migrations 명령은 Newton 릴리스 이전에 생성된 인스턴스에 대해 "
"missing virtual_interfaces 레코드를 채워넣을 것이다. 이는 "
"https://launchpad.net/bugs/1751923에 대한修复을 포함한다. 이修复은 nova-compute 서비스의 "
"_heal_instance_info_cache 주기적 일정 task를 neutron port list에서 현재 neutron port "
"list를 사용하여 인스턴스 네트워크 정보 캐시를 다시 생성하게 한다. 이때 virtualInterfaces 레코드가 필요하다. "
"인스턴스에서 포트의 순서를 유지하기 때문이다."

#: ../../<reno.sphinxext unmaintained/yoga>:415
msgid ""
"The ``nova-manage image_property set`` command can be used to update the "
"stored image properties stored in the database for a given instance and "
"image properties."
msgstr ""
"``nova-manage image_property set`` 명령은 데이터베이스에 저장된 이미지 프로퍼티를 업데이트하여 주어지는 이미지"
" 프로퍼티와 인스턴스에 저장된 이미지 프로퍼티를 업데이트할 수 있는 명령입니다."

#: ../../<reno.sphinxext unmaintained/yoga>:412
msgid ""
"The ``nova-manage image_property show`` command can be used to show the "
"current stored image property value for a given instance and property."
msgstr ""
"``nova-manage image_property show`` 명령은 특정 인스턴스와 속성을 사용하여 현재 저장된 이미지 속성 값이 "
"표시됩니다."

#: ../../<reno.sphinxext stable/2025.1>:326
msgid ""
"The ``nova-manage limits migrate_to_unified_limits`` command will now scan "
"the API and cell databases to detect resource classes that do not have "
"registered limits set in Keystone and report them to the console."
msgstr ""
"nova-manage limits migrate_to_unified_limits 명령은 현재 Keystone에서 등록된 제한이 없는 "
"리소스 클래스를 API 및 세ลล 데이터베이스에서 감지하고 콘솔에 báo cáo합니다."

#: ../../<reno.sphinxext stable/train>:1447
msgid ""
"The ``nova-manage placement heal_allocations`` `CLI`_ has been extended to "
"heal missing port allocations which are possible due to `bug 1819923`_ ."
msgstr ""
"``nova-manage placement heal_allocations`` `CLI`_은 `bug 1819923`_ 의缘에 "
"missing port 할당이 가능한 경우에 healing이 가능하도록 확장되었습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:454
msgid ""
"The ``nova-manage placement heal_allocations`` `CLI`_ now allows "
"regenerating the placement allocation of servers with ports using minimum "
"guaranteed packet rate QoS policy rules."
msgstr ""
"``nova-manage placement heal_allocations`` `CLI`_ 현재 포트가 있는 서버의 배치 할당을 최소 보장"
" 패킷 दर QoS 정책 규칙에 따라 재생성할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1109
msgid ""
"The ``nova-manage project quota_usage_refresh`` and its alias ``nova-manage "
"account quota_usage_refresh`` commands have been renamed ``nova-manage quota"
" refresh``. Aliases are provided but these are marked as deprecated and will"
" be removed in the next release of nova."
msgstr ""
"``nova-manage project quota_usage_refresh`` 및 그_alias``nova-manage account "
"quota_usage_refresh`` 명령어는 ``nova-manage quota refresh``로 이름이 변경되었습니다. 이 "
"aliases는 제공되지만, deprecated로 표시되어 다음 nova 릴리스에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/pike>:1714
msgid ""
"The ``nova-manage quota refresh`` command has been deprecated and is now a "
"no-op since quota usage is counted from resources instead of being tracked "
"separately. The command will be removed during the Queens cycle."
msgstr ""
"``nova-manage quota refresh`` 명령은 quota 사용이 리소스 대신 separately 추적되는 대신 카운트되는 "
"경우에만 사용되기 때문에 now deprecated 및 no-op이 된 것으로 나타납니다. 명령은 Queens cycle 동안 제거되게 "
"됩니다."

#: ../../<reno.sphinxext unmaintained/xena>:355
msgid ""
"The ``nova-manage volume_attachment get_connector`` command can be used to "
"get updated host connector for the localhost."
msgstr ""
"nova-manage volume_attachment get_connector 명령은 로컬 호스트의 업데이트된 호스트 कनेक터를 얻을 "
"수 있습니다."

#: ../../<reno.sphinxext unmaintained/xena>:351
msgid ""
"The ``nova-manage volume_attachment show`` command can be used to show the "
"current volume attachment information for a given volume and instance."
msgstr ""
"``nova-manage volume_attachment show`` 명령은 특정 볼륨과 인스턴스를 사용하여 현재 볼륨 액세서리 정보를 "
"표시할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:935
msgid ""
"The ``nova-manage`` set of commands would previously exit with return code 1"
" due to any unexpected error. However, some commands, such as ``nova-manage "
"db archive_deleted_rows``, ``nova-manage cell_v2 map_instances`` and ``nova-"
"manage placement heal_allocations`` use return code 1 for flow control with "
"automation. As a result, the unexpected error return code has been changed "
"from 1 to 255 for all ``nova-manage`` commands."
msgstr ""
"``nova-manage`` 명령어의 집합은 이전에는 bất ngờ 오류가 발생할 경우 1로의 리턴 코드로 종료되었습니다. 그러나 일부 "
"명령,例如 ``nova-manage db archive_deleted_rows``, ``nova-manage cell_v2 "
"map_instances`` 및 ``nova-manage placement heal_allocations``은 자동화와 함께 흐름 제어를"
" 위해 리턴 코드 1을 사용합니다. 따라서 모든 ``nova-manage`` 명령어의 비预期 오류 리턴 코드는 1에서 255로 "
"변경되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:1003
msgid ""
"The ``nova-xvpvncproxy`` service has been deprecated since the 19.0.0 Stein "
"release and has now been removed. The following configuration options have "
"also been removed:"
msgstr ""
"``nova-xvpvncproxy`` 서비스는 19.0.0 스티恩 릴리즈 이후에弃용되었으며 현재 제거되었습니다. 다음 구성 옵션도 "
"제거되었습니다."

#: ../../<reno.sphinxext stable/queens>:1139
msgid ""
"The ``nova.conf`` file should have the ``auth_schemes`` parameter in the "
"``vnc`` group set. If there are a mix of compute nodes, some with VeNCrypt "
"enabled and others with it disabled, then the ``auth_schemes`` configuration"
" option should be set to ``['vencrypt', 'none']``."
msgstr ""
"``nova.conf`` 파일에 ``auth_schemes`` 매개 변수가 ``vnc`` 그룹에 설정되어 있어야 합니다. compute "
"노드가 혼합되어 있으면, 일부가 VeNCrypt 활성화되어 있고 다른 일부가 비활성화되어 있으면, ``auth_schemes`` 구성 "
"옵션은 ``['vencrypt', 'none']``로 설정되어야 합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:609
msgid ""
"The ``nova.image.download`` entry point hook has been removed, per the "
"deprecation announcement in the 17.0.0 (Queens) release."
msgstr ""
"nova.image.download entry point hook가 제거되었습니다. 17.0.0 (Queens) 릴리스의 비활성화 발표에"
" 따라."

#: ../../<reno.sphinxext stable/2023.2>:390
msgid ""
"The ``nova.quota.DbQuotaDriver`` is marked as deprecated and the default "
"quota driver configuration is planned to be changed to the "
"``nova.quota.UnifiedLimitsDriver`` in the 29.0.0 (2024.1 Caracal) release."
msgstr ""
"``nova.quota.DbQuotaDriver``은弃용된 것으로 표시되어 29.0.0 (2024.1 Caracal) 린스에서 "
"``nova.quota.UnifiedLimitsDriver``로 기본 할당량 구성이 변경될 예정입니다."

#: ../../<reno.sphinxext stable/pike>:1128 stable/pike>:1158
msgid ""
"The ``nova.virt.libvirt.firewall.IptablesFirewallDriver`` firewall driver is"
" enabled"
msgstr ""
"``nova.virt.libvirt.firewall.IptablesFirewallDriver``防火walls 구현자는 활성화되었습니다."

#: ../../<reno.sphinxext stable/pike>:1315
msgid ""
"The ``nova.virt.libvirt.volume.glusterfs.LibvirtGlusterfsVolumeDriver`` "
"volume driver has been removed. The GlusterFS volume driver in Cinder was "
"deprecated during the Newton release and was removed from Cinder in the "
"Ocata release so it is effectively not maintained and therefore no longer "
"supported."
msgstr ""
"``nova.virt.libvirt.volume.glusterfs.LibvirtGlusterfsVolumeDriver`` 집합 드라이버가"
" 제거되었다. 시더에서 사용하는 글러스터FS 집합 드라이버는 뉴턴 릴리스 시에 비상상적이되었으며, 오카타 릴리스 시에 시더에서 제거되어서"
" 실제로 유지 관리가 되지 않아 더 이상 지원되지 않는다."

#: ../../<reno.sphinxext stable/pike>:1337
msgid ""
"The ``nova.virt.libvirt.volume.scality.LibvirtScalityVolumeDriver`` volume "
"driver has been removed. The Scality volume driver in Cinder was deprecated "
"during the Newton release and was removed from Cinder in the Ocata release "
"so it is effectively not maintained and therefore no longer supported."
msgstr ""
"``nova.virt.libvirt.volume.scality.LibvirtScalityVolumeDriver`` 집합 드라이버가 "
"제거되었다. 시클리 volume 드라이버는 뉴턴 릴리스 동안 deprecated되었으며, 오카타 릴리스에서 시클리에서 제거되어서 실제로 "
"유지 관리되지 않으며 therefore 더 이상 지원되지 않는다."

#: ../../<reno.sphinxext stable/2025.2>:218
msgid ""
"The ``one_time_use`` tag was added to the PCI ``device_spec`` description, "
"which allows leaving devices in reserved state after they have been assigned"
" to an instance. This is useful for data cleaning, firmware updates, and "
"other operator-specific workflows."
msgstr ""
"``one_time_use`` 태그가 PCI ``device_spec`` 설명에 추가되었으며, 인스턴스에 할당된 후 장치를 예약 상태로 "
"남겨두는 것을 허용합니다. 이는 데이터 청소, 펄리먼트 업데이트 및 기타 운영자-specific 워크플로우에 유용합니다."

#: ../../<reno.sphinxext stable/rocky>:1045
msgid ""
"The ``os-aggregates`` compute API ``add_host`` and ``remove_host`` actions "
"will automatically add/remove compute node resource providers from resource "
"provider aggregates in the placement service if the ``nova-api`` service is "
"configured to communicate with the placement service, so this command is "
"mostly useful for existing deployments with host aggregates which are not "
"yet mirrored in the placement service."
msgstr ""
"``os-aggregates`` 컴퓨터 API의 ``add_host`` 및 ``remove_host`` hành động은 ``nova-"
"api`` 서비스가 배치 서비스와 통신할 수 있게 설정된 경우에 따라 컴퓨터 노드 리소스 제공자를 리소스 제공자 집합에 추가/제거합니다."
" 이로 인해 배치 서비스에 아직 반영되지 않은 호스트 집합이 있는 currently existing deployments에 이 명령은 "
"mostly 유용합니다."

#: ../../<reno.sphinxext stable/pike>:1704
msgid ""
"The ``os-hosts`` API is deprecated as of the 2.43 microversion. Requests "
"made with microversion >= 2.43 will result in a 404 error. To list and show "
"host details, use the ``os-hypervisors`` API. To enable or disable a "
"service, use the ``os-services`` API. There is no replacement for the "
"`shutdown`, `startup`, `reboot`, or `maintenance_mode` actions as those are "
"system-level operations which should be outside of the control of the "
"compute service."
msgstr ""
"``os-hosts`` API는 2.43 마이크로 버전부터弃기되었습니다. 2.43 이상의 마이크로 버전으로 요청하면 404 오류가 "
"발생합니다. 호스트รายละเอียด을 목록화하고 표시하려면 ``os-hypervisors`` API를 사용하십시오. 서비스를 "
"활성화하거나 비활성화하려면 ``os-services`` API를 사용하십시오. `shutdown`, `startup`, `reboot`,"
" 또는 `maintenance_mode` 액션의 대체는 없습니다. 이 액션은 컴퓨터 서비스의 제어권이 خارج된 시스템 수준의 연산이므로"
" 대체가 없습니다."

#: ../../<reno.sphinxext stable/pike>:53 stable/queens>:186 stable/rocky>:447
#: stable/stein>:1349
msgid ""
"The ``os-simple-tenant-usage`` pagination has been fixed. In some cases, "
"nova usage-list would have returned incorrect results because of this. See "
"bug https://launchpad.net/bugs/1796689 for details."
msgstr ""
"``os-simple-tenant-usage`` 페이지네이션은 수정되었습니다. 일부 경우, nova usage-list는 이 수정 때문에"
" 오류를 반환했습니다. 더 많은 정보는 https://launchpad.net/bugs/1796689 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:1163
msgid ""
"The ``os_compute_api:flavors`` policy deprecated in 16.0.0 has been removed."
msgstr "``os_compute_api:flavors`` 정책은 16.0.0 버전에서弃용되었으며 제거되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:601
msgid ""
"The ``os_compute_api:os-extended-server-attributes`` policy controls which "
"users a number of server extended attributes are shown to. Configuring "
"visiblity of the ``OS-EXT-SRV-ATTR:hostname`` attribute via this policy has "
"now been deprecated and will be removed in a future release. Upon removal, "
"this attribute will be shown for all users regardless of policy "
"configuration."
msgstr ""
"``os_compute_api:os-extended-server-attributes`` 정책은 여러 서버에 대한 확장 속성의 사용자에게 "
"보이도록 설정하는 것을 제어합니다. 이 정책을 사용하여 ``OS-EXT-SRV-ATTR:hostname`` 속성의 보이기 "
"visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 "
"보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 "
"속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 "
"사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 "
"정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. "
"이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 "
"구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 "
"visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 "
"보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 "
"속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 "
"사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 "
"정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. "
"이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 "
"구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 "
"visibility를 구성했습니다. 이 정책을 사용하여 속성의 보이기 visibility를 구성했습니다. 이 정책을 사용하여 속성의 "
"보이기 visibility를 구성했습니다. 이"

#: ../../<reno.sphinxext stable/stein>:1167
msgid ""
"The ``os_compute_api:os-flavor-manage`` policy has been removed because it "
"has been deprecated since 16.0.0. Use the following policies instead:"
msgstr ""
"``os_compute_api:os-flavor-manage`` 정책은 16.0.0부터弃기되었기 때문에 제거되었습니다. 대신 사용할 수 "
"있는 정책은 다음과 같습니다."

#: ../../<reno.sphinxext stable/stein>:1176
msgid ""
"The ``os_compute_api:os-server-groups`` policy deprecated in 16.0.0 has been"
" removed."
msgstr "``os_compute_api:os-server-groups`` 정책은 16.0.0 버전에서弃용되었으며 제거되었습니다."

#: ../../<reno.sphinxext stable/queens>:756
msgid ""
"The ``personality`` parameter is removed from the server create and rebuild "
"APIs. Use the ``user_data`` parameter instead."
msgstr "``인격`` 매개 변수는 서버 생성 및 재건 API에서 제거되었습니다. 대신 ``사용자 데이터`` 매개 변수를 사용하십시오."

#: ../../<reno.sphinxext stable/rocky>:764
msgid ""
"The ``policies`` and ``metadata`` fields have been removed from the response"
" body of POST, GET ``/os-server-groups`` API and GET ``/os-server-"
"groups/{server_group_id}`` API."
msgstr ""
"``policies`` 및 ``metadata`` 필드는 POST, GET ``/os-server-groups`` API 및 GET "
"``/os-server-groups/{server_group_id}`` API의 응답 바디에서 제거되었습니다."

#: ../../<reno.sphinxext stable/rocky>:761
msgid ""
"The ``policy`` and ``rules`` fields will be returned in response body of "
"POST, GET ``/os-server-groups`` API and GET ``/os-server-"
"groups/{server_group_id}`` API."
msgstr ""
"``정책`` 및 ``규칙`` 필드는 POST, GET ``/os-server-groups`` API 및 GET ``/os-server-"
"groups/{server_group_id}`` API의 응답 바디에서 반환됩니다."

#: ../../<reno.sphinxext stable/train>:1032
msgid ""
"The ``populate_queued_for_delete`` and ``populate_user_id`` online data "
"migrations must be completed before usage can be counted from placement. "
"Until the data migration is complete, the system will fall back to legacy "
"quota usage counting from cell databases depending on the result of an "
"``EXISTS`` database query during each quota check, if "
"``[quota]count_usage_from_placement`` is set to ``True``.  Operators who "
"want to avoid the performance hit from the ``EXISTS`` queries should wait to"
" set the ``[quota]count_usage_from_placement`` configuration option to "
"``True`` until after they have completed their online data migrations via "
"``nova-manage db online_data_migrations``."
msgstr ""
"``populate_queued_for_delete`` 및 ``populate_user_id`` 온라인 데이터 이식이 완료되지 않은 "
"경우, 사용자에게 할당된 자원 수를 카운트할 수는 없습니다. 데이터 이식이 완료되지 않은 경우, 시스템은 legacy 자원 사용 카운트를"
" cell 데이터베이스에 의존하여 ``EXISTS`` 데이터베이스 쿼리의 결과에 따라 fallback합니다. "
"``[quota]count_usage_from_placement``가 ``True``로 설정된 경우, 각 자원 확인 시 "
"``EXISTS`` 데이터베이스 쿼리를 수행하여 자원 사용 카운트를 수행합니다. "
"``[quota]count_usage_from_placement``를 ``True``로 설정하고자 하는 연산자는 ``nova-manage"
" db online_data_migrations``를 통해 온라인 데이터 이식을 완료한 후에 ``True``로 설정해야 합니다."

#: ../../<reno.sphinxext stable/pike>:1586
msgid ""
"The ``quota_usage_refresh`` sub-command has been renamed to ``nova-manage "
"quota refresh``. This new command should be used instead."
msgstr ""
"``quota_usage_refresh`` 명령은 ``nova-manage quota refresh``로 이름이 변경되었습니다. 이 "
"새로운 명령은 사용해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1487
msgid ""
"The ``remap_vbd_dev`` option is deprecated and will be removed in a future "
"release."
msgstr "``remap_vbd_dev`` 옵션은 현재 버전에서 사용할 수 없으며 향후 버전에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/rocky>:643
msgid ""
"The ``request_id`` field has been added to all instance action and instance "
"update versioned notification payloads. Note that notifications triggered by"
" periodic tasks will have the ``request_id`` field set to be ``None``."
msgstr ""
"``request_id`` 필드는 모든 인스턴스 액션과 인스턴스 업데이트 버전화된 알림 패키지에 추가되었다.  주기적 일정을 트리거하는 "
"알림은 ``request_id`` 필드를 ``None``으로 설정한다."

#: ../../<reno.sphinxext stable/rocky>:1150
msgid ""
"The ``required`` and ``member_of`` query parameters for a given group are "
"optional.  That is, you may specify ``resources42=XXX`` without a "
"corresponding ``required42=YYY`` or ``member_of42=ZZZ``. However, the "
"reverse (specifying ``required42=YYY`` or ``member_of42=ZZZ`` without "
"``resources42=XXX``) will result in an error."
msgstr ""
"``required`` 및 ``member_of`` query parameter의 특정 그룹에 대한 ``required`` 및 "
"``member_of``는 선택적입니다.  즉, ``resources42=XXX``를 ``required42=YYY`` 또는 "
"``member_of42=ZZZ``와 함께 지정하지 않으면 ``resources42=XXX``를 지정할 수 있습니다. 그러나 반대로 "
"(``required42=YYY`` 또는 ``member_of42=ZZZ``를 ``resources42=XXX``와 함께 지정하지 "
"않음)는 오류를 발생시킵니다."

#: ../../<reno.sphinxext stable/ussuri>:1367
msgid ""
"The ``resize`` and ``migrate`` server action APIs used to synchronously "
"block until a destination host is selected by the scheduler. Those APIs now "
"asynchronously return a response to the user before scheduling. The response"
" code remains 202 and users can monitor the operation via the ``status`` and"
" ``OS-EXT-STS:task_state`` fields on the server resource and also by using "
"the ``os-instance-actions`` API. The most notable change is ``NoValidHost`` "
"will not be returned in a 400 error response from the API if scheduling "
"fails but that information is available via the instance actions API "
"interface."
msgstr ""
"``resize`` 및 ``migrate`` 서버 액션 API는 스케줄러가 목적지 호스트를 선택할 때까지 동기적으로 블록합니다. 이 "
"API는 현재 비동기적으로 사용자에게 반응을 반환합니다. 반응 코드는 202로 유지되고, 사용자는 ``status`` 및 ``OS-"
"EXT-STS:task_state`` 필드에 있는 서버 리소스에서 작업의 상태를 확인할 수 있습니다. 또한 ``os-instance-"
"actions`` API를 사용하여 작업의 상태를 확인할 수 있습니다. 가장 중요한 변경 사항은 ``NoValidHost``가 400 "
"오류 반응에서 반환되지 않지만, 작업이 실패할 때만 정보가 available입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1303
msgid ""
"The ``scheduler_json_config_location`` configuration option has not been "
"used since the 13.0.0 Mitaka release and has been removed."
msgstr ""
"``scheduler_json_config_location`` 설정 옵션은 13.0.0 미타카 릴리스 이후 사용되지 않았으며 제거되었다."

#: ../../<reno.sphinxext stable/pike>:1192
msgid ""
"The ``ssl`` options were only used by Nova code that interacts with Glance "
"client. These options are now defined and read by Keystoneauth. "
"``api_insecure`` option from glance group is renamed to ``insecure``. The "
"following ''ssl'' options are moved to ``glance`` group"
msgstr ""
"``ssl`` 옵션은 Nova 코드가 Glance 클라이언트와 상호작용하는 경우에만 사용되었다. 이 옵션은 현재 Keystoneauth에"
" 의해 정의되고 읽어집니다. Glance 그룹의 ``api_insecure`` 옵션은 ``insecure``로 이름이 변경되었습니다. "
"다음 ``ssl`` 옵션은 Glance 그룹으로 이동되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:383
msgid ""
"The ``task_log`` database table contains instance usage audit records if "
"``nova-compute`` has been configured with ``[DEFAULT]instance_usage_audit = "
"True``. This will be the case if OpenStack Telemetry is being used in the "
"deployment, as the option causes Nova to generate audit notifications that "
"Telemetry consumes from the message bus."
msgstr ""
"``task_log`` 데이터베이스 테이블에는 ``nova-compute``가 ``[DEFAULT]instance_usage_audit "
"= True``로 구성된 경우에 instance 사용 audit 기록이 포함됩니다. 이 경우 OpenStack Telemetry가 "
"배포에서 사용되는 경우, 옵션은 Nova가 메시지 버스에서 소비하는 audit 알림을 생성하여 Telemetry를 알림합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1492
msgid ""
"The ``topic`` config options are now deprecated and will be removed in the "
"next release. The deprecated options are as below:"
msgstr ""
"``topic`` config 옵션은 현재 비고정화되어 있으며 다음 릴리스에서 제거될 예정입니다. 비고정화된 옵션은 아래와 같습니다."

#: ../../<reno.sphinxext stable/rocky>:1319
msgid ""
"The ``trusted_image_certificates`` parameter will be in the response body of"
" the following APIs (not restricted by policy):"
msgstr ""
"``trusted_image_certificates`` 파라미터는 다음 API의 응답 바디에 포함될 수 있습니다. (정책에 의해 제한되지"
" 않음)"

#: ../../<reno.sphinxext stable/rocky>:1306
msgid ""
"The ``trusted_image_certificates`` request parameter can be passed to the "
"server create and rebuild APIs (if allowed by policy):"
msgstr ""
"``trusted_image_certificates`` 요청 매개 변수는 정책에 의해 허용되는 경우에만 서버를 생성하고 재건 APIs에 "
"전달할 수 있습니다."

#: ../../<reno.sphinxext stable/2025.1>:364
msgid ""
"The ``unified_limits_resource_list`` list can also be set to an empty list."
msgstr "``unified_limits_resource_list`` 집합은 비어 있는 집합으로 설정될 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:755
msgid "The ``user_data`` parameter is added to the server rebuild API."
msgstr "``사용자 데이터`` 속성은 서버 재건 API에 추가됩니다."

#: ../../<reno.sphinxext stable/train>:1206
msgid ""
"The ``vcpu_pin_set`` configuration option has been deprecated. You should "
"migrate host CPU configuration to the ``[compute] cpu_dedicated_set`` or "
"``[compute] cpu_shared_set`` config options, or both. Refer to the help text"
" of these config options for more information."
msgstr ""
"``vcpu_pin_set`` 설정 옵션은弃용되었습니다. 호스트 CPU 설정을 ``[compute] cpu_dedicated_set`` "
"또는 ``[compute] cpu_shared_set`` config 옵션 중 하나 또는 ambos로 mig리트해야 합니다. 더 많은 "
"정보를 얻으려면 config 옵션의 도움말 텍스트를 참조하십시오."

#: ../../<reno.sphinxext stable/pike>:818
msgid ""
"The ``virtio-forwarder`` VNIC type has been added to the list of VNICs. This"
" VNIC type is intended to request a low-latency virtio port inside the "
"instance, likely backed by hardware acceleration. Currently the Agilio OVS "
"external Neutron and OS-VIF plugins provide support for this VNIC mode."
msgstr ""
"``virtio-forwarder`` VNIC 유형이 VNIC 목록에 추가되었다. 이 VNIC 유형은 인스턴스 내에서 저-latency "
"virtio 포트를 요청하는 것을 목표로 한다. 이 virtio 포트는 likely 하드웨어 가속화에 의해 지원된다. 현재 Agilio "
"OVS external Neutron 및 OS-VIF 플러그는 이 VNIC 모드를 지원한다."

#: ../../<reno.sphinxext unmaintained/wallaby>:235 unmaintained/xena>:263
#: unmaintained/yoga>:600
msgid ""
"The `bug 1952941`_  is fixed where a pre-Victoria server with pinned CPUs "
"cannot be migrated or evacuated after the cloud is upgraded to Victoria or "
"newer as the scheduling fails with ``NotImplementedError: Cannot load "
"'pcpuset'`` error."
msgstr ""
"`bug 1952941`_ 은 비クト리아 이전 서버에서 CPU를 고정하여 migRATE 또는 추출할 수 없게 된 이유는 클라우드가 "
"비クト리아 또는 newer 버전으로 업그레이드된 후 scheduling이 ``NotImplementedError: Cannot load "
"'pcpuset'`` 에러로 실패하는 때문입니다."

#: ../../<reno.sphinxext unmaintained/xena>:180 unmaintained/yoga>:622
msgid ""
"The `bug 1960401`_  is fixed which can cause invalid `BlockDeviceMappings` "
"to accumulate in the database. This prevented the respective volumes from "
"being attached again to the instance."
msgstr ""
"`bug 1960401`_ 은 `BlockDeviceMappings`가 데이터베이스에 쌓이게 되면 무效로 인해 volume이 다시 "
"인스턴스에 연결되지 않는 것을 방지하기 위해修正되었습니다."

#: ../../<reno.sphinxext stable/pike>:744
msgid ""
"The `discover_hosts_in_cells_interval` periodic task in the scheduler is now"
" more efficient in that it can specifically query unmapped compute nodes "
"from the cell databases instead of having to query them all and compare "
"against existing host mappings."
msgstr ""
"`discover_hosts_in_cells_interval` 스케줄러의 주기적 일정은 현재 더 효율적이게 작동합니다. 이는 세ลล "
"데이터베이스에서 비mapped 컴퓨터를เฉพาะ적으로 확인할 수 있기 때문입니다. 이전에는 모든 컴퓨터를 확인하고 존재하는 호스트 매핑과"
" 비교해야했습니다."

#: ../../<reno.sphinxext stable/queens>:144 stable/rocky>:294
#: stable/stein>:954
msgid ""
"The `long term solution`_ to these issues is to recalculate the XML on the "
"destination node. When this work is completed, the restriction on live "
"migration with NUMA topologies will be lifted."
msgstr ""
"`long term solution`_을 이 문제에 대한는 XML을 대상 노드에 재คำนว치하는 것이다. 이 작업이 완료되면, "
"NUMA拓도에 대한 live migration에 대한 제한은 해제된다."

#: ../../<reno.sphinxext stable/pike>:715
msgid ""
"The `nova-compute` worker can automatically disable itself in the service "
"database if consecutive build failures exceed a set threshold. The "
"``[compute]/consecutive_build_service_disable_threshold`` configuration "
"option allows setting the threshold for this behavior, or disabling it "
"entirely if desired. The intent is that an admin will examine the issue "
"before manually re-enabling the service, which will avoid that compute node "
"becoming a black hole build magnet."
msgstr ""
"`nova-compute` 노드가 서비스 데이터베이스에 tự động적으로 비활성화되도록 설정할 수 있다. 연속적인 빌드 실패가 설정된 "
"임계치를 초과할 때. `[compute]/consecutive_build_service_disable_threshold` 구성 옵션을 "
"사용하여 이 행동의 임계치를 설정하거나, 원하는 경우 전부 비활성화할 수 있다. 이 목적은 관리자가 수동으로 서비스를 재启동할 때, "
"컴퓨터 노드가 블랙홀 빌드 매그넘으로 변하는 것을 피하기 위해 이스UES를 검토하는 것이다."

#: ../../<reno.sphinxext stable/queens>:1588
msgid ""
"The `nova-manage cell` command has been deprecated. This command configures "
"various aspects of the Cells v1 functionality. Cells v1 has been deprecated,"
" thus, this command is also deprecated. It will be removed in its entirety "
"when Cells v1 is removed."
msgstr ""
"`nova-manage cell` 명령은弃용되었습니다. 이 명령은 셀스 v1 기능의 다양한 측면을 구성합니다. 셀스 v1도弃용되었습니다."
" 따라서, 이 명령도弃용되었습니다. 셀스 v1이弃용되면 이 명령은 전부 제거됩니다."

#: ../../<reno.sphinxext unmaintained/victoria>:217 unmaintained/wallaby>:931
msgid ""
"The `os-resetState`_ API will now log an instance action when called. The "
"resulting instance action being visable via the `os-instance-actions`_ API "
"to users and admins, resolving `bug 1911924`_."
msgstr ""
"`os-resetState` API는 현재 호출될 때 인스턴스 액션을 로그합니다. 결과적으로 인스턴스 액션은 `os-instance-"
"actions` API를 통해 사용자와 관리자에게 보이게 되고, `bug 1911924`를 해결합니다."

#: ../../<reno.sphinxext stable/queens>:165 stable/rocky>:252
#: stable/stein>:379 stable/train>:1316
msgid ""
"The `os-volume_attachments`_ update API, commonly referred to as the swap "
"volume API will now return a ``400`` (BadRequest) error when attempting to "
"swap from a multi attached volume with more than one active read/write "
"attachment resolving `bug #1775418`_."
msgstr ""
"`os-volume_attachments`_ 업데이트 API, 일반적으로 스위핑 볼륨 API라고도 알려진,는 현재 다중 연결 볼륨에서 "
"하나 이상의 활성 읽기/쓰기 연결을 가진 경우에 400 (BadRequest) 오류를 반환합니다. 이 오류는 bug #1775418_에 "
"대한 해결책을 제공합니다."

#: ../../<reno.sphinxext unmaintained/zed>:220
msgid ""
"The `unshelve` instance API action now provides a new `host` parameter with "
"2.91 microversion (for only admins)."
msgstr ""
"`unshelve` 인스턴스 API 액션은 now 2.91 마이크로 버전 (admin만) 이하의 `host` 매개 변수를 제공합니다."

#: ../../<reno.sphinxext stable/train>:725
msgid "The affected APIs are as follows:"
msgstr "다음과 같은 API가 영향을 받습니다."

#: ../../<reno.sphinxext unmaintained/zed>:539
msgid ""
"The algorithm that is used to see if a multi NUMA guest fits to a multi NUMA"
" host has been optimized to speed up the decision on hosts with high number "
"of NUMA nodes ( > 8). For details see `bug 1978372`_"
msgstr ""
"다중 NUMA 게스트가 다중 NUMA 호스트에 적합한지 확인하는 알고리즘은, 다중 NUMA 노드가 많은 호스트에 대한 결정을 빠르게 할 "
"수 있도록 최적화되었다. 8개 이상의 NUMA 노드가 있는 호스트에 대한詳細 정보는 `bug 1978372`_를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:770
msgid ""
"The amount of PCI Express ports (slots in virtual motherboard) can now be "
"configured using ``num_pcie_ports`` option in ``libvirt`` section of "
"``nova.conf`` file.  This affects x86-64 with ``hw_machine_type`` set to "
"'pc-q35' value and AArch64 instances of 'virt' ``hw_machine_type`` (which is"
" default for that architecture). Due to QEMU's memory map limits on "
"aarch64/virt maximum value is limited to 28."
msgstr ""
"PCI Express 포트의 수 (virtually Motherboard의 슬롯)가 now `num_pcie_ports` 옵션을 통해 "
"`libvirt` 섹션의 `nova.conf` 파일에서 구성할 수 있습니다.  이는 x86-64와 `hw_machine_type`를 "
"'pc-q35'로 설정하고 AArch64 인스턴스의 'virt' `hw_machine_type` (이 아키텍처의 디폴트입니다.)에 영향을"
" 미칩니다.  QEMU의 메모리 맵 제한은 aarch64/virt의 최대 값이 28으로 제한됩니다."

#: ../../<reno.sphinxext stable/pike>:1443
msgid ""
"The base ``3.0`` version is identical to v2 and it was introduced in the "
"Newton release of OpenStack. In case you need Nova to continue using the v2 "
"version you can point it towards that by setting the ``catalog_info`` option"
" in the ``nova.conf`` file under the ``cinder`` section, like::"
msgstr ""
"``3.0`` 기본 버전은 v2와 동일하며 OpenStack Newton 릴리스에서 도입되었다. Nova를 v2 버전을 계속 사용하고 "
"싶다면, `catalog_info` 옵션을 `nova.conf` 파일의 `cinder` 섹션에서 설정하여 해당 버전을 指할 수 있다."

#: ../../<reno.sphinxext stable/2023.2>:431 unmaintained/2023.1>:101
msgid ""
"The behavior has been changed to archive batches of complete parent + child "
"rows trees while limiting each batch when it has reached >= max_rows "
"records. This allows the size of the database transaction to be controlled "
"by the user and enables more rows to be archived per invocation of ``nova-"
"manage db archive_deleted_rows`` when there are a large number of foreign "
"key related records."
msgstr ""
"행동은 max_rows 레코드가 >= 인 경우 각 배치에서 완전한 부모 + 자식 레코드의 트리 집합을 아카이브 करन을 위해 "
"변경되었습니다. 이로 인해 사용자가 데이터베이스 트랜잭션의 크기를 제어할 수 있으며, foreign key와 관련된 레코드가 많은 경우 "
"nova-manage db archive_deleted_rows의 invocation에 따라 더 많은 레코드를 아카이브할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:475 stable/rocky>:2027
msgid ""
"The behaviour of ImagePropertiesFilter when using multiple architectures in "
"a cloud can be unpredictable for a user if they forget to set the "
"architecture property in their image.  Nova now allows the deployer to "
"specify a fallback in "
"``[filter_scheduler]image_properties_default_architecture`` to use a default"
" architecture if none is specified.  Without this, it is possible that a VM "
"would get scheduled on a compute node that does not support the image."
msgstr ""
"이미지 속성 필터(ImagePropertiesFilter)의 행동은 클라우드에서 여러 아키텍처를 사용할 때 사용자에게 예측이 불가할 수 "
"있습니다. 사용자가 이미지 속성 속성(architecture property)을 설정하지 않으면.  Nova는 now deployer가 "
"``[filter_scheduler]image_properties_default_architecture``를 사용하여 기본 아키텍처를 "
"사용할 수 있도록 fallback을 spécificar할 수 있습니다.  이 경우, none이 지정되지 않으면 VM은 compute "
"노드가 이미지를 지원하지 않는 경우에 scheduling에 subject가 될 수 있습니다."

#: ../../<reno.sphinxext stable/2025.2>:376
msgid ""
"The below service-to-service APIs policy rule default value ``role:admin or "
"role:service`` is deprecated and will be changed to ``role:service`` in "
"future release:"
msgstr ""
"아래 서비스 간 API 정책 규칙의 기본值 \"role:admin or role:service\"은弃용되고 미래 릴리스에서 "
"\"role:service\"로 변경될 예정입니다."

#: ../../<reno.sphinxext stable/train>:1183
msgid ""
"The block-storage (cinder) version 3.44 API is now required when working "
"with volume attachments. A check has been added to the ``nova-status upgrade"
" check`` command for this requirement."
msgstr ""
"ブロック 스토리지 (Cinder) 버전 3.44 API는 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할"
" 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 "
"액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now"
" 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 "
"함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now "
"필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께"
" 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. "
"볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 "
"now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 "
"액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now"
" 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다. 볼륨 액세서리와 "
"함께 작업할 때 now 필요합니다. 볼륨 액세서리와 함께 작업할 때 now 필요합니다."

#: ../../<reno.sphinxext stable/rocky>:220 stable/stein>:1302
msgid ""
"The bug fix applies to both the ``nova-osapi_compute`` and ``nova-compute`` "
"service so older compute services will need to be patched."
msgstr ""
"``nova-osapi_compute`` 및 ``nova-compute`` 서비스에 대한 버그修复이 적용되므로 older compute "
"서비스가 패치가 필요하다."

#: ../../<reno.sphinxext stable/stein>:576
msgid ""
"The cache can be cleared manually at any time by sending SIGHUP to the "
"compute process. This will cause the cache to be repopulated the next time "
"the data is accessed."
msgstr ""
"캐시를 수동으로 청소할 수 있습니다. SIGHUP를 컴퓨터 프로세스에 보냈을 때, 캐시는 다음 데이터를 접근할 때 다시 복원됩니다."

#: ../../<reno.sphinxext stable/pike>:1332
msgid ""
"The cells topic configuration option has been removed. Please make sure your"
" cells related message queue topic is 'cells'."
msgstr "세ลล스 주제 구성 옵션은 제거되었습니다. 세ลล스와 관련된 메시지 큐 주제가 '세ลล스' 인지 확인해 주세요."

#: ../../<reno.sphinxext stable/train>:1456
msgid ""
"The code for the `placement service <https://docs.openstack.org/placement>`_"
" was moved to its own `repository "
"<https://git.openstack.org/cgit/openstack/placement>`_ in Stein. The "
"placement code in nova has been deleted."
msgstr ""
"`placement service <https://docs.openstack.org/placement>`_의 코드는 Stein에서 자신의"
" `repository <https://git.openstack.org/cgit/openstack/placement>`_ 에서 "
"이동되었다. nova에서 placement 코드는 삭제되었다."

#: ../../<reno.sphinxext unmaintained/yoga>:430
msgid ""
"The code gracefully handles the lack of the capability since it is optional "
"or Libvirt may not support it in a particular release."
msgstr ""
"코드는 능력의 부족을 조용히 처리하며, 이 능력은 선택적이거나, 리버트는 특정 릴리즈에서 이 능력을 지원하지 않을 수 있기 때문이다."

#: ../../<reno.sphinxext unmaintained/2023.1>:394
msgid ""
"The compute manager now uses a local file to provide node uuid persistence "
"to guard against problems with renamed services, among other things. "
"Deployers wishing to ensure that *new* compute services get a predicatble "
"uuid before initial startup may provision that file and nova will use it, "
"otherwise nova will generate and write one to a `compute_id` file in "
"`CONF.state_path` the first time it starts up. Accidental renames of a "
"compute node's hostname will be detected and the manager will exit to avoid "
"database corruption. Note that none of this applies to Ironic computes, as "
"they manage nodes and uuids differently."
msgstr ""
"compute manager는 서비스 이름이 변경되는 문제를 방지하기 위해 노드 UUID persistence를 제공하기 위해 로컬 "
"파일을 사용합니다. 다른 것과 함께. *new* compute 서비스가 초기 시작 전 예측 가능한 UUID를 얻을 수 있도록 "
"wishful deployers가 파일을 프로비전하고 nova가 사용할 수 있습니다. 그 외에 nova는 `compute_id` 파일에 "
"`CONF.state_path`에 첫 번째 시작 시에 하나를 생성하고 쓰게 됩니다. compute 노드의 호스트 이름이 부정적으로 "
"변경되면 관리자는 데이터베이스 오류를 피하기 위해.exit합니다. Ironic computes는 노드와 UUID을 다르게 관리하므로 이 "
"모든 것이 적용되지 않습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1344
msgid ""
"The concept that ``service manager`` were replaceable components was "
"deprecated in Mitaka, so following config options are removed."
msgstr ""
"``service manager``이 교체 가능한 구성 요소였던 개념이 미타카에서 비상시되었기 때문에 다음 구성 옵션들이 제거되었다."

#: ../../<reno.sphinxext origin/stable/ocata>:1425
msgid ""
"The config options ``multi_instance_display_name_template`` and "
"``null_kernel`` in the ``DEFAULT`` group are now deprecated and may be "
"removed as early as the 16.0.0 release. These options are deprecated to keep"
" API behaviour consistent across deployments."
msgstr ""
"`multi_instance_display_name_template` 및 `null_kernel` 속성은 `DEFAULT` 그룹에 있는 "
"`multi_instance_display_name_template` 및 `null_kernel` 속성은 현재 비고화되었습니다. 이 "
"속성은 API 행동을 배포에 대한 일관성을 유지하기 위해 비고화되었습니다."

#: ../../<reno.sphinxext stable/stein>:570
msgid ""
"The configuration option ``[compute]resource_provider_association_refresh`` "
"can now be set to zero to disable refresh entirely. This follows on from "
"`bug 1767309`_ allowing more aggressive reduction in the amount of traffic "
"to the placement service."
msgstr ""
"`[compute]resource_provider_association_refresh` 옵션은 이제 0으로 설정할 수 있습니다. 이는 "
"`bug 1767309`_에 의해 허용되는 `placement service`에 대한 트래픽의 양을 더 강력하게 줄이는 것을 허용합니다."

#: ../../<reno.sphinxext stable/stein>:449
msgid ""
"The configuration option ``[compute]resource_provider_association_refresh`` "
"can now be set to zero to disable refresh entirely. This should be useful "
"for large-scale deployments."
msgstr ""
"`[compute]resource_provider_association_refresh` 옵션은 이제 0으로 설정될 수 있으며, 이는 "
"refresh를 완전히 비활성화하는 것을 의미합니다. 이는 대규모 배포에 유용한 기능입니다."

#: ../../<reno.sphinxext stable/pike>:1494 stable/queens>:1519
msgid ""
"The configuration options ``baremetal_enabled_filters`` and "
"``use_baremetal_filters`` are deprecated in Pike and should only be used if "
"your deployment still contains nodes that have not had their resource_class "
"attribute set. See `Ironic release notes "
"<https://docs.openstack.org/releasenotes/ironic/>`_ for upgrade concerns."
msgstr ""
"`baremetal_enabled_filters` 및 `use_baremetal_filters`를 사용하는 것은 Pike에서弃용되었으며,"
" 배포가 여전히 리소스 클래스 속성에 대한 설정이 없는 노드가 존재하는 경우만 사용해야 합니다.  upgrade에 대한 우려 사항은 "
"`Ironic release notes <https://docs.openstack.org/releasenotes/ironic/>`_를 "
"참조하십시오."

#: ../../<reno.sphinxext stable/stein>:560
msgid ""
"The configured maximum is enforced during server create, rebuild, evacuate, "
"unshelve, live migrate, and attach volume. When the maximum is exceeded "
"during server create, rebuild, evacuate, unshelve, or live migrate, the "
"server will go into ``ERROR`` state and the server fault message will "
"indicate the failure reason. When the maximum is exceeded during a server "
"attach volume API operation, the request will fail with a ``403 "
"HTTPForbidden`` error."
msgstr ""
"구성된 최대는 서버 생성, 재건, 추출, 라이브 마이그레이션, 및 볼륨 연결 시 enforce됩니다. 최대가 초과되면 서버 생성, 재건,"
" 추출, 라이브 마이그레이션, 또는 볼륨 연결 API 연산 시, 서버는 \"ERROR\" 상태에 들어가고, 서버 오류 메시지는 실패 "
"이유를 나타냅니다. 최대가 초과되면 볼륨 연결 API 연산 시, 요청은 \"403 HTTPForbidden\" 오류로 실패합니다."

#: ../../<reno.sphinxext stable/stein>:841
msgid ""
"The configured maximum is not enforced on shelved offloaded servers, as they"
" have no compute host."
msgstr "구성된 최대는 보관된 오프로드드 서버에 적용되지 않으며, 그들은 컴퓨터 호스트가 없기 때문이다."

#: ../../<reno.sphinxext unmaintained/xena>:548
msgid ""
"The database migration engine has changed from `sqlalchemy-migrate`__ to "
"`alembic`__. For most deployments, this should have minimal to no impact and"
" the switch should be mostly transparent. The main user-facing impact is the"
" change in schema versioning. While sqlalchemy-migrate used a linear, "
"integer-based versioning scheme, which required placeholder migrations to "
"allow for potential migration backports, alembic uses a distributed version "
"control-like schema where a migration's ancestor is encoded in the file and "
"branches are possible. The alembic migration files therefore use a arbitrary"
" UUID-like naming scheme and the ``nova-manage db sync`` and ``nova-manage "
"api_db sync`` commands now expect such an version when manually specifying "
"the version that should be applied. For example::"
msgstr ""
"데이터베이스 이민지 엔진은 `sqlalchemy-migrate`__에서 `alembic`__으로 변경되었다. 대부분의 배포에서 이 변경은"
" 최소한의 영향을 미치고 전환은 mostly 투명할 것이다. 가장 사용자-facing한 영향을는 스키마 버전의 변경이다. "
"sqlalchemy-migrate는 선형, 정수 기반 버전화 체계를 사용했으며, 이민지의 복원으로부터 가능성 있는 이민지의 경우에 "
"placeholder 이민지를 사용해야 했다. 반면 alembic는 분산 버전 관리와 같은 스키마를 사용한다. 이민지의 조상은 파일에 "
"인코딩되어 있으며 branches가 가능하다. 따라서 alembic 이민지 파일은 무작위 UUID-like 이름의 스키마를 사용하고, "
"`nova-manage db sync` 및 `nova-manage api_db sync` 명령은 수동으로 적용할 버전을 명시할 때 이러한"
" 버전을 예상해야 한다. 예를 들어:"

#: ../../<reno.sphinxext stable/stein>:933
msgid ""
"The defalut value for policy rule "
"``os_compute_api:servers:create:zero_disk_flavor`` has changed from "
"``rule:admin_or_owner`` to ``rule:admin_api`` which means that by default, "
"users without the admin role will not be allowed to create servers using a "
"flavor with ``disk=0`` *unless* they are creating a volume-backed server. If"
" you have these kinds of flavors, you may need to take action or temporarily"
" override the policy rule. Refer to `bug 1739646 "
"<https://launchpad.net/bugs/1739646>`_ for more details."
msgstr ""
"``os_compute_api:servers:create:zero_disk_flavor`` 정책 규칙의 기본 giá trị는 "
"``rule:admin_or_owner``에서 ``rule:admin_api``로 변경되었습니다. 이는 사용자가 admin 역할을 가지고"
" 있지 않아도 ``disk=0``의 플레버를 사용하여 서버를 생성할 수 없다는 것을 의미합니다. *만약* 사용자는 볼륨-backed "
"서버를 생성하는 경우, 사용자들은 이러한 플레버를 사용할 수 있습니다. 이러한 플레버가 있는 경우, 사용자는 acción을 취하거나 "
"임시적으로 정책 규칙을 오버라이드해야 할 수 있습니다. 더 많은 정보를 얻으려면 `bug 1739646 "
"<https://launchpad.net/bugs/1739646>`_을 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:880
msgid ""
"The default QEMU machine type for ARMv7 architecture is now changed to "
"``virt`` (from the older ``vexpress-a15``, which is a particular ARM "
"development board).  The ``virt`` board is the recommended default for "
"ARMv7, which is explicitly designed to be used with virtual machines.  It is"
" more flexible, supports PCI and 'virtio' devices, has decent RAM limits, "
"and so forth.  For pre-existing Nova guests on ARMv7 to acquire the ``virt``"
" machine type: (a) upgrade Nova with this fix; (b) explicitly start and stop"
" the guests, then they will pick up the 'virt' machine type."
msgstr ""
"ARMv7 아키텍처의 기본 QEMU मशीन 타입은 현재 ``virt`` (이전 ``vexpress-a15``에서 사용되는 고유의 ARM"
" 개발 보드)로 변경되었다.  ``virt`` 보드는 ARMv7의 권장된 기본으로, 가상 머신과 함께 사용할 수 있는 것으로 명시적으로 "
"설계되었다.  더 flexibility가 있으며 PCI 및 'virtio' 장치에 지원을 제공하며, RAM 제한이 적절하고 같은 것과 "
"같은 것.  ARMv7의 이전에 존재하는 노바 게스트가 ``virt`` मशीन 타입을 얻으려면: (a) 노바를 이修을 위해 업그레이드;"
" (b) 명시적으로 게스트를 시작하고 중단하면, 'virt' मशीन 타입을 선택할 것이다."

#: ../../<reno.sphinxext unmaintained/zed>:441
msgid ""
"The default ``api-paste.ini`` file has been updated and now the Metadata API"
" pipeline includes the ``HTTPProxyToWSGI`` middleware."
msgstr ""
"기본 `api-paste.ini` 파일이 업데이트되어 현재 메타데이터 API 파이프라인에 `HTTPProxyToWSGI` 미들웨어가 "
"포함되어 있습니다."

#: ../../<reno.sphinxext stable/rocky>:14 stable/stein>:14 stable/train>:155
#: stable/ussuri>:178 unmaintained/victoria>:579
msgid ""
"The default for ``[glance] num_retries`` has changed from ``0`` to ``3``. "
"The option controls how many times to retry a Glance API call in response to"
" a HTTP connection failure. When deploying Glance behind HAproxy it is "
"possible for a response to arrive just after the HAproxy idle time. As a "
"result, an exception will be raised when the connection is closed resulting "
"in a failed request. By increasing the default value, Nova can be more "
"resilient to this scenario were HAproxy is misconfigured by retrying the "
"request."
msgstr ""
"``[glance] num_retries``의 기본값이 ``0``에서 ``3``로 변경되었다. 이 옵션은 HTTP 연결 실패에 반응하여 "
"Glance API 호출을 몇 번 retry할지 제어한다. HAproxy 뒤에 Glance를 배포할 때, HAproxy의 비대기 시간이 "
"끝나기 전에 응답이 도착할 수 있다. 이로 인해 연결이 닫히면 예외가 발생하고 요청이 실패한다. 기본값을 증가시켜야 하기 때문에 "
"HAproxy가 잘못 구성된 경우에 Nova는 이 상황에 대처할 수 있다. 요청을 다시 시도하여 resilient해진다."

#: ../../<reno.sphinxext unmaintained/2023.1>:571
msgid ""
"The default initial allocation ratios enabled ram over commit by default "
"with a factor of ``1.5``. This value was chosen early in nova's history as "
"the predominant workload was web hosting or other light weight "
"virtualization. Similarly the default initial cpu allocation ratio defaulted"
" to 16. As more demanding workload from telco, enterprise, scientific and "
"governmental users became the norm the initial values we had chosen became "
"less and less correct overtime. These have now been updated to reflect a "
"more reasonable default for the majority of our users. As of this release "
"the initial ram allocation value is 1.0 disabling overcommit by default for "
"new compute nodes and the initial cpu allocation ratio is now 4.0 which is a"
" more reasonable overcommit for non idle workloads."
msgstr ""
"default initial allocation ratios가 ram over commit을 기본적으로 활성화하고, 1.5의 가중치로 "
"설정되었다. 이 값은 nova의 역사에서 초기에 선택된 것으로, 웹 호스팅 또는 다른 हल부 가중 virtualization의 주류 로드"
" 로드가-dominated했다. 비슷하게, default initial cpu allocation ratio는 16로 기본적으로 "
"설정되었다. 그러나 telco, enterprise, 과학적 및 정부 사용자에서 비즈니스 로드가 증가하는 norm이 norm이되면서, "
"초기에 선택된 가중치가 점점 더 적절하지 못해졌다. 이러한 가중치를 현재 사용자 대부분을 반영하는 적절한 기본 가중치를 반영하기 위해 "
"업데이트되었다. 이 릴리스에서, 초기 ram allocation value는 1.0으로, new compute nodes에 대해 "
"overcommit을 비활성화하고, initial cpu allocation ratio는 4.0으로, non idle workloads에"
" 대해 적절한 overcommit이되도록 설정되었다."

#: ../../<reno.sphinxext stable/pike>:137 stable/queens>:1269
msgid ""
"The default list of non-inherited image properties to pop when creating a "
"snapshot has been extended to include image signature properties. The "
"properties ``img_signature_hash_method``, ``img_signature``, "
"``img_signature_key_type`` and ``img_signature_certificate_uuid`` are no "
"longer inherited by the snapshot image as they would otherwise result in a "
"Glance attempting to verify the snapshot image with the signature of the "
"original."
msgstr ""
"기본적인 상속되지 않은 이미지 속성의 목록을 생성할 때 스냅샘을 생성할 때 팝업하는 속성 목록이 확장되어 이미지 서명 속성도 "
"포함되었습니다. 속성 ``img_signature_hash_method``, ``img_signature``, "
"``img_signature_key_type`` 및 ``img_signature_certificate_uuid``는 스냅샘 이미지에 "
"상속되지 않게되는데, 그 이유는 원본 이미지의 서명과 스냅샘 이미지의 서명을 일치시키려고 Glance가 시도하는 것을 피하기 때문입니다."

#: ../../<reno.sphinxext stable/pike>:1281
msgid ""
"The default policy for os_compute_api:os-quota-sets:detail has been changed "
"to permit listing of quotas with details to project users, not only to "
"admins."
msgstr ""
"os_compute_api:os-quota-sets:detail의 기본 정책이 프로젝트 사용자에게 only admins와 함께 "
"쿼타의รายละเอียด을 listing करन을 허용하는 것을 허용한다."

#: ../../<reno.sphinxext stable/pike>:1226
msgid ""
"The default policy on os-server-tags has been changed from ``RULE_ANY`` "
"(allow all) to ``RULE_ADMIN_OR_OWNER``. This is because server tags should "
"only be manipulated on servers owned by the user or admin. This doesn't have"
" any affect on how the API works."
msgstr ""
"os-server-tags에 대한 기본 정책은 ``RULE_ANY`` (모두 허용)에서 ``RULE_ADMIN_OR_OWNER`` 로 "
"변경되었다. 이 이유는 사용자 또는 관리자가 소유하는 서버에서만 서버 태그를 조정해야 할 때문이다. 이 변경은 API의 작동 방식에 "
"영향을 미치지 않는다."

#: ../../<reno.sphinxext unmaintained/zed>:226
msgid ""
"The default system scope is removed from all APIs hence finishing to "
"implement `phase #1 of new RBAC guidelines`__ that are opt-in."
msgstr ""
"기본 시스템 스코프가 모든 API에서 제거되며, `새로운 RBAC 지침의 Phase #1`을 구현하는 것을 완료합니다. (기본 시스템 스코프 제거) \n"
"\n"
"기본 시스템 스코프 제거는 모든 API에서 제거되며, 새로운 RBAC 지침 Phase #1을 구현하는 것을 완료합니다."

#: ../../<reno.sphinxext stable/stein>:892
msgid ""
"The default value for the \"cpu_allocation_ratio\", \"ram_allocation_ratio\""
" and \"disk_allocation_ratio\" configurations have been changed to ``None``."
msgstr ""
"default value for \"cpu_allocation_ratio\", \"ram_allocation_ratio\" and "
"\"disk_allocation_ratio\" configurations have been changed to ``None``."

#: ../../<reno.sphinxext stable/stein>:1033
msgid ""
"The default value for the ``[compute]/live_migration_wait_for_vif_plug`` "
"configuration option has been changed to True. As noted in the help text for"
" the option, some networking backends will not work with this set to True, "
"although OVS and linuxbridge will."
msgstr ""
"``[compute]/live_migration_wait_for_vif_plug`` 설정 옵션의 디폴트 값은 True로 변경되었다. "
"옵션의 도움말 텍스트에 noted한 것과 같이, 이 설정을 True로 설정하면 일부 네트워킹 백엔드가 작동하지 않을 수 있지만 OVS와 "
"linuxbridge는 작동한다."

#: ../../<reno.sphinxext origin/stable/ocata>:1192 stable/pike>:1116
msgid ""
"The default value for the ``[xenserver]/vif_driver`` configuration option "
"has been changed to ``nova.virt.xenapi.vif.XenAPIOpenVswitchDriver`` to "
"match the default configuration of ``[DEFAULT]/use_neutron=True``."
msgstr ""
"[xenserver]/vif_driver의 기본값이 [DEFAULT]/use_neutron=True의 기본값과 일치하기 위해 "
"nova.virt.xenapi.vif.XenAPIOpenVswitchDriver로 변경되었습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:550
msgid ""
"The default value of ``[oslo_policy] policy_file`` config option has been "
"changed from ``policy.json`` to ``policy.yaml``. Nova policy new defaults "
"since 21.0.0 and current default value of ``[oslo_policy] policy_file`` "
"config option (``policy.json``) does not work when ``policy.json`` is "
"generated by `oslopolicy-sample-generator "
"<https://docs.openstack.org/oslo.policy/latest/cli/oslopolicy-sample-"
"generator.html>`_  tool. Refer to `bug 1875418 "
"<https://bugs.launchpad.net/nova/+bug/1875418>`_ for more details. Also "
"check `oslopolicy-convert-json-to-yaml "
"<https://docs.openstack.org/oslo.policy/latest/cli/oslopolicy-convert-json-"
"to-yaml.html>`_ tool to convert the JSON to YAML formatted policy file in "
"backward compatible way."
msgstr ""
"`[oslo_policy] policy_file` 구성 옵션의 기본값이 `policy.json`에서 `policy.yaml`로 "
"변경되었습니다. Nova policy는 21.0.0부터 새로운 기본값을 가지고 있으며, `policy.json`이 `oslopolicy-"
"sample-generator` 도구에 의해 생성된 경우 `policy.json`의 기본값은 작동하지 않습니다. 더 많은 정보는 `bug"
" 1875418`에 대한 참조를 통해 얻을 수 있습니다. 또한, JSON을 YAML 형식으로 formatted policy 파일을 "
"backwardly compatible한 방식으로 변환하는 `oslopolicy-convert-json-to-yaml` 도구를 "
"확인하십시오."

#: ../../<reno.sphinxext stable/pike>:1230
msgid ""
"The default value of the ``[DEFAULT]/firewall_driver`` configuration option "
"has been changed to ``nova.virt.firewall.NoopFirewallDriver`` to coincide "
"with the default value of ``[DEFAULT]/use_neutron=True``."
msgstr ""
"``[DEFAULT]/firewall_driver`` 설정 옵션의 기본값이 "
"``nova.virt.firewall.NoopFirewallDriver``로 변경되었습니다. 이 변경은 "
"``[DEFAULT]/use_neutron=True``의 기본값과 일치합니다."

#: ../../<reno.sphinxext stable/rocky>:1772
msgid ""
"The default value of the configuration attribute ``[libvirt]/rng_dev_path`` "
"is now set to ``/dev/urandom``.  Refer to the documentation of "
"``rng_dev_path`` for details."
msgstr ""
"default configuration attribute의 기본값은 now /dev/urandom으로 설정되었습니다. "
"rng_dev_path의 설명을 참조하십시오."

#: ../../<reno.sphinxext stable/pike>:1289
msgid ""
"The deprecated /os-cloudpipe API endpoint has been removed. Whenever calls "
"are made to that endpoint it now returns a 410 response."
msgstr ""
"Deprecated /os-cloudpipe API endpoint가 제거되었습니다. 그 API 엔드포인트에 호출을 시도할 때는 now "
"410 응답을 반환합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1295
msgid "The deprecated S3 image backend has been removed."
msgstr "Deprecated S3 image backend가 제거되었습니다."

#: ../../<reno.sphinxext stable/train>:523
msgid ""
"The deprecated ``Cells V1`` feature (not to be confused with `Cells V2`_) "
"has been removed."
msgstr "Deprecated ``Cells V1`` 기능 (Cells V2와 혼동하지 마세요)은 제거되었습니다."

#: ../../<reno.sphinxext stable/2024.1>:534
msgid "The deprecated ``[api] use_forwarded_for`` option has been removed."
msgstr "Deprecated `[API] use_forwarded_for` 옵션은 제거되었습니다."

#: ../../<reno.sphinxext stable/2024.1>:530
msgid "The deprecated ``[upgrade_levels] cert`` option has been removed."
msgstr "Deprecated ``[upgrade_levels] cert`` 옵션은 제거되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1324
msgid "The deprecated ``cert_topic`` configuration option has been removed."
msgstr "Deprecated ``cert_topic`` 구성 옵션은 제거되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1379
msgid ""
"The deprecated compute config option ``snapshot_name_template`` has been "
"removed. It is not used anywhere and has no effect on any code, so there is "
"no impact."
msgstr ""
"Deprecated compute config 옵션 `snapshot_name_template`은 사용되지 않으며 어떤 코드에도 영향을 "
"미치지 않기 때문에 영향을 받지 않는다."

#: ../../<reno.sphinxext origin/stable/ocata>:1385
msgid ""
"The deprecated config option ``compute_available_monitors`` has been removed"
" from the ``DEFAULT`` config section. Use setuptools entry points to list "
"available monitor plugins."
msgstr ""
"구성 옵션 `compute_available_monitors`은 `DEFAULT` 구성 섹션에서 제거되었다. "
"`compute_available_monitors` 옵션은 사용 가능한 모니터 플러그인 목록을 liệt어내는 `setuptools` "
"엔트리 포인트를 사용하여 사용할 수 없다."

#: ../../<reno.sphinxext stable/pike>:1285
msgid ""
"The deprecated nova cert daemon is now removed. The /os-certificates API "
"endpoint that depended on this service now returns 410 whenever it is "
"called."
msgstr ""
"Deprecated nova cert daemon이 이제는 제거되었습니다. 이 서비스에 의존하는 /os-certificates API "
"endpoint가 호출될 때마다 410을 반환합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1541
msgid ""
"The direct use of the encryption provider classes such as "
"nova.volume.encryptors.luks.LuksEncryptor is now deprecated and will be "
"blocked in the Pike release of Nova. The use of out of tree encryption "
"provider classes is also deprecated and will be blocked in the Pike release "
"of Nova."
msgstr ""
"nova.volume.encryptors.luks.LuksEncryptor과 같은 암호화 제공자 클래스의 직접적인 사용은 현재 "
"deprecated되어 Pike 릴리스의 Nova에서 블록 될 예정입니다.  나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 "
"경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의"
" 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, "
"나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우,"
" 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 "
"경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의"
" 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, "
"나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우,"
" 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 "
"경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의"
" 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, "
"나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우, 나의 경우"

#: ../../<reno.sphinxext stable/pike>:1948
msgid ""
"The disabled ``os-pci`` API has been removed. This API was originally added "
"to the v3 API which over time finally became the v2.1 API and the initial "
"microversion is backward compatible with the v2.0 API, where the ``os-pci`` "
"extension did not exist. The ``os-pci`` API was never enabled as a "
"microversion in the v2.1 API and at this time no longer aligns with Nova "
"strategically and is therefore just technical debt, so it has been removed. "
"Since it was never enabled or exposed out of the compute REST API endpoint "
"there was no deprecation period for this."
msgstr ""
"``os-pci`` API가 제거되었다. 이 API는 원래 v3 API에 추가되었으며, 시간이 지남에 따라 finally v2.1 "
"API가 되었다. 이 microversion의 초기 버전은 v2.0 API와 backward compatible하다. 그러나 ``os-"
"pci`` 확장 기능이 존재하지 않기 때문에. ``os-pci`` API는 v2.1 API에서 ever microversion으로 "
"활성화되지 않았으며, 현재 Nova strategically와 일치하지 않기 때문에 기술 부담이 되고 therefore 제거되었다. 이 "
"API는 ever compute REST API endpoint의 outside에서 활성화되거나 노출되지 않았기 때문에 이에 대한 "
"비활성화 시기가 없었다."

#: ../../<reno.sphinxext unmaintained/wallaby>:577
msgid ""
"The dnspython 2.0.0 package is incompatible with even the latest eventlet "
"package version. This makes nova-novncproxy service to fail if the version "
"of the dnspython package is equal or greater than 2.0.0. See `eventlet issue"
" 619`_ for more details"
msgstr ""
"dnspython 2.0.0 패키지는 가장 mới한 eventlet 패키지 버전과도 충돌한다. 이로 인해 nova-novncproxy "
"서비스가 dnspython 패키지 버전이 2.0.0 이상이면 실패한다. 더 많은 정보는 `eventlet issue 619`_에서 확인할"
" 수 있다."

#: ../../<reno.sphinxext stable/queens>:862
msgid ""
"The embedded flavor description will not be included in server "
"representations."
msgstr ""
"집합 {aggregate} \n"
"\n"
"서버 표현에서 집합 {aggregate}를 포함하지 않습니다."

#: ../../<reno.sphinxext stable/2024.1>:415
msgid ""
"The enforcement is implemented via a timer mechanism, initiating when users "
"access the console and concluding upon the expiration of the set console "
"token."
msgstr "집합은 타이머 메커니즘을 통해 구현되며, 사용자가 콘솔에 접근할 때 시작되고, 설정된 콘솔 토큰의 만료 시에 종료됩니다."

#: ../../<reno.sphinxext unmaintained/xena>:588
msgid ""
"The existing config options in the ``[devices]`` group for managing virtual "
"GPUs are now renamed in order to be more generic since the mediated devices "
"framework from the linux kernel can support other devices:"
msgstr ""
"existing config options in the ``[devices]`` group for managing virtual GPUs"
" are now renamed in order to be more generic since the mediated devices "
"framework from the linux kernel can support other devices:"

#: ../../<reno.sphinxext origin/stable/ocata>:25 stable/pike>:25
#: stable/queens>:95 stable/rocky>:187 stable/stein>:333 stable/train>:1279
msgid ""
"The fault ``details``, which are only exposed to users with the admin role, "
"will continue to include the traceback and also include the exception value "
"which for non-nova exceptions is what used to be exposed in the fault "
"``message`` field. Meaning, the information that admins could see for server"
" faults is still available, but the exception value may be in ``details`` "
"rather than ``message`` now."
msgstr ""
"fault `details` , which are only exposed to users with the admin role, will "
"continue to include the traceback and also include the exception value which"
" for non-nova exceptions is what used to be exposed in the fault `message` "
"field. Meaning, the information that admins could see for server faults is "
"still available, but the exception value may be in `details` rather than "
"`message` now."

#: ../../<reno.sphinxext stable/stein>:489
msgid ""
"The field ``instance_name`` has been added to the ``InstanceCreatePayload`` "
"in the following versioned notifications:"
msgstr "``인스턴스 이름``은 다음 버전의 알림에서 ``InstanceCreatePayload``에 추가되었다."

#: ../../<reno.sphinxext stable/pike>:657
msgid ""
"The fields ``locked`` and ``display_description`` have been added to "
"InstancePayload. Versioned notifications for instance actions will include "
"these fields."
msgstr ""
"``locked`` 및 ``display_description`` 필드는 InstancePayload에 추가되었다. 버전화된 알림은 "
"인스턴스 액션에 대한 알림을 포함할 것이다."

#: ../../<reno.sphinxext unmaintained/wallaby>:819
msgid ""
"The fields have been removed as the information they provided was frequently"
" misleading or outright wrong, and more accurate information can now be "
"queried from placement."
msgstr ""
"집합은 제공한 정보가 자주 오류가 있거나 완전히 잘못된 정보를 제공했기 때문에 제거되었습니다. 현재 배치에서 더 chính確한 정보를 "
"querying할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1073
msgid ""
"The filter and sort query parameters for server list API are now limited "
"according to whitelists. The whitelists are different for admin and non-"
"admin users."
msgstr ""
"서버 목록 API의 필터 및 정렬 쿼리 파라미터는 현재 하위 목록에 따라 제한되었습니다. 하위 목록은 관리자와 비 관리자 사용자에 따라 "
"다르며, 관리자는 비 관리자 사용자보다 더 많은 하위 목록을 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1910
msgid ""
"The filter scheduler will now attempt to claim a number of resources in the "
"placement API after determining a list of potential hosts. We attempt to "
"claim these resources for each instance in the build request, and if a claim"
" does not succeed, we try this claim against the next potential host the "
"scheduler selected. This claim retry process can potentially attempt claims "
"against a large number of hosts, and we do not limit the number of hosts to "
"attempt claims against. Claims for resources may fail due to another "
"scheduler process concurrently claiming resources against the same compute "
"node. This concurrent resource claim is normal and the retry of a claim "
"request should be unusual but harmless."
msgstr ""
"필터 스케줄러는 현재 호스트 목록을 xác định한 후 배치 API에서 자원들을 claim할 것이다. 각 인스턴스에 대해 claim을 "
"시도하고 claim이 성공하지 않으면 다음 potential host를 스케줄러가 선택한 다음 claim을 시도한다. claim "
"retry 프로세스는 potencially claim을 여러 호스트에 시도할 수 있으며 claim을 시도하는 호스트의 수를 제한하지 "
"않는다. claim은 자원 claim에 대해 failure할 수 있으며 다른 스케줄러가 same compute node에 resource"
" claim을 시도하는 경우에 failure할 수 있다. 이 concurrent resource claim은 normal이며 claim "
"request retry가 unusual but harmless하다."

#: ../../<reno.sphinxext ../source/newton.rst:34 origin/stable/ocata>:282
#: stable/pike>:410 stable/queens>:1681
msgid ""
"The fix for `OSSA-2017-005`_ (CVE-2017-16239) was too far-reaching in that "
"rebuilds can now fail based on scheduling filters that should not apply to "
"rebuild. For example, a rebuild of an instance on a disabled compute host "
"could fail whereas it would not before the fix for CVE-2017-16239. "
"Similarly, rebuilding an instance on a host that is at capacity for vcpu, "
"memory or disk could fail since the scheduler filters would treat it as a "
"new build request even though the rebuild is not claiming *new* resources."
msgstr ""
"`OSSA-2017-005`_ (CVE-2017-16239) 문제를แก는 방법은 재건이 스케줄 필터에 따라 실패할 수 있게 된 것이었다."
" 예를 들어, 컴퓨터 호스트가 비활성화된 경우 인스턴스 재건이 실패할 수 있지만 이전에 CVE-2017-16239 문제를แก인 후는 "
"ऐस한 경우가 없기 때문이다. 비슷하게, vcpu, 메모리 또는 디스크가 부족한 호스트에 인스턴스를 재건하면, 스케줄러 필터가 새로운 "
"빌드 요청으로 간주하여 재건이 새로운 리소스를 claim하지 않더라도 실패할 수 있다."

#: ../../<reno.sphinxext stable/pike>:397 stable/queens>:1653
msgid ""
"The fix for errata in `OSSA-2017-005`_ (CVE-2017-16239) will need to be "
"applied in addition to this fix."
msgstr "`OSSA-2017-005`_ (CVE-2017-16239) 에 대한 오류에 대한修复을 추가로 적용해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:341 stable/pike>:460
#: stable/queens>:1636
msgid "The fix is in the `nova-api` and `nova-conductor` services."
msgstr "`nova-api` 및 `nova-conductor` 서비스에.fix이 있습니다."

#: ../../<reno.sphinxext stable/pike>:395 stable/queens>:1651
msgid "The fix is in the `nova-api` and `nova-scheduler` services."
msgstr "`nova-api` 및 `nova-scheduler` 서비스에 위치한修复이 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:38 stable/2024.2>:38 stable/2025.1>:26
#: stable/2025.2>:511
msgid "The fix optimizes uptime retrieval by:"
msgstr "집합을 사용하여 uptime 수집을 최적화합니다."

#: ../../<reno.sphinxext ../source/newton.rst:45 origin/stable/ocata>:293
#: stable/pike>:421 stable/queens>:1692
msgid ""
"The fix relies on a ``RUN_ON_REBUILD`` variable which is checked for all "
"scheduler filters during a rebuild. The reasoning behind the value for that "
"variable depends on each filter. If you have out-of-tree scheduler filters, "
"you will likely need to assess whether or not they need to override the "
"default value (False) for the new variable."
msgstr ""
"``RUN_ON_REBUILD`` 변수에 의존하여修复이 이루어집니다. 이 변수는 모든 스케줄러 필터에 대해 재건 시에 확인됩니다. 이 "
"변수의 값은 각 필터에 따라 달라집니다. 스케줄러 필터가外部 트리에서 제공되는 경우, 새로운 변수의 디폴트 값 (False)으로 "
"오버라이드해야 하는지 여부를 평가해야 할 가능성이 있습니다."

#: ../../<reno.sphinxext stable/pike>:1662
msgid ""
"The following APIs which are considered as proxies of Neutron networking "
"API, are deprecated and will result in a 404 error response in microversion "
"`2.44`::"
msgstr ""
"다음 API는 네트워크 API의 프록시로 간주되는 API이며, 버전 `2.44`의 마이크로 버전에서 404 오류를 발생시킬 수 있습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:326
msgid ""
"The following SPICE-related options are added to the ``spice`` configuration"
" group of a Nova configuration:"
msgstr "다음 SPICE 관련 옵션들이 Nova 구성의 ``spice`` 구성 그룹에 추가됩니다."

#: ../../<reno.sphinxext stable/queens>:1421 stable/ussuri>:903
msgid "The following ``nova-manage`` commands have been removed."
msgstr "다음의 `nova-manage` 명령은 제거되었습니다."

#: ../../<reno.sphinxext stable/2025.2>:348
msgid ""
"The following aliases for ``[api]`` options have been removed. They have "
"been deprecated since the 15.0.0 (Ocata) release."
msgstr "다음과 같은 API 옵션의 별명이 제거되었다. 15.0.0 (Ocata) 릴리스부터는 deprecated되었다."

#: ../../<reno.sphinxext stable/rocky>:1398
msgid ""
"The following are the short names and description of the plugins which they "
"represent:"
msgstr ""
"다음은 플러그인에 대한 짧은 이름과 설명입니다.\n"
"\n"
"(Plugs-in에 대한 짧은 이름과 설명입니다.)"

#: ../../<reno.sphinxext stable/queens>:1374
msgid ""
"The following commands are no longer required to be listed in your rootwrap "
"configuration: blkid; blockdev; cat; chown; cryptsetup; dd; ebrctl; ifc_ctl;"
" kpartx; losetup; lvcreate; lvremove; lvs; mkdir; mm-ctl; mount; nova-"
"idmapshift; parted; ploop; prl_disk_tool; qemu-nbd; readlink; shred; tee; "
"touch; umount; vgs; vrouter-port-control; and xend."
msgstr ""
"다음 명령어는 루트 wraps 구성에서 더 이상 표시되지 않습니다. blkid; 블록 디스크; cat; chown; 크립트 셋업; dd;"
" ebrctl; ifc_ctl; kpartx; losetup; lvcreate; lvremove; lvs; mkdir; mm-ctl; "
"mount; nova-idmapshift; parted; ploop; prl_disk_tool; qemu-nbd; readlink; "
"shred; tee; touch; umount; vgs; vrouter-port-control; 및 xend."

#: ../../<reno.sphinxext stable/rocky>:1606
msgid ""
"The following commands are no longer required to be listed in your rootwrap "
"configuration: e2fsck; mkfs; tune2fs; xenstore_read."
msgstr ""
"다음 명령은 루트 wraps 구성에서 더 이상 표시되지 않습니다. : e2fsck; mkfs; tune2fs; xenstore_read."

#: ../../<reno.sphinxext stable/ussuri>:1088
msgid ""
"The following conf options have been moved to the ``[image_cache]`` group "
"and renamed accordingly. The old option paths are deprecated and will be "
"removed in a future release."
msgstr ""
"다음 conf 옵션은 ``[image_cache]`` 그룹에 이동되어 이름이 변경되었습니다. 이전 옵션 경로는 비고된 상태로 남아 있으며"
" 향후 릴리스에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/ussuri>:918
msgid ""
"The following config options only applied when using the *nova-network* "
"network driver which has now been removed. The config options have therefore"
" been removed also."
msgstr ""
"다음 config 옵션은 *nova-network* 네트워크 드라이버를 사용할 때만 적용되었으며, 이 드라이버는 현재 제거되었습니다. "
"따라서 config 옵션도 제거되었습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:666
msgid ""
"The following config options only apply when using the ``XenAPI`` virt "
"driver which has now been removed. The config options have therefore been "
"removed also."
msgstr ""
"다음 config 옵션은 XenAPI virt 드라이버를 사용할 때만 적용되며, XenAPI virt 드라이버가 현재 제거되었다는 것을 "
"고려할 때 config 옵션도 제거되었다."

#: ../../<reno.sphinxext stable/2024.1>:497
msgid ""
"The following config options which only apply for the ``HyperV`` virt driver"
" or RDP console APIs also have been removed:"
msgstr "다음 config 옵션은 ``HyperV`` virt 드라이버 또는 RDP 콘솔 API에만 적용되는 옵션은 모두 제거되었다."

#: ../../<reno.sphinxext stable/queens>:1571
msgid "The following configuration options are deprecated for removal:"
msgstr "다음 configuration 옵션은 제거를 위해 deprecated되어 있습니다."

#: ../../<reno.sphinxext stable/queens>:1465
msgid "The following configuration options have been renamed:"
msgstr "다음 configuration options이 이름이 변경되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1904
msgid ""
"The following configuration options in the ``[upgrade_levels]`` group have "
"been deprecated:"
msgstr ""
"다음 configuration options은 ``[upgrade_levels]`` 그룹에 있는 것으로 deprecated되었습니다:"

#: ../../<reno.sphinxext stable/stein>:1188
msgid ""
"The following configuration options in the ``quota`` group have been removed"
" because they have not been used since 17.0.0."
msgstr ""
"다음 configuration options은 17.0.0 이후에 사용되지 않기 때문에 \"quota\" 그룹의 configuration"
" options이 제거되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1698
msgid ""
"The following configuration options were deprecated for removal in the "
"17.0.0 Queens release and have now been removed:"
msgstr "다음 configuration 옵션은 17.0.0 퀸즈 릴리스에서 비활성화되었으며 현재 제거되었습니다."

#: ../../<reno.sphinxext stable/pike>:1321 stable/pike>:1344
msgid ""
"The following configuration options, previously found in the ``libvirt`` "
"group, have been removed:"
msgstr "다음 configuration 옵션, 이전에 ``libvirt`` 그룹에서 찾은 옵션은 제거되었습니다."

#: ../../<reno.sphinxext stable/stein>:1137
msgid "The following deprecated Policy Rules have been removed:"
msgstr "다음은弃용된 Policy Rule을 제거했습니다."

#: ../../<reno.sphinxext stable/rocky>:1664
msgid ""
"The following deprecated configuration options have been removed from the "
"``api`` section of ``nova.conf``:"
msgstr "다음은 `nova.conf`의 `api` 섹션에서 제거된 deprecated 구성 옵션입니다."

#: ../../<reno.sphinxext stable/queens>:1389 stable/rocky>:1677
msgid ""
"The following deprecated configuration options have been removed from the "
"``compute`` section of ``nova.conf``:"
msgstr "다음은 `compute` 섹션의 `nova.conf`에서 제거된 deprecated 구성 옵션입니다."

#: ../../<reno.sphinxext stable/queens>:1435
msgid ""
"The following deprecated configuration options have been removed from the "
"``xenserver`` section of ``nova.conf``:"
msgstr "다음은 `nova.conf`의 `xenserver` 섹션에서 제거된 deprecated 구성 옵션입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1391 origin/stable/ocata>:1400
msgid "The following deprecated nova-manage commands have been removed:"
msgstr "다음은 deprecated nova-manage 명령이 제거된 것입니다."

#: ../../<reno.sphinxext stable/rocky>:1687
msgid ""
"The following deprecated options have been removed from the ``placement`` "
"group of ``nova.conf``:"
msgstr "다음은 `nova.conf`의 `placement` 그룹에서 제거된 deprecated 옵션입니다."

#: ../../<reno.sphinxext unmaintained/victoria>:640
msgid "The following deprecated scheduler filters have been removed."
msgstr "다음은弃용된 스케줄러 필터가 제거되었습니다."

#: ../../<reno.sphinxext unmaintained/zed>:348
msgid ""
"The following enlightenments are now added by default to the libvirt XML for"
" Windows guests:"
msgstr ""
"다음은 Windows 게스트에 대한 libvirt XML의 기본적으로 추가된 enlightenment입니다.\n"
"\n"
"- \"enlightenment\" → \"명명\"\n"
"- \"default\" → \"기본\"\n"
"- \"for\" → \"위치\"\n"
"- \"Windows\" → \"윈도우\"\n"
"- \"guests\" → \"게스트\"\n"
"- \"libvirt\" → \"libvirt\"\n"
"- \"XML\" → \"XML\"\n"
"- \"to\" → \"위치\"\n"
"- \"the\" → \"the\"\n"
"- \"following\" → \"다음\"\n"
"- \"are\" → \"는\"\n"
"- \"now\" → \"이제\"\n"
"- \"added\" → \"추가\"\n"
"- \"by\" → \"by\"\n"
"- \"default\" → \"기본\"\n"
"- \"to\" → \"위치\"\n"
"- \"the\" → \"the\"\n"
"- \"libvirt\" → \"libvirt\"\n"
"- \"XML\" → \"XML\"\n"
"- \"for\" → \"위치\"\n"
"- \"Windows\" → \"윈도우\"\n"
"- \"guests\" → \"게스트\""

#: ../../<reno.sphinxext stable/2024.1>:521
msgid ""
"The following extra specs which only apply for the ``HyperV`` virt driver "
"have been removed."
msgstr "다음과 같은 추가 속성은 ``HyperV`` virt 드라이버에만 적용되는 속성들이 제거되었습니다."

#: ../../<reno.sphinxext stable/queens>:838
msgid "The following instance action records have been added:"
msgstr "다음 인스턴스 액션 레코드가 추가되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:750 stable/queens>:883
#: stable/rocky>:1019
msgid ""
"The following legacy notifications have been transformed to a new versioned "
"payload:"
msgstr "다음은 legacy notification을 새로운 버전화된ayload로 변형되었습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:656
msgid ""
"The following new ``nova-manage`` commands have been introduced to help "
"operators manage the ``hw_machine_type`` image property:"
msgstr ""
"다음 nova-manage 명령어는 hw_machine_type hình ảnh 속성을 관리하는 운영자가 도움을 줄 수 있는 새로운 "
"명령어입니다."

#: ../../<reno.sphinxext stable/pike>:1361
msgid "The following options are removed:"
msgstr "다음 옵션들이 제거되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1446 stable/pike>:1521
#: stable/rocky>:1869 stable/stein>:1267
msgid ""
"The following options, found in ``DEFAULT``, were only used for configuring "
"nova-network and are, like nova-network itself, now deprecated."
msgstr ""
"다음 옵션, ``DEFAULT``에서 찾은 옵션은 nova-network를 구성할 때만 사용되었으며, nova-network 자체와 "
"마찬가지로 현재 deprecated되어 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1478
msgid "The following options, found in ``quota``, are also deprecated."
msgstr "다음 옵션, `quota`에서 찾을 수 있는 옵션도弃용되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1641
msgid ""
"The following options, previously found in the ``[crypto]`` group, have been"
" removed:"
msgstr "다음 옵션, 이전에 ``[crypto]`` 그룹에 존재했던 것들,는 제거되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:875
msgid "The following policies have also been removed."
msgstr "다음 정책도 제거되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1312
msgid ""
"The following policy rules were added to restrict the usage of the "
"``trusted_image_certificates`` request parameter in the server create and "
"rebuild APIs:"
msgstr ""
"다음 정책 규칙이 서버 생성 및 재건 API에서 ``trusted_image_certificates`` 요청 매개 변수의 사용을 제한하기"
" 위해 추가되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:896
msgid "The following related config options have been removed."
msgstr "다음과 관련된 구성 옵션은 제거되었습니다."

#: ../../<reno.sphinxext stable/pike>:1502 stable/queens>:1527
msgid ""
"The following scheduler filters are deprecated in Pike: ``ExactRamFilter``, "
"``ExactCoreFilter`` and ``ExactDiskFilter`` and should only be used if your "
"deployment still contains nodes that have not had their resource_class "
"attribute set. See `Ironic release notes "
"<https://docs.openstack.org/releasenotes/ironic/>`_ for upgrade concerns."
msgstr ""
"다음 스케줄러 필터가 피크에서弃용되었습니다: `ExactRamFilter`, `ExactCoreFilter` 및 "
"`ExactDiskFilter` 및 이 필터를 사용할 수 있는 것은 배포가 여전히 리소스_클래스 속성을 설정하지 않은 노드가 있는 "
"경우에만입니다.  Ironic 릴리스 노트 <https://docs.openstack.org/releasenotes/ironic/> _을"
" 참조하십시오."

#: ../../<reno.sphinxext stable/2025.1>:308
msgid ""
"The following share attach and share detach versioned notifications have "
"been added to the nova-compute service: * instance.share_attach.start * "
"instance.share_attach.end * instance.share_detach.start * "
"instance.share_detach.end"
msgstr ""
"다음 nova-compute 서비스에 versioned通知을 추가한 share attach 및 share detach 버전이 "
"추가되었습니다. * instance.share_attach.start * instance.share_attach.end * "
"instance.share_detach.start * instance.share_detach.end"

#: ../../<reno.sphinxext origin/stable/ocata>:583
msgid ""
"The following versioned swap volume notifications have been added in the "
"compute manager:"
msgstr "다음 버전된 스위프 볼륨 알림이 컴퓨터 관리자에 추가되었습니다."

#: ../../<reno.sphinxext stable/pike>:958
msgid ""
"The following volume attach and volume detach versioned notifications have "
"been added to the nova-compute service:"
msgstr "다음과 같은 볼륨 연결 및 볼륨 해제 버전이 nova-compute 서비스에 추가되었습니다."

#: ../../<reno.sphinxext stable/2025.1>:477
msgid ""
"The following volume drivers of the libvirt virt driver have been deprecated"
" and will be removed in a future release. The corresponding volume drivers "
"in cinder were all marked unsupported and will be removed."
msgstr ""
"다음 libvirt virt 드라이버의 volume 드라이버가 deprecated되었으며 향후 릴리스에서 제거될 예정입니다. "
"corresponding volume 드라이버는 cinder에서 모든 것을 비지니컬리한 것으로 표시되었으며 제거될 예정입니다."

#: ../../<reno.sphinxext stable/ussuri>:291 unmaintained/victoria>:571
msgid ""
"The former combination is invalid as it would suggest reserved memory is "
"greater than total memory available, while the latter is considered "
"incorrect behavior as reserving of file-backed memory can and should be "
"achieved by reducing the filespace allocated as memory by modifying "
"``[libvirt] file_backed_memory``."
msgstr ""
"이전의 조합은 reserved memory가 총 메모리 사용 가능한 메모리보다 더 많다는 것을 의미하는 thing이 therefore "
"invalid으로 간주됩니다. 반면에, 후자는 file-backed memory를 reserve하는 behavior가 "
"incorrect으로 간주됩니다. file-backed memory를 reserve하는 것은 filespace가 memory로 할당된 "
"것을 수정하여 memoryspace가 더 적게 할당된 것을 의미하는 thing이므로, memoryspace가 더 적게 할당된 것을 "
"의미하는 thing을 수정하여 reserve하는 것이 가능하고 should be done이라고 간주됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:719
msgid ""
"The guest OS type must be specified in order to properly spawn the VMs. It "
"can be specifed through the image property \"os_type\", and the acceptable "
"values are \"windows\" or \"linux\"."
msgstr ""
"가UEST OS 타입은 적절하게 VMs를 생성하기 위해 명시되어야 합니다. 이 값은 \"os_type\" 속성에 의해 명시될 수 있으며,"
" \"windows\" 또는 \"linux\"와 같은 적합한 값이 있습니다."

#: ../../<reno.sphinxext stable/2023.2>:396 unmaintained/2023.1>:181
msgid ""
"The hyperv driver is marked as experimental and may be removed in a future "
"release. The driver is not tested by the OpenStack project and does not have"
" a clear maintainer."
msgstr ""
"하이퍼바이저 드라이버는 실험적으로 표시되어 향후 릴리스에서 제거될 수 있습니다. 드라이버는 오픈 스텝 프로젝트에 의해 테스트되지 않으며 "
"명확한 유지자가 없습니다."

#: ../../<reno.sphinxext stable/pike>:1820
msgid ""
"The i/o performance for Quobyte volumes has been increased significantly by "
"disabling xattrs."
msgstr "Quobyte 볼륨의 I/O 성능이 xattrs를 비활성화하면 상당히 증가했습니다."

#: ../../<reno.sphinxext stable/ussuri>:642
msgid ""
"The image properties ``cinder_encryption_key_id`` and "
"``cinder_encryption_key_deletion_policy`` are absolutely non-inheritable "
"regardless of the ``non_inheritable_image_properties`` setting."
msgstr ""
"이미지 속성 \"cinder_encryption_key_id\"과 "
"\"cinder_encryption_key_deletion_policy\"은 "
"\"non_inheritable_image_properties\" 설정에 관계없이 absolutelly 비상속 가능한 것으로 간주됩니다."

#: ../../<reno.sphinxext stable/train>:562
msgid ""
"The information exposed by this API is admin or owner only by default, "
"controlled by rule:"
msgstr "이 API가 노출하는 정보는 기본적으로 관리자 또는 소유자만이 접근할 수 있으며, 규칙에 의해 제어됩니다."

#: ../../<reno.sphinxext stable/pike>:1070
msgid ""
"The information in the network.json metadata has been amended, for IPv6 "
"networks under Neutron control, the ``type`` field has been changed from "
"being always set to ``ipv6_dhcp`` to correctly reflecting the "
"``ipv6_address_mode`` option in Neutron."
msgstr ""
"네트워크.json metadata에 있는 네트워크 정보가 수정되었습니다. IPv6 네트워크가 Neutron의 통제하에 있는 경우, "
"type 필드는 항상 설정되지 않도록 설정된 것을 ipv6_dhcp로 변경되어 Neutron의 ipv6_address_mode 옵션을 "
"correctly 반영하는 데 사용됩니다."

#: ../../<reno.sphinxext stable/queens>:267 stable/rocky>:1441
msgid ""
"The initial implementation of native LUKS decryption within Libvirt 2.2.0 "
"had a `known issue`_ with the use of passphrases that were a multiple of 16 "
"bytes in size. This was `resolved`_ in the upstream 3.3.0 release of Libvirt"
" and has been backported to various downstream distribution specific "
"versions."
msgstr ""
"초기 LUKS 디코딩 native implementation은 Libvirt 2.2.0에서 `유명 문제`_가 사용된 패스프레이즈가 16 "
"바이트의 배수인 경우에 대한 문제가있었습니다. 이 문제는 upstream 3.3.0 Libvirt 릴리스에서 `해결되었습니다._` 및 "
"다양한 하위 배포 spécifique 버전에 대한 백포트가있었습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:669
msgid ""
"The instance must have a ``vm_state`` of ``STOPPED``, ``SHELVED`` or "
"``SHELVED_OFFLOADED``."
msgstr ""
"인스턴스는 ``vm_state``의 ``STOPPED``, ``SHELVED`` 또는 ``SHELVED_OFFLOADED`` 중 하나가 "
"있어야 합니다."

#: ../../<reno.sphinxext stable/pike>:510 stable/queens>:1858
msgid ""
"The ironic driver will automatically migrate instance flavors for resource "
"classes at runtime. If you are not able to run the compute and ironic "
"services at pike because you are automating an upgrade past this release, "
"you can use the ``nova-manage db ironic_flavor_migration`` to push the "
"migration manually. This is only for advanced users taking on the risk of "
"automating the process of upgrading through pike and is not recommended for "
"normal users."
msgstr ""
"ironic 드라이버는 런타임에 리소스 클래스에 대한 인스턴스 플레버를 tự động으로 이식할 수 있습니다. Pike 버전 이전에 "
"업그레이드하는 것을 위해 컴퓨터 및 ironic 서비스를 pike에서 실행할 수 없다면, nova-manage db "
"ironic_flavor_migration 명령을 사용하여 수동으로 이식할 수 있습니다. 이는 pike를 통해 업그레이드를 tự "
"động화하는 것을 위험으로 인한 고급 사용자만이 사용할 수 있는 기능이며, 일반 사용자에게는 추천되지 않습니다."

#: ../../<reno.sphinxext stable/pike>:1825 stable/queens>:1779
msgid ""
"The ironic virt driver no longer reports an empty inventory for bare metal "
"nodes that have instances on them. Instead the custom resource class, VCPU, "
"memory and disk are reported as they are configured on the node."
msgstr ""
"ironic virt driver는 더 이상 비메탈 노드에 인스턴스가 있는 경우 비어 있는 인ベント리报告를 더 이상하지 않습니다. 대신 "
"custom resource class VCPU, memory 및 disk가 노드에 구성된 상태로 보고됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1025
msgid ""
"The ivs-ctl command has been added to the rootwrap filters in "
"compute.filters. Deployments needing support for BigSwitch no longer need to"
" add the filters manually nor include network.filters at installation."
msgstr ""
"ivs-ctl 명령은 compute.filters에 있는 rootwrap 필터에 추가되었다. BigSwitch에 대한 지원이 필요하는 "
"배포는 더 이상 필터를 수동으로 추가하거나 network.filters를 설치 시 포함하지 않아야 한다."

#: ../../<reno.sphinxext origin/stable/ocata>:528
msgid ""
"The latest API microversion supported for Ocata is v2.42. Details on REST "
"API microversions added since the 14.0.0 Newton release can be found in the "
"`REST API Version History`_ page."
msgstr ""
"마이너 버전 v2.42이 Ocata에 지원되는 가장 최신 API microversion입니다. Newton 14.0.0 릴리스 이후 "
"추가된 REST API microversion의 세부 사항은 `REST API Version History`_ page에서 찾을 수 "
"있습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:281
msgid "The latest Compute API microversion supported for 2023.1 is `v2.95`__."
msgstr "2023.1 Compute API의 latest microversion은 `v2.95`입니다."

#: ../../<reno.sphinxext stable/2023.2>:191
msgid "The latest Compute API microversion supported for 2023.2 is `v2.95`__."
msgstr "2023.2에서 지원되는 가장 mới한 Compute API 마이크로 버전은 `v2.95`입니다."

#: ../../<reno.sphinxext stable/2024.1>:307
msgid "The latest Compute API microversion supported for 2024.1 is `v2.96`__."
msgstr "2024.1에서 지원되는 가장 mới한 Compute API 마이크로 버전은 `v2.96`입니다."

#: ../../<reno.sphinxext stable/2025.1>:186
msgid ""
"The latest Compute API microversion supported for 2024.2 is `v2.100`__."
msgstr "2024.2에서 지원되는 가장 mới한 Compute API 마이크로 버전은 `v2.100`입니다."

#: ../../<reno.sphinxext stable/2024.2>:233
msgid "The latest Compute API microversion supported for 2024.2 is `v2.96`__."
msgstr "2024.2에서 지원되는 가장 mới한 Compute API 마이크로 버전은 `v2.96`입니다."

#: ../../<reno.sphinxext stable/2025.2>:26
msgid ""
"The latest Compute API microversion supported for 2025.2 is `v2.100 "
"<https://docs.openstack.org/nova/latest/reference/api-microversion-"
"history.html#maximum-in-2025-1-epoxy-and-2025-2-flamingo>`_."
msgstr ""
"2025.2에서 지원되는 가장 최근의 Compute API 마이크로 버전은 `v2.100 "
"<https://docs.openstack.org/nova/latest/reference/api-microversion-"
"history.html#maximum-in-2025-1-epoxy-and-2025-2-flamingo>`_."

#: ../../<reno.sphinxext stable/pike>:597
msgid ""
"The latest Compute API microversion supported for Pike is v2.53. Details on "
"REST API microversions added since the 15.0.0 Ocata release can be found in "
"the `REST API Version History`_ page."
msgstr ""
"마이크로 버전 v2.53이 Pike에서 지원되는 가장 최근의 컴퓨팅 API입니다. 15.0.0 Ocata 릴리스 이후 추가된 REST "
"API 마이크로 버전에 대한รายละเอ기는 `REST API Version History`_ page에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:593
msgid ""
"The latest Compute API microversion supported for Queens is v2.60. Details "
"on REST API microversions added since the 16.0.0 Pike release can be found "
"in the `REST API Version History`_ page."
msgstr ""
"latest Compute API microversion supported for Queens은 v2.60입니다. 16.0.0 Pike "
"릴리스 이후 추가된 REST API microversion의 세부 사항은 `REST API Version History`_ page에서 "
"찾을 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:511
msgid ""
"The latest Compute API microversion supported for Rocky is v2.65. Details on"
" REST API microversions added since the 17.0.0 Queens release can be found "
"in the `REST API Version History`_ page."
msgstr ""
"마이크로 버전 v2.65이 로키에 대한 가장 최근의 컴퓨팅 API를 지원합니다. 17.0.0 퀸즈 릴리스 이후 추가된 REST API "
"마이크로 버전에 대한รายละเอ기는 `REST API Version History`_ 페이지에서 찾을 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:412
msgid ""
"The latest Compute API microversion supported for Stein is v2.72. Details on"
" REST API microversions added since the 18.0.0 Rocky release can be found in"
" the `REST API Version History`_ page."
msgstr ""
"마이크로 버전 v2.72이 Stein에서 지원되는 가장 최근의 컴퓨팅 API입니다. 18.0.0 로키 릴리스 이후 추가된 REST API"
" 마이크로 버전에 대한รายละเอ기는 `REST API Version History`_ page에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:464
msgid ""
"The latest Compute API microversion supported for Train is v2.79. Details on"
" REST API microversions added since the 19.0.0 Stein release can be found in"
" the `REST API Version History`_ page."
msgstr ""
"latest Compute API microversion supported for Train은 v2.79입니다. 19.0.0 Stein "
"릴리스 이후 추가된 REST API microversion의 세부 사항은 `REST API Version History`_ page에서 "
"찾을 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:340
msgid ""
"The latest Compute API microversion supported for Ussuri is v2.87. Details "
"on REST API microversions added since the 20.0.0 Train release can be found "
"in the `REST API Version History`__ page."
msgstr ""
"최근est Compute API microversion으로 Ussuri를 지원하는 가장 최신 버전은 v2.87입니다. REST API "
"microversions에 대한 세부 정보는 20.0.0 Train 릴리스 이후 추가된 API 버전 역사에 대한 정보가 `REST API"
" Version History`__ page에 있습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:326
msgid ""
"The latest Compute API microversion supported for Victoria is v2.87. No new "
"microversions were added during this cycle but you can find all of them in "
"the `REST API Version History`__ page."
msgstr ""
"latest Compute API microversion supported for Victoria은 v2.87입니다. 이 시기 동안 "
"새로운 마이크로 버전이 추가되지 않았지만, `REST API Version History`__ page에서 모든 마이크로 버전을 찾을 수"
" 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:391
msgid ""
"The latest Compute API microversion supported for Wallaby is v2.88. Details "
"on REST API microversions added since the 22.0.0 Victoria release can be "
"found in the `REST API Version History`__ page."
msgstr ""
"latest Compute API microversion supported for Wallaby는 v2.88입니다. Wallaby에서 "
"지원되는 REST API microversion에 대한 세부 정보는 `REST API Version History`__ page에서 찾을"
" 수 있습니다."

#: ../../<reno.sphinxext unmaintained/xena>:297
msgid ""
"The latest Compute API microversion supported for Xena is v2.90. Details on "
"REST API microversions added since the 23.0.0 Wallaby release can be found "
"in the `REST API Version History`__ page."
msgstr ""
"latest Compute API microversion supported for Xena은 v2.90입니다. REST API "
"microversions에 대한 세부 정보는 23.0.0 Wallaby 릴리스 이후 추가된 API 버전 역사에 대한 정보가 `REST "
"API Version History`__ page에 있습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:270
msgid ""
"The latest Compute API microversion supported for Yoga is `v2.90`__ (same as"
" the Xena release)."
msgstr ""
"최근est Compute API microversion은 Yoga에서 지원되는 `v2.90`__ (Xena 릴리스와 동일)입니다."

#: ../../<reno.sphinxext unmaintained/zed>:193
msgid "The latest Compute API microversion supported for Zed is `v2.93`__."
msgstr "Zed에서 지원되는 가장 mới한 Compute API microversion은 `v2.93`입니다."

#: ../../<reno.sphinxext stable/2023.2>:349
msgid ""
"The legacy ``sqlalchemy-migrate`` migrations, which have been deprecated "
"since Wallaby, have been removed. There should be no end-user impact."
msgstr ""
"어제의 전통적인 `sqlalchemy-migrate` 마이그레이션,_wallaby 이후부터 deprecated가 된 것은 제거되었다. 이"
" 마이그레이션은 사용자에게 영향을 미치지 않는다."

#: ../../<reno.sphinxext stable/train>:994
msgid ""
"The legacy naive behavior is dependent on the value of the "
"``[workarounds]/enable_numa_live_migration`` option. Refer to the "
"Deprecations sections for more details."
msgstr ""
"기반의 무지한 행동은 `[workarounds]/enable_numa_live_migration` 옵션의 giá trị에 의존합니다. 더"
" 많은 정보는 deprecations 섹션을 참조하십시오."

#: ../../<reno.sphinxext stable/2023.2>:201
msgid ""
"The legacy quota driver is now deprecated and a `nova-manage limits command "
"<https://docs.openstack.org/nova/latest/cli/nova-manage.html#limits-migrate-"
"to-unified-limits>`_ is provided in order to migrate the legacy limits into "
"Keystone. We plan to change the default quota driver to the unified limits "
"driver in an upcoming release (hopefully 2024.1 Caracal). It is recommended "
"that you begin planning and executing a migration to unified limits as soon "
"as possible."
msgstr ""
"기상 제한 드라이버는 현재 비상grade되었으며, `nova-manage limits 명령 "
"<https://docs.openstack.org/nova/latest/cli/nova-manage.html#limits-migrate-"
"to-unified-limits>`_이 제공되었습니다. 이로써 기상 제한을 Keystonie로 이식할 수 있습니다. upcoming "
"release (hopefully 2024.1 Caracal)에서 기상 제한 드라이버를 unified limits 드라이버로 변경할 "
"계획입니다. 이로 인해 unified limits로 이식하는 것을 가능한 한 빠르게 planning 및 execution을 시작하는 것이"
" 추천됩니다."

#: ../../<reno.sphinxext stable/2025.2>:387
msgid ""
"The legacy v2 API is now deprecated for removal, and the status in the root "
"version document (``/``) has been changed from ``SUPPORTED`` to "
"``DEPRECATED``. This will cause some clients like keystoneauth to ignore the"
" endpoint by default. Users are encouraged to switch the v2.1 API, which is "
"functionally identical to the v2 API when using the ``2.1`` API "
"microversion."
msgstr ""
"기반 v2 API는 현재 제거를 위해 비활성화되었으며, 루트 버전 문서 (``/``)에서 ``SUPPORTED``에서 "
"``DEPRECATED``로 변경되었습니다. 이로 인해 keystoneauth와 같은 일부 클라이언트는 기본적으로 엔드포인트를 "
"무시합니다. 사용자는 v2.1 API를 사용하여 v2 API와 동일한 기능을 제공하는 ``2.1`` API 마이크로 버전을 사용하는 경우"
" v2.1 API를 사용하는 것을 khuyến진합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:405
msgid ""
"The libvirt RBD image backend module can now handle a Glance multistore "
"environment where multiple RBD clusters are in use across a single "
"Nova/Glance deployment, configured as independent Glance stores. In the case"
" where an instance is booted with an image that does not exist in the RBD "
"cluster that Nova is configured to use, Nova can ask Glance to copy the "
"image from whatever store it is currently in to the one that represents its "
"RBD cluster. To enable this feature, set "
"``[libvirt]/images_rbd_glance_store_name`` to tell Nova the Glance store "
"name of the RBD cluster it uses."
msgstr ""
"libvirt RBD 이미지 백엔드 모듈은 현재 여러 RBD 클러스터가 하나의 Nova/Glance 배포에서 사용되는 Glance 다중 스토어 환경을 처리할 수 있습니다. 이 경우, 인스턴스가 RBD 클러스터에 있는 이미지와 일치하지 않는 이미지로 부트되면, Nova는 Glance를 통해 RBD 클러스터에 있는 스토어 이름을 알려주어야 합니다. 이 경우, Nova는 현재 사용 중인 스토어에서 이미지의 복사본을 RBD 클러스터에 있는 스토어로 복사합니다. \n"
"\n"
"이 기능을 활성화하려면, `libvirt/images_rbd_glance_store_name`를 설정하여 Nova에 Glance 스토어 이름을 알려줍니다."

#: ../../<reno.sphinxext stable/queens>:602
msgid ""
"The libvirt and xenapi compute drivers now have (experimental) native "
"support for virtual GPU devices. See the `virtual GPU`_ admin guide for more"
" details."
msgstr ""
"libvirt 및 xenapi 컴퓨터 드라이버는 현재 ( experimentally ) 가상 그래픽 장치에 대한 원 native 지원을 "
"제공합니다. 더 많은 정보는 `가상 그래픽`_ admin 가이드를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:1351
msgid ""
"The libvirt compute driver now allows users to create instances with SR-IOV "
"virtual functions which will be configured as trusted."
msgstr ""
"libvirt 컴퓨팅 드라이버는 현재 사용자들이 SR-IOV 가상 함수를 사용하여 인스턴스를 생성할 수 있게 하여, 이 가상 함수가 "
"신뢰할 수 있는 것으로 구성됩니다."

#: ../../<reno.sphinxext stable/pike>:752
msgid ""
"The libvirt compute driver now supports attaching volumes of type \"drbd\". "
"See the `DRBD documentation`_ for more information."
msgstr ""
"libvirt 컴퓨트 드라이버는 현재 \"DRBD\" 타입의 볼륨을 연결할 수 있습니다.  더 많은 정보는 `DRBD 문서`_에서 확인할"
" 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:935
msgid ""
"The libvirt compute driver now supports connecting to Veritas HyperScale "
"volume backends."
msgstr "libvirt 컴퓨팅 드라이버는 현재 Veritas HyperScale 볼륨 백엔드와 연결할 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:533
msgid ""
"The libvirt compute driver now supports trusted image certificates when "
"using the 2.63 compute API microversion. See the `image signature "
"certificate validation`_ documentation for more details."
msgstr ""
"libvirt compute 드라이버는 2.63 compute API microversion을 사용할 때, 신뢰할 수 있는 이미지 서명서"
" 인증서를 지원합니다. 더 많은 정보는 `이미지 서명서 인증서 유효성 확인`_ 문서를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:605
msgid ""
"The libvirt compute driver now supports volume multi-attach when using the "
"2.60 compute API microversion. See the cinder admin guide for more details "
"about volume multi-attach support in OpenStack."
msgstr ""
"libvirt 컴퓨트 드라이버는 2.60 컴퓨트 API 마이크로 버전을 사용할 때 볼륨 다중 연결을 지원합니다. OpenStack의 볼륨"
" 다중 연결 지원에 대한 더 많은 정보는 cinder admin guide에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:1010
msgid ""
"The libvirt compute driver will \"reshape\" VGPU inventories and allocations"
" on start of the ``nova-compute`` service. This will result in moving VGPU "
"inventory from the root compute node resource provider to a nested (child) "
"resource provider in the tree and move any associated VGPU allocations with "
"it. This will be a one-time operation on startup in Stein. There is no end-"
"user visible impact for this; it is for internal resource tracking purposes."
" See the `spec`__ for more details."
msgstr ""
"libvirt compute 드라이버는 `nova-compute` 서비스가 시작될 때 \"reshape\" VGPU 집합과 할당을 "
"\"reshape\"합니다. 이것은 VGPU 집합을 루트 컴퓨터 노드 리소스 제공자에서 내부(자녀) 리소스 제공자로 이동하고 이에 따라 "
"관련된 VGPU 할당을 함께 이동하는 것을 의미합니다. 이것은 스티恩에서 시작할 때 한 번의 작업이며, 내부 리소스 트랙킹 목적으로만 "
"사용되며, 사용자에게는 보이는 영향을 없으며, `spec`__에 더 많은รายละเอียด을 확인하십시오."

#: ../../<reno.sphinxext stable/train>:645
msgid ""
"The libvirt driver can now support requests for guest RAM to be encrypted at"
" the hardware level, if there are compute hosts which support it.  Currently"
" only AMD SEV (Secure Encrypted Virtualization) is supported, and it has "
"certain minimum version requirements regarding the kernel, QEMU, and "
"libvirt."
msgstr ""
"libvirt 드라이버는 현재 guest RAM를 하드웨어 수준에서 암호화하는 요청을 지원할 수 있습니다. 이 경우에는 compute "
"호스트가 암호화 지원을 제공하는 경우에만 지원됩니다. 현재는 AMD SEV (Secure Encrypted Virtualization)만"
" 지원하고 있으며, 커널, QEMU, libvirt와 관련하여 특정 최소 버전의 요구 사항이 있습니다."

#: ../../<reno.sphinxext stable/2025.2>:176
msgid ""
"The libvirt driver can now support requests for guest RAM to be encrypted "
"using the AMD SEV-ES(Secure Encrypted Virtualization-Encrypted State), "
"instead of AMD SEV."
msgstr ""
"libvirt 드라이버는 현재 AMD SEV-ES(안드레아드 보안 암호화된 가상화-암호화된 상태)로 가상 guest RAM를 암호화하는 "
"요청을 지원할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:553
msgid ""
"The libvirt driver has added support for hardware-offloaded OVS with vDPA "
"(vhost Data Path Acceleration) type interfaces. vDPA allows virtio net "
"interfaces to be presented to the guest while the datapath can be offloaded "
"to a software or hardware implementation. This enables high performance "
"networking with the portablity of standard virtio interfaces."
msgstr ""
"libvirt 드라이버는 하드웨어 오프loads OVS를 지원하는 vDPA (vhost Data Path Acceleration) "
"type 인터페이스를 추가했습니다. vDPA는 guest에게 virtio net 인터페이스를呈现할 수 있지만 datapath를 소프트웨어"
" 또는 하드웨어 implementation으로 오프loads할 수 있습니다. 이것은 표준 virtio 인터페이스의 mobility를 "
"제공하는 고성능 네트워킹을 possible합니다."

#: ../../<reno.sphinxext stable/train>:803
msgid ""
"The libvirt driver has been extended to support user configurable "
"performance monitoring unit (vPMU) virtualization. This is particularly "
"useful for real-time workloads. A pair of boolean flavor extra spec and "
"image metadata properties ``hw:pmu`` and ``hw_pmu`` have been added to "
"control the emulation of the vPMU. By default the behavior of vPMU emulation"
" has not been changed. To take advantage of this new feature, the operator "
"or tenant will need to update their flavors or images to define the new "
"property."
msgstr ""
"libvirt 드라이버는 사용자-configurable performance monitoring unit (vPMU) "
"virtualization을 지원하기 위해 확장되었다. 이 기능은 특히 실시간 작업에 유용하다. vPMU의 simulation을 제어하기"
" 위해 boolean flavor extra spec과 image metadata properties인 ``hw:pmu``와 "
"``hw_pmu``의 쌍이 추가되었다. 기본적으로 vPMU simulation의 행동은 변경되지 않았다. 이 새로운 기능을 사용하려면 "
"운영자 또는 tenant은 새로운 속성을 정의하기 위해.flavor 또는 image를 업데이트해야 한다."

#: ../../<reno.sphinxext unmaintained/wallaby>:447
msgid ""
"The libvirt driver now allows explicitly disabling CPU flags for guests via "
"the ``[libvirt]cpu_model_extra_flags`` config attribute. This is possible "
"via a ``+`` / ``-`` notation, where if you specify a CPU flag prefixed with "
"a ``+`` sign (without quotes), it will be enabled for the guest, while a "
"prefix of ``-`` will disable it.  If neither ``+`` nor ``-`` is specified, "
"the CPU flag will be enabled, which is the default behaviour."
msgstr ""
"libvirt 드라이버는 now guest에 대해 명시적으로 CPU 플래그를 비활성화할 수 있습니다. 이는 "
"[libvirt]cpu_model_extra_flags config 속성을 통해 가능합니다. 이 속성은 + / - 표기법을 사용하여 "
"구현됩니다. + 기호가 preceding CPU 플래그에 사용되면, guest에 대해 활성화됩니다. 반면 - 기호가 preceding "
"CPU 플래그에 사용되면, 비활성화됩니다. neither + nor - 기호가 사용되면, CPU 플래그는 활성화되며, 기본 "
"behaviour입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:217 stable/pike>:243
#: stable/queens>:510
msgid ""
"The libvirt driver now allows specifying individual CPU feature flags for "
"guests, via a new configuration attribute "
"``[libvirt]/cpu_model_extra_flags`` -- only with ``custom`` as the "
"``[libvirt]/cpu_model``.  Refer to its documentation in ``nova.conf`` for "
"usage details."
msgstr ""
"libvirt 드라이버는 now guest에 individual CPU feature flags를 specifying할 수 있습니다.  "
"new configuration attribute ``[libvirt]/cpu_model_extra_flags``를 통해 -- "
"custom``이 ``[libvirt]/cpu_model``의 sole value일 때만.  nova.conf의 "
"documentation을 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:900
msgid ""
"The libvirt driver now allows specifying individual CPU feature flags for "
"guests, via a new configuration attribute "
"``[libvirt]/cpu_model_extra_flags`` -- this is valid in combination with all"
" the three possible values for ``[libvirt]/cpu_mode``: ``custom``, ``host-"
"model``, or ``host-passthrough``.  The ``cpu_model_extra_flags`` also allows"
" specifying multiple CPU flags. Refer to its documentation in ``nova.conf`` "
"for usage details."
msgstr ""
"libvirt 드라이버는 now guest에 individual CPU feature flags를 spécifying 할 수 있습니다."
"  new configuration attribute ``[libvirt]/cpu_model_extra_flags``를 통해 -- "
"3가지의 possible value 중 하나인 ``[libvirt]/cpu_mode``와 함께 사용할 수 있습니다.  "
"``custom``, ``host-model``, or ``host-passthrough``.  "
"``cpu_model_extra_flags``는 여러 CPU flags를 spécifying 할 수 있습니다.  nova.conf의 "
"documentation을 참조하십시오."

#: ../../<reno.sphinxext unmaintained/yoga>:463
msgid ""
"The libvirt driver now allows using Native NVMeoF multipathing for NVMeoF "
"connector, via the configuration attribute in nova-cpu.conf "
"``[libvirt]/volume_use_multipath``, defaulting to False (disabled)."
msgstr ""
"libvirt 드라이버는 현재 Native NVMeoFultipathing을 NVMeoF कनेक터에 사용할 수 있도록 nova-"
"cpu.conf의 구성 속성인 `[libvirt]/volume_use_multipath`에서 사용할 수 있습니다. 기본적으로 False "
"(disabled)로 설정됩니다."

#: ../../<reno.sphinxext stable/rocky>:923
msgid ""
"The libvirt driver now allows utilizing file backed memory for qemu/KVM "
"virtual machines, via a new configuration attribute "
"``[libvirt]/file_backed_memory``, defaulting to 0 (disabled)."
msgstr ""
"libvirt 드라이버는 현재 qemu/KVM 가상 머신을 위해 파일 기반 메모리를 사용할 수 있도록 새로운 구성 "
"속성``[libvirt]/file_backed_memory``를 추가하여 사용할 수 있습니다. 기본적으로 0 (비활성화)으로 설정됩니다."

#: ../../<reno.sphinxext stable/2025.2>:207
msgid ""
"The libvirt driver now automatically enables ``autodeflate`` and "
"``freePageReporting`` features for virtio memory balloon devices. The "
"``autodeflate`` feature allows the QEMU virtio memory balloon to release "
"memory at the last moment before a guest process is killed by the Out of "
"Memory killer. The ``freePageReporting`` feature enables the memory balloon "
"to return unused pages back to the hypervisor for use by other guests or "
"processes, improving overall memory efficiency on the compute host."
msgstr ""
"libvirt 드라이버는 현재 virtio memory balloon 장치에 대해 ``autodeflate`` 및 "
"``freePageReporting`` 기능을 tự động 활성화합니다. ``autodeflate`` 기능은 QEMU virtio "
"memory balloon이 guest 프로세스가 Out of Memory killer에 의해 죽은 마지막 순간에 메모리를释放할 수 "
"있도록 허용합니다. ``freePageReporting`` 기능은 메모리-balloon이 unused page를 Hypervisor에 "
"반환하여 다른 guest 또는 프로세스에 사용되도록 improving overall memory efficiency on the "
"compute host."

#: ../../<reno.sphinxext stable/2024.2>:380
msgid ""
"The libvirt driver now ensures the ``swtpm_ioctl`` binary, which is used to "
"terminate swtpm processes, is present when ``[libvirt] swtpm_enabled`` is "
"set to ``True``."
msgstr ""
"libvirt 드라이버는 `swtpm_ioctl` 바이너리를 사용하여 swtpm 프로세스를 종료하는 `swtpm_ioctl` 바이너리가 "
"`libvirt] swtpm_enabled`가 `True`로 설정되면 존재하는 것을 보장합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:665
msgid ""
"The libvirt driver now has a ``live_migration_scheme`` configuration option "
"which should be used where the ``live_migration_uri`` would previously have "
"been configured with non-default scheme."
msgstr ""
"libvirt 드라이버는 현재 live_migration_scheme 설정 옵션을 가지고 있으며, live_migration_uri가 "
"이전에 비 디폴트 스킵으로 구성된 곳에서 사용해야 합니다."

#: ../../<reno.sphinxext stable/stein>:736
msgid ""
"The libvirt driver now supports \"QEMU-native TLS\" transport for live "
"migration.  This will provide encryption for all migration streams, namely: "
"guest RAM, device state and disks on a non-shared setup that are transported"
" over NBD (Network Block Device), also called as \"block migration\"."
msgstr ""
"libvirt 드라이버는 현재 \"QEMU-native TLS\" 전송을 지원합니다.  이로 인해 모든 이주 스트림에 암호화가 "
"제공됩니다.  namely: 게스트 RAM, 장치 상태 및 비공유 설정에서 전송되는 NBD (네트워크 블록 장치)로 전송되는 디스크 및 "
"같은, namely: 게스트 RAM, 장치 상태 및 디스크."

#: ../../<reno.sphinxext stable/rocky>:820
msgid ""
"The libvirt driver now supports additional Cinder front-end QoS specs, "
"allowing the specification of additional IO burst limits applied for each "
"attached disk, individually."
msgstr ""
"libvirt 드라이버는 추가적인 Cinder 프런트 엔드 QoS 스펙을 지원합니다. 이로 인해 각 연결 디스크에 적용되는 추가적인 "
"I/O 부스트 제한을 individually 지정할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:637
msgid ""
"The libvirt driver now supports booting instances by asking for virtual "
"GPUs. In order to support that, the operators should specify the enabled "
"vGPU types in the nova-compute configuration file by using the configuration"
" option ``[devices]/enabled_vgpu_types``. Only the enabled vGPU types can be"
" used by instances."
msgstr ""
"libvirt 드라이버는 now virtual GPUs를 사용하여 인스턴스를 부트로딩하는 것을 지원합니다. 그에 따라, "
"operators는 nova-compute 구성 파일에서 ``[devices]/enabled_vgpu_types`` 옵션을 사용하여 "
"활성화된 vGPU 타입을 지정해야 합니다. 활성화된 vGPU 타입만 인스턴스에 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:574
msgid ""
"The libvirt driver now supports booting instances with virtual persistent "
"memory (vPMEM), also called persistent memory (PMEM) namespaces. To enable "
"vPMEM support, the user should specify the PMEM namespaces in the "
"``nova.conf`` by using the configuration option "
"``[libvirt]/pmem_namespaces``. For example::"
msgstr ""
"libvirt 드라이버는 현재 가상 지속 메모리 (vPMEM)와 같은 가상 지속 메모리 (PMEM) 네임스페이스를 사용하여 인스턴스를 "
"부트로드하는 것을 지원합니다. vPMEM 지원을 활성화하려면 사용자가 ``nova.conf``에 PMEM 네임스페이스를 지정하여 "
"``[libvirt]/pmem_namespaces``라는 구성 옵션을 사용해야 합니다. 예를 들어:"

#: ../../<reno.sphinxext stable/ussuri>:603
msgid ""
"The libvirt driver now supports defining different virtual GPU types for "
"each physical GPU. See the ``[devices]/enabled_vgpu_types`` configuration "
"option for knowing how to do it. Please refer to "
"https://docs.openstack.org/nova/latest/admin/virtual-gpu.html for further "
"documentation."
msgstr ""
"libvirt 드라이버는 각 물리적 GPU에 대해 다른 가상 GPU 타입을 정의할 수 있습니다. \"devices\" 디렉터의 "
"\"enabled_vgpu_types\" 설정 옵션을 확인하여 어떻게 하는지 알려줍니다. 더 많은 정보는 "
"https://docs.openstack.org/nova/latest/admin/virtual-gpu.html에 있습니다."

#: ../../<reno.sphinxext stable/2025.1>:226
msgid ""
"The libvirt driver now supports hw_vif_model=igb image property if the "
"hypervisor has libvirt version 9.3.0 and qemu version 8.0.0 or higher."
msgstr ""
"libvirt 드라이버는 하이퍼바이저가 libvirt 버전 9.3.0 및 qemu 버전 8.0.0 이상일 때 "
"hw_vif_model=igb image 속성을 지원합니다."

#: ../../<reno.sphinxext stable/ussuri>:421
msgid ""
"The libvirt driver now supports live migration with virtual persistent "
"memory (vPMEM), which requires QEMU as hypervisor. In virtualization layer, "
"QEMU will copy vpmem over the network like volatile memory, due to the "
"typical large capacity of vPMEM, it may takes longer time for live "
"migration."
msgstr ""
"libvirt 드라이버는 현재 virtio-PMEM를 사용하여 live migration을 지원합니다. virtio-PMEM은 QEMU를"
" 하이퍼바이저로 사용해야 합니다. virtio-PMEM은 volatile memory와 마찬가지로 네트워크를 통해 복사됩니다. "
"virtio-PMEM의 일반적인 큰 용량으로 인해 live migration은 더 오래 걸릴 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:350
msgid ""
"The libvirt driver now supports requesting a configurable memory address "
"space for the instances. This allows `instances with large RAM requirements "
"<https://specs.openstack.org/openstack/nova-"
"specs/specs/2024.1/implemented/libvirt-maxphysaddr-support.html#flavor-"
"extra-specs>`_ to be created by specifying either "
"``hw:maxphysaddr_mode=emulate`` and ``hw:maxphysaddr_bits`` flavor extra "
"specs or  ``hw_maxphysaddr_mode`` and ``hw_maxphysaddr_bits``image "
"properties. The ``ImagePropertiesFilter`` and ``ComputeCapabilitiesFilter`` "
"filters are required to support this functionality."
msgstr ""
"libvirt 드라이버는 현재 인스턴스에 configurable memory address space를 요청할 수 있습니다. 이것은 "
"인스턴스에 큰 RAM 필요가 있는 경우에 `instances with large RAM requirements "
"<https://specs.openstack.org/openstack/nova-"
"specs/specs/2024.1/implemented/libvirt-maxphysaddr-support.html#flavor-"
"extra-specs>`_을 생성할 수 있도록 allowing합니다. 이 기능을 지원하려면 "
"`hw:maxphysaddr_mode=emulate`와 `hw:maxphysaddr_bits` flavor extra specs 또는 "
"`hw_maxphysaddr_mode`와 `hw_maxphysaddr_bits` image properties를 사용하여 either "
"specifying합니다. `ImagePropertiesFilter`와 `ComputeCapabilitiesFilter` 필터는 이 "
"기능을 지원하기 위해 필요합니다."

#: ../../<reno.sphinxext stable/pike>:1152
msgid ""
"The libvirt driver port filtering feature will now ignore the "
"``allow_same_net_traffic`` config option."
msgstr "libvirt 드라이버 포트 필터링 기능은 현재 ``allow_same_net_traffic`` 구성 옵션을 무시합니다."

#: ../../<reno.sphinxext stable/pike>:1122
msgid ""
"The libvirt driver port filtering feature will now ignore the ``use_ipv6`` "
"config option."
msgstr "libvirt 드라이버 포트 필터링 기능은 현재 ``use_ipv6`` 설정 옵션을 무시합니다."

#: ../../<reno.sphinxext stable/pike>:1125 stable/pike>:1155
msgid ""
"The libvirt driver provides port filtering capability. This capability is "
"enabled when the following is true:"
msgstr "libvirt 드라이버는 포트 필터링 기능을 제공합니다. 이 기능은 다음 조건이 true일 때 활성화됩니다."

#: ../../<reno.sphinxext stable/train>:922
msgid ""
"The libvirt driver's RBD imagebackend no longer supports setting "
"force_raw_images to False. Setting force_raw_images = False and images_type "
"= rbd in nova.conf will cause the nova compute service to refuse to start. "
"To fix this, set force_raw_images = True. This change was required to fix "
"the `bug 1816686`_."
msgstr ""
"libvirt 드라이버의 RBD 이미지 백엔드가 더 이상 force_raw_images를 False로 설정할 수 없으며, "
"force_raw_images = False와 images_type = rbd를 nova.conf에 설정하면 nova 컴퓨터 서비스가 "
"시작하지 않게 됩니다. 이 문제를 해결하려면 force_raw_images = True를 설정해야 합니다. 이 변경은 bug "
"1816686_를修正하기 위해 필요했습니다."

#: ../../<reno.sphinxext stable/2025.2>:29
msgid ""
"The libvirt guest XML now includes additional flavor and image metadata "
"fields so that it can be used during troubleshooting or services like "
"Ceilometer can retrieve accurate instance information directly, without "
"performing extra Nova API calls."
msgstr ""
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있도록 추가적인 flavor 및 image metadata fields가 포함되어 있습니다. 이로써 libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있도록 추가적인 flavor 및 image metadata fields가 포함되어 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스를 위해 accurate instance information을 retrieves 할 수 있습니다. \n"
"\n"
"libvirt guest XML은 troubleshooting 또는 Ceilometer와 같은 서비스"

#: ../../<reno.sphinxext unmaintained/victoria>:125 unmaintained/wallaby>:198
#: unmaintained/xena>:224 unmaintained/yoga>:511
msgid ""
"The libvirt virt driver in Nova implements power on and hard reboot by "
"destroying the domain first and unpluging the vifs then recreating the "
"domain and replugging the vifs. However nova does not wait for the network-"
"vif-plugged event before unpause the domain. This can cause the domain to "
"start running and requesting IP via DHCP before the networking backend has "
"finished plugging the vifs. The config option "
"[workarounds]wait_for_vif_plugged_event_during_hard_reboot has been added, "
"defaulting to an empty list, that can be used to ensure that the libvirt "
"driver waits for the network-vif-plugged event for vifs with specific "
"``vnic_type`` before it unpauses the domain during hard reboot. This should "
"only be used if the deployment uses a networking backend that sends such "
"event for the given ``vif_type`` at vif plug time. The ml2/ovs and the "
"networking-odl Neutron backend is known to send plug time events for ports "
"with ``normal`` ``vnic_type``.  For more information see "
"https://bugs.launchpad.net/nova/+bug/1946729"
msgstr ""
"libvirt virt 드라이버가 Nova에서 전력 부여 및 하드 리부트를 implements한다. 전력 부여와 하드 리부트를 위해 "
"도메인을 first로 파괴하고 vifs를 제거하고, 도메인을 재생성하고 vifs를 재가속시킨다. 그러나 Nova는 네트워크-vif-"
"plugged 이벤트를 대기하지 않고 도메인을 일시정지시한다. 이것은 도메인이 시작되어 DHCP를 통해 IP를 요청하고 네트워크 백엔드가"
" vifs를 가속시킨 후에만 일시정지시되는 것을 유발할 수 있다. "
"[workarounds]wait_for_vif_plugged_event_during_hard_reboot라는 config 옵션은, "
"기본적으로 비어있는 목록으로, libvirt 드라이버가 vifs의 특정 \"vnic_type\"에 대해 네트워크-vif-plugged "
"이벤트를 대기하는 것을 보장하는 데 사용할 수 있다. 이것은 only 사용할 수 있는 경우가 있으며, 배포가 네트워크 백엔드가 vif "
"type에 대해 plug time 이벤트를 전달하는 경우에만 사용해야 한다. ml2/ovs 및 networking-odl Neutron "
"backend는 \"normal\" vnic_type의 포트에 대해 plug time 이벤트를 전달한다. 더 많은 정보는 "
"https://bugs.launchpad.net/nova/+bug/1946729에서 확인할 수 있다."

#: ../../<reno.sphinxext unmaintained/wallaby>:415
msgid "The libvirt virt driver now supports new features :"
msgstr "libvirt virt 드라이버는 새로운 기능을 지원합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:227 unmaintained/wallaby>:951
msgid ""
"The libvirt virt driver will no longer attempt to fetch volume encryption "
"metadata or the associated secret key when attaching ``LUKSv1`` encrypted "
"volumes if a libvirt secret already exists on the host."
msgstr ""
"libvirt virt 드라이버는 호스트에 이미 존재하는 libvirt secret가 있는 경우, LUKSv1로 암호화된 볼륨에 대해 "
"볼륨 암호화 메타데이터 또는 관련 secret key를 가져오지 않게 할 것이다."

#: ../../<reno.sphinxext unmaintained/wallaby>:648
msgid ""
"The libvirt virt driver will now attempt to record the machine type of an "
"instance at startup and when launching an instance if the machine type is "
"not already recorded in the image metadata associated with the instance."
msgstr ""
"libvirt virt 드라이버는 현재 인스턴스의 máy체형을 시작時に 기록하고 인스턴스를 시작할 때, 인스턴스와 관련된 이미지 "
"메타데이터에 기재된 máy체형이 이미 기록되어 있지 않다면 기록할 것이다."

#: ../../<reno.sphinxext stable/ussuri>:414
msgid ""
"The lists of operations that are supported or unsupported for instances with"
" accelerators are listed in `accelerator operation guide "
"<https://docs.openstack.org/api-guide/compute/accelerator-support.html>`_"
msgstr ""
"`인스턴스에 가속기가 있는 경우 지원 또는 비 지원되는 연산의 목록은 <https://docs.openstack.org/api-"
"guide/compute/accelerator-support.html>`_ 가속기 연산 지침에 listed됩니다."

#: ../../<reno.sphinxext stable/rocky>:1799
msgid "The live, rolling upgrade has all compute hosts running Rocky code"
msgstr "live, rolling upgrade는 모든 compute host가 Rocky 코드를 실행합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:936 stable/pike>:1009
msgid ""
"The live-migration progress timeout controlled by the configuration option "
"``[libvirt]/live_migration_progress_timeout`` has been discovered to "
"frequently cause live-migrations to fail with a progress timeout error, even"
" though the live-migration is still making good progress. To minimize "
"problems caused by these checks we have changed the default to 0, which "
"means do not trigger a timeout. To modify when a live-migration will fail "
"with a timeout error, please now look at "
"``[libvirt]/live_migration_completion_timeout`` and "
"``[libvirt]/live_migration_downtime``."
msgstr ""
"live-migration progress timeout controlled by the configuration option "
"``[libvirt]/live_migration_progress_timeout``는 일반적으로 live-migration이 성공적으로 "
"진행되지만 시간이 지남에 따라 live-migration이 실패하는 경우가 많다. 이러한 확인을 통해 발생하는 문제를 최소화하기 위해 "
"기본적으로 0으로 설정되었습니다. 이는 시간 초과 오류가 발생하지 않도록 트리거하지 않습니다. live-migration이 시간 초과 "
"오류로 실패하는 경우를 수정하려면 ``[libvirt]/live_migration_completion_timeout``와 "
"``[libvirt]/live_migration_downtime``를 확인하세요."

#: ../../<reno.sphinxext unmaintained/wallaby>:672
msgid ""
"The machine type is supported. The supported list includes alias and "
"versioned types of ``pc``, ``pc-i440fx``, ``pc-q35``, ``q35``, ``virt``, "
"``s390-ccw-virtio``, ``hyperv-gen1`` and ``hyperv-gen2`` as supported by the"
" hyperv driver."
msgstr ""
"기계 유형은 지원됩니다. 지원 목록에는 alias 및 버전화된 유형의 ``pc``, ``pc-i440fx``, ``pc-q35``, "
"``q35``, ``virt``, ``s390-ccw-virtio``, ``hyperv-gen1`` 및 ``hyperv-gen2``가 "
"포함됩니다. 이러한 유형은 하이퍼바이저 드라이버에 의해 지원됩니다."

#: ../../<reno.sphinxext stable/stein>:993
msgid ""
"The main advantage to this is to catch invalid configurations as early as "
"possible so that we can return a useful error to the user rather than fail "
"later on much further down the stack where the operator would have to get "
"involved."
msgstr ""
"이것의 주요 이점은 불법적인 구성으로부터 가장 sớm으로 잡아 early configuration validation을 통해 사용자에게 "
"유용한 오류를 반환할 수 있기 때문입니다. 이로 인해 연쇄적으로 오류가 발생할 때 오퍼레이터가 관여할 필요가 없게 됩니다."

#: ../../<reno.sphinxext stable/rocky>:633
msgid ""
"The microversion 2.62 adds ``host`` (hostname) and ``hostId`` (an obfuscated"
" hashed host id string) fields to the instance action ``GET "
"/servers/{server_id}/os-instance-actions/{req_id}`` API. The display of the "
"newly added ``host`` field will be controlled via policy rule "
"``os_compute_api:os-instance-actions:events``, which is the same policy used"
" for the ``events.traceback`` field. If the user is prevented by policy, "
"only ``hostId`` will be displayed."
msgstr ""
"microversion 2.62에서 `host` (호스트 이름)과 `hostId` (호스트 ID를 가공한 해시된 문자열) field를 "
"인스턴스 액션 `GET /servers/{server_id}/os-instance-actions/{req_id}` API에 추가합니다. "
"새로운 `host` field의 표시는 policy rule `os_compute_api:os-instance-"
"actions:events`를 통해 제어됩니다. 이 policy는 `events.traceback` field와 동일한 policy를 "
"사용합니다. 사용자가 정책에 의해 차단되면, only `hostId`만 표시됩니다."

#: ../../<reno.sphinxext ../source/newton.rst:22 origin/stable/ocata>:270
#: stable/pike>:338
msgid ""
"The migration is optional and can be postponed if you have not been affected"
" by the bug. The bug manifests itself through \"Data too long for column "
"'spec'\" database errors."
msgstr ""
"집합은 선택적이며, 버그에 영향을 받지 않은 경우에만 지연될 수 있습니다. 버그는 \"Data too long for column "
"'spec'\" 데이터베이스 오류를 통해 itself를 나타냅니다."

#: ../../<reno.sphinxext stable/queens>:430 stable/rocky>:1522
msgid ""
"The minimum ``nova-osapi_compute`` service version is less than 15 in any "
"given cell"
msgstr "nova-osapi_compute 서비스의 최소 버전은任何เซลล에서 15보다 낮은もの입니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:758
msgid ""
"The minimum required version of QEMU used by the `nova-compute` service is "
"now 4.2.0. The next minimum required version to be used in a future release "
"is 5.2.0."
msgstr ""
"nova-compute 서비스가 사용하는 QEMU의 최소한의 필요한 버전은 현재 4.2.0이며, 미래의 릴리스에서 사용할 수 있는 "
"최소한의 필요한 버전은 5.2.0입니다."

#: ../../<reno.sphinxext stable/2023.2>:326
msgid ""
"The minimum required version of libvirt by the `nova-compute` service is now"
" 7.0.0, and the minimum required version of QEMU is 5.2.0. Failing to meet "
"these minimum versions when using the libvirt compute driver will result in "
"the `nova-compute` service not starting."
msgstr ""
"`nova-compute` 서비스에 의해 libvirt의 최소 필요 버전은 7.0.0이며, QEMU의 최소 필요 버전은 5.2.0입니다."
" libvirt compute 드라이버를 사용할 때, 이러한 최소 버전을 충족하지 않으면 `nova-compute` 서비스가 시작되지 "
"않습니다."

#: ../../<reno.sphinxext stable/2025.1>:426
msgid ""
"The minimum required version of libvirt by the `nova-compute` service is now"
" 8.0.0, and the minimum required version of QEMU is 6.2.0. Failing to meet "
"these minimum versions when using the libvirt compute driver will result in "
"the `nova-compute` service not starting. The next minimum required version "
"of libvirt to be used in a future release is 10.0.0, while the next minimum "
"QEMU is 8.2.2."
msgstr ""
"`nova-compute` 서비스의 libvirt의 최소 버전은 8.0.0이며, QEMU의 최소 버전은 6.2.0입니다. libvirt "
"compute 드라이버를 사용할 때, libvirt의 최소 버전이 8.0.0 이상이 아니면 `nova-compute` 서비스가 시작되지 "
"않을 수 있습니다. 미래 릴리스에서 사용할 수 있는 libvirt의 최소 버전은 10.0.0이며, QEMU의 최소 버전은 "
"8.2.2입니다."

#: ../../<reno.sphinxext stable/pike>:1236
msgid ""
"The minimum required version of libvirt used by the `nova-compute` service "
"is now 1.2.9. The minimum required version of QEMU used by the `nova-"
"compute` service is now 2.1.0. Failing to meet these minimum versions when "
"using the libvirt compute driver will result in the `nova-compute` service "
"not starting."
msgstr ""
"`nova-compute` 서비스에 사용되는 libvirt의 최소 필요 버전은 1.2.9이며, QEMU의 최소 필요 버전은 "
"2.1.0입니다. libvirt compute 드라이버를 사용할 때, 이러한 최소 버전을 충족하지 않으면 `nova-compute` "
"서비스가 시작되지 않습니다."

#: ../../<reno.sphinxext stable/rocky>:1747
msgid ""
"The minimum required version of libvirt used by the `nova-compute` service "
"is now 1.3.1.  And the minimum required version of QEMU used by the `nova-"
"compute` service is now 2.5.0. Failing to meet these minimum versions when "
"using the libvirt compute driver will result in the `nova-compute` service "
"not starting."
msgstr ""
"`nova-compute` 서비스에 사용되는 `libvirt` 라이브러리 버전은 현재 1.3.1이며, `nova-compute` 서비스에"
" 사용되는 `QEMU` 버전은 현재 2.5.0입니다. `libvirt` 컴퓨터 드라이버를 사용할 때, 이러한 최소 버전을 충족하지 않으면"
" `nova-compute` 서비스가 시작되지 않습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:709
msgid ""
"The minimum required version of libvirt used by the `nova-compute` service "
"is now 5.0.0. The minimum required version of QEMU used by the `nova-"
"compute` service is now 4.0.0. Failing to meet these minimum versions when "
"using the libvirt compute driver will result in the `nova-compute` service "
"not starting."
msgstr ""
"`nova-compute` 서비스에 사용되는 `libvirt` 라이브러리 버전은 현재 5.0.0이며, `nova-compute` 서비스에"
" 사용되는 QEMU 버전은 현재 4.0.0입니다. `libvirt` 컴퓨터 드라이버를 사용할 때, 이러한 최소 버전을 충족하지 않으면 "
"`nova-compute` 서비스가 시작되지 않습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:754
msgid ""
"The minimum required version of libvirt used by the `nova-compute` service "
"is now 6.0.0. The next minimum required version to be used in a future "
"release is 7.0.0."
msgstr ""
"`nova-compute` 서비스에 사용되는 `libvirt`의 최소 필요 버전은 현재 6.0.0이며, 미래의 릴리즈에서 사용할 수 있는"
" 최소 필요 버전은 7.0.0입니다."

#: ../../<reno.sphinxext stable/queens>:1332
msgid "The minimum required version of shred has been increased to 8.22."
msgstr "집합의 최소 필요 버전이 8.22로 증가했습니다."

#: ../../<reno.sphinxext stable/rocky>:1462
msgid ""
"The minimum version of libvirt on AArch64 architecture that nova compute "
"will interoperate with is now 3.6.0. Deployments using older versions of "
"libvirt on AArch64 should upgrade."
msgstr ""
"libvirt의 AArch64 아키텍처에서 nova compute와 interoperability를 위해 최소한의 버전은 "
"3.6.0입니다. AArch64에서 libvirt의 older 버전을 사용하는 배포는 업그레이드해야 합니다."

#: ../../<reno.sphinxext stable/pike>:692
msgid ""
"The model name vhostuser_vrouter_plug is set by the neutron contrail plugin "
"during a VM (network port) creation. The libvirt compute driver now supports"
" plugging virtual interfaces of type \"contrail_vrouter\" which are provided"
" by the contrail-nova-vif-driver plugin [1]. [1] "
"https://github.com/Juniper/contrail-nova-vif-driver"
msgstr ""
"모델 이름 vhostuser_vrouter_plug은 네트워크 포트 (VM) 생성 시 네트론 (Neutron) 컨트레일 플러그인에 의해 "
"설정됩니다. libvirt 컴퓨터 드라이버는 현재 \"contrail_vrouter\" 타입의 가상 인터페이스를 지원합니다. 이 가상 "
"인터페이스는 contrail-nova-vif-driver 플러그인 [1]에서 제공됩니다. [1] "
"https://github.com/Juniper/contrail-nova-vif-driver"

#: ../../<reno.sphinxext stable/queens>:1841
msgid "The motivation behind these changes are:"
msgstr "이 변경의 배경은 다음과 같습니다."

#: ../../<reno.sphinxext stable/pike>:677
msgid ""
"The network.json metadata format has been amended for IPv6 networks under "
"Neutron control. The type that is shown has been changed from being always "
"set to ``ipv6_dhcp`` to correctly reflecting the ``ipv6_address_mode`` "
"option in Neutron, so the type now will be ``ipv6_slaac``, "
"``ipv6_dhcpv6-stateless`` or ``ipv6_dhcpv6-stateful``."
msgstr ""
"네트워크.json 메타데이터 형식은 네트워크를 제어하는 네트론(Neutron) 하에 IPv6 네트워크에 대한 수정이 이루어졌습니다. "
"표시된 유형은 항상 설정되어 있는 유형에서 ``ipv6_dhcp``에서 ``ipv6_address_mode`` 옵션을 correctly "
"반영하기 위해 변경되었습니다. 따라서 현재 유형은 ``ipv6_slaac``, ``ipv6_dhcpv6-stateless`` 또는 "
"``ipv6_dhcpv6-stateful``입니다."

#: ../../<reno.sphinxext stable/2025.1>:368
msgid ""
"The new ``[libvirt] volume_enforce_multipath`` option has been added. When "
"this option is set to ``True``, attachment of volumes is aborted when "
"multipathd is not running in the host. Otherwise it falls back to single "
"path. This option also makes the libvirt driver to check multipathd during "
"initialization, and the compute service fails to start if mulitipathd is not"
" running."
msgstr ""
"``[libvirt] volume_enforce_multipath`` 옵션은 새로 추가되었습니다. 이 옵션을 ``True`` 로 설정하면"
" 호스트에서 multipathd가 실행되지 않으면 볼륨의 연결이 취소되며, 반대로 single path로 fallback합니다. 이 "
"옵션은 libvirt 드라이버가 초기화 시 multipathd를 확인하고, compute 서비스가 실행되지 않게 됩니다."

#: ../../<reno.sphinxext stable/rocky>:1579
msgid ""
"The new ``[scheduler]workers`` configuration option defaults to ``ncpu`` "
"workers if using the ``filter_scheduler`` scheduler driver. If you are "
"running *nova-scheduler* on the same host as other services, you may want to"
" change this default value, or to otherwise account for running other "
"instances of the *nova-scheduler* service."
msgstr ""
"새로운 `[scheduler]workers` 구성 옵션은 `filter_scheduler` 스케줄러 드라이버를 사용할 때 `ncpu` "
"노드의 노드 수를 기본으로 설정합니다. *nova-scheduler*를 다른 서비스와 cùng 호스트를 사용하는 경우, 이 기본值를 "
"변경하거나 다른 *nova-scheduler* 서비스 인스턴스를 실행하는 경우에 대해 다른 인스턴스를 실행하는 경우에 대해 계정해야 할 "
"수 있습니다."

#: ../../<reno.sphinxext stable/pike>:783
msgid ""
"The new ``img_hide_hypervisor_id`` image metadata property hides the "
"hypervisor signature for the guest."
msgstr "새로운 `img_hide_hypervisor_id` 이미지 메타데이터 속성은 게스트에 대한 하이퍼바이저 서명이 숨겨진다."

#: ../../<reno.sphinxext stable/2024.1>:400
msgid ""
"The new config option ``[libvirt]migration_inbound_addr`` is now used to "
"determine the address for incoming move operations (cold migrate, resize, "
"evacuate). This config is defaulted to [DEFAULT]my_ip to keep the "
"configuration backward compatible. However it allows an explicit hostname or"
" FQDN to be specified, or allows to specify '%s' that is then resolved to "
"the hostname of compute host. Note that this config should only be changed "
"from its default after every compute is upgraded."
msgstr ""
"새로운 구성 옵션 ``[libvirt]migration_inbound_addr``는 현재 이동 연산 (cold migrate, "
"resize, evacuate)에서 incoming 연산에 대한 주소를 결정하는 데 사용됩니다. 이 구성은 [DEFAULT]my_ip로 "
"기본적으로 설정되어 backwardly compatible를 유지합니다. 그러나 명시적인 호스트 이름 또는 FQDN을 지정하거나 "
"'%s'를 지정하여 호스트 이름을 컴퓨터 호스트의 호스트 이름으로解析할 수 있습니다. 그러나 이 구성은 컴퓨터가 업그레이드된 후에만 "
"변경되어야 합니다."

#: ../../<reno.sphinxext stable/pike>:1102
msgid ""
"The new configuration option "
"``[compute]/consecutive_build_service_disable_threshold`` defaults to a "
"nonzero value, which means multiple failed builds will result in a compute "
"node auto-disabling itself."
msgstr ""
"새로운 구성 옵션 ``[compute]/consecutive_build_service_disable_threshold`` 기본적으로 비 "
"zero 가치로 설정되어 있으며, 이는 여러 실패 builds가 컴퓨터 노드가 자체적으로 자체 비활성화되는 것을 의미합니다."

#: ../../<reno.sphinxext stable/stein>:925
msgid ""
"The new configuration option, ``[compute]/max_disk_devices_to_attach`` "
"defaults to ``-1`` (unlimited). Users of the libvirt driver should be "
"advised that the default limit for non-ide disk buses has changed from 26 to"
" unlimited, upon upgrade to Stein. The ``ide`` disk bus continues to be "
"limited to 4 attached devices per server."
msgstr ""
"새한 구성 옵션, ``[compute]/max_disk_devices_to_attach``, 기본적으로 ``-1`` (무한)으로 "
"설정된다. libvirt 드라이버 사용자는 Stein 업그레이드에 따라 non-ide 디스크 버스에 대한 기본 제한이 26에서 무한으로 "
"변경되었다고 경고해야 한다. ``ide`` 디스크 버스는 각 서버에 4개의 연결 thiết bị만 제한된다."

#: ../../<reno.sphinxext stable/rocky>:1384
msgid ""
"The new style ``policy`` field has been added to ``ServerGroupPayload``. The"
" ``server_group.create``, ``server_group.delete`` and "
"``server_group.add_member`` versioned notifications will be updated to "
"include the new ``policy`` and ``rules`` field. The ``policies`` field is "
"deprecated for removal but still put into the notification payload for "
"backward compatibility."
msgstr ""
"새로운 스타일 \"policy\" 필드는 \"ServerGroupPayload\"에 추가되었다. "
"\"server_group.create\", \"server_group.delete\" 및 "
"\"server_group.add_member\" 버전화된 알림은 새로운 \"policy\" 및 \"rules\" 필드를 포함하여 "
"업데이트된다. \"policies\" 필드는 제거를 위해弃기되었지만 여전히 알림 파이프로 후 backward compatibility를 "
"위해 추가된다."

#: ../../<reno.sphinxext stable/2023.2>:332
msgid ""
"The next minimum required version of libvirt to be used in a future release "
"is 8.0.0, while the next minimum QEMU is 6.2.0."
msgstr ""
"다음 릴리스에서 사용할 수 있는 libvirt의 최소 버전은 8.0.0이며, 다음 릴리스에서 사용할 수 있는 QEMU의 최소 버전은 "
"6.2.0입니다."

#: ../../<reno.sphinxext stable/2024.2>:56 stable/2025.1>:142
#: stable/2025.2>:536
msgid ""
"The nova (metadata)api wsgi application will now detect fatal errors "
"(configuration, et al) on startup and lock into a permanent error state "
"until fixed and restarted. This solves a problem with some wsgi runtimes "
"ignoring initialization errors and continuing to send requests to the half-"
"initialized service. See https://bugs.launchpad.net/nova/+bug/2103811 for "
"more details."
msgstr ""
"nova (metadata) API WSGI 응용 프로그램은 이제 시작時に configuration, etc.와 같은_fatal 오류를 "
"감지하고 영구 오류 상태로 고정되며 고정되지 않으면 재시작되지 않도록 고정된다. 이는 일부 WSGI 런타임이 초기화 오류를 무시하고 "
"반복적으로 요청을 보냈을 때 발생하는 문제를 해결한다. 더 많은รายละเอียด은 "
"https://bugs.launchpad.net/nova/+bug/2103811 에서 확인할 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:671
msgid ""
"The nova Hyper-V driver can now plug OVS VIFs. This means that neutron-ovs-"
"agent can be used as an L2 agent instead of neutron-hyperv-agent. In order "
"to plug OVS VIFs, the configuration option \"vswitch_name\" from the "
"\"hyperv\" section must be set to the vSwitch which has the OVS extension "
"enabled. Hot-plugging is only supported on Windows / Hyper-V Server 2016 + "
"Generation 2 VMs. Older Hyper-V versions only support attaching vNICs while "
"the VM is turned off."
msgstr ""
"nova Hyper-V 드라이버는 현재 OVS VIF를 연결할 수 있습니다. 이것은 neutron-ovs-agent가 neutron-"
"hyperv-agent 대신 L2.agent로 사용할 수 있음을 의미합니다. OVS VIF를 연결하려면 \"hyperv\" 섹션의 "
"\"vswitch_name\" 설정 옵션을 OVS 확장 기능이 활성화된 vSwitch에 설정해야 합니다. Windows / Hyper-V"
" Server 2016 + Generation 2 VMs에서만 Hot-plugging이 지원됩니다. older Hyper-V 버전은 "
"VM이.off 상태에서 vNIC을 연결할 수만 지원됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:675
msgid ""
"The nova Hyper-V driver now supports adding PCI passthrough devices to "
"Hyper-V instances (discrete device assignment). This feature has been "
"introduced in Windows / Hyper-V Server 2016 and offers the possibility to "
"attach some of the host's PCI devices (e.g.: GPU devices) directly to "
"Hyper-V instances. In order to benefit from this feature, Hyper-V compute "
"nodes must support SR-IOV and must have assignable PCI devices. This can "
"easily be checked by running the following powershell commands::"
msgstr ""
"nova Hyper-V 드라이버는 현재 하이퍼-V 인스턴스 (분리 장치 할당) 에서 PCI 패스-through 장치를 추가할 수 "
"있습니다. 이 기능은 Windows / 하이퍼-V Server 2016에서 도입되었으며, 하이퍼-V 인스턴스에 일부 호스트의 PCI 장치"
" (예: 그래픽 장치)를 직접 연결할 수 있는 가능성이 있습니다. 이 기능을 이익받으려면 하이퍼-V 컴퓨터 노드는 SR-IOV를 지원해야"
" 하며, 할당 가능한 PCI 장치가 있어야 합니다. 이 기능을 확인하기 위해 다음의 powershell 명령을 실행할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:710
msgid ""
"The nova Hyper-V driver now supports symmetric NUMA topologies. This means "
"that all the NUMA nodes in the NUMA topology must have the same amount of "
"vCPUs and memory. It can easily be requested by having the flavor extra_spec"
" \"hw:numa_nodes\", or the image property \"hw_numa_nodes\". An instance "
"with NUMA topology cannot have dynamic memory enabled. Thus, if an instance "
"requires a NUMA topology, it will be spawned without dynamic memory, "
"regardless of the value set in the \"dynamic_memory_ratio\" config option in"
" the compute node's \"nova.conf\" file. In order to benefit from this "
"feature, the host's NUMA spanning must be disabled. Hyper-V does not "
"guarantee CPU pinning, thus, the nova Hyper-V driver will not spawn "
"instances with the flavor extra_spec \"hw:cpu_policy\" or image property "
"\"hw_cpu_policy\" set to \"dedicated\"."
msgstr ""
"nova 하이퍼-V 드라이버는 현재 대칭형 NUMA topology를 지원합니다. 이 의미는 NUMA topology의 모든 NUMA "
"노드가 동일한 vCPUs 및 메모리를 가지고 있어야 합니다. 이 기능을 사용하고 싶다면, flavor extra_spec "
"\"hw:numa_nodes\" 또는 image property \"hw_numa_nodes\"를 사용하여 쉽게 요청할 수 있습니다. "
"NUMA topology를 가진 인스턴스는 동적 메모리를 활성화할 수 없습니다. 따라서 NUMA topology를 가진 인스턴스는 "
"\"nova.conf\" 파일의 \"dynamic_memory_ratio\" config 옵션의 값이 설정된 것과 상관없이 동적 메모리 "
"활성화가 안 된 인스턴스를 생성합니다. 이 기능을 이익으로 얻으려면 호스트의 NUMA spanning이 비활성화되어야 합니다. 하이퍼-"
"V는 CPU pinning에 대한 보증을 제공하지 않기 때문에, nova 하이퍼-V 드라이버는 flavor extra_spec "
"\"hw:cpu_policy\" 또는 image property \"hw_cpu_policy\"를 \"dedicated\"로 설정하여 "
"인스턴스를 생성하지 않습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:270 unmaintained/xena>:686
msgid ""
"The nova libvirt driver supports two independent features, virtual CPU "
"topologies and virtual NUMA topologies. Previously, when "
"``hw:cpu_max_sockets``, ``hw:cpu_max_cores`` and ``hw:cpu_max_threads`` were"
" specified for pinned instances (``hw:cpu_policy=dedicated``) without "
"explicit ``hw:cpu_sockets``, ``hw:cpu_cores``, ``hw:cpu_threads`` extra "
"specs or their image equivalent, nova failed to generate a valid virtual CPU"
" topology. This has now been fixed and it is now possible to use max CPU "
"constraints with pinned instances. e.g. a combination of  "
"``hw:numa_nodes=2``, ``hw:cpu_max_sockets=2``, ``hw:cpu_max_cores=2``, "
"``hw:cpu_max_threads=8`` and ``hw:cpu_policy=dedicated`` can now generate a "
"valid topology using a flavor with 8 vCPUs."
msgstr ""
"nova libvirt driver는 가상 CPU topology 및 가상 NUMA topology를 지원합니다. 이전에, pinned "
"instance (``hw:cpu_policy=dedicated``)에서 ``hw:cpu_sockets``, "
"``hw:cpu_cores``, ``hw:cpu_threads``를 명시적으로 추가한 스펙이나 그 equivalent image가 없는 "
"경우, ``hw:cpu_max_sockets``, ``hw:cpu_max_cores`` 및 ``hw:cpu_max_threads``가 "
"지정된 경우 nova는 가상 CPU topology를 생성할 수 없었습니다. 이 문제는 현재 해결되었으며 pinned instance에서"
" max CPU constraint를 사용할 수 있습니다. 예를 들어, 2개의 NUMA 노드, 2개의 CPU 스ockets, 2개의 "
"CPU 코어, 8개의 CPU 스레드 및 ``hw:cpu_policy=dedicated``를 사용하여 8개의 vCPU가 있는 플레바를 "
"사용하여 가상 CPU topology를 생성할 수 있습니다."

#: ../../<reno.sphinxext stable/train>:190 stable/ussuri>:1379
msgid ""
"The nova libvirt virt driver supports creating instances with multi-queue "
"virtio network interfaces. In previous releases nova has based the maximum "
"number of virtio queue pairs that can be allocated on the reported kernel "
"major version. It has been reported in `bug #1847367`_ that some distros "
"have backported changes from later major versions that make major version "
"number no longer suitable to determine the maximum virtio queue pair count. "
"A new config option has been added to the libvirt section of the nova.conf. "
"When defined nova will now use the ``[libvirt]/max_queues`` option to define"
" the max queues that can be configured, if undefined it will fallback to the"
" previous kernel version approach."
msgstr ""
"nova libvirt virt 드라이버는 다중 큐 virtio 네트워크 인터페이스를 사용하여 인스턴스를 생성할 수 있습니다. 이전 "
"릴리스에서 nova는 virtio 큐 pair의 최대 수를 할당할 수 있는 최대ernel 메이저 버전을 기반으로했습니다. `bug "
"#1847367`_에 따르면 일부 배포판은 나중에 버전의 변경 사항을 후원하여 메이저 버전 번호가 더 이상 virtio 큐 pair "
"카운트를 결정하는 데 적합하지 않게 변경되었다고 보고되었습니다. nova.conf의 libvirt 섹션에 새로운 구성 옵션을 "
"추가했습니다. nova가 정의되면 now virtio 큐 pair의 최대 수를 정의하는 `[libvirt]/max_queues` 옵션을 "
"사용합니다. undefined일 경우 이전 커널 버전의 접근 방식을 fallback합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1612
msgid ""
"The nova metadata service will now pass a nove service token to the external"
" vendordata server. These options can be configured using various Keystone-"
"related options available in the ``vendordata_dynamic_auth`` group. A new "
"service token has been created for this purpose. Previously, the requesting "
"user's keystone token was passed through to the external vendordata server "
"if available, otherwise no token is passed. This resolves issues with "
"scenarios such as cloud-init's use of the metadata server on first boot to "
"determine configuration information.  Refer to the blueprints at "
"http://specs.openstack.org/openstack/nova-"
"specs/specs/ocata/approved/vendordata-reboot-ocata.html for more "
"information."
msgstr ""
"nova 메타데이터 서비스는 이제 외부 비즈니스 데이터 서버에 노바 서비스 토큰을 전달합니다. 이 옵션은 "
"`vendordata_dynamic_auth` 그룹에 있는 다양한 Keyston과 관련된 옵션을 사용하여 구성할 수 있습니다. 새로운 "
"서비스 토큰가 이 목적을 위해 생성되었습니다. 이전에, 요청하는 사용자의 Keyston 토큰이 외부 비즈니스 데이터 서버로 전달되면, "
"accessible 경우, otherwise 토큰이 전달되지 않습니다. 이는 클라우드-인िट의 메타데이터 서버의 첫 부트 시에 구성 "
"정보를 결정하는 경우와 같은 시나리오에 대한 문제를 해결합니다.  더 많은 정보를 얻으려면 "
"http://specs.openstack.org/openstack/nova-"
"specs/specs/ocata/approved/vendordata-reboot-ocata.html에 대한 블루프린트를 참조하십시오."

#: ../../<reno.sphinxext stable/2025.1>:257
msgid ""
"The nova scheduler now supports enabling the nova cell discover hosts "
"perodic task on multiple schedulers. In prior release enabling this feature "
"required setting the discover_hosts_in_cells_interval option to a value "
"greater than 0 in at most one scheduler, with the 2025.1 release it was "
"possible to enable the feature on multiple schedulers via the introduction "
"of leader election. This simplifies deployment of nova in kubernetes by "
"allowing the operator to deploy multiple schedulers and have them elect a "
"single leader that will run the discover hosts perodic task."
msgstr ""
"nova 스케줄러는 현재 nova 세ลล์ discovery 호스트를 여러 스케줄러에서 지원하는 periodic task를 활성화할 수 "
"있습니다. 이전 버전에서는 이 기능을 활성화하려면 discover_hosts_in_cells_interval 옵션을 0보다 큰 giá치로"
" 설정하여 최소한 한 스케줄러에서만 활성화해야했습니다. 2025.1 버전에서는 리더 선출을 도입하여 여러 스케줄러에서 이 기능을 활성화할"
" 수 있습니다. 이는 nova를 kubernetes에서 배포하는 데 도움이 됩니다. 운영자는 여러 스케줄러를 배포할 수 있으며, 그 중 "
"하나를 리더로 선출하여 discover hosts periodic task를 실행할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1280
msgid ""
"The nova-all binary to launch all services has been removed after a "
"deprecation period. It was only intended for testing purposes and not "
"production use. Please use the individual Nova binaries to launch services."
msgstr ""
"nova-all은 모든 서비스를 시작하기 위해 사용하는 binary를 deprecation period이 끝난 후 제거했습니다. 이 "
"binary는 단지 테스트 용도로만 사용되었으며 프로덕션 용도로 사용하지 않습니다. individual Nova binaries를 "
"사용하여 서비스를 시작하세요."

#: ../../<reno.sphinxext stable/2025.2>:237
msgid ""
"The nova-api and nova-metadata services now can be run in native threading "
"mode instead of with eventlet. This is an experimental feature that is "
"disabled by default. Please test the native threading mode in pre-production"
" before enabling it in production. If you do so please let us now how it "
"went on the mailing list openstack-discuss@lists.openstack.org. Please read "
"the `concurrency "
"<https://docs.openstack.org/nova/latest/admin/concurrency.html>`__ guide for"
" more details."
msgstr ""
"nova-api 및 nova-metadata 서비스는 native threading mode에서 실행할 수 있습니다. 이 기능은 "
"기본적으로 비활성화되어 있습니다. production에서 활성화할 때는 pre-production에서 native threading "
"mode를 테스트하여야 합니다. 테스트를 완료한 경우 openstack-discuss@lists.openstack.org의 mailing"
" list에 알려 주세요. 더 많은 정보를 얻으려면 `concurrency "
"<https://docs.openstack.org/nova/latest/admin/concurrency.html>`__ guide를 "
"읽어보세요."

#: ../../<reno.sphinxext stable/rocky>:955
msgid ""
"The nova-api service needs to understand how to connect to the placement "
"service in order for this mirroring process to work. Administrators should "
"ensure that there is a ``[placement]`` section in the nova.conf file which "
"is used by the nova-api service, and that credentials for interacting with "
"placement are contained in that section."
msgstr ""
"nova-api 서비스는 mirroring 프로세스를 작동시키기 위해 placement 서비스와 연결하는 방법을 이해해야 합니다. "
"관리자는 nova.conf 파일에 ``[placement]`` 섹션이 있는지 확인해야 하며, nova-api 서비스가 사용하는 "
"credentails가 đó 섹션에 포함되어 있는지 확인해야 합니다."

#: ../../<reno.sphinxext stable/queens>:490 stable/rocky>:1977
msgid ""
"The nova-compute service now allows specifying the interval for updating "
"nova-compute-side cache of the compute node resource provider's aggregates "
"and traits info via a new config option called "
"``[compute]/resource_provider_association_refresh`` which defaults to 300. "
"This was previously hard-coded to run every 300 seconds which may be too "
"often in a large deployment."
msgstr ""
"nova-compute 서비스는 현재 nova-compute-side 캐시의 업데이트 интер벌을 설정할 수 있습니다. nova-"
"compute-side 캐시의 업데이트 인터벌은 compute 노드 리소스 제공자 집합 및 속성 정보의 "
"`compute/resource_provider_association_refresh` 옵션을 통해 설정할 수 있습니다. 이 옵션은 "
"기본적으로 300초로 설정되어 있으며, 이전에는 300초마다 실행되었습니다. 그러나 이러한 설정은 큰 배포에서 너무 자주 실행되었습니다."

#: ../../<reno.sphinxext branch>:14 current
msgid ""
"The nova-conductor services now can be run in native threading mode instead "
"of with eventlet. This is an experimental feature that is disabled by "
"default. Please test the native threading mode in pre-production before "
"enabling it in production. Please read the `concurrency "
"<https://docs.openstack.org/nova/latest/admin/concurrency.html>`__ guide for"
" more details."
msgstr ""
"nova-conductor 서비스는 native threading 모드에서 실행할 수 있습니다. 이 기능은 기본적으로 비활성화되어 "
"있습니다. production에서 활성화할 때는 pre-production에서 native threading 모드를 테스트하세요. 더 "
"많은 정보는 <https://docs.openstack.org/nova/latest/admin/concurrency.html>의 "
"guide를 참조하세요."

#: ../../<reno.sphinxext stable/rocky>:551
msgid "The nova-consoleauth service has been deprecated."
msgstr "nova-consoleauth 서비스는弃용되었습니다."

#: ../../<reno.sphinxext stable/queens>:1369
msgid ""
"The nova-idmapshift binary has been removed. This has been replaced by "
"internal functionality using privsep."
msgstr "nova-idmapshift 바이너리가 제거되었다. 이에 대체된 것은 privsep를 사용한 내부 기능이다."

#: ../../<reno.sphinxext stable/rocky>:1219
msgid ""
"The nova-manage command now has a 'db purge' command that will delete data "
"from the shadow tables after 'db archive_deleted_rows' has been run. There "
"is also now a ``--purge`` option for 'db archive_deleted_rows' that will "
"automatically do a full purge after archiving."
msgstr ""
"nova-manage 명령은 jetzt 'db purge' 명령이 있습니다. 이 명령은 'db archive_deleted_rows' "
"명령이 실행된 후 스토어 테이블에 있는 데이터를 삭제합니다. 또한 'db archive_deleted_rows' 명령에는 '--"
"purge' 옵션도 있습니다. 이 옵션은 아카이브를 사용한 후에 전체적으로 데이터를 제거합니다."

#: ../../<reno.sphinxext stable/pike>:275 stable/queens>:566 stable/rocky>:797
msgid ""
"The nova-manage discover_hosts command now has a ``--by-service`` option "
"which allows discovering hosts in a cell purely by the presence of a nova-"
"compute binary. At this point, there is no need to use this unless you're "
"using ironic, as it is less efficient. However, if you are using ironic, "
"this allows discovery and mapping of hosts even when no ironic nodes are "
"present."
msgstr ""
"nova-manage discover_hosts 명령은 현재 ``--by-service`` 옵션을 가지고 있으며, nova-compute"
" 바이너리의 존재만으로 세ลล에서 호스트를 발견할 수 있습니다. 이때는 이 옵션을 사용할 필요가 없을 때는 ironic을 사용하지 "
"않으면서도 효율성이 더 낮은 경우에만 사용해야 합니다. 그러나 ironic을 사용하는 경우, 이 옵션은 ironic 노드가 존재하지 "
"않더라도 호스트를 발견하고 매핑할 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:901
msgid ""
"The nova-manage online_data_migrations command now prints a tabular summary "
"of completed and remaining records. The goal here is to get all your numbers"
" to zero. The previous execution return code behavior is retained for "
"scripting."
msgstr ""
"nova-manage online_data_migrations 명령은 현재 완료된 및 남은 레코드의 표준화된 요약을 출력합니다. 이 "
"목표는 모든 숫자를 0으로 만드는 것입니다. 이전 실행의 리턴 코드 행동은 스크립팅에 유지됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1247
msgid ""
"The nova-network service was deprecated in the 14.0.0 Newton release. In the"
" 15.0.0 Ocata release, nova-network will only work in a Cells v1 deployment."
" The Neutron networking service is now the default configuration for new "
"deployments based on the ``use_neutron`` configuration option."
msgstr ""
"nova-network 서비스는 Newton 릴리스(14.0.0)에서弃기되었다.  Ocata 릴리스(15.0.0)에서 nova-"
"network 서비스는 Cells v1 배포에서만 작동할 것이다.  Neutron 네트워크 서비스는 now use_neutron 설정 "
"옵션에 따라 새로운 배포에서 기본 구성으로 사용된다."

#: ../../<reno.sphinxext stable/pike>:1685
msgid ""
"The nova-network specific API to query the server's interfaces is "
"deprecated::"
msgstr "nova-network 특이한 API를 서버의 인터페이스를 확인하기 위한 것은弃용되었습니다::"

#: ../../<reno.sphinxext stable/2025.2>:248
msgid ""
"The nova-scheduler now can be run in native threading mode instead of with "
"eventlet. This is an experimental feature that is disabled by default. "
"Please test the native threading mode in pre-production before enabling it "
"in production. If you do so please let us now how it went on the mailing "
"list openstack-discuss@lists.openstack.org. Please read the `concurrency "
"<https://docs.openstack.org/nova/latest/admin/concurrency.html>`__ guide for"
" more details."
msgstr ""
"nova-scheduler는 native threading mode에서 실행할 수 있습니다. 이 기능은 기본적으로 비활성화되어 있습니다."
" production에서 활성화할 때는 pre-production에서 native threading mode를 테스트하여야 합니다. "
"테스트를 완료한 경우 mailing list openstack-discuss@lists.openstack.org에 알려 주세요. 더 많은"
" 정보를 얻으려면 `concurrency "
"<https://docs.openstack.org/nova/latest/admin/concurrency.html>`__ 가이드를 "
"읽어보세요."

#: ../../<reno.sphinxext stable/2025.1>:200
msgid ""
"The nova-scheduler service now does leader election in an attempt to have "
"only one run host discovery periodically (if configured) instead of all of "
"them running it all the time in parallel."
msgstr ""
"nova-scheduler 서비스는 단일 호스트를 발견하기 위해 주기적으로 (구성된 경우) 한 번에 모든 호스트를 동시에 실행하지 않도록"
" 노바 스케줄러 서비스가 리더 선출을 시도하고 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:407
msgid ""
"The nova-scheduler service now verifies which compute nodes can be used for "
"instances having `requested networks or ports`__ by looking at the network "
"segments related to the compute nodes."
msgstr ""
"nova-scheduler 서비스는 현재 인스턴스에 `requested networks or ports`__을 사용할 수 있는 컴퓨터 "
"노드가 있는지 확인합니다. 컴퓨터 노드와 관련된 네트워크 구간을 확인하여 컴퓨터 노드가 사용할 수 있는 인스턴스에 대한 유효성 확인을 "
"수행합니다."

#: ../../<reno.sphinxext stable/stein>:1262
msgid ""
"The nova-xvpvncproxy service is deprecated as it is Xen specific and has "
"effectively been replaced by noVNC and the nova-novncproxy service."
msgstr ""
"nova-xvpvncproxy 서비스는 Xen 특성에 따라弃기되었으며, noVNC와 nova-novncproxy 서비스에 의해 효과적으로"
" 대체되었다."

#: ../../<reno.sphinxext stable/train>:25 stable/ussuri>:71
#: unmaintained/victoria>:165 unmaintained/wallaby>:325 unmaintained/xena>:627
msgid ""
"The novnc, serial, and spice console proxies will now reject requests that "
"pass a redirection URL beginning with \"//\" with a 400 Bad Request."
msgstr ""
"novnc, serial, spice console proxies는 \"//\"로 시작하는 리다이렉션 URL을ผ่าน하는 모든 요청을 "
"400 Bad Request로 거부합니다."

#: ../../<reno.sphinxext stable/queens>:1399
msgid ""
"The old deprecated ``keymgr`` options have been removed. Configuration "
"options using the ``[keymgr]`` group will not be applied anymore. Use the "
"``[key_manager]`` group from Castellan instead. The Castellan ``api_class`` "
"options should also be used instead, as most of the options that lived in "
"Nova have migrated to Castellan."
msgstr ""
"기존에弃용된 `keymgr` 옵션은 제거되었다. `keymgr` 그룹을 사용하는 구성 옵션은 더 이상 적용되지 않는다. 대신 "
"`key_manager` 그룹을 사용하도록 하자. Castellan에서 사용하는 `key_manager` 그룹을 사용하도록 하자. "
"Castellan의 `api_class` 옵션도 사용하도록 하자. 대부분의 옵션은 Nova에서 살았던 옵션들이 Castellan으로 "
"이동했다."

#: ../../<reno.sphinxext stable/stein>:969
msgid ""
"The online data migration ``migrate_instances_add_request_spec``, which was "
"added in the 14.0.0 Newton release, has now been removed. Compatibility code"
" in the controller services for old instances without a matching "
"``request_specs`` entry in the ``nova_api`` database is also gone. Ensure "
"that the ``Request Spec Migration`` check in the ``nova-status upgrade "
"check`` command is successful before upgrading to the 19.0.0 Stein release."
msgstr ""
"``migrate_instances_add_request_spec`` 온라인 데이터 이민이 14.0.0 뉴턴 릴리스에서 추가되었지만 현재"
" 제거되었다. 舊 인스턴스에 ``request_specs`` 엔트리가 ``nova_api`` 데이터베이스에 매치되지 않는 경우 컨트롤러 "
"서비스의 호환성 코드도 제거되었다. 19.0.0 스티न 릴리스로 업그레이드하기 전에 ``Request Spec Migration`` "
"확인을 성공적으로 완료해야 한다."

#: ../../<reno.sphinxext stable/rocky>:1354
msgid ""
"The operator will have to create pools of devices with tag trusted=true."
msgstr "트루스트가 true인 태그를 가진 장치 집합을 만들해야 합니다."

#: ../../<reno.sphinxext stable/queens>:1085
msgid ""
"The operators should specify a vGPU resource in the flavor's extra_specs::"
msgstr "연산자는 flavor의 extra_specs::에서 vGPU 리소스를 지정해야 합니다."

#: ../../<reno.sphinxext stable/2025.1>:359
msgid ""
"The options should be configured for the :program:`nova-api` and "
":program:`nova-conductor` services. The :program:`nova-conductor` service "
"performs quota enforcement when ``[quota]recheck_quota`` is ``True`` (the "
"default)."
msgstr ""
":program:`nova-api` 및 :program:`nova-conductor` 서비스에 대한 옵션을 구성해야 합니다. "
":program:`nova-conductor` 서비스는 ``[quota]recheck_quota``가 ``True`` (기본값)일 때 "
"부트스트 랜더를 사용하여 할당량을 확인합니다."

#: ../../<reno.sphinxext stable/pike>:1094
msgid ""
"The os-volume_attachments APIs no longer check ``os_compute_api:os-volumes``"
" policy. They do still check ``os_compute_api:os-volumes-attachments`` "
"policy rules. Deployers who have customized policy should confirm that their"
" settings for os-volume_attachments policy checks are sufficient."
msgstr ""
"os-volume_attachments APIs는 더 이상 `os_compute_api:os-volumes` 정책을 확인하지 않습니다. "
"그러나 `os_compute_api:os-volumes-attachments` 정책 규칙을 확인합니다. customized policy를"
" 사용하는 배포자는 os-volume_attachments 정책 확인 설정이 충분한지 확인해야 합니다."

#: ../../<reno.sphinxext stable/rocky>:1327
msgid ""
"The payload of the ``instance.create.start`` and ``instance.create.end`` and"
" ``instance.create.error`` versioned notifications have been extended with "
"the ``trusted_image_certificates`` field that contains the list of trusted "
"certificate IDs used when the instance is created."
msgstr ""
"``instance.create.start`` 및 ``instance.create.end`` 및 "
"``instance.create.error`` 버전화된 알림의 파이로드에 ``trusted_image_certificates`` 필드가 "
"추가되어 instance가 생성될 때 사용되는 trusted certificate IDs의 목록이 포함됩니다."

#: ../../<reno.sphinxext stable/rocky>:1332
msgid ""
"The payload of the ``instance.rebuild.start`` and ``instance.rebuild.end`` "
"and ``instance.rebuild.error`` versioned notifications have been extended "
"with the ``trusted_image_certificates`` field that contains the list of "
"trusted certificate IDs used when the instance is rebuilt. This change also "
"causes the type of the payload object to change from "
"``InstanceActionPayload`` version 1.6 to ``InstanceActionRebuildPayload`` "
"version 1.7. See the `notification dev reference`_ for the sample file of "
"``instance.rebuild.start`` as an example."
msgstr ""
"``instance.rebuild.start`` 및 ``instance.rebuild.end`` 및 "
"``instance.rebuild.error`` 버전화된 알림의 패지로드에 ``trusted_image_certificates`` 필드가"
" 추가되었다. 이 변경은 인스턴스 재건 시 사용되는 신뢰할 수 있는 자격 증명 ID의 목록을 포함하는 "
"``trusted_image_certificates`` 필드를 추가한다. 이 변경은 패지로드 오브젝트의 유형이 "
"``InstanceActionPayload`` 버전 1.6에서 ``InstanceActionRebuildPayload`` 버전 1.7으로"
" 변경된다. `notification dev reference`_에示例로 ``instance.rebuild.start`` 패지로드를 "
"참조하십시오."

#: ../../<reno.sphinxext stable/queens>:870
msgid ""
"The payload of the ``instance.snapshot.start`` and ``instance.snapshot.end``"
" notifications have been extended with the ``snapshot_image_id`` field that "
"contains the image id of the snapshot created. This change also causes that "
"the type of the payload object has been changed from "
"``InstanceActionPayload`` version 1.5 to ``InstanceActionSnapshotPayload`` "
"version 1.6. See the `notification dev reference`_ for the sample file of "
"``instance.snapshot.start`` as an example."
msgstr ""
"``instance.snapshot.start`` 및 ``instance.snapshot.end`` 알림의.payload의 "
"``snapshot_image_id`` 필드가 snapshot 생성 시에 생성된 이미지 ID를 포함하는 필드를 추가했습니다. 이 변경 "
"사항은 ``InstanceActionPayload`` 버전 1.5에서 ``InstanceActionSnapshotPayload`` 버전 "
"1.6로 payload 오브젝트의 유형이 변경되었습니다.  `notification dev reference`_에示例 파일의 샘플을 "
"참조하십시오."

#: ../../<reno.sphinxext origin/stable/ocata>:843
msgid "The placement API is only available to admin users."
msgstr "집합 API는 관리자 사용자만 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:861
msgid ""
"The placement API service can now be configured to support `CORS "
"<http://docs.openstack.org/developer/oslo.middleware/cors.html>`_. If a "
"`cors` configuration group is present in the service's configuration file "
"(currently `nova.conf`), with `allowed_origin` configured, the values within"
" will be used to configure the middleware. If `cors.allowed_origin` is not "
"set, the middleware will not be used."
msgstr ""
"`placement API 서비스는 현재 `CORS "
"<http://docs.openstack.org/developer/oslo.middleware/cors.html>`_를 지원하는 구성이 "
"가능합니다. `cors` 구성 그룹이 서비스의 구성 파일 (`nova.conf`)에 존재하고 `allowed_origin`가 구성된 "
"경우, 구성 파일 내의 값이 미드웨어를 구성하는 데 사용됩니다. `cors.allowed_origin`가 설정되지 않은 경우 미드웨어는 "
"사용되지 않습니다."

#: ../../<reno.sphinxext stable/rocky>:546
msgid ""
"The placement service now supports granular RBAC policy rules configuration."
" See the `placement policy`_ documentation for details."
msgstr ""
"집합 서비스는 현재 fine-grained RBAC 정책 규칙을 세부적으로 구성할 수 있습니다.  자세한 내용은 `placement "
"policy`_ 문서를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:596
msgid ""
"The placement service should be upgraded before the nova controller and "
"compute services. See the `Pike Upgrade Notes for Queens`_ for more details."
msgstr ""
"`nova` 컨트롤러와 컴퓨터 서비스를 업그레이드하기 전에 배치 서비스를 업그레이드해야 합니다. 더 많은 정보는 `Pike Upgrade"
" Notes for Queens`_에 있습니다."

#: ../../<reno.sphinxext unmaintained/zed>:467
msgid ""
"The powervm virt driver has been removed. The driver was not tested by the "
"OpenStack project nor did it have clear maintainers and thus its quality "
"could not be ensured."
msgstr ""
"powervm virt 드라이버가 제거되었다. 드라이버는 오픈 스텝 프로젝트에 의해 테스트되지 않았으며, 또한 명확한 유지자들이 없기 "
"때문에 its quality가 보장되지 못했다."

#: ../../<reno.sphinxext unmaintained/yoga>:567
msgid ""
"The powervm virt driver is deprecated and may be removed in a future "
"release. The driver is not tested by the OpenStack project nor does it have "
"clear maintainers and thus its quality can not be ensured."
msgstr ""
"powervm virt driver는 현재 deprecated되어 향후 릴리스에서 제거될 수 있습니다. powervm virt "
"driver는 OpenStack 프로젝트에 의해 테스트되지 않으며, 또한 명확한 유지자들이 없기 때문에 chất lượng을 보장할 수 "
"없습니다."

#: ../../<reno.sphinxext stable/2025.1>:330
msgid ""
"The purpose of the flavor scan is to assist operators who are migrating from"
" legacy quotas to unified limits quotas. The current behavior with unified "
"limits is to fail quota checks if resources requested are missing registered"
" limits in Keystone. With flavor scanning in ``migrate_to_unified_limits``, "
"operators can easily determine what resource classes for which they need to "
"create registered limits."
msgstr ""
"flavor 스캔의 목적은 legacy quotas에서 unified limits quotas로 mig리팅하는 운영자에게 도움을 주는 "
"것이다. unified limits는 Keystone에서 등록된 limits가 없는 리소스를 요청한 경우 quota checks가 "
"실패하는 현재 행동을 가지고 있다. flavor 스캔은 \"migrate_to_unified_limits\"에서 unified "
"limits를 사용할 때 필요한 리소스 클래스를 쉽게 결정할 수 있도록 한다."

#: ../../<reno.sphinxext stable/queens>:748
msgid ""
"The query parameter schema of the ``GET /os-migrations`` API no longer "
"allows additional properties."
msgstr "``GET /os-migrations`` API의 쿼리 파라미터 스키마는 더 이상 추가 속성들을 허용하지 않는다."

#: ../../<reno.sphinxext stable/pike>:612
msgid ""
"The quota system has been reworked to `count resources`_ at the point of "
"creation rather than using a reserve/commit/rollback approach. No operator "
"impacts are expected."
msgstr ""
"quotas 시스템은 생성 시점에 리소스를 `count resources`_으로 계산하는 대신 보관/Commit/Rollback 접근 "
"방식을 사용하지 않도록 수정되었다. No operator impacts are expected."

#: ../../<reno.sphinxext stable/stein>:294
msgid ""
"The reasons this is happening are discussed in bug 1829062_. There are three"
" workarounds available:"
msgstr "이것이 발생하는 이유는 버그 1829062_ 에서 논의되어 있습니다. 세 가지 작업을 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1033 stable/queens>:1182
msgid ""
"The recommended work around is to assign a resource class to all ironic "
"nodes, and use it for scheduling of bare metal instances."
msgstr ""
"집합을 모든 이론적 노드에 할당하고, Bare Metal 인스턴스를 스케줄링하기 위해 사용하는 것이 권장된 workaround입니다."

#: ../../<reno.sphinxext stable/2025.2>:328
msgid ""
"The remote debugger has been removed from the codebase. If you were using "
"it, you will need to find an alternative solution. The remote debugger was "
"un-maintained and was first introduced in the early days of OpenStack when "
"we used python 2.7. While it was useful at the time, it has not been "
"maintained and updated to work with new versions of python and eventlet. As "
"a result it is being removed until we complete the eventlet removal."
msgstr ""
"다istant 디버거가 코드베이스에서 제거되었다. 사용하고 있는 경우, 대체 giải pháp을 찾을 필요가 있다. 다istant "
"디버거는 유지 관리가 중단되었으며, OpenStack의 초기 시기에는 Python 2.7을 사용했을 때 처음 소개되었다. 그 때는 "
"유용했지만, Python과 eventlet의 새로운 버전과 함께 유지 관리 및 업데이트가 이루어지지 않았기 때문에, 새로운 버전과 함께 "
"작동하지 않는다. 따라서, eventlet 제거를 완료할 때까지 제거된다."

#: ../../<reno.sphinxext stable/train>:1463
msgid ""
"The reporting for bytes available for RBD has been enhanced to accomodate "
"`unrecommended <http://docs.ceph.com/docs/luminous/start/hardware-"
"recommendations/#hard-disk-drives>`_ Ceph deployments where multiple OSDs "
"are running on a single disk. The new reporting method takes the number of "
"configured replicas into consideration when reporting bytes available."
msgstr ""
"RBD에 사용 가능한 바이트의 보고는 `unrecommended "
"<http://docs.ceph.com/docs/luminous/start/hardware-recommendations/#hard-"
"disk-drives>`_ Ceph 배포에서 여러 OSD가 한 디스크에 실행되는 경우에 개선되었다. 새로운 보고 방법은 보고된 바이트의 "
"수를 고려하여 구성된 복사본의 수를 고려한다."

#: ../../<reno.sphinxext stable/pike>:1077
msgid ""
"The required ironic API version is updated to 1.32. The ironic service must "
"be upgraded to an ironic release > 8.0 before nova is upgraded, otherwise "
"all ironic intergration will fail."
msgstr ""
"이ronic API 버전은 1.32로 업데이트되었습니다. ironic 서비스는 nova가 업데이트되기 전에 ironic release >"
" 8.0 이상으로 업그레이드되어야 합니다. 그 외의 경우 ironic integration은 모두 실패합니다."

#: ../../<reno.sphinxext stable/pike>:793
msgid "The result should not be (for KVM hypervisor)::"
msgstr "결과는 (KVM 하이퍼바이저)에서 (for KVM hypervisor)::"

#: ../../<reno.sphinxext origin/stable/ocata>:655
msgid ""
"The same policy rule (os_compute_api:os-server-groups) was being used for "
"all actions (show, index, delete, create) for server_groups REST APIs. It "
"was thus impossible to provide different RBAC for specific actions based on "
"roles. To address this changes were made to have separate policy rules for "
"each action. The original rule (os_compute_api:os-server-groups) is left "
"unchanged for backward compatibility."
msgstr ""
"다음은 원문을 한국어로 번역한 결과입니다.\n"
"\n"
"same policy rule (os_compute_api:os-server-groups)가 모든 hành động (show, index, delete, create)에서 사용되는 것과 같이 모든 server_groups REST API에 사용되었다. 따라서 특정 hành động에 따라 role에 따라 RBAC를 다르게 할 수 없었다. 이러한 문제를 해결하기 위해 각 hành động에 riêng된 policy rule을 만들었다. 원래의 rule (os_compute_api:os-server-groups)은 역 backwards compatibility를 위해 변경되지 않았다."

#: ../../<reno.sphinxext stable/train>:786
msgid ""
"The scheduler can now use placement to more efficiently query for hosts that"
" support the disk_format of the image used in a given request. The "
"``[scheduler]/query_placement_for_image_type_support`` config option enables"
" this behavior, but must not be turned on until all computes have been "
"upgraded to this version and thus are exposing image type support traits."
msgstr ""
"스케줄러는 이미지를 사용한 요청에 대한 디스크 포맷을 지원하는 호스트를 더 효율적으로查색할 수 있습니다. "
"`[scheduler]/query_placement_for_image_type_support` 설정 옵션을 활성화하면 이 기능이 "
"활성화되지만, 모든 컴퓨터가 이 버전으로 업그레이드되어 이메지 type support trait를 노출하는 경우까지 활성화되지 않습니다."

#: ../../<reno.sphinxext stable/rocky>:1276
msgid ""
"The scheduler can now use placement to more efficiently query for hosts "
"within a tenant-restricted aggregate. This requires that a host aggregate is"
" created in nova with the ``filter_tenant_id`` key (optionally suffixed with"
" any string for multiple tenants, like ``filter_tenant_id3=$tenantid``) and "
"the same aggregate is created in placement with an identical UUID. The "
"``[scheduler]/limit_tenants_to_placement_aggregate`` config option enables "
"this behavior and ``[scheduler]/placement_aggregate_required_for_tenants`` "
"makes it either optional or mandatory, allowing only some tenants to be "
"restricted. For more information, see the schedulers section__ of the "
"administration guide."
msgstr ""
"scheduler는 now tenant-restricted aggregate 내에서 hosts를 더 효율적으로 검색할 수 있습니다. "
"이것은 tenant-restricted aggregate가 nova에 filter_tenant_id key (optionally "
"filter_tenant_id3=$tenantid와 같은 string으로 확장될 수있는)와 identical UUID가있는 "
"placement에 same aggregate를 생성하는 것을 필요로합니다. scheduler의 behavior를 활성화하는 "
"[scheduler]/limit_tenants_to_placement_aggregate config 옵션과 "
"[scheduler]/placement_aggregate_required_for_tenants config 옵션은 tenant-"
"restricted aggregate가 optional 또는 mandatory로 설정되도록 allowing only some "
"tenants를 allow합니다. 더 많은 정보는 administration guide의 schedulers section에서 확인할 수"
" 있습니다."

#: ../../<reno.sphinxext stable/rocky>:670
msgid ""
"The scheduler can now use placement to more efficiently query for hosts "
"within an availability zone. This requires that a host aggregate is created "
"in nova with the ``availability_zone`` key set, and the same aggregate is "
"created in placement with an identical UUID. The "
"``[scheduler]/query_placement_for_availability_zone`` config option enables "
"this behavior and, if enabled, eliminates the need for the "
"``AvailabilityZoneFilter`` to be enabled."
msgstr ""
"scheduler는 now availability_zone에 대한 호스트를 더 효율적으로 검색할 수 있습니다. 이에 대해 호스트 집합을 "
"nova에 availability_zone key를 설정하여 생성하고, identical UUID를 사용하여 placement에 "
"identical aggregate를 생성해야 합니다. scheduler/scheduler "
"query_placement_for_availability_zone config option이 활성화되면, "
"AvailabilityZoneFilter를 활성화하는 필요가 없게 됩니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:515
msgid ""
"The scheduler can now verify if the requested networks or the port are "
"related to Neutron `routed networks`_ with some specific segments to use. In"
" this case, the routed networks prefilter will require the related "
"aggregates to be reported in Placement, so only hosts within the asked "
"aggregates would be accepted. In order to support this behaviour, operators "
"need to set the "
"``[scheduler]/query_placement_for_routed_network_aggregates`` configuration "
"option which defaults to ``False``."
msgstr ""
"scheduler는 현재 요청된 네트워크 또는 포트가 Neutron `routed networks`_에 관련된 특정 구간을 사용할 수 "
"있는지 확인할 수 있습니다. 이 경우, routed 네트워크의 전처리 will require related aggregates to be"
" reported in Placement, so only hosts within the asked aggregates would be "
"accepted. In order to support this behaviour, operators need to set the "
"``[scheduler]/query_placement_for_routed_network_aggregates`` configuration "
"option which defaults to ``False``."

#: ../../<reno.sphinxext stable/pike>:1424
msgid ""
"The scheduler now requests allocation candidates from the Placement service "
"during scheduling. The allocation candidates information was introduced in "
"the Placement API 1.10 microversion, so you should upgrade the placement "
"service before the Nova scheduler service so that the scheduler can take "
"advantage of the allocation candidate information."
msgstr ""
"현재 스케줄러는 배치 서비스에서 배치 후보자를 요청하여 배치하는 동안 배치 후보자 정보를 요청합니다. 배치 후보자 정보는 배치 API "
"1.10 마이크로 버전에서 도입되었으므로 배치 서비스를 노바 스케줄러 서비스보다 먼저 업그레이드해야 합니다. 이로써 스케줄러는 배치 "
"후보자 정보를 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:983
msgid ""
"The scheduler will pass required traits to the ``GET "
"/allocation_candidates`` endpoint in the Placement API to include only "
"resource providers that can satisfy the required traits. Currently the only "
"valid value is ``required``. Any other value will be considered invalid."
msgstr ""
"배치 API의 ``GET /allocation_candidates`` 엔드포인트에 필요한 특성들을 포함하여만 리소스 제공자만이 필요 "
"trait를 충족할 수 있는지 확인할 수 있는 것을 scheduler가 제공합니다. 현재는 ``required``만 유효한 값으로만 "
"간주됩니다. 다른 값은 무效로 간주됩니다."

#: ../../<reno.sphinxext stable/rocky>:851
msgid ""
"The scheduler will pass the forbidden traits to the ``GET "
"/allocation_candidates`` endpoint in the Placement API to include only "
"resource providers that do not include the forbidden traits. Currently the "
"only valid values are ``required`` and ``forbidden``. Any other values will "
"be considered invalid."
msgstr ""
"scheduler는 `GET /allocation_candidates` 엔드포인트에 `Placement API`에서만 허용되지 않는 "
"특성만 포함된 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 "
"포함하여 `GET /allocation_candidates` 엔드포인트에 forbidden 특성을 포함하지 않는 리소스 제공자만을 "
"포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 "
"포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 "
"포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 "
"리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 "
"`forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 "
"제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 "
"`forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 "
"제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 "
"`forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 "
"제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 "
"`forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 "
"제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 "
"`forbidden` 특성을 포함하지 않는 리소스 제공자만을 포함하는 것을 포함하여 `forbidden` 특성을 포함하지 않는 리소스 "
"제공자만을 포함하는 것을 포함하여 `for"

#: ../../<reno.sphinxext origin/stable/ocata>:687
msgid ""
"The script above will print a list of assignable PCI devices available on "
"the host, and if the host supports SR-IOV."
msgstr "다음 스크립트는 호스트에 있는 PCI 장치의 할당 가능한 목록을 출력하고, 호스트가 SR-IOV를 지원하는 경우에만."

#: ../../<reno.sphinxext unmaintained/2023.1>:498
msgid ""
"The second option allows operators to set the delay between the QEMU "
"announce_self commands in seconds for subsequent announce_self commands with"
"  ``qemu_announce_self_interval``"
msgstr ""
"두 번째 옵션은 QEMU announce_self 명령의 후속 announce_self 명령에 대해 초기 시간을 초당 秒으로 설정할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/rocky>:885
msgid ""
"The semantic of the (unnumbered) ``resources`` and ``trait`` keys is "
"unchanged: the resources and traits specified thereby may be satisfied by "
"any provider on the same host or associated via aggregate."
msgstr ""
"``resource`` 및 ``trait`` 키의 의미는 변경되지 않습니다. 이러한 리소스 및 특성은 동일한 호스트 또는 집합을 통해 "
"제공자에 의해 제공되거나 연결될 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:1156
msgid ""
"The semantic of the (unnumbered) ``resources``, ``required``, and "
"``member_of`` query parameters is unchanged: the resources, traits, and "
"aggregate associations specified thereby may be satisfied by any provider in"
" the same non-sharing tree or associated via the specified aggregate(s)."
msgstr ""
"``자원``(unnumbered), ``required``, 및 ``member_of`` query parameter의 의미는 unchanged입니다. \n"
"자원, trait, 및 집합 연관성으로 정의된 자원, trait, 및 집합 연관성은 동일한 비공유 트리 내의 제공자ใด도 또는 그에 의해 정의된 집합(s)와 연관ized 할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:597
msgid ""
"The server ``evacuate``, ``os-migrateLive`` and ``unshelve`` action APIs now"
" support servers with neutron ports having resource requests, e.g. ports "
"that have QoS minimum bandwidth rules attached."
msgstr ""
"서버 \"evacuate\", \"os-migrateLive\" 및 \"unshelve\" hành động API는 현재 네트워크 "
"포트가 자원 요청을 받는 네트워크 포트를 가진 서버에 지원됩니다. 예를 들어 QoS 최소帯宽 규칙이 부착된 포트가 있습니다."

#: ../../<reno.sphinxext stable/rocky>:838
msgid ""
"The shutdown retry interval in powering off instances can now be set using "
"the configuration setting ``shutdown_retry_interval``, in the compute "
"configuration group."
msgstr ""
"shutdown retry interval은 인스턴스를 비활성화하는 동안 재시도 interval을 설정할 수 있는 설정으로 "
"``shutdown_retry_interval``이 now 사용할 수 있습니다. compute configuration group에서."

#: ../../<reno.sphinxext stable/train>:882
msgid ""
"The support for guest RAM encryption using AMD SEV (Secure Encrypted "
"Virtualization) added in Train is incompatible with a number of image "
"metadata options:"
msgstr ""
"트레인에서 추가된 AMD SEV (Secure Encrypted Virtualization)를 사용하여 게스트 RAM 암호화에 대한 "
"지원은 여러 이미지 메타데이터 옵션에 대한 호환성이 없습니다."

#: ../../<reno.sphinxext stable/rocky>:575
msgid ""
"The support to abort live migrations with ``queued`` and ``preparing`` "
"status using ``DELETE /servers/{server_id}/migrations/{migration_id}`` API "
"has been added in microversion 2.65."
msgstr ""
"``queued`` 및 ``preparing`` 상태의 live migration을 취소하는 지원이 microversion 2.65에서 추가되었다. \n"
"\n"
"``DELETE /servers/{server_id}/migrations/{migration_id}`` API를 사용하여."

#: ../../<reno.sphinxext origin/stable/ocata>:1188
msgid ""
"The three configuration options ``cpu_allocation_ratio``, "
"``ram_allocation_ratio`` and ``disk_allocation_ratio`` for the nova compute "
"are now checked against negative values. If any of these three options is "
"set to negative value then nova compute service will fail to start."
msgstr ""
"nova 컴퓨터의 세 가지 구성 옵션인 `cpu_allocation_ratio` , `ram_allocation_ratio` 및 "
"`disk_allocation_ratio` 가 현재 부정적인 값에 대한 확인을 수행하고 있습니다. 이 세 가지 옵션 중ใด도 부정적인 "
"값으로 설정되면 nova 컴퓨터 서비스는 시작할 수 없습니다."

#: ../../<reno.sphinxext stable/train>:1291
msgid ""
"The transition from rootwrap (or sudo) to privsep has been completed for "
"nova. The only case where rootwrap is still used is to start privsep "
"helpers. All other rootwrap configurations for nova may now be removed."
msgstr ""
"nova에서 rootwrap (또는 sudo)에서 privsep로 전환은 완료되었습니다. rootwrap가 여전히 사용되는 유일한 경우는"
" privsep helper를 시작하는 경우입니다. nova에서 모든 rootwrap 구성은 이제 제거할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1083
msgid ""
"The type of following config options have been changed from string to URI. "
"They are checked whether they follow the URI format or not and its scheme."
msgstr "다음 config 옵션의 유형이 string에서 URI로 변경되었다. URI 형식과.scheme에 따라 확인된다."

#: ../../<reno.sphinxext unmaintained/wallaby>:680
msgid ""
"The update will not move the instance between an alias and versioned machine"
" type or vice versa. For example, ``pc`` to ``pc-1.2.3`` or ``pc-1.2.3`` to "
"``pc``."
msgstr ""
"업데이트는_alias와 버전화된 머신 타입 사이에 인스턴스를 이동하지 않습니다. 예를 들어, `pc`를 `pc-1.2.3`로, 또는 "
"`pc-1.2.3`를 `pc`로."

#: ../../<reno.sphinxext unmaintained/wallaby>:677
msgid ""
"The update will not move the instance between underlying machine types. For "
"example, ``pc`` to ``q35``."
msgstr ""
"업데이트는 하위 मशीन 타입을 통해 인스턴스를 이동하지 않습니다. 예를 들어, ``pc``를 ``q35``로 이동하지 않습니다."

#: ../../<reno.sphinxext unmaintained/zed>:473
msgid ""
"The upgrade check tooling now returns a non-zero exit code in the presence "
"of compute node services that are too old. This is to avoid situations in "
"which Nova control services fail to start after an upgrade."
msgstr ""
"업그레이드 확인 도구는 현재 컴퓨터 노드 서비스가 너무 오래된 경우에 비슷한.exit 코드를 반환합니다. 이로 인해 노바 컨트롤 서비스가"
" 업그레이드 후 시작하지 못하는 상황을 피할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1080
msgid ""
"The values of ``model-name ( RO):`` from the output of the above commands "
"are the vGPU type names which you can choose from to set the nova configure "
"- ``[devices]/enabled_vgpu_types``. Please choose only one vGPU type to be "
"enabled."
msgstr ""
"``model-name ( RO):``의 output에서 제공되는 ``model-name ( RO):``의 값은 nova 구성 - "
"``[devices]/enabled_vgpu_types``를 설정하기 위해 선택할 수 있는 vGPU 타입 이름입니다. 단, 하나만 "
"선택해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:886
msgid ""
"The vendordata metadata system now caches boot time roles.  Some external "
"vendordata services want to provide metadata based on the role of the user "
"who started the instance. It would be confusing if the metadata returned "
"changed later if the role of the user changed, so we cache the boot time "
"roles and then pass those to the external vendordata service."
msgstr ""
"구매 데이터 메타데이터 시스템은 현재 부트 타임 역할을 캐시합니다.  일부 외부 구매 데이터 서비스는 사용자가 인스턴스를 시작한 사용자 "
"역할에 기반하여 메타데이터를 제공하고자 합니다.  사용자가 인스턴스를 시작한 사용자 역할이 변경되면 메타데이터가 변경된 후에 변경되면 "
"혼란스럽습니다.  따라서 부트 타임 역할을 캐시하고 그 afterward를 외부 구매 데이터 서비스에 전달합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:894
msgid ""
"The vendordata metadata system now supports a hard failure mode. This can be"
" enabled using the ``api.vendordata_dynamic_failure_fatal`` configuration "
"option.  When enabled, an instance will fail to start if the instance cannot"
" fetch dynamic vendordata."
msgstr ""
"구매 데이터 메타데이터 시스템은 현재 하드 패스 패스 모드의 지원을 추가했습니다. 이 모드는 "
"`api.vendordata_dynamic_failure_fatal` 구성 옵션을 사용하여 활성화할 수 있습니다. 활성화된 경우, "
"인스턴스는 동적 구매 데이터를 로드할 수 없을 때 인스턴스가 시작하지 못합니다."

#: ../../<reno.sphinxext stable/rocky>:1290
msgid ""
"The versioned ``instance.lock`` and ``instance.unlock`` notifications have "
"been added. These notifications are emitted as a result of the respective "
"server ``lock`` and server ``unlock`` REST API calls."
msgstr ""
"``instance.lock`` 및 ``instance.unlock`` 알림의 버전이 추가되었습니다. 이 알림은 respective "
"server의 ``lock`` 및 server의 ``unlock`` REST API 호출의 결과로 발신됩니다."

#: ../../<reno.sphinxext stable/stein>:775
msgid ""
"The versioned notification interface of nova is now complete and in feature "
"parity with the legacy interface. The emitted notifications are documented "
"in `notification dev ref`_ with full sample files. The deprecation of the "
"legacy notification interface is under dicussion and will be handled "
"separately."
msgstr ""
"nova의 버전화된通知 인터페이스는 현재 완성되어 legacy 인터페이스의 기능 동등성에 도달했다. legacy 인터페이스의 비상대적通知"
" 인터페이스는 `notification dev ref`_에 fully sample files가 포함되어 있다. legacy "
"notification interface의 비상대적通知 인터페이스는 현재 논의 중이며 별개로處理될 예정이다."

#: ../../<reno.sphinxext stable/pike>:639
msgid ""
"The versioned_notifications_topic configuration option; This enables one to "
"configure the topics used for versioned notifications."
msgstr "집합 버전화通知_topic 구성 옵션; 이 구성 옵션은 버전화通知를 사용하는 Topic을 구성할 수 있게 합니다."

#: ../../<reno.sphinxext stable/ussuri>:1082
msgid ""
"The vmwareapi driver is deprecated in this release and may be removed in a "
"future one. The driver is not tested by the OpenStack Nova project and does "
"not have a clear maintainer."
msgstr ""
"vmwareapi 드라이버는 이 릴리즈에서弃기되어 다음 릴리즈에서 제거될 수 있습니다. 드라이버는 OpenStack Nova 프로젝트가 "
"테스트하지 않았으며 명확한 유지자가 없으며."

#: ../../<reno.sphinxext stable/2023.2>:402
msgid ""
"The vmwareapi driver is marked as experimental and may be removed in a "
"future release. The driver is not tested by the OpenStack project and does "
"not have a clear maintainer."
msgstr ""
"vmwareapi 드라이버는 실험적으로 표시되어 향후 릴리스에서 제거될 수 있습니다. 드라이버는 오픈 스톡 프로젝트가 테스트하지 않았으며"
" 명확한 유지자가 없으며."

#: ../../<reno.sphinxext unmaintained/victoria>:732
msgid ""
"The vmwareapi driver was deprecated in Ussuri due to missing third-party CI "
"coverage and a clear maintainer. These issues have been addressed during the"
" Victoria cycle and the driver is now undeprecated."
msgstr ""
"vmwareapi 드라이버는 Ussuri에서 3차-party CI 커버지와 명확한 유지자가 부족하여 deprecated되었습니다. 이러한"
" 문제는 Victoria 주기 동안 해결되었으며 드라이버는 now deprecated가なくな되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1079
msgid "The whitelist for REST API filters for admin users:"
msgstr "REST API filter whitelist for admin users:"

#: ../../<reno.sphinxext origin/stable/ocata>:1134
msgid "The whitelist for sort keys for admin users:"
msgstr "admin 사용자에 대한 sort key whitelist"

#: ../../<reno.sphinxext stable/train>:1234
msgid ""
"The xenapi driver is deprecated and may be removed in a future release. The "
"driver is not tested by the OpenStack project nor does it have clear "
"maintainer(s) and thus its quality can not be ensured. If you are using the "
"driver in production please let us know in freenode IRC and/or the "
"openstack-discuss mailing list."
msgstr ""
"xenapi 드라이버는 향후 릴리스에서 제거될 수 있으며, 드라이버는 오픈 스텝 프로젝트에 의해 테스트되지 않으며, 또한 명확한 "
"유지자(s)가 없기 때문에品질을 보장할 수 없습니다. production에서 드라이버를 사용하고 있으시면, freenode IRC "
"및/or openstack-discuss mailing list에 알려 주시기 바랍니다."

#: ../../<reno.sphinxext stable/queens>:1089
msgid ""
"Then users can use the flavor to boot instances with a vGPU attached. At the"
" moment, XenServer doesn't support multiple vGPUs for a single instance, so "
"``resources:VGPU`` in the flavor's extra_specs should always be ``1``."
msgstr ""
"그 후 사용자는 flavor를 사용하여 인스턴스를 부트로드할 수 있습니다. 현재 XenServer는 단일 인스턴스에 대해 여러 vGPU를"
" 지원하지 않기 때문에 flavor의 extra_specs의 resources:VGPU는 항상 1이must be."

#: ../../<reno.sphinxext stable/2023.2>:188 stable/2024.1>:304
#: stable/2024.2>:230 stable/2025.1>:183 stable/2025.2>:23 stable/stein>:409
#: stable/train>:461 stable/ussuri>:337 unmaintained/2023.1>:278
#: unmaintained/victoria>:323 unmaintained/wallaby>:388 unmaintained/xena>:294
#: unmaintained/yoga>:267 unmaintained/zed>:190
msgid ""
"There are a few major changes worth mentioning. This is not an exhaustive "
"list:"
msgstr "집합이 몇 가지 주요 변경 사항이 worth mentioning. 이 목록은 exhaustive가 아니기 때문에."

#: ../../<reno.sphinxext unmaintained/wallaby>:860
msgid "There are some known caveats with this:"
msgstr "이것에 대한 일부 알려진 제한점이 있습니다."

#: ../../<reno.sphinxext stable/train>:1015
msgid ""
"There are some things to note when opting in to counting quota usage from "
"placement:"
msgstr "집합 사용quota 카운팅에 opt-in 할 때는 다음 것에 주의해야 합니다."

#: ../../<reno.sphinxext stable/stein>:1109
msgid ""
"There are still known race issues with concurrently building some types of "
"resources and workloads, such as anything that requires PCI/NUMA or "
"(anti-)affinity groups. However, those races also existed with the "
"``caching_scheduler`` driver."
msgstr ""
"집합은 여전히 동시적으로 일부 리소스와 워크로드를 xây dựng하는 경우에 대한 경쟁 문제가 존재한다. 예를 들어, PCI/NUMA "
"또는 (anti-)affinity 그룹이 필요하는 모든 것과 같은 경우. 그러나 이러한 경쟁은 또한 `caching_scheduler` "
"드라이버와 함께 존재했다."

#: ../../<reno.sphinxext stable/queens>:1002
msgid "There are three policies supported:"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1365
msgid ""
"There had been `bug 1777591`_ that placement filters out the specified "
"target host when deploying an instance by the random limitation. In previous"
" releases the bug has been worked around by unlimiting the results from the "
"Placement service if the target host is specified. From this release, the "
"Nova scheduler uses more optimized path retrieving only the target host "
"information from placement. Note that it still uses the unlimit workaround "
"if a target host is specified without a specific node and multiple nodes are"
" found for the target host. This can happen in some of the virt drivers such"
" as the Ironic driver."
msgstr ""
"`bug 1777591`_가 배포 시 인스턴스를 randomness로 제한하여 특정 목표 호스트를 제외하는 것이 있었다. 이전 "
"릴리즈에서는 목표 호스트가 지정된 경우 배치 서비스의 결과를 제한하지 않도록 unlimiting을 사용하여 버그를 해결했다. 이 "
"릴리즈부터 노바 스케줄러는 목표 호스트 정보만을 retrieves하여 path를 최적화한다. 그러나 목표 호스트가 특정 노드가 지정되지 "
"않아 여러 노드가 목표 호스트에 해당하는 경우, 여전히 unlimiting을 사용한다. 이 경우 virt driver와 같은 일부 "
"virt driver에서 발생할 수 있다. 예를 들어 Ironic driver."

#: ../../<reno.sphinxext origin/stable/ocata>:21 stable/pike>:21
#: stable/queens>:91 stable/rocky>:183 stable/stein>:329 stable/train>:1275
msgid ""
"There is a behavior change where non-nova exceptions will only record the "
"exception class name in the fault ``message`` field which is exposed to all "
"users, regardless of the admin role."
msgstr ""
"집합에서 비nova 예외는 오류 메시지 필드의 예외 클래스 이름만 기록합니다. 이 메시지는 모든 사용자에게 노출되며, 관리자 역할에 "
"상관없이입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:444
msgid ""
"There is a known regression in Ocata reported in `bug 1671648`_ where server"
" build failures on a compute node are not retried on another compute node. "
"The fix for this bug is being worked and will be provided shortly in a "
"15.0.2 release."
msgstr ""
"`Ocata`에서 알려진 버그 `bug 1671648`_은 컴퓨터 노드에서 서버 빌드가 실패하고 다른 컴퓨터 노드에 다시 시도되지 "
"않는다. 이 버그의修复이 underway이며, 15.0.2 버전에서 shortly 제공될 예정이다."

#: ../../<reno.sphinxext stable/rocky>:1784
msgid ""
"There is a new configuration option ``[workarounds]/enable_consoleauth`` for"
" use by operators who:"
msgstr "``[workarounds]/enable_consoleauth``는 운영자에게 사용할 수 있는 새로운 구성 옵션입니다."

#: ../../<reno.sphinxext stable/2024.1>:433
msgid ""
"There is a new nova-manage command ``db ironic_compute_node_move`` that can "
"be used to move ironic nodes, and the associated instances, between nova-"
"compute services. This is useful when migrating from the legacy hash ring "
"based HA towards the new sharding approach."
msgstr ""
"```\n"
"nova-manage db ironic_compute_node_move``는 nova-compute 서비스를 통해 ironic 노드와 관련된 인스턴스를 이동할 수 있는 새로운 nova-manage 명령입니다. 이 명령은 legacy hash ring based HA에서 새로운 sharding 접근 방식을 migrate 할 때 유용합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:551
msgid ""
"There is a new nova-status command that gives operators a better view of "
"their cloud. In particular, a new subcommand called \"upgrade\" allows "
"operators to run a pre-flight check on their deployment before upgrading. "
"This helps them to proactively identify potential upgrade issues that could "
"occur."
msgstr ""
"집합에 새로운 nova-status 명령어가 존재하며, 이 명령어는 운영자들에게 클라우드의 더 좋은 시각을 제공합니다. 특히, "
"\"업그레이드\"라는 이름의 새로운 서브 명령어가 존재하며, 이 명령어는 배포를 업그레이드하기 전에 전파 확인을 수행할 수 있습니다. "
"이로 인해 운영자는 전파 확인을 통해 전파 업그레이드에 대한潜在 문제를 미리 확인할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:498
msgid "There is no known workaround within OpenStack Nova to this issue."
msgstr "OpenStack Nova에서 이 문제에 대한 알려진 workaround은 없습니다."

#: ../../<reno.sphinxext stable/stein>:245
msgid ""
"There is one caveat, which is that the wsgi app configuration must be left "
"as the default ``threads=1`` or set explicitly to ``threads=1`` to ensure "
"that reconnection will work properly. When threads > 1, it is not guaranteed"
" that oslo.messaging will reconnect to rabbitmq when the wsgi app resumes "
"after pausing during idle time. Threads are used internally by "
"oslo.messaging for heartbeats and more, and it may fail in a variety of ways"
" if run under eventlet with an app that violates eventlet's threading "
"guarantees. When oslo.messaging does not reconnect to rabbitmq after a wsgi "
"app pause, RPC requests will fail with a ``MessagingTimeout`` error. So, it "
"is necessary to have the wsgi app configured with ``threads=1`` for "
"reconnection to work properly."
msgstr ""
"`threads=1`를 기본으로 유지하거나 명시적으로 `threads=1`로 설정하여 재연결이 제대로 작동하는 것을 보장해야 합니다. "
"`threads > 1`일 때, `oslo.messaging`가 `rabbitmq`에 재연결하는 것을 보장할 수 없습니다. "
"`threads`는 `oslo.messaging`에서 심장เต시와 더불어 내부적으로 사용됩니다. `eventlet`와 함께 앱이 "
"`eventlet`의 스레딩 보장에 위반되는 경우, 다양한 방법으로 실패할 수 있습니다. `oslo.messaging`가 "
"`rabbitmq`에 재연결하지 않으면, `WSGI` 앱이 일시 중단 후 재시작할 때 `RPC` 요청은 `MessagingTimeout`"
" 오류로 실패합니다. 따라서 `WSGI` 앱이 제대로 재연결을 작동하는 것을 보장하기 위해 `threads=1`로 구성해야 합니다."

#: ../../<reno.sphinxext stable/pike>:1877
msgid ""
"There really never was a good reason to disable or enable non-compute "
"services anyway since that would not do anything. The nova-scheduler and "
"nova-api services are checking the ``status`` and ``forced_down`` fields to "
"see if instance builds can be scheduled to a compute host or if instances "
"can be evacuated from a downed compute host. There is nothing that relies on"
" a disabled or downed nova-conductor or nova-scheduler service."
msgstr ""
"집합에 대한 실제로 어떤 이유가 있은가? 비 컴퓨터 서비스를 비활성화하거나 활성화하는 것은 아무런 효과가 없기 때문이다. nova-"
"scheduler와 nova-api 서비스는 `status`와 `forced_down` 필드를 확인하여 인스턴스 빌드를 컴퓨터 호스트에 "
"스케줄링할 수 있는지, 또는 컴퓨터 호스트가 down된 경우 인스턴스를 비우는지 확인한다. 비활성화된 또는 down된 nova-"
"conductor 또는 nova-scheduler 서비스에 의존하는อะไร도 없기 때문이다."

#: ../../<reno.sphinxext ../source/newton.rst:42 origin/stable/ocata>:290
#: stable/pike>:418 stable/queens>:1689
msgid ""
"Therefore this release contains a fix for those regressions in scheduling "
"behavior on rebuild while maintaining the original fix for CVE-2017-16239."
msgstr ""
"따라서 이 릴리스에는 재건 시 스케줄izing 행동에 대한 오류가 있는 those regressions에 대한修复이 포함되어 있으며 원래"
" CVE-2017-16239에 대한修复이 유지되었습니다."

#: ../../<reno.sphinxext stable/pike>:1619
msgid ""
"These commands will be removed in their entirety during the Queens cycle."
msgstr "이 명령은 그 entirety로 Queen의 시리즈 동안 제거됩니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:335
msgid ""
"These configuration options can be used to enable and set the SPICE "
"compression settings for libvirt (QEMU/KVM) provisioned instances. Each "
"configuration option is optional and can be set explictly to configure the "
"associated SPICE compression setting for libvirt. If all configuration "
"options are not set, then none of the SPICE compression settings will be "
"configured for libvirt, which corresponds to the behavior before this "
"change. In this case, the built-in defaults from the libvirt backend (e.g. "
"QEMU) are used."
msgstr ""
"이 configuration 옵션은 libvirt (QEMU/KVM) 제공된 인스턴스를 위한 SPICE 압축 설정을 활성화하고 설정할 수"
" 있습니다. 각 configuration 옵션은 선택적이며, libvirt와 관련된 SPICE 압축 설정을 명시적으로 설정할 수 "
"있습니다. 모든 configuration 옵션을 설정하지 않으면, libvirt에 대한 SPICE 압축 설정이 none이 됩니다. 이 "
"경우, libvirt backend (예: QEMU)에서 내장된 기본 설정이 사용됩니다."

#: ../../<reno.sphinxext stable/rocky>:1653
msgid "These have not been used in recent releases."
msgstr "이것은 최근 릴리스에서 사용되지 않았습니다."

#: ../../<reno.sphinxext stable/stein>:903
msgid ""
"These options help operators specify initial virtual CPU/ram/disk to "
"physical CPU/ram/disk allocation ratios. These options are only used when "
"initially creating the ``computes_nodes`` table record for a given nova-"
"compute service."
msgstr ""
"이 옵션은 운영자가 가상 CPU/RAM/디스크의 초기 비율을 물리 CPU/RAM/디스크 할당 비율과 일치시키는 것을 도와줍니다. 이 "
"옵션은 nova-compute 서비스에 대한 ``computes_nodes`` 테이블 레코드를 초기화할 때만 사용됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1069
msgid "These options should no longer be included in the ``DEFAULT`` group."
msgstr "이 옵션은 더 이상 ``DEFAULT`` 그룹에 포함되지 않아야 합니다."

#: ../../<reno.sphinxext stable/rocky>:1693
msgid ""
"These were deprecated in 17.0.0 as they have been superseded by their "
"respective keystoneauth1 Adapter configuration options."
msgstr ""
"이것은 17.0.0에서弃용되었으며, respective keystoneauth1 Adapter configuration options에 "
"의해 대체되었다."

#: ../../<reno.sphinxext stable/queens>:1394 stable/rocky>:1682
msgid ""
"These were deprecated in the 15.0.0 release as they allowed for inconsistent"
" API behavior across deployments."
msgstr "이것은 15.0.0 릴리스에서弃용되었으며, 이들은 배포를 통해 불일관된 API 행동을 허용했다."

#: ../../<reno.sphinxext stable/rocky>:1669
msgid ""
"These were deprecated in the 16.0.0 release as they allowed inconsistent API"
" behavior across deployments. To disable snapshots in the ``createImage`` "
"server action API, change the ``os_compute_api:servers:create_image`` and "
"``os_compute_api:servers:create_image:allow_volume_backed`` policies."
msgstr ""
"이것은 16.0.0 릴리스에서弃용되었으며, 배포에 대한 API 행동이 불일치하는 것을 허용했습니다. snapshots을 비활성화하려면 "
"`os_compute_api:servers:create_image` API에서 "
"`os_compute_api:servers:create_image:allow_volume_backed` 정책을 변경합니다."

#: ../../<reno.sphinxext stable/stein>:1158
msgid ""
"These were deprecated in the 17.0.0 release as nova removed the concept of "
"API extensions."
msgstr "이것은 17.0.0 릴리스에서弃용되었으며 nova는 API 확장의 개념을 제거했다."

#: ../../<reno.sphinxext stable/ussuri>:908
msgid ""
"These were only useful for the now-removed *nova-network* service and have "
"been deprecated since the 15.0.0 (Ocata) release."
msgstr "이것은 현재 제거된 *nova-network* 서비스에만 유용했으며, 15.0.0 (Ocata) 릴리스 이후에弃용되었습니다."

#: ../../<reno.sphinxext stable/queens>:1431
msgid "These were previously deprecated in 16.0.0."
msgstr "이것은 이전에 16.0.0에서弃용되었다."

#: ../../<reno.sphinxext stable/pike>:1327
msgid ""
"These were used by the now-removed ``LibvirtGlusterfsVolumeDriver`` volume "
"driver and therefore no longer had any effect."
msgstr ""
"이것은 현재 제거된 `LibvirtGlusterfsVolumeDriver` 볼륨 드라이버에 의해 사용되었기 때문에 더 이상 효과가 "
"없었습니다."

#: ../../<reno.sphinxext stable/pike>:1350
msgid ""
"These were used by the now-removed ``LibvirtScalityVolumeDriver`` volume "
"driver and therefore no longer had any effect."
msgstr ""
"이것은 현재 제거된 `LibvirtScalityVolumeDriver` 볼륨 드라이버에 의해 사용되었기 때문에 더 이상ใด도 효과가 "
"없었습니다."

#: ../../<reno.sphinxext stable/queens>:919
msgid ""
"These will allow using a file trigger for the reports, which is particularly"
" useful for Windows nodes where the default signals are not available. Also,"
" specifying a log directory will allow the reports to be generated at a "
"specific location instead of stdout."
msgstr ""
"이것은 보고서를 사용할 수 있는 파일 트리거를 허용합니다. 보고서에 대한 파일 트리거는 특히 Windows 노드에서 기본 신호가 "
"unavailable인 경우 특히 유용합니다. 또한 로그 디렉터리를 지정하면 보고서가 특정 위치에 생성되기 대신 stdout에 "
"생성됩니다."

#: ../../<reno.sphinxext stable/train>:558
msgid ""
"This API provides information about the NUMA topology of a server, including"
" instance to host CPU pin mappings, if CPU pinning is used, and pagesize "
"information."
msgstr ""
"이 API는 서버의 NUMA topology에 대한 정보를 제공합니다. 이에 포함된 정보는 CPU pinning이 사용되는 경우 "
"인스턴스와 호스트 CPU pin mapping, page size information입니다."

#: ../../<reno.sphinxext unmaintained/zed>:360
msgid "This adds to the list of already existing enlightenments, namely:"
msgstr "이것은 이미 존재하는 지식의 목록에 추가되며, namely: 집합"

#: ../../<reno.sphinxext stable/train>:1408
msgid ""
"This aims to address issues reported in `bug 1834048`_ where failures to "
"initially connect to a RBD cluster left the nova-compute service inoperable "
"due to constant RPC timeouts being hit."
msgstr ""
"이것은 `bug 1834048`_에서 보고된 문제를 해결하기 위해 설계되었습니다. 초기에 RBD 클러스터와의 연결을 성공적으로 할 수 "
"없게 된 nova-compute 서비스는 constantr RPC 타임아웃이 발생할 때 인опер블리게 남아있었습니다."

#: ../../<reno.sphinxext stable/ussuri>:1345
msgid ""
"This aims to address the issues reported in `bug 1842149`_, where it "
"describes that the proxy services can inherit insecure TLS ciphers and "
"protocol versions from the compiled-in defaults of the OpenSSL library on "
"the underlying system.  The proxy services provided no way to override such "
"insecure defaults with current day generally accepted secure TLS settings."
msgstr ""
"이것은 `bug 1842149`_에서 보고된 문제를 해결하기 위해 설계되었습니다. 이 문제는 OpenSSL 라이브러리에 의해 컴파일된 "
"기본 설정의 불안정한 TLS 암호화 기법과 프로토콜 버전을 상속받아 proxy 서비스가 불안정한 TLS 암호화 기법과 프로토콜 버전을 "
"상속받을 수 있음을 설명합니다.  이러한 불안정한 기본 설정을 현재의 일상에서 일반적으로 수용되는 안전한 TLS 설정과 대체할 수 있는 "
"방법이 제공되지 않았습니다."

#: ../../<reno.sphinxext stable/pike>:1591
msgid ""
"This allows for the creation, deletion, update and listing of \"agent "
"builds\". Operators should use the equivalent resources in the `REST API`__ "
"instead."
msgstr ""
"이것은 \" एजент 빌드 \"의 생성, 삭제, 업데이트 및 목록 표시를 허용합니다. 운영자는 `REST API`__의 equivalent 리소스를 사용해야 합니다.\n"
"\n"
"* \"agent builds\" : 집합\n"
"* \"REST API\" : API endpoint"

#: ../../<reno.sphinxext stable/pike>:1582
msgid ""
"This allows for the creation, deletion, update and listing of user and "
"project quotas. Operators should use the equivalent resources in the `REST "
"API`__ instead."
msgstr ""
"이것은 사용자 및 프로젝트의 квOTA를 생성, 삭제, 업데이트하고 listing 할 수 있도록 허용합니다. REST API__ "
"equivalent resources를 사용하는 운영자에게는 권한이 있습니다."

#: ../../<reno.sphinxext stable/pike>:1602
msgid ""
"This allows for the filtering of errors from nova's logs and extraction of "
"all logs from syslog. This command has not been actively maintained in a "
"long time, is not tested, and can be achieved using `journalctl` or by "
"simply grepping through ``/var/log``. It will simply be removed."
msgstr ""
"이것은 nova 로그의 오류를 필터링하고 syslog 로그의 모든 로그를 추출할 수 있습니다. 이 명령은 오랜 시간 동안 활성적으로 유지"
" 관리되지 않았으며 테스트되지 않았으며, `journalctl` 또는 단순히 `/var/log`를 grepping 하기만으로에 달리 할 "
"수 있습니다. 이 명령은 단순히 제거됩니다."

#: ../../<reno.sphinxext stable/pike>:1597
msgid ""
"This allows for the listing of compute hosts. Operators should use the "
"equivalent resources in the `REST API`__ instead."
msgstr ""
"이것은 컴퓨터 호스트를 목록화할 수 있도록 허용합니다. 운영자는 `REST API`__ equivalent 리소스를 사용해야 합니다.\n"
"\n"
"* REST API : API endpoint\n"
"* equivalent : same, equivalent\n"
"* REST API : API endpoint"

#: ../../<reno.sphinxext stable/2024.2>:286
msgid ""
"This also simplifies deployment with other WSGI servers that expect module "
"paths such as gunicorn."
msgstr ""
"이 또한 다른 WSGI 서버와 함께 배포를 간소화합니다. 예를 들어, gunicorn과 같은 모듈 경로를 기대하는 다른 WSGI 서버와 "
"함께 배포를 간소화합니다."

#: ../../<reno.sphinxext stable/stein>:742
msgid ""
"This can be configured via a new configuration attribute "
"``[libvirt]/live_migration_with_native_tls``.  Refer to its documentation in"
" ``nova.conf`` for usage details.  Note that this is the preferred the way "
"to secure all migration streams in an OpenStack network, instead of "
"``[libvirt]/live_migration_tunnelled``."
msgstr ""
"이 설정은 새로운 구성 속성 ``[libvirt]/live_migration_with_native_tls``를 통해 구성할 수 있습니다."
"  nova.conf에 대한 사용 방법에 대한 설명은 참조하십시오.  이 설정은 오픈 스텝 네트워크에서 모든 이민 스트림을 보안하는 데 "
"선호되는 방법입니다.  대신 ``[libvirt]/live_migration_tunnelled``를 사용합니다."

#: ../../<reno.sphinxext stable/pike>:1249
msgid ""
"This change can have impacts, however, to deployment tooling that relies on "
"parts of the API, like listing compute hosts, `before` the compute hosts are"
" mapped using the ``nova-manage cell_v2 discover_hosts`` command."
msgstr ""
"이 변경 사항은 API의 일부를 기반으로 하는 배포 도구에 영향을 미칠 수 있으며, 예를 들어, 컴퓨터 호스트를 listing하는 것과 "
"`before` 컴퓨터 호스트가 mapping이 완료되기 전에 `nova-manage cell_v2 discover_hosts` 명령을 "
"사용하여 호스트를 mapping하는 것과 같은 경우에 특히이다."

#: ../../<reno.sphinxext stable/2023.2>:256
msgid ""
"This change ensures the synchronization of volume attachments between Nova "
"and Cinder, by deleting any dangling volume attachments and maintaining "
"consistency between two databases."
msgstr ""
"이 변경은 Nova와 Cinder 간의 볼륨 연결의 동기화를 보장하기 위해, dangling 볼륨 연결을 제거하고 두 데이터베이스 사이의"
" 정합성을 유지하는 것을確保합니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:544
msgid ""
"This change replaces the usage of older API, compareCPU(), with the new one,"
" compareHypervisorCPU()."
msgstr ""
"이 변경은 older API, compareCPU(),의 사용을 새로운 API, compareHypervisorCPU(),로 대체합니다."

#: ../../<reno.sphinxext stable/queens>:1834
msgid ""
"This change should be transparent to end users and does not affect existing "
"volume attachments. Also, this does not affect how new volumes are created "
"and attached during boot-from-volume when the "
"``block_device_mapping_v2.source_type`` is ``blank``, ``image`` or "
"``snapshot`` and the ``block_device_mapping_v2.destination_type`` is "
"``volume``."
msgstr ""
"이 변경 사항은 사용자에게 투명해야 하며, 기존 볼륨에 대한 연결이 영향을 받지 않는다. 또한, "
"``block_device_mapping_v2.source_type``가 ``blank``, ``image`` 또는 "
"``snapshot`` 인 경우, ``block_device_mapping_v2.destination_type``가 ``volume`` "
"인 경우에 new volumes가 생성되고 연결되는 동안 영향을 받지 않는다."

#: ../../<reno.sphinxext stable/2024.1>:51 stable/2024.2>:51 stable/2025.1>:39
#: stable/2025.2>:524
msgid ""
"This change significantly reduces response times for the hypervisor detail "
"API in large deployments while maintaining backward compatibility."
msgstr ""
"이 변경은 대규모 배포에서 하이퍼바이저รายละเอียด API의 반응 시간을 크게 감소시ки며, 역 backwards "
"compatibility를 유지합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:690
msgid ""
"This command will list instance UUIDs that do not have a machine type "
"recorded. An optional cell UUID can be provided to list on instances without"
" a machine type from that cell."
msgstr ""
"이 명령은 기계 유형이 기록되지 않은 인스턴스 UUID를 listing 할 수 있는 명령입니다. 선택적으로 세ลล UUID를 제공할 수 "
"있으며, 이 세ลล에서 기계 유형이 기록되지 않은 인스턴스를 listing 할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:661
msgid ""
"This command will print the current machine type if set in the image "
"metadata of the instance."
msgstr "이 명령은 인스턴스의 이미지 메타데이터에 설정된 현재 머신 타입을 출력할 것이다."

#: ../../<reno.sphinxext unmaintained/wallaby>:666
msgid ""
"This command will set or update the machine type of the instance assuming "
"the following criteria are met:"
msgstr "이 명령은 다음 조건이 충족되는 경우 인스턴스의 머신 타입을 설정하거나 업데이트합니다."

#: ../../<reno.sphinxext stable/2023.2>:296
msgid ""
"This config option can be used to configure tb_cache size for guest VMs, "
"it's only applicable with ``virt_type=qemu``."
msgstr ""
"이 config 옵션은 게스트 VM의 tb_cache 크기를 구성할 수 있습니다. virt_type=qemu와 함께 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:418
msgid ""
"This ensures the graceful closure of console sessions on the server side, "
"aligning with security best practices."
msgstr "이것은 서버 측에서 콘솔 세션의 조용한 종료를 보장하며, 보안 최선의 관행에 맞추어 align합니다."

#: ../../<reno.sphinxext stable/queens>:1471
msgid ""
"This establishes a consistent naming between VNC and Spice options and "
"removes some unnecessary duplication."
msgstr "이것은 VNC와 Spice 옵션 간의 일관된 이름을 설정하고 일부 불필요한 복제를 제거합니다."

#: ../../<reno.sphinxext stable/pike>:930
msgid ""
"This feature is already implemented for Nova interaction with the Cinder and"
" Neutron APIs in Ocata."
msgstr "이 기능은 Ocata에서 Nova와 Cinder 및 Neutron API 간의 상호작용에 대해 이미 집합화되어 있습니다."

#: ../../<reno.sphinxext stable/queens>:1020
msgid ""
"This feature is currently only supported by the libvirt compute driver and "
"only then if qemu<2.10 or libvirt>3.10 on the compute host."
msgstr ""
"이 기능은 현재 libvirt 컴퓨터 드라이버만 지원하고, qemu<2.10 또는 libvirt>3.10이면 컴퓨터 호스트에만 "
"지원됩니다."

#: ../../<reno.sphinxext stable/ussuri>:685
msgid ""
"This feature is disabled by default can be enabled via config option "
"``[oslo_policy]enforce_scope`` in ``nova.conf``"
msgstr ""
"이 기능은 기본적으로 비활성화되어 있으며, `nova.conf`에서 `oslo_policy`의 "
"`[oslo_policy]enforce_scope` 옵션을 통해 활성화할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:501
msgid "This feature is only supported with the Libvirt driver."
msgstr "이 기능은 only Libvirt 드라이버와 함께 지원됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:728
msgid ""
"This feature is supported on Windows / Hyper-V Server 2012 R2 for Windows "
"guests, and Windows / Hyper-V Server 2016 for both Windows and Linux guests."
msgstr ""
"이 기능은 Windows / Hyper-V Server 2012 R2에서 Windows 게스트에 대해 지원되고 있으며 Windows / "
"Hyper-V Server 2016에서 Windows 및 Linux 게스트에 대해 지원됩니다."

#: ../../<reno.sphinxext unmaintained/yoga>:486
msgid ""
"This feature relies on Neutron being upgraded to the corresponding release "
"of OpenStack and having an appropriate backend capable of binding "
"``VNIC_TYPE_REMOTE_MANAGED`` ports (at the time of writing, ML2 with the OVN"
" ML2 mechanism driver is the only supported backend, see the Neutron "
"documentation for more details)."
msgstr ""
"이 기능은 OpenStack의 corresponding release와 함께 Neutron을 업그레이드하고, "
"VNIC_TYPE_REMOTE_MANAGED 포트를 결합할 수 있는 적합한 백엔드를 사용해야 합니다. (현재 작성 시점에, ML2와 "
"OVN ML2 메커니즘 드라이버가 지원되는 백엔드만 지원됩니다. 더 많은 정보는 Neutron 문서를 참조하십시오.)"

#: ../../<reno.sphinxext unmaintained/2023.1>:494
msgid ""
"This fix introduces two operator configurable options. The first option sets"
" the number of times the QEMU monitor announce_self command is called - "
"``qemu_announce_self_count``"
msgstr ""
"이修复은 두 개의 오퍼레이터 구축 가능한 옵션을 도입합니다. 첫 번째 옵션은 QEMU 모니터 announce_self 명령을 몇 번 "
"호출할지 설정합니다 - `qemu_announce_self_count`"

#: ../../<reno.sphinxext stable/2024.1>:411
msgid ""
"This is a security-enhancing feature that automatically closes console "
"sessions exceeding a defined timeout period. To enable this functionality, "
"operators are required to set the 'enforce_session_timeout' boolean "
"configuration option to True."
msgstr ""
"이것은 콘솔 세션이 정의된 타임아웃 기간을 초과할 때 tự động 닫는 보안 확장 기능입니다. 이 기능을 활성화하려면 운영자들은 "
"'enforce_session_timeout' boolean 구성 옵션을 True로 설정해야 합니다."

#: ../../<reno.sphinxext stable/stein>:691
msgid ""
"This is a special operation that should only be used in rare cases of "
"resource provider topology changing when inventory is in use. Only use this "
"if you are really sure of what you are doing."
msgstr ""
"이것은 리소스 제공자 topology가 변경되는 드물은 경우에만 사용해야 하는 특별한 연산입니다. 사용할 때는 realmente 무엇을 "
"하는지แน란히 sicher히 하십시오."

#: ../../<reno.sphinxext stable/pike>:1609
msgid ""
"This is an alias for `account` and has been deprecated for the same reasons."
msgstr "이것은 `account`의 별명으로, 동일한 이유로 비활성화되었다."

#: ../../<reno.sphinxext stable/ussuri>:529
msgid ""
"This is different than the ``os_compute_api:servers:show:host_status`` "
"policy rule which controls whether a user can view all possible host status "
"in the aforementioned APIs including ``UP``, ``DOWN``, ``MAINTENANCE``, and "
"``UNKNOWN``."
msgstr ""
"이것은 `os_compute_api:servers:show:host_status` 정책 규칙과 달리 사용자가 aforementioned API에서 모든 possible host status를 viewing 할 수 있는지 여부를 제어하는 규칙입니다. \n"
"\n"
"이 규칙은 `UP`, `DOWN`, `MAINTENANCE`, `UNKNOWN`와 같은 API에서 host status를 viewing 할 수 있는지 여부를 제어하는 규칙입니다."

#: ../../<reno.sphinxext stable/queens>:391 stable/rocky>:2074
msgid ""
"This is disabled by default for backward compatibilty and because the "
"compute service cannot reliably determine which types of virtual interfaces "
"(``port.binding:vif_type``) will send ``network-vif-plugged`` events without"
" an accompanying port ``binding:host_id`` change. Open vSwitch and "
"linuxbridge should be OK, but OpenDaylight is at least one known backend "
"that will not currently work in this case, see bug "
"https://launchpad.net/bugs/1755890 for more details."
msgstr ""
"이것은 기본적으로 후 backward compatibility 및 compute 서비스가 신뢰할 수 있는 방식으로 virtual "
"interfaces의 유형 (``port.binding:vif_type``)가 ``network-vif-plugged`` 이벤트를 보낼 "
"때 ``binding:host_id``를 함께 보낸다. Open vSwitch 및 linuxbridge는 정상적으로 작동합니다. 그러나 "
"OpenDaylight은 현재 이 경우에 작동하지 않을 것으로 알려진至少 하나의 backend입니다. 더 많은 세부 사항은 "
"https://launchpad.net/bugs/1755890에 있습니다."

#: ../../<reno.sphinxext stable/pike>:1001
msgid ""
"This is not a regression, just a note about functionality that is not yet "
"available. Support for modeling shared storage providers will be worked on "
"in the Queens release."
msgstr ""
"이것은 리그레션이 아니라, 기능성에 대한 노트입니다. 현재 사용 가능한 기능이 아니며, 공유 스토리지 제공자 모델링에 대한 지원은 퀸즈 "
"릴리스에서 작업할 예정입니다."

#: ../../<reno.sphinxext stable/pike>:1415
msgid "This is now an optional positional argument. For example::"
msgstr "이것은 now optional positional argument입니다. 예를 들어:"

#: ../../<reno.sphinxext unmaintained/2023.1>:356
msgid ""
"This is now possible to configure nova-compute services using libvirt driver"
" by setting ``[libvirt]cpu_power_management`` to ``True`` in order to let "
"the service to powering down or up physical CPUs depending on whether those "
"CPUs are pinned or not to instances. In order on to support this feature, "
"the compute service needs to be set with ``[compute]cpu_dedicated_set``. If "
"so, all the related CPUs will be powering down until they are used by an "
"instance where the related pinned CPU will be powering up just before "
"starting the guest. If ``[compute]cpu_dedicated_set`` isn't set, then the "
"compute service will refuse to start. By default the power strategy will "
"offline CPUs when powering down and online the CPUs on powering up but "
"another strategy is possible by using "
"``[libvirt]cpu_power_management_strategy=governor`` which will rather modify"
" the related CPU governor using ``[libvirt]cpu_power_governor_low`` and "
"``[libvirt]cpu_power_governor_high`` configuration values (respective "
"defaults being  ``powersave`` and ``performance``)"
msgstr ""
"이러한 nova-compute 서비스를 libvirt 드라이버로 구성할 수 있게 된 것은 ``[libvirt]cpu_power_management``을 ``True``로 설정하여 서비스가 물리 CPU를 제어할 수 있는지 여부를 결정하는 것에 있습니다. 이 서비스가 물리 CPU를 제어할 수 있으면, 서비스가 CPU를 제어할 수 있는지 여부에 따라 서비스가 CPU를 down하거나 up할 수 있습니다. CPU가 pinned된 경우, 서비스가 CPU를 down할 수 있습니다. 그러나 CPU가 pinned되지 않은 경우, 서비스가 CPU를 down할 수 없습니다. 또한, CPU가 pinned된 경우, 서비스가 CPU를 up할 수 있습니다. 그러나 CPU가 pinned되지 않은 경우, 서비스가 CPU를 up할 수 없습니다. \n"
"\n"
"이 기능을 지원하기 위해, 컴퓨터 서비스가 ``[compute]cpu_dedicated_set``를 설정해야 합니다. 이 설정이 설정되면, 모든 관련 CPU가 down할 때까지 사용되지 않은 경우에만 down할 수 있습니다. 그러나 관련된 pinned CPU가 down할 때까지 사용되지 않은 경우에만 up할 수 있습니다. 관련된 pinned CPU가 down할 때까지 사용되지 않은 경우에만 up할 수 있습니다. 그러나 관련된 pinned CPU가 down할 때까지 사용되지 않은 경우에만 up할 수 있습니다. \n"
"\n"
"``[compute]cpu_dedicated_set``가 설정되지 않으면, 서비스가 시작할 수 없습니다. \n"
"\n"
"기본적으로, 서비스가 down할 때는 offline CPU를 사용하고, 서비스가 up할 때는 online CPU를 사용합니다. 그러나 다른 전략이 가능합니다. libvirt 드라이버를 사용하여 ``[libvirt]cpu_power_management_strategy=governor``를 설정하여, 관련된 CPU를 사용하는 governor를 ``[libvirt]cpu_power_governor_low``와 ``[libvirt]cpu_power_governor_high``의 설정 값으로 수정할 수 있습니다. (default는 ``powersave``와 ``performance``입니다.)"

#: ../../<reno.sphinxext unmaintained/wallaby>:652
msgid ""
"This machine type will then be used when the instance is restarted or "
"migrated as it will now appear as an image metadata property associated with"
" the instance."
msgstr ""
"이 मशीन 타입은 인스턴스가 재시작되거나 이식되면 이제 이미지는 인스턴스와 관련된 이미지 메타데이트 속성으로 나타날 것이므로 이 "
"मशीन 타입을 사용할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:430
msgid ""
"This means if you are using system scope token to access Nova API then the "
"request will be failed with 403 error code. Also, new defaults will be "
"enforced by default. To know about the new defaults of each policy rule, "
"refer to the `Policy New Defaults`_. For more detail about the Nova API "
"policies changes, refer to `Policy Concepts`_."
msgstr ""
"이것은 시스템 스코프 토큰을 사용하여 Nova API에 접근하는 경우, 403 오류 코드가 발생하여 요청이 실패합니다. 또한, 새로운 "
"기본 설정은 기본적으로 강제됩니다. 각 정책 규칙의 새로운 기본 설정에 대한 정보를 biết하려면 `Policy New "
"Defaults`를 참조하십시오. Nova API 정책 변경에 대한 더 많은 정보는 `Policy Concepts`를 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:1761
msgid ""
"This means it is no longer possible to class-load custom out-of-tree quota "
"drivers."
msgstr ""
"이것은 더 이상 custom out-of-tree quota driver를 class-load 할 수 없다는 것을 의미합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:223 unmaintained/xena>:466
msgid ""
"This option is primarily intended for CI and development clouds as a bridge "
"for operators to mitigate the issue while they work with their upstream "
"image vendors."
msgstr ""
"이 옵션은 CI 및 개발 클라우드에 대한 주요 목적이며, 업스트림 이미지 वENDOR와 함께 작업하는 동안 운영자들이 문제를 완화하기 "
"위해桥梁 역할을 할 수 있는 bridge로 설계되었다."

#: ../../<reno.sphinxext stable/stein>:758
msgid ""
"This parameter (``changes-before``) does not change any read-deleted "
"behavior in the os-instance-actions or os-migrations APIs. The os-instance-"
"actions API with the 2.21 microversion allows retrieving instance actions "
"for a deleted server resource. The os-migrations API takes an optional "
"``instance_uuid`` filter parameter but does not support returning deleted "
"migration records."
msgstr ""
"이 매개 변수 (``changes-before``)는 os-instance-actions 또는 os-migrations APIs의 "
"read-deleted 행동을 변경하지 않는다. os-instance-actions API는 2.21 마이크로 버전을 사용하여 삭제된 "
"서버 리소스를 retrieve하는 인스턴스 액션을 얻을 수 있다. os-migrations API는 옵션의 "
"``instance_uuid`` 필터 매개 변수를 사용하지만 삭제된 이민 레코드를 반환하지 않는다."

#: ../../<reno.sphinxext stable/2023.2>:160 stable/2024.1>:593
#: unmaintained/2023.1>:228 unmaintained/zed>:66
msgid ""
"This patch fixes this situation by translating VF capabilities reported by "
"Libvirt to Neutron port binding profiles. Other VF capabilities are "
"translated as well for possible future use."
msgstr ""
"이 패치는 리버트(Libvirt)에서รายงาน된 VF 능력과 네트워크 포트 결합 프로필(Neutron port binding "
"profiles)에 대한 번역을 통해 이 situación을 해결합니다. 다른 VF 능력도 possible future use에 대하여도"
" 번역됩니다."

#: ../../<reno.sphinxext stable/rocky>:1403
msgid ""
"This plugin directly processes the VHD files in XenServer SR(Storage "
"Repository). So this plugin only works when the host's SR type is file "
"system based e.g. ext, nfs.  This is the default plugin."
msgstr ""
"이 플러그인은 XenServer SR(Storage Repository)에서 VHD 파일을 직접 처리한다. 따라서 이 플러그인은 호스트의"
" SR 유형이 파일 시스템 기반인 경우만 작동한다. 예를 들어, ext, nfs와 같은.  이 플러그인은 기본적으로 작동한다."

#: ../../<reno.sphinxext stable/rocky>:1417
msgid "This plugin implements an image proxy in nova compute service."
msgstr "이 플러그인은 nova 컴퓨터 서비스에서 이미지 프록시를 구현합니다."

#: ../../<reno.sphinxext stable/rocky>:1409
msgid ""
"This plugin implements an image upload method which attaches the VDI as a "
"local disk in the VM in which the OpenStack Compute service runs. It uploads"
" the raw disk to glance when creating an image; When booting an instance "
"from a glance image, it downloads the image and streams it into the disk "
"which is attached to the compute VM."
msgstr ""
"이 플러그인은 OpenStack Compute 서비스가 실행되는 VM 내에서 VDI를 로컬 디스크로 연결하여 이미지 업로드 방법을 "
"구현합니다. 이미지를 생성할 때 raw 디스크를 glance에 업로드합니다. glance 이미지에서 인스턴스를 부트면, 이미지를 "
"다운로드하고 compute VM에 연결된 디스크에 스트리밍합니다."

#: ../../<reno.sphinxext stable/2024.2>:330
msgid ""
"This release adds a new config option require_secure to the spice "
"configuration group. Defaulting to false to match the previous behavior, if "
"set to true the SPICE consoles will require TLS protected connections. "
"Unencrypted connections will be gracefully redirected to the TLS port via "
"the SPICE protocol."
msgstr ""
"이 릴리즈는 스피시 구성 그룹에 새로운 구성 옵션인 require_secure를 추가한다. 기본적으로는 false로 설정되어 이전 동작과"
" 일치한다. require_secure가 true로 설정되면 SPICE 콘솔은 TLS 보호된 연결을 필요로 한다. 비보안 연결은 "
"SPICE 프로토콜을 통해 TLS 포트로 유연하게 리다이렉트된다."

#: ../../<reno.sphinxext stable/pike>:814
msgid ""
"This release adds support for Netronome's Agilio OVS VIF type. In order to "
"use the accelerated plugging modes, external Neutron and OS-VIF plugins are "
"required. Consult https://github.com/Netronome/agilio-ovs-openstack-plugin "
"for installation and operation instructions. Consult the Agilio "
"documentation available at https://support.netronome.com/ for more "
"information about the plugin compatibility and support matrix."
msgstr ""
"이 릴리즈는 Netronome의 Agilio OVS VIF 유형을 지원합니다. 가속화된 연결 모드를 사용하려면 외부 Neutron 및 "
"OS-VIF 플러그인을 사용해야 합니다. 설치 및 운영 지침은 https://github.com/Netronome/agilio-ovs-"
"openstack-plugin 을 참조하십시오. Agilio 문서가 https://support.netronome.com/ 에 "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available available "
"available available available available available available"

#: ../../<reno.sphinxext stable/2025.1>:269
msgid ""
"This release adds support for SR-IOV devices using the new kernel VFIO SR-"
"IOV variant driver interface. See the `OpenStack pci-passthrough "
"documentation`__ for more details."
msgstr ""
"이 릴리스에서는 새로운 커널 VFIo SR-IOV 버전 드라이버 인터페이스를 사용하여 SR-IOV 장치를 지원합니다. 더 많은 정보는 "
"`OpenStack pci-passthrough documentation`__에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/stein>:802
msgid ""
"This release adds support for ``direct`` and ``virtio-forwarder`` VNIC types"
" to the ``vrouter`` VIF type. In order to use these VNIC types, support is "
"required from the version of OpenContrail, Contrail or Tungsten Fabric that "
"is installed, as well the required hardware. At this time, the reference os-"
"vif plugin is hosted on OpenContrail at https://github.com/Juniper/contrail-"
"nova-vif-driver but is expected to transition to Tungsten Fabric in the "
"future. Version 5.1 or later of the plugin is required to use these new VNIC"
" types. Consult the `Tungsten Fabric documentation "
"<https://tungstenfabric.github.io/website/>`_ for release notes, when "
"available, about hardware support. For commercial support, consult the "
"release notes from a downstream vendor."
msgstr ""
"이 릴리즈에서는 `direct` 및 `virtio-forwarder` VNIC 유형을 `vrouter` VIF 유형에 추가 지원을 "
"제공합니다. 이러한 VNIC 유형을 사용하려면 OpenContrail, Contrail 또는 Tungsten Fabric가 설치된 버전의"
" 지원이 필요하며, 이에 따라 필요한 하드웨어도 필요합니다. 현재, reference os-vif 플러그인은 OpenContrail에 "
"https://github.com/Juniper/contrail-nova-vif-driver 에서 호스팅됩니다. 그러나 이 future "
"에서 Tungsten Fabric로 전환될 것으로 예상됩니다. 버전 5.1 이상의 플러그인은 이러한 새로운 VNIC 유형을 사용하기 위해"
" 필요합니다. release notes에 대한 하드웨어 지원에 대한 정보는 Tungsten Fabric 문서 "
"<https://tungstenfabric.github.io/website/>에 consult하십시오. 상업 지원을 원한다면, 하위 "
"벤더의 release notes를 consult하십시오."

#: ../../<reno.sphinxext stable/2025.1>:294
msgid ""
"This release adds support for migrating SR-IOV devices using the new kernel "
"VFIO SR-IOV variant driver interface. See the `OpenStack configuration "
"documentation`__ for more details."
msgstr ""
"이 릴리스는 새로운 커널 VFIO SR-IOV 버전 드라이버 인터페이스를 사용하여 SR-IOV 장치를 mig리그레이션하는 지원을 "
"추가했습니다. 더 많은 정보는 `OpenStack 구성 문서`__에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1014
msgid ""
"This release adds support to attach a volume to multiple server instances. "
"The feature can only be used in Nova if the volume is created in Cinder with"
" the **multiattach** flag set to True. It is the responsibility of the user "
"to use a proper filesystem in the guest that supports shared read/write "
"access."
msgstr ""
"이 릴리즈는 여러 서버 인스턴스를 연결할 수 있는 볼륨을 연결하는 기능을 추가했습니다. 이 기능은 볼륨이 시더에 "
"**multiattach** 플래그를 True로 설정하여 생성된 경우에만 Nova에서 사용할 수 있습니다. 사용자는 공유 읽기/쓰기 "
"접근을 지원하는 적절한 파일 시스템을 게스트에서 사용해야 합니다."

#: ../../<reno.sphinxext stable/ussuri>:1157
msgid ""
"This release contains a fix for `bug 1748697`_ which distinguishes between "
"resize and cold migrate such that the ``allow_resize_to_same_host`` config "
"option is no longer used to determine if a server can be cold migrated to "
"the same compute service host. Now when requesting a cold migration the API "
"will check if the source compute node resource provider has the "
"``COMPUTE_SAME_HOST_COLD_MIGRATE`` trait and if so the scheduler can select "
"the source host. Note that the only in-tree compute driver that supports "
"cold migration to the same host is ``VMwareVCDriver``."
msgstr ""
"이 릴리즈에는 `bug 1748697`_을แก리는 것이 포함되어 있으며, resize와 cold migrate를 구별하여, "
"`allow_resize_to_same_host` 구성 옵션을 더 이상 서버가 same compute service 호스트에 cold "
"migrate 할 수 있는지 결정하는 데 사용되지 않는다. 현재冷 마이그레이션을 요청할 때, API는 소스 컴퓨터 노드 리소스 제공자에 "
"`COMPUTE_SAME_HOST_COLD_MIGRATE` trait가 있는지 확인하고, 그러면 스케줄러가 소스 호스트를 선택할 수 "
"있다. note가 있다. VMwareVCDriver만 in-tree compute driver가 same host에 cold "
"migration을 지원하는 것은 것이다."

#: ../../<reno.sphinxext stable/train>:315 stable/ussuri>:1223
msgid ""
"This release contains a fix for `bug 1856925`_ such that ``resize`` and "
"``migrate`` server actions will be rejected with a 409 ``HTTPConflict`` "
"response if the source compute service is down."
msgstr ""
"이 릴리스에는 `bug 1856925`_을 해결하기 위해 `resize` 및 `migrate` 서버 액션을 409 "
"`HTTPConflict`응답과 함께 거부할 수 있는ように 소스 컴퓨터 서비스가 down일 때를対象으로 하는 것이 포함됩니다."

#: ../../<reno.sphinxext unmaintained/victoria>:767
msgid ""
"This release contains a fix for `bug 1874032`_ which delegates snapshot "
"upload into a dedicated thread. This ensures nova compute service stability "
"on busy environment during snapshot, when concurrent snapshots or any other "
"tasks slow down storage performance."
msgstr ""
"이 릴리즈에는 `bug 1874032`_을 위한修复이 포함되어 있습니다. 이修复은 스냅샘 업로드를 고유 스레드에 위임하여 nova 컴퓨터"
" 서비스의 안정성을 busy 환경에서 스냅샘에 대한 동시 스냅샘 또는 다른 task가 스토리지 성능을 느리게 만드는 경우에 도달합니다."

#: ../../<reno.sphinxext stable/stein>:95 stable/train>:304
#: stable/ussuri>:1202
msgid ""
"This release contains a fix for a `regression`__ introduced in 15.0.0 "
"(Ocata) where server create failing during scheduling would not result in an"
" instance action record being created in the cell0 database. Now when "
"creating a server fails during scheduling and is \"buried\" in cell0 a "
"``create`` action will be created with an event named "
"``conductor_schedule_and_build_instances``."
msgstr ""
"이 릴리즈에는 15.0.0 (Ocata)에서 `regression`__이 도입된 `서버 생성이 스케줄링 중에 실패하여 인스턴스 액션 "
"레코드가 세ลล0 데이터베이스에 생성되지 않도록`에 대한修复이 포함됩니다. 현재 스케줄링 중에 서버 생성이 실패하고 세ลล0에 "
"\"埋葬\"되어 있는 경우, \"create\" 액션은 \"conductor_schedule_and_build_instances\"라는 "
"이벤트와 함께 생성됩니다."

#: ../../<reno.sphinxext ../source/newton.rst:17 origin/stable/ocata>:265
#: stable/pike>:333
msgid ""
"This release contains a schema migration for the ``nova_api`` database in "
"order to address bug 1738094:"
msgstr "이 릴리스에는 `nova_api` 데이터베이스의 스키마 전환을 포함하여 버그 1738094를 해결하기 위해 사용됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:17 stable/pike>:17
#: stable/queens>:87 stable/rocky>:179 stable/stein>:325 stable/train>:1271
msgid ""
"This release contains a security fix for `bug 1837877`_ where users without "
"the admin role can be exposed to sensitive error details in the server "
"resource fault ``message``."
msgstr ""
"이 릴리즈에는 `bug 1837877`_의 보안修复이 포함되어 있으며, 관리자 역할이 없는 사용자는 서버 리소스 오류 fault "
"`message`에 대한 민감한 오류 디tails에 노출될 수 있습니다."

#: ../../<reno.sphinxext ../source/newton.rst:54 origin/stable/ocata>:311
#: stable/pike>:430 stable/queens>:1741
msgid ""
"This release includes a fix for `bug 1733886`_ which was a regression "
"introduced in the 2.36 API microversion where the ``force`` parameter was "
"missing from the ``PUT /os-quota-sets/{tenant_id}`` API request schema so "
"users could not force quota updates with microversion 2.36 or later. The bug"
" is now fixed so that the ``force`` parameter can once again be specified "
"during quota updates. There is no new microversion for this change since it "
"is an admin-only API."
msgstr ""
"이 릴리즈에는 `bug 1733886`_을修正한 것이 2.36 API 마이크로 버전에서 `force` 매개 변수가 `PUT /os-"
"quota-sets/{tenant_id}` API 요청 스키마에서 missing이었습니다. 이로 인해 2.36 이상 마이크로 버전에서는 "
"force quota updates를 할 수 없습니다. 그러나 이 버그는 이제 `force` 매개 변수를 다시quota 업데이트에서 "
"지정할 수 있습니다. 이 변경은 admin-only API이므로 새로운 마이크로 버전이 없습니다."

#: ../../<reno.sphinxext ../source/mitaka.rst:17 ../source/newton.rst:141
#: origin/stable/ocata>:151 origin/stable/ocata>:407 stable/pike>:220
#: stable/pike>:584
msgid "This release includes fixes for security vulnerabilities."
msgstr "이 릴리스에는 보안 위험성에 대한修复이 포함되어 있습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:668
msgid ""
"This release includes work in progress support for Keystone's unified "
"limits. This should not be used in production. It is included so we can "
"collect early feedback from operators around the performance of the new "
"limits system. There is currently no way to export your existing quotas and "
"import them into Keystone. There is also no proxy API to allow you to update"
" unified limits via Nova APIs. All the update APIs behave as if you are "
"using the noop driver when the unified limits quota driver is configured."
msgstr ""
"이 릴리즈에는 Keyston의統합 제한을 지원하는 작업이 포함되어 있습니다. 이 릴리즈는 프로덕션에서 사용하지 마세요. 이 릴리즈는 현재"
" 제한 시스템의 성능에 대한 운영자들의 초기 피드백을 수집하기 위해 포함되었습니다. 현재 현재의 제한을 내보내고 "
"Keyston에.import할 수 있는 방법이 없습니다. 또한 Keyston의 제한을 Nova API를 통해 업데이트할 수 있는 프록시 "
"API가 없습니다. 모든 업데이트 API는 unified limits quota driver가 구성된 경우 noop driver를 "
"사용하는 것처럼 행동합니다."

#: ../../<reno.sphinxext stable/stein>:1044
msgid ""
"This release moves the ``vrouter`` VIF plug and unplug code to a separate "
"package called ``contrail-nova-vif-driver``. This package is a requirement "
"on compute nodes when using Contrail, OpenContrail or Tungsten Fabric as a "
"Neutron plugin. At this time, the reference plugin is hosted on OpenContrail"
" at https://github.com/Juniper/contrail-nova-vif-driver but is expected to "
"transition to Tungsten Fabric in the future. Release ``r5.1.alpha0`` or "
"later of the plugin is required, which will be included in Tungsten Fabric "
"5.1."
msgstr ""
"이 릴리스에서는 `vrouter` VIF 플러그 및 언플러그 코드를 `contrail-nova-vif-driver`라는 이름의 별개된 "
"패키지로 이동했습니다. 이 패키지는 Contrail, OpenContrail 또는 Tungsten Fabric를 Neutron "
"플러그인으로 사용할 때 컴퓨터 노드에서 필요합니다. 현재, 참조 플러그는 OpenContrail의 "
"https://github.com/Juniper/contrail-nova-vif-driver에 호스팅되어 있지만 향후 Tungsten "
"Fabric로 전환될 것으로 예상됩니다. 릴리스 `r5.1.alpha0` 이상의 플러그는 Tungsten Fabric 5.1에 포함될 "
"예정이며, 이 릴리스는 Tungsten Fabric 5.1의 일부입니다."

#: ../../<reno.sphinxext stable/rocky>:1568
msgid ""
"This release moves the livirt driver ``IVS`` VIF plug-unplug to a separate "
"package called ``os-vif-bigswitch``. This package is a requirement on "
"compute nodes when using ``networking-bigswitch`` as neutron ML2 and L3 "
"driver. Releases are available on https://pypi.org/project/os-vif-"
"bigswitch/. Major version for the package matches upstream neutron version "
"number. Minor version tracks compatiblity with Big Cloud Fabric (BCF) "
"releases, and typically is set to the lowest supported BCF release."
msgstr ""
"이 릴리즈는 livirt 드라이버 \"IVS\" VIF를 separate 패키지 \"os-vif-bigswitch\"로 이동했습니다. 이"
" 패키지는 \"networking-bigswitch\"를 사용하여 neutron ML2 및 L3 드라이버로 사용할 때 compute "
"노드에서 필요합니다. 릴리즈는 https://pypi.org/project/os-vif-bigswitch/에서 사용할 수 있습니다. "
"패키지의 주요 버전은 upstream neutron 버전 번호와 일치합니다. 패키지의 부수 버전은 Big Cloud Fabric "
"(BCF) 릴리즈의 호환성과 일치하며, 일반적으로는 가장 지원되는 BCF 릴리즈로 설정됩니다."

#: ../../<reno.sphinxext stable/2024.2>:318
msgid ""
"This release removes the limit of the number of tenants that can be "
"specified for an aggregate and honored by the "
"`AggregateMultitenancyIsolation` filter. It now respects multiple keys "
"prefixed by `filter_tenant_id` like the request filter implementation.  You "
"can use `filter_tenant_id` as a prefix to set an infinite number of "
"properties for tenant IDs on the aggregate. This change has been implemented"
" in a manner that preserves backward compatibility. Existing configurations "
"using `filter_tenant_id` will continue to function as expected."
msgstr ""
"이 릴리스에서는 집합에 지정할 수 있는 tenant의 수를 제한하는 limitation을 제거하고, "
"`AggregateMultitenancyIsolation` 필터가 honoring하는 것을 제거했습니다.  현재, "
"`filter_tenant_id`를 prefix로 사용하여 여러 키가 존재합니다.  이 필터 implementation와 마찬가지로.  "
"`filter_tenant_id`를 prefix로 사용하여 tenant ID에 infinite number of properties를 "
"설정할 수 있습니다.  이 변경 사항은 backward compatibility를 유지하기 위해 implementation이 "
"수행되었습니다.  현재의 구성 using `filter_tenant_id`는 예상대로 작동합니다."

#: ../../<reno.sphinxext stable/queens>:989
msgid ""
"This requires that the Placement API version 1.17 is available before the "
"``nova-scheduler`` service can use this feature."
msgstr ""
"이 기능을 사용하려면 Placement API 버전 1.17이 preceding `nova-scheduler` 서비스에서 사용할 수 "
"있는지 확인해야 합니다."

#: ../../<reno.sphinxext stable/rocky>:857
msgid ""
"This requires that the Placement API version 1.22 is available before the "
"``nova-scheduler`` service can use this feature."
msgstr ""
"이 기능을 사용하려면 Placement API 버전 1.22이 preceding `nova-scheduler` 서비스가 사용할 수 있는지"
" 확인해야 합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:231 unmaintained/wallaby>:955
msgid ""
"This resolves `bug 1905701`_ where instances with ``LUKSv1`` encrypted "
"volumes could not be restarted automatically by the ``nova-compute`` service"
" after a host reboot when the ``[DEFAULT]/resume_guests_state_on_host_boot``"
" configurable was enabled."
msgstr ""
"이 resolves `bug 1905701`_에서, `nova-compute` 서비스가 호스트 재부팅 시 `LUKSv1` 암호화된 볼륨을"
" 사용하는 인스턴스들이 `resume_guests_state_on_host_boot` 설정이 활성화된 경우, 자동으로 재시작할 수 "
"없게되었습니다."

#: ../../<reno.sphinxext stable/2024.1>:276 stable/2024.2>:413
msgid ""
"This resolves `bug 1907775`_ where after such move the instance become stuck"
" in between availability zones."
msgstr "이 resolves `bug 1907775`_에서 such move 이후 인스턴스는 가용 구역 사이에서 고정되게 된다."

#: ../../<reno.sphinxext stable/pike>:1614
msgid ""
"This starts the Python interactive interpreter. It is a clone of the same "
"functionality found in Django's `django-manage` command. This command hasn't"
" been actively maintained in a long time and is not tested. It will simply "
"be removed."
msgstr ""
"이것은 파이썬의 인터랙티브 인터프리터가 시작됩니다. 이것은 Django의 `django-manage` 명령의 동일한 기능을 가진 "
"클론입니다. 이 명령은 오랜 시간 동안 활성화되지 않았으며 테스트되지 않았습니다. 이 명령은 단순히 제거됩니다."

#: ../../<reno.sphinxext stable/train>:896
msgid ""
"This will be resolved in a future patch release. For more information, refer"
" to `bug 1845986`__"
msgstr "이 문제는 향후 패치 릴리스에서 해결될 것이다. 더 많은 정보를 얻으려면 `bug 1845986`__를 참조하십시오."

#: ../../<reno.sphinxext stable/ussuri>:1428
msgid "This workaround does not currently support extending attached volumes."
msgstr "이 workaround은 현재 확장된 연결 볼륨을 지원하지 않는다."

#: ../../<reno.sphinxext stable/ussuri>:1407 stable/ussuri>:1430
msgid ""
"This workaround is temporary and will be removed during the W release once "
"all impacted distributions have been able to update their versions of the "
"libgcrypt library."
msgstr ""
"이 workaround은暂시적이며 W 릴리스에서 모든 영향을 받은 배포가 libgcrypt 라이브러리 버전을 업데이트할 때까지 "
"제거됩니다."

#: ../../<reno.sphinxext unmaintained/yoga>:145 unmaintained/zed>:414
msgid ""
"This workaround will be deprecated and removed once Nova replaces the older "
"libvirt APIs with their newer counterparts. The work is being tracked via "
"this `blueprint cpu-selection-with-hypervisor-consideration`_."
msgstr ""
"이 workaround은 노바가 더 오래된 libvirt API를 newer counterpart로 대체할 때까지 deprecated 및"
" 제거될 것이다. 이 작업은 `blueprint cpu-selection-with-hypervisor-consideration`_를 통해"
" 추적되고 있다."

#: ../../<reno.sphinxext stable/pike>:1683
msgid ""
"Those server actions can be replaced by calling the Neutron API directly."
msgstr "네트로恩 API를 직접 호출하여 those server actions를 대체할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1181
msgid ""
"Three live-migration related configuration options were restricted by "
"minimum values since 16.0.0 and will now raise a ValueError if these "
"configuration options' values less than minimum values, instead of logging "
"warning before. These configuration options are:"
msgstr ""
"세 가지 live-migration 관련 설정 옵션은 16.0.0 이후에 최소한의 giá치로 제한되었으며, 이때는 설정 옵션의 giá치가"
" 최소한의 giá치보다 낮을 때 ValueError가 발생하게 되고, 이전에는 경고 메시지만 출력하도록 하였다. 이 설정 옵션은 다음과 "
"같다."

#: ../../<reno.sphinxext stable/queens>:932
msgid ""
"Throughout the Placement API, in microversion 1.15, 'last-modified' headers "
"have been added to GET responses and those PUT and POST responses that have "
"bodies. The value is either the actual last modified time of the most "
"recently modified associated database entity or the current time if there is"
" no direct mapping to the database. In addition, 'cache-control: no-cache' "
"headers are added where the 'last-modified' header has been added to prevent"
" inadvertent caching of resources."
msgstr ""
"Placement API의 microversion 1.15에서, GET 응답에서 'last-modified' 헤더가 추가되었으며, PUT"
" 및 POST 응답에서 바디가 있는 경우. 'last-modified' 헤더의 값은, 가장 최근에 수정된 연관된 데이터베이스 엔티티의 "
"실제 마지막 수정 시간이거나, 데이터베이스에 직접 매핑되지 않는 경우 현재 시간이 될 수 있습니다. 또한, 'last-modified' "
"헤더가 추가된 경우 'cache-control: no-cache' 헤더가 추가되어, 자발적으로 캐싱되는 자원을 방지하기 위해."

#: ../../<reno.sphinxext stable/pike>:765
msgid ""
"To address backwards compatibility, the new rules added to the "
"flavor_manage.py policy file, default to the existing rule, "
"``os_compute_api:os-flavor-manage``, if it is set to a non-default value."
msgstr ""
"backward compatibility를 address하기 위해, flavor_manage.py policy file에 추가된 새로운 "
"규칙은, 기본적으로 existing rule, ``os_compute_api:os-flavor-manage``에 설정된 경우에만 non-"
"default value로 설정됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1021
msgid ""
"To allow access to the versions REST API from diverse origins, CORS support "
"has been added to the 'oscomputeversions' pipeline in '/etc/nova/api-"
"paste.ini'. Existing deployments that wish to enable support should add the "
"'cors' filter at the start of the 'oscomputeversions' pipeline."
msgstr ""
"다양한 원천에서 REST API 버전을 접근할 수 있도록, CORS 지원이 'oscomputeversions' 파이프라인에 추가되었다. "
"'/etc/nova/api-paste.ini'에 있는 'oscomputeversions' 파이프라인에 'cors' 필터를 시작에 추가하여"
" 지원을 활성화하고자 하는 существ하는 배포를 추가해야 한다."

#: ../../<reno.sphinxext stable/2025.2>:86
msgid ""
"To avoid any change in ``admin`` permissions, Nova uses "
"``PROJECT_MANAGER_OR_ADMIN`` as a default where manager access is granted. "
"In this release, the below APIs policy are newly defaulted to "
"``PROJECT_MANAGER_OR_ADMIN``:"
msgstr ""
"``admin`` 권한에 대한任何 변경을 피하기 위해, Nova는 ``PROJECT_MANAGER_OR_ADMIN``를 기본으로 사용하여"
" 관리자에 대한 접근이 허가되는 ``admin`` 권한을 사용합니다. 이 릴리즈에서, 아래 API 정책이 "
"``PROJECT_MANAGER_OR_ADMIN``로 새로 기본화되었습니다."

#: ../../<reno.sphinxext unmaintained/zed>:282
msgid ""
"To enable this, provide `hw:viommu_model` in flavor extra spec or equivalent"
" image metadata property `hw_viommu_model` and with the guest CPU "
"architecture and OS allows, we will enable viommu in Libvirt driver. Support"
" values intel|smmuv3|virtio|auto. Default to ``auto``. Which ``auto`` will "
"automatically select ``virtio`` if Libvirt supports it, else ``intel`` on "
"X86 (Q35) and ``smmuv3`` on AArch64. vIOMMU config will raise invalid "
"exception if the guest architecture is neither X86 (Q35) or AArch64."
msgstr ""
"`hw:viommu_model`을 플레버의 추가 속성 spec 또는 equivalent image metadata 속성 "
"`hw_viommu_model`를 제공하여 이 기능을 활성화할 수 있습니다. guest CPU 아rchitecture와 OS를 허용하면,"
" viommu를 Libvirt 드라이버에서 활성화합니다. 지원하는 값은 intel|smmuv3|virtio|auto입니다. 기본적으로는 "
"`auto`입니다. `auto`는 Libvirt가 지원하는 경우에 `virtio`를 선택합니다. 반면 X86 (Q35)와 "
"AArch64에서 `intel`과 `smmuv3`를 사용합니다. guest 아rchitecture가 X86 (Q35) 또는 "
"AArch64가 아닌 경우 viommu 구성은 무效한 예외를 발생시킵니다."

#: ../../<reno.sphinxext stable/2025.2>:322
msgid ""
"To expose the newly added attributes in the libvirt domain metadata in an "
"upgraded environment, already running instances will need to be shutdown, "
"restarted, cold migrated or shelved and unshelved."
msgstr ""
"libvirt 도메인 메타데이터에 새로운 속성들을 노출시키기 위해 업그레이드된 환경에서 이미 실행 중인 인스턴스를 종료, 재시작, 냉동 "
"이식 또는 보관 및 보관 해제해야 합니다."

#: ../../<reno.sphinxext unmaintained/yoga>:40 unmaintained/zed>:458
msgid ""
"To facilitate the transition to no Nova default for ``<cputune><shares>``, "
"its value will be removed during live migration unless a value is set in the"
" ``quota:cpu_shares`` extra spec. This can cause temporary CPU starvation "
"for the live migrated instance if other instances on the destination host "
"still have the old default ``<cputune><shares>`` value. To fix this, hard "
"reboot, cold migrate, or live migrate the other instances."
msgstr ""
"Nova의 기본 `cputune` 및 `shares` 가용性는 live migration 시에 제거되며, "
"`quota:cpu_shares` 추가 스펙이 설정되지 않으면, live migration된 인스턴스에서 임시 CPU "
"starvation이 발생할 수 있습니다. 다른 목적지 호스트에 있는 다른 인스턴스에서 old default `cputune` 및 "
"`shares` 가용性的值이 남아 있으면, 이 문제를 해결하기 위해 다른 인스턴스를 하드 리부트,冷 마이그레이션, 또는 live "
"migration으로 해결할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:362 unmaintained/xena>:732
msgid ""
"To fix `device detach issues`__ in the libvirt driver the detach logic has "
"been changed from a sleep based retry loop to waiting for libvirt domain "
"events. During this change we also introduced two new config options to "
"allow fine tuning the retry logic. For details see the description of the "
"new ``[libvirt]device_detach_attempts`` and "
"``[libvirt]device_detach_timeout`` config options."
msgstr ""
"`기기 분리 문제`를 libvirt 드라이버에서修正하기 위해, 분리 로직은 잠시 대기 기반 재시도 루프에서 libvirt 도메인 이벤트를"
" 대기하기로 변경되었다. 이 변경 사항 동안, 두 가지 새로운 구성 옵션을 도입하여 재시도 로직의 fine tuning을 허용했다. "
"자세한 내용은 새로운 `libvirt]device_detach_attempts` 및 "
"`libvirt]device_detach_timeout` 구성 옵션의 설명을 참조하십시오."

#: ../../<reno.sphinxext stable/ussuri>:696
msgid "To implement the reader roles, Below policies are made more granular"
msgstr "집합을 reader 역할을 implementing 하기 위해, 아래 정책은 더 세부적이게 만들어졌습니다."

#: ../../<reno.sphinxext stable/ussuri>:683
msgid "To know each policy scope_type, please refer the `Policy Reference`_"
msgstr "`Policy Reference`_을 확인하여 각 policy scope_type을 biết하시길 바랍니다."

#: ../../<reno.sphinxext ../source/newton.rst:1104 origin/stable/ocata>:1608
msgid ""
"To make live-migration consistent with resize, confirm-resize and revert-"
"resize operations, the migration status is changed to 'error' instead of "
"'failed' in case of live-migration failure. With this change the periodic "
"task '_cleanup_incomplete_migrations' is now able to remove orphaned "
"instance files from compute nodes in case of live-migration failures. There "
"is no impact since migration status 'error' and 'failed' refer to the same "
"failed state."
msgstr ""
"live-migration을 confirm-resize와 revert-resize와 일관되게 작동하기 위해, live-migration에"
" 실패할 경우 'error'로 thay đổi된 migration status는 'failed'와 동일한 실패 상태를 나타내는 "
"'failed' 대신 'error'로 바뀌게 됩니다. 이 변경 사항으로 인해 주기적인 task "
"'_cleanup_incomplete_migrations'는 live-migration 실패 시 compute 노드에 남아 있는 "
"orphaned instance files를 제거할 수 있습니다. 이 경우에는 migration status 'error'와 "
"'failed'는 동일한 실패 상태를 나타내기 때문에 영향을 받지 않습니다."

#: ../../<reno.sphinxext stable/queens>:1133
msgid ""
"To make use of VeNCrypt, configuration steps are required for both the "
"`nova-novncproxy` service and libvirt on all the compute nodes. The "
"``/etc/libvirt/qemu.conf`` file should be modified to set the ``vnc_tls`` "
"option to ``1``, and optionally the ``vnc_tls_x509_verify`` option to ``1``."
" Certificates must also be deployed on the compute node."
msgstr ""
"VeNCrypt를 사용하려면 `nova-novncproxy` 서비스와 libvirt는 모든 컴퓨팅 노드에 대해 구성步骤이 필요합니다. "
"`/etc/libvirt/qemu.conf` 파일은 `vnc_tls` 옵션을 `1`으로 설정해야 하며, 선택적으로 "
"`vnc_tls_x509_verify` 옵션을 `1`으로 설정해야 합니다. 또한, 컴퓨팅 노드에 대한 자격 증명이 배포되어야 합니다."

#: ../../<reno.sphinxext stable/stein>:1114
msgid ""
"To migrate from the CachingScheduler to the FilterScheduler, operators can "
"leverage the ``nova-manage placement heal_allocations`` command:"
msgstr ""
"`nova-manage placement heal_allocations` 명령을 사용하여 캐싱 "
"스케줄러(CachingScheduler)에서 필터 스케줄러(FilterScheduler)로 mig리gate할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:324 stable/rocky>:1948
msgid ""
"To mitigate potential issues with compute nodes disabling themselves in "
"response to failures that were either non-fatal or user-generated, the "
"consecutive build failure counter functionality in the compute service has "
"been changed to advise the scheduler of the count instead of self-disabling "
"the service upon exceeding the threshold. The "
"``[compute]/consecutive_build_service_disable_threshold`` configuration "
"option still controls whether the count is tracked, but the action taken on "
"this value has been changed to a scheduler weigher. This allows the "
"scheduler to be configured to weigh hosts with consecutive failures lower "
"than other hosts, configured by the "
"``[filter_scheduler]/build_failure_weight_multiplier`` option. If the "
"compute threshold option is nonzero, computes will report their failure "
"count for the scheduler to consider. If the threshold value is zero, then "
"computes will not report this value and the scheduler will assume the number"
" of failures for non-reporting compute nodes to be zero. By default, the "
"scheduler weigher is enabled and configured with a very large multiplier to "
"ensure that hosts with consecutive failures are scored low by default."
msgstr ""
"집합을 사용하여 컴퓨터 노드가 자체적으로 실패를 인지하고 자체적으로 비활성화되는 경우의 가능성에 대한 문제를 완화하기 위해, 컴퓨터 "
"서비스의 연속적인 건설 실패 카운터 기능이 변경되어 스케줄러에게 카운트를 조언하는 대신 서비스가 임계치 초과하면 자체적으로 비활성화되는 "
"대신 스케줄러가 임계치를 초과하면 가중치를 사용하는 hành động으로 변경되었다. "
"``[compute]/consecutive_build_service_disable_threshold`` 구성 옵션은 카운트가 추적되는지 "
"여부를still 제어하지만, 이 값에 대한 hành động이 변경되어 스케줄러 가중치로 변경되었다. 이 allows 스케줄러를 구성하여"
" 연속적인 실패가 임계치 이하인 호스트를 다른 호스트와 비교하여 가중치를 부여할 수 있다. "
"``[filter_scheduler]/build_failure_weight_multiplier`` 옵션에 의해 구성된. If the "
"compute threshold option is nonzero, computes will report their failure "
"count for the scheduler to consider. If the threshold value is zero, then "
"computes will not report this value and the scheduler will assume the number"
" of failures for non-reporting compute nodes to be zero. By default, the "
"scheduler weigher is enabled and configured with a very large multiplier to "
"ensure that hosts with consecutive failures are scored low by default."

#: ../../<reno.sphinxext ../source/newton.rst:203 origin/stable/ocata>:964
msgid ""
"To mitigate this issue Nova no longer generates an empty path attribute to "
"the script element when defining an interface. This resolves the issue with "
"regards to virtual machine creation. To resolve the issue with regards to "
"existing virtual machines a change to Libvirt is required, this is being "
"tracked in `Bugzilla 1412834`_"
msgstr ""
"이 문제를 해결하기 위해 노바는 인터페이스를 정의할 때 스크립트 요소에 비어 있는 경로 속성을 생성하지 않도록 하는 것을 중단했다. 이로"
" 인해 가상 머신 생성과 관련된 문제가 해결된다. 가상 머신이 이미 존재하는 경우, 이 문제를 해결하기 위해 리버트에 대한 변경이 "
"필요하다. 이 변경 사항은 `Bugzilla 1412834`_에追蹄 중이다."

#: ../../<reno.sphinxext stable/rocky>:811
msgid ""
"To place a workload's emulator threads on a set of isolated physical CPUs, "
"set the ``[compute]/cpu_shared_set`` configuration option to the set of host"
" CPUs that should be used for best-effort CPU resources. Then set a flavor "
"extra spec to ``hw:emulator_threads_policy=share`` to instruct nova to place"
" that workload's emulator threads on that set of host CPUs."
msgstr ""
"`workload의 simulatore 스레드들을 isolated physical CPU의 집합에 배치하려면, best-effort "
"CPU 리소스를 최적화하기 위해 사용할 host CPU의 집합을 `compute` 구성 옵션에 설정해야 합니다. 그 다음, "
"`hw:emulator_threads_policy=share`를 `flavor` 추가 속성으로 설정하여 nova가 workload의 "
"simulatore 스레드를 đó 집합의 host CPU에 배치하도록 지시해야 합니다."

#: ../../<reno.sphinxext stable/pike>:1690
msgid ""
"To query attached neutron interfaces for a specific server, the API `GET "
"/servers/{server_uuid}/os-interface` can be used."
msgstr ""
"`네트론 인터페이스를 연결된 네트워크 인터페이스에 대해 특정 서버를 queried하려면, API `GET "
"/servers/{server_uuid}/os-interface`를 사용할 수 있습니다.`"

#: ../../<reno.sphinxext stable/train>:1189
msgid ""
"To resolve `bug 1805659`_ the default value of "
"``[notifications]/notification_format`` is changed from ``both`` to "
"``unversioned``. For more information see the `documentation of the config "
"option`_. If you are using versioned notifications, you will need to adjust "
"your config to ``versioned``\""
msgstr ""
"`bug 1805659`_의 기본값 `[notifications]/notification_format`의 `both`에서 "
"`unversioned`로 변경되었습니다. 더 많은 정보는 `config 옵션의 문서`_를 참조하십시오. 버전화된 알림을 사용하는 경우,"
" config을 `versioned`로 수정해야 합니다."

#: ../../<reno.sphinxext stable/ussuri>:1166
msgid ""
"To support rolling upgrades with N-1 computes if a node does not report the "
"trait and is old the API will fallback to the ``allow_resize_to_same_host`` "
"config option value. That compatibility will be removed in a subsequent "
"release."
msgstr ""
"N-1 컴퓨터를 사용하여 롤링 업그레이드를 지원하기 위해 노드가 trait를 báo하지 않으면서도 오래된 경우 API는 "
"allow_resize_to_same_host 설정 옵션의 giá치로 fallback합니다. 이 호환성은 다음 릴리스에서 제거됩니다."

#: ../../<reno.sphinxext stable/pike>:789
msgid "To verify if hiding hypervisor id is working on Linux based system::"
msgstr "집합을 사용하여 하이퍼바이저 ID를 감추는 것이 리눅스 기반 시스템에서 작동하는지 확인하는 방법입니다."

#: ../../<reno.sphinxext stable/queens>:1843
msgid ""
"Track volume attachment state in the block storage service rather than the "
"compute service (separation of duties, simplicity, etc)"
msgstr "트랙 볼륨을 블록 스토리지 서비스에서 컴퓨터 서비스에서 attaching 상태를 추적하는 대신 (무분리, 단순성, etc)"

#: ../../<reno.sphinxext stable/rocky>:1181
msgid ""
"Trait names which are empty, do not exist, or are otherwise invalid will "
"result in a 400 error."
msgstr "집합 이름이 비어 있거나 존재하지 않거나 다른 경우에는 400 오류가 발생합니다."

#: ../../<reno.sphinxext stable/pike>:870
msgid "Traits are added to the placement with Microversion 1.6."
msgstr "Traits는 Microversion 1.6에서 배치에 속성을 추가합니다."

#: ../../<reno.sphinxext stable/queens>:610
msgid ""
"Traits-based scheduling is now available for the ironic compute driver. For "
"more details, see the `ironic docs for scheduling based on traits`_."
msgstr ""
"Traits-based scheduling은 현재 ironic compute driver에 대해 उपलबitez다. 더 많은 정보는 "
"`ironic docs for scheduling based on traits`_를 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:301
msgid ""
"Turn off threading in the web server. For example, if using ``mod_wsgi`` or "
"``uwsgi``, set ``threads=1`` in their respective configurations."
msgstr ""
"서버에서 스레딩을 비활성화 하십시오. 예를 들어, `mod_wsgi` 또는 `uwsgi`를 사용하는 경우, 그들의 respective "
"구성에 `threads=1`를 설정하십시오."

#: ../../<reno.sphinxext stable/rocky>:1842
msgid "Two keymap-related configuration options have been deprecated:"
msgstr "두 키맵 관련 구성 옵션은弃용되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:309
msgid ""
"Two new ``nova-manage`` CLI commands can be used for `checking`__ the volume"
" attachment connection information and for `refreshing`__ it if the "
"connection is stale (for example with a Ceph backing store and MON IP "
"addresses). Some documentation on how to use them can be found `here`__."
msgstr ""
"``nova-manage`` CLI 명령어의 두 가지 새로운 명령어를 사용하여 볼륨 연결 정보를 확인하고, 연결이 오래된 경우에 "
"refresh할 수 있습니다. 예를 들어 Ceph 백업 스토어와 MON IP 주소가 사용되는 경우. 연결 정보를 확인하고 refresh할"
" 수 있는 방법에 대한 일부 문서는 `여기`에서 찾을 수 있습니다."

#: ../../<reno.sphinxext stable/rocky>:599
msgid ""
"Two new options ``--enable`` and ``--disable`` have been added to the "
"``nova-manage cell_v2 update_cell`` command. Using these flags users can "
"enable or disable scheduling to a cell."
msgstr ""
"``--enable`` 및 ``--disable`` 옵션을 추가하여 ``nova-manage cell_v2 update_cell`` "
"명령을 사용할 수 있습니다. 이 옵션을 사용하면 사용자가 세ลล에 스케줄링을 활성화하거나 비활성화할 수 있습니다."

#: ../../<reno.sphinxext stable/2023.2>:216
msgid ""
"Two new scheduler weighers have been introduced. One helps `sorting the "
"nodes by the number of active instances they run "
"<https://docs.openstack.org/nova/latest/configuration/config.html#filter_scheduler.num_instances_weight_multiplier>`_,"
" the other helps `sorting by the hypervisor version each compute runs "
"<https://docs.openstack.org/nova/latest/configuration/config.html#filter_scheduler.hypervisor_version_weight_multiplier>`_."
" Accordingly, you can place your instances with different strategies, eg. by"
" allocating them to more recent nodes or by reducing the number of noisy "
"instance neighbors."
msgstr ""
"다음 두 개의 새로운 스케줄러 가중치가 도입되었습니다. 하나는 노드의 활성 인스턴스 수에 따라 정렬을 도와주고 "
"<https://docs.openstack.org/nova/latest/configuration/config.html#filter_scheduler.num_instances_weight_multiplier>,"
" 다른 하나는 컴퓨터가 실행하는 하이퍼바이저 버전에 따라 정렬을 도와주고 "
"<https://docs.openstack.org/nova/latest/configuration/config.html#filter_scheduler.hypervisor_version_weight_multiplier>."
" 따라서, 다른 전략으로 인스턴스를 배치할 수 있습니다. 예를 들어, 노드에 할당하는 것을 더 최근 노드로 할당하거나, noisy "
"인스턴스 이웃의 수를 줄이면 됩니다."

#: ../../<reno.sphinxext stable/pike>:1477
msgid ""
"TypeAffinityFilter is deprecated for removal in the 17.0.0 Queens release. "
"There is no replacement planned for this filter. It is fundamentally flawed "
"in that it relies on the ``flavors.id`` primary key and if a flavor "
"\"changed\", i.e. deleted and re-created with new values, it will result in "
"this filter thinking it is a different flavor, thus breaking the usefulness "
"of this filter."
msgstr ""
"TypeAffinityFilter는 17.0.0 Queens 릴리스에서 제거하기 위해弃용되었습니다. 이 필터에 대체 계획은 없습니다. 이"
" 필터는 fundamentally flawed 인 이유는 flavors.id primary key에 의존하여 \"flavor\"가 "
"변경되면, 즉 삭제되어 새로운 값으로 다시 생성되면, 이 필터는 다른 flavor로 간주하여 이 필터의 usefulness가 깨지게 "
"됩니다."

#: ../../<reno.sphinxext stable/2025.1>:436
msgid ""
"Ubuntu 24.04 does not support the QXL video model or spice. Operators using "
"instances with this video model will be unable to start them if moving to "
"Ubuntu 24.04. CentOS 9 and RHEL 9 previously compiled out support for spice,"
" and as QXL was implemented in libspice, support for QXL was removed as a "
"result for the removal of support for spice. The QXL model was a spice "
"optimized video model and is no longer supported by several distros. To "
"avoid issues with starting instances, operators should change the spice "
"video model from QXL to virtio and replace the spice console with vnc. For "
"existing instances the video model can be updated with the ``nova-manage "
"image_property set`` command. When updating the embedded image properties, "
"operators should evaluate if any glance images request the QXL video model "
"and update them or work with the image owner to have them move to a modern "
"video model."
msgstr ""
"Ubuntu 24.04는 QXL 비디오 모델 또는 spice를 지원하지 않습니다. 이 비디오 모델을 사용하는 인스턴스를 Ubuntu "
"24.04로 이동하면 시작할 수 없습니다. CentOS 9 및 RHEL 9는 spice 지원을 제거한 이전에 support를 제거한 것이"
" spice 지원을 제거한 결과로 QXL 비디오 모델의 지원을 제거했습니다. QXL 모델은 spice 최적화된 비디오 모델이었습니다. "
"여러 배포에서 더 이상 지원되지 않습니다. 시작할 수 있는 인스턴스 문제를 피하기 위해, 운영자 spice 비디오 모델을 virtio로 "
"변경하고 spice 콘솔을 vnc로 대체해야 합니다. 현재 인스턴스에서는 ``nova-manage image_property set`` "
"명령을 사용하여 비디오 모델을 업데이트할 수 있습니다. 업데이트된 이미지를 사용하는 경우, 운영자 QXL 비디오 모델을 요청하는 글라ンス"
" 이미지의 경우를 평가하고 업데이트하거나 이미지 소유자가 modern 비디오 모델으로 이동하는 것을 작업해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:463
msgid ""
"Unfortunately the ``nova-manage project quota_usage_refresh`` command will "
"not reset the usages to fix this problem once encountered."
msgstr ""
"Unfortunately the `nova-manage project quota_usage_refresh` 명령은 이 문제를 해결하기 "
"위해 사용량을 다시 설정할 수 없습니다."

#: ../../<reno.sphinxext stable/2025.1>:209
msgid ""
"Unified limits are no longer experimental since we added a nova-manage tool "
"for migrating existing quota limits to Keystone automatically and given we "
"added new configuration options for telling which resource classes are "
"either required or ignored."
msgstr ""
"统일한 제한은 더 이상 실험적이지 않으며, nova-manage 도구를 통해 현재의 자격 제한을 tự động으로 keystone에 "
"이식하고, 또한 새로운 구성 옵션을 통해 자원 클래스가 필요하거나 무시되는지 알 수 있는 새로운 구성 옵션을 추가했습니다."

#: ../../<reno.sphinxext stable/train>:1347
msgid ""
"Unsetting '[DEFAULT] dhcp_domain' will now correctly result in the metadata "
"service/config drive providing an instance hostname of '${hostname}' instead"
" of '${hostname}None', as was previously seen."
msgstr ""
"[DEFAULT] dhcp_domain을 제거하면 이제 metadata service/config drive는 이전에 "
"'${hostname}None'와 같이 보였던 '${hostname}'의 인스턴스 호스트 이름을 '${hostname}'으로 제공합니다."

#: ../../<reno.sphinxext stable/train>:915
msgid ""
"Until all the ``nova-compute`` services that run the ironic driver are "
"upgraded to the Train code that handles the ``power-update`` callbacks from "
"ironic, the ``[nova]/send_power_notifications`` config option can be kept "
"disabled in ironic."
msgstr ""
"nova-compute 서비스가 ironic 드라이버를 사용하여 실행되는 모든 서비스가 Train 코드에 의해 power-update "
"콜백을 처리하는 코드로 업그레이드될 때까지, nova/send_power_notifications 설정 옵션을 비활성화할 수 있다."

#: ../../<reno.sphinxext stable/train>:1379
msgid ""
"Update the way QEMU cache mode is configured for Nova guests: If the file "
"system hosting the directory with Nova instances is capable of Linux's "
"O_DIRECT, use ``none``; otherwise fallback to ``writeback`` cache mode.  "
"This improves performance without compromising data integrity.  `Bug "
"1818847`_."
msgstr ""
"QEMU 캐시 모드의 Nova 게스트를 업데이트하십시오. Nova 인스턴스를 호스팅하는 디렉토리의 파일 시스템이 리눅스의 ODirect를"
" 지원하는 경우, `none`으로 설정하십시오; 그 외에는 `writeback` 캐시 모드를 fallback으로 사용하십시오. 이 방법은"
" 데이터의完整성을 보장하지 않지만 성능을 개선합니다.  `_Bug 1818847_`"

#: ../../<reno.sphinxext origin/stable/ocata>:568
msgid ""
"Updates the network metadata that is passed to configdrive by the Ironic "
"virt driver. The metadata now includes network information about port groups"
" and their associated ports. It will be used to configure port groups on the"
" baremetal instance side."
msgstr ""
"네트워크 메타데이터를 configdrive에 전달하는 Ironic virt 드라이버에 의해 업데이트됩니다. 현재의 메타데이터에는 포트 "
"그룹과 그에关联된 포트의 네트워크 정보가 포함됩니다. 이 메타데이터는 Bare Metal 인스턴스 측의 포트 그룹을 구성하는 데 "
"사용됩니다."

#: ../../<reno.sphinxext ../source/liberty.rst:28 ../source/mitaka.rst:94
#: ../source/mitaka.rst:378 ../source/newton.rst:13 ../source/newton.rst:630
#: origin/stable/ocata>:157 origin/stable/ocata>:261 origin/stable/ocata>:985
#: stable/2023.2>:10 stable/2023.2>:311 stable/2024.1>:178 stable/2024.1>:472
#: stable/2024.2>:340 stable/2025.1>:379 stable/2025.2>:273 stable/pike>:121
#: stable/pike>:294 stable/pike>:493 stable/pike>:526 stable/pike>:1050
#: stable/queens>:130 stable/queens>:284 stable/queens>:408 stable/queens>:538
#: stable/queens>:1193 stable/rocky>:10 stable/rocky>:274 stable/rocky>:374
#: stable/rocky>:460 stable/rocky>:1458 stable/stein>:10 stable/stein>:876
#: stable/train>:151 stable/train>:351 stable/train>:905 stable/ussuri>:119
#: stable/ussuri>:174 stable/ussuri>:280 stable/ussuri>:650
#: unmaintained/2023.1>:10 unmaintained/2023.1>:241 unmaintained/2023.1>:414
#: unmaintained/victoria>:24 unmaintained/victoria>:262
#: unmaintained/victoria>:506 unmaintained/wallaby>:10
#: unmaintained/wallaby>:251 unmaintained/wallaby>:623 unmaintained/xena>:10
#: unmaintained/xena>:474 unmaintained/yoga>:29 unmaintained/yoga>:73
#: unmaintained/yoga>:531 unmaintained/zed>:90 unmaintained/zed>:425
msgid "Upgrade Notes"
msgstr "업그레이드 노트"

#: ../../<reno.sphinxext stable/train>:355 stable/ussuri>:801
msgid ""
"Upgrading to Train on a deployment with a large database may hit `bug "
"1862205`_, which results in instance records left in a bad state, and "
"manifests as instances not being shown in list operations. Users upgrading "
"to Train for the first time will definitely want to apply a version which "
"includes this fix.  Users already on Train should upgrade to a version "
"including this fix to ensure the problem is addressed."
msgstr ""
"트레이닝을 배포에 large database를 사용하여 업그레이드 할 때, `bug 1862205`_이 발생할 수 있으며, 이로 인해 "
"인스턴스 레코드가 나쁜 상태에 남아 있고, list operation에서 인스턴스가 표시되지 않는다.  트레이닝을 처음으로 업그레이드하는"
" 사용자들은 이修复 버전을 포함하는 버전을 적용하고 싶을 것이다.  트레이닝에 이미 업그레이드된 사용자들은 이修复 버전을 포함하는 "
"버전으로 업그레이드하여 문제가 해결되도록 할 것이다."

#: ../../<reno.sphinxext unmaintained/xena>:389
msgid ""
"Usage data can also be later retrieved by calling the ``/os-"
"instance_usage_audit_log`` REST API [1]."
msgstr "사용 데이터는 `/os-instance_usage_audit_log` REST API를 호출하여 나중에 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/2025.2>:180
msgid ""
"Usage of AMD SEV-ES for memory encryption can be required either via a "
"flavor which has the ``hw:mem_encryption_model`` extra spec set to ``amd-"
"sev-es``, or via an image which has the ``hw_mem_encryption_model`` property"
" set to ``amd-sev-es``. In case the extra spec and the property are unset or"
" set to ``amd-sev``, then AMD SEV is used for memory encryption."
msgstr ""
"AMD SEV-ES를 메모리 암호화에 사용하는 방법은 다음 두 가지 방법 중 하나를 사용할 수 있습니다. \n"
"\n"
"1. flavor에 hw:mem_encryption_model extra spec을 amd-sev-es로 설정하면 amd-sev-es를 사용합니다. \n"
"2. image에 hw_mem_encryption_model 속성을 amd-sev-es로 설정하면 amd-sev-es를 사용합니다. \n"
"\n"
"extra spec과 속성이 비어 있거나 amd-sev-es로 설정된 경우 AMD SEV가 메모리 암호화에 사용됩니다."

#: ../../<reno.sphinxext stable/stein>:260
msgid "Use the eventlet wsgi server instead of uWSGI or mod_wsgi, or"
msgstr "이벤트레트 WSGI 서버를 uWSGI 또는 mod_wsgi 대신 사용하십시오."

#: ../../<reno.sphinxext stable/2024.1>:463
msgid ""
"Useful references: https://libvirt.org/formatdomain.html#virtio-related-"
"options https://docs.oasis-"
"open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html "
"https://specs.openstack.org/openstack/nova-"
"specs/specs/2023.2/approved/virtio_packedring_configuration_support.html"
msgstr ""
"사용 가능한 참조: https://libvirt.org/formatdomain.html#virtio-related-options "
"https://docs.oasis-"
"open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html "
"https://specs.openstack.org/openstack/nova-"
"specs/specs/2023.2/approved/virtio_packedring_configuration_support.html"

#: ../../<reno.sphinxext stable/stein>:437
msgid ""
"Users are now able to create servers with Neutron ports that have QoS "
"minimum bandwidth rules when using the 2.72 compute API microversion. See "
"the `using ports with resource request`_ documentation for more details."
msgstr ""
"사용자는 2.72 컴퓨팅 API 마이크로 버전을 사용할 때, QoS 최소帯width 규칙이 있는 Neutron 포트를 사용하여 서버를 "
"생성할 수 있습니다.  `ports with resource request`_ 문서를 참조하십시오."

#: ../../<reno.sphinxext stable/queens>:742
msgid ""
"Users can also use ``changes-since`` filter to filter the results based on "
"the last time the migration record was updated."
msgstr ""
"사용자는 migration 레코드가 마지막으로 업데이트된 시간에 따라 결과를 필터링하기 위해 `changes-since` 필터를 사용할 "
"수 있습니다."

#: ../../<reno.sphinxext stable/stein>:427
msgid ""
"Users can now specify a volume type when creating servers when using the "
"2.67 compute API microversion. See the `block device mapping`_ documentation"
" for more details."
msgstr ""
"사용자는 2.67 compute API microversion을 사용하여 서버를 생성할 때 볼륨 타입을 지정할 수 있습니다. 더 많은 "
"정보는 `블록 장치 매핑`_ documentation을 참조하십시오."

#: ../../<reno.sphinxext stable/stein>:550
msgid ""
"Usually, disk bus is determined automatically from the device type or disk "
"device, and the virtualization type. However, disk bus can also be specified"
" via a block device mapping or an image property. See the ``disk_bus`` field"
" in https://docs.openstack.org/nova/latest/user/block-device-mapping.html "
"for more information about specifying disk bus in a block device mapping, "
"and see https://docs.openstack.org/glance/latest/admin/useful-image-"
"properties.html for more information about the ``hw_disk_bus`` image "
"property."
msgstr ""
"일반적으로 디스크 버스는 장치 유형 또는 디스크 장치, 및 가상화 유형에 따라 tự động으로 결정됩니다. 그러나 디스크 버스는 또한 "
"블록 장치 매핑 또는 이미지 속성을 통해 지정할 수 있습니다. 더 많은 블록 장치 매핑에서 디스크 버스를 지정하는 방법에 대한 정보는 "
"https://docs.openstack.org/nova/latest/user/block-device-mapping.html에あり고, 더"
" 많은 hw_disk_bus 이미지 속성을 사용하는 방법에 대한 정보는 "
"https://docs.openstack.org/glance/latest/admin/useful-image-"
"properties.html에あります."

#: ../../<reno.sphinxext stable/rocky>:1240
msgid ""
"Utilizing recent changes in oslo.messaging, the `rpc_response_timeout` value"
" can now be increased significantly if needed or desired to solve issues "
"with long-running RPC calls timing out before completing due to legitimate "
"reasons (such as live migration prep). If `rpc_response_timeout` is "
"increased beyond the default, nova will request active call monitoring from "
"oslo.messaging, which will effectively heartbeat running activities to avoid"
" a timeout, while still detecting failures related to service outages or "
"message bus congestion in a reasonable amount of time. Further, the "
"`[DEFAULT]/long_rpc_timeout` option has been added which allows setting an "
"alternate timeout value for longer-running RPC calls which are known to take"
" a long time. The default for this is 1800 seconds, and the "
"`rpc_response_timeout` value will be used for the heartbeat frequency "
"interval, providing a similar failure-detection experience for these calls "
"despite the longer overall timeout. Currently, only the live migration RPC "
"call uses this longer timeout value."
msgstr ""
"오스로 메시징의 최근 변경 사항을 사용하여, `rpc_response_timeout`의值를 필요에 따라 크게 증가시킬 수 있습니다. 이 "
"경우, 오래-running RPC 호출이 완료되기 전에 타임아웃하는 문제를 해결하기 위해.-legitimate한 이유(예: live "
"migration prep)와 같은 합리적인 이유로. `rpc_response_timeout`가 기본값을 초과하면, nova는 "
"oslo.messaging에서 활성적인 호출 모니터링을 요청할 것이며, 이는 효과적으로 활성화된 활동을 피하여 타임아웃을 피할 것이며, "
"여전히 서비스 오류 또는 메시지 버스 오버플로와 관련된 실패를reasonable한 시간 내에 감지할 것이다. 또한, "
"`[DEFAULT]/long_rpc_timeout` 옵션을 추가했습니다. 이 옵션은 더 오래-running RPC 호출을 사용하는 것으로"
" 알려진 오래-running RPC 호출에 대하여 대체 타임아웃 값을 설정할 수 있습니다. 기본값은 1800초이며, "
"`rpc_response_timeout`의erval은 피드백 주기 간격을 제공하며, 이러한 호출에 대한 유사한 실패 감지 경험을 "
"제공합니다. 현재, live migration RPC 호출만이 이 더 오래-running 타임아웃 값을 사용합니다."

#: ../../<reno.sphinxext unmaintained/zed>:382
msgid ""
"VDPA Hot-plug live migration requires all compute services to be upgraded to"
" service level 63 to be enabled. Similarly suspend resume need service level"
" 63 and attach/detach require service level 62. As such it will not be "
"available to use during a rolling upgrade but will become available when all"
" host are upgraded to the 26.0.0 (Zed) release."
msgstr ""
"VDPA Hot-plug live migration은 모든 컴퓨팅 서비스가 서비스 수준 63으로 업그레이드되어 활성화되도록 해야 합니다."
" similarly suspend resume는 서비스 수준 63이 필요하고 attach/detach는 서비스 수준 62이 필요합니다. "
"따라서 이 기능은 롤링 업그레이드 중에 사용할 수 없지만 모든 호스트가 26.0.0 (Zed) 릴리스로 업그레이드되면 사용 가능합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:878
msgid "VLAN tags are currently only supported via the Libvirt driver."
msgstr "현재 VLAN 태그는 only Libvirt 드라이버를 통해 지원됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:869
msgid ""
"VLAN tags associated with instance network interfaces are now exposed via "
"the metadata API and instance config drives and can be consumed by the "
"instance. This is an extension of the device tagging mechanism added in past"
" releases. This is useful for instances utilizing SR-IOV physical functions "
"(PFs). The VLAN configuration for the guest's virtual interfaces associated "
"with these devices cannot be configured inside the guest OS from the host, "
"but nonetheless must be configured with the VLAN tags of the device to "
"ensure packet delivery. This feature makes this possible."
msgstr ""
"인스턴스 네트워크 인터페이스와 관련된 VLAN 태그가 now metadata API와 인스턴스 구성 드라이브를 통해 노출되어 인스턴스에 "
"의해 소비될 수 있습니다. 이는 이전 릴리스에 추가된 장치 태그 메커니즘의 확장입니다. SR-IOV 물리 기능 (PFs)을 사용하는 "
"인스턴스에 유용합니다. 게스트의 가상 인터페이스와 관련된 VLAN 구성은 호스트에서 게스트 OS 내에서 구성할 수 없습니다. 그러나 이 "
"장치의 VLAN 태그와 함께 구성되어야 packet 전달을 보장합니다. 이 기능은 이 가능성을 제공합니다."

#: ../../<reno.sphinxext stable/2024.1>:455
msgid ""
"VMs using virtio-net will see an increase in performance. The increase can "
"be anywhere between 10/20% (see DPDK Intel Vhost/virtio perf. reports) and "
"75% (using Napatech SmartNICs)."
msgstr ""
"virtio-net를 사용하는 VMs는 성능이 증가할 수 있습니다. 성능 증가의 범위는 10/20% (DPDK Intel "
"Vhost/virtio perf. reports를 참조하십시오)에서 75% (Napatech SmartNICs를 사용하는 "
"경우)까지입니다."

#: ../../<reno.sphinxext stable/queens>:1102
msgid ""
"VMware serial console log is completed. `VSPC`_ must be deployed along with "
"nova-compute and configured properly. The ``[vmware]/serial_log_dir`` config"
" option must have the same value in both nova.conf and vspc.conf."
msgstr ""
"VMware 시리얼 콘솔 로그가 완료되었습니다. `VSPC`_ must be deployed along with nova-compute "
"and configured properly. The ``[vmware]/serial_log_dir`` config option must "
"have the same value in both nova.conf and vspc.conf."

#: ../../<reno.sphinxext stable/ussuri>:382
msgid ""
"Validation for `known flavor extra specs with recognized namespaces`__."
msgstr "`인식된 네임스페이스와 알려진 플레버에 추가 속성`의 유효성 확인"

#: ../../<reno.sphinxext stable/queens>:608
msgid ""
"Version 1.0.0 of the `osc-placement plugin`_ has been released which "
"provides CLI support for interacting directly with the Placement API."
msgstr ""
"`osc-placement plugin`_ 버전 1.0.0이 출시되었으며, `Placement API`에 직접 상호작용하는 CLI 지원을"
" 제공합니다."

#: ../../<reno.sphinxext stable/pike>:902
msgid ""
"Versioned instance.update notification will be sent when server's tags field"
" is updated."
msgstr "VER전ioned 인스턴스.update 알림이 서버의 태그 필드가 업데이트되면 전송됩니다."

#: ../../<reno.sphinxext stable/2025.1>:483
msgid "Virtuozzo Storage"
msgstr "Virtuozzo Storage"

#: ../../<reno.sphinxext origin/stable/ocata>:599
msgid "Virtuozzo hypervisor now supports ephemeral disks for containers."
msgstr "Virtuozzo 하이퍼바이저 현재 컨테이너에 대한 임시 디스크를 지원합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:647
msgid ""
"Virtuozzo hypervisor now supports libvirt callback to set admin password. "
"Requires libvirt>=2.0.0."
msgstr ""
"Virtuozzo 하이퍼바이저 현재 libvirt 콜백을 사용하여 관리자 패스워드를 설정할 수 있습니다. libvirt>=2.0.0을 "
"사용해야합니다."

#: ../../<reno.sphinxext unmaintained/zed>:216
msgid ""
"Volume-backed instances (instances with root disk attached as a volume) can "
"now be rebuilt by specifying a 2.93 microversion instead of returning a "
"HTTP400 exception."
msgstr ""
"집합에 연결된 루트 디스크가 있는 볼륨-backed 인스턴스 (instances)가 now 2.93 microversion을 "
"사용하여指定하면 HTTP400 예외를 반환하지 않고 다시 rebuilding이 가능합니다."

#: ../../<reno.sphinxext stable/rocky>:519
msgid ""
"Volume-backed instances will no longer report ``root_gb`` usage for new "
"instances and existing instances will heal during move operations."
msgstr ""
"집합-backed 인스턴스는 더 이상 새로운 인스턴스와 기존 인스턴스에 대해 ``root_gb`` 사용량을รายงาน하지 않으며, 이동 "
"연산을 수행할 때 기존 인스턴스는 heal 할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:1463
msgid ""
"WSGI application scripts ``nova-api-wsgi`` and ``nova-metadata-wsgi`` are "
"now available. They allow running the compute and metadata APIs using a WSGI"
" server of choice (for example nginx and uwsgi, apache2 with mod_proxy_uwsgi"
" or gunicorn). The eventlet-based servers are still available, but the WSGI "
"options will allow greater deployment flexibility."
msgstr ""
"WSGI 애플리케이션 스크립트 \"nova-api-wsgi\"과 \"nova-metadata-wsgi\"가 현재 उपलब 있습니다.它们는"
" 선택한 WSGI 서버를 사용하여 컴퓨터 API와 메타데이터 API를 실행할 수 있습니다. 예를 들어 nginx와 uwsgi, "
"apache2와 mod_proxy_uwsgi 또는 gunicorn을 사용할 수 있습니다. 이벤트레트 기반 서버가 여전히 उपलब but "
"WSGI 옵션은 배포 Flexibility를 더 높일 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:702
msgid ""
"We are working actively to remove or workaround those caveats, but please "
"understand that for the moment this feature is experimental given all the "
"above."
msgstr ""
"집합을 제거하거나 workaround하는 데 적극적으로 노력하고 있지만, 위의 모든 요인에 따라 현재 이 기능은 실험적입니다."

#: ../../<reno.sphinxext stable/2023.2>:378
msgid ""
"We have deprecated the configuration ``[ironic]peer_list``, along with our "
"support for a group of ironic nova-compute processes targeting a shared set "
"of Ironic nodes. There are so many bugs in this support we now prefer "
"statically sharding the nodes between multiple nova-compute processes. Note "
"that the ironic nova-compute process is stateless, and the identity of the "
"service is defined by the config option ``[DEFAULT]host``. As such, you can "
"use an active-passive HA solution to ensure at most one nova-compute process"
" is running for each Ironic node shard."
msgstr ""
"ironic peer_list  구성은 ironic 노드 그룹에 대한 지원을 함께 해제되었습니다. 또한 ironic nova-"
"compute 프로세스 그룹이 공유된 ironic 노드에 집중하는 지원을 함께 해제되었습니다. 이 지원에 대한 많은 버그가 있습니다. "
"따라서 현재는 노드들을 여러 nova-compute 프로세스 사이에 정적으로 분할하여 스테이시적으로 분할하는 것을 선호합니다. "
"ironic nova-compute 프로세스는 상태less이며, 서비스의 식별자는 [DEFAULT]host 옵션에 의해 정의됩니다. "
"따라서 각 ironic 노드 shard에 대해 최소한 하나의 nova-compute 프로세스가 실행되도록 활성-비활성 HA giải "
"pháp을 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/2023.2>:372
msgid ""
"We have renamed ``[ironic]partition_key`` to ``[ironic]conductor_group``. "
"The config option is still used to specify which Ironic conductor group the "
"ironic driver in the nova compute process should target."
msgstr ""
"우리는 `[ironic]partition_key`를 `[ironic]conductor_group`로 이름을 변경했습니다. config "
"옵션은 여전히 Ironic 컨더더 그룹을 specify하는 것을 위해 Nova Compute 프로세스에서 ironic 드라이버를 표적해야"
" 할 것을 명시합니다."

#: ../../<reno.sphinxext stable/rocky>:948
msgid ""
"We now attempt to mirror the association of compute host to host aggregate "
"into the placement API. When administrators use the ``POST /os-"
"aggregates/{aggregate_id}/action`` Compute API call to add or remove a host "
"from an aggregate, the nova-api service will attempt to ensure that a "
"corresponding record is created in the placement API for the resource "
"provider (compute host) and host aggregate UUID."
msgstr ""
"우리는 현재 컴퓨터 호스트를 호스트 집합과 연관성의镜像을 placement API에 반영하려고 시도합니다. administrators가 "
"``POST /os-aggregates/{aggregate_id}/action`` Compute API를 사용하여 호스트를 집합에 추가 "
"또는 제거할 때, nova-api 서비스는 호스트와 호스트 집합 UUID에 corresponding record가 생성되는지 확인하기 "
"위해 리소스 제공자 (compute 호스트)와 호스트 집합 UUID에 corresponding record가 생성되는지 확인하기 위해 "
"placement API에 record를 생성하려고 시도합니다."

#: ../../<reno.sphinxext unmaintained/zed>:336
msgid ""
"We recommend to enable the both scope as well new defaults together "
"otherwise you may experience some late failures with unclear error messages."
msgstr ""
"집합 및 새로운 기본값을 함께 활성화하는 것을 추천합니다. 그렇지 않으면 unclear error message와 함께 일부 late "
"failures를 경험할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1061
msgid ""
"When ``IP address substring filtering`` extension is available in Neutron, "
"Nova will proxy the instance list with ``ip`` or ``ip6`` filter to Neutron "
"for better performance."
msgstr ""
"Neutron에서 \"IP 주소 서브스tring 필터링\" 확장 기능이 उपलबуществ을 있으면, Nova는 \"ip\" 또는 "
"\"ip6\" 필터를 사용하여 인스턴스 목록을 Neutron에 proxy합니다."

#: ../../<reno.sphinxext stable/train>:598
msgid ""
"When ``enable_dhcp`` is set on a subnet but there is no DHCP port on neutron"
" then the ``dhcp_server`` value in meta hash will contain the subnet gateway"
" IP instead of being absent."
msgstr ""
"``enable_dhcp``이 네트워크 서브넷에 설정되면, 네트워크에서 DHCP 포트가 없을 때, ``dhcp_server``의 메타 "
"해시 값은 서브넷 게이트웨이 IP 대신에 존재합니다."

#: ../../<reno.sphinxext stable/2025.1>:353
msgid ""
"When ``unified_limits_resource_strategy = ignore``, if a resource in "
"``unified_limits_resource_list`` is requested and has no registered limit "
"set, the quota limit for that resource will be considered to be unlimited "
"and all requests to allocate that resource will be accepted. Any resource "
"not in the list will be considered to have 0 quota."
msgstr ""
"``unified_limits_resource_strategy = ignore``의 경우, ``unified_limits_resource_list``에 있는 리소스가 요청되어 등록된 제한이 none인 경우, 그 리소스의 제한 제한은 무제한으로 간주되며, 그 리소스를 할당하는 모든 요청이 수용된다. \n"
"``unified_limits_resource_list``에 없는 리소스는 제한 0으로 간주된다."

#: ../../<reno.sphinxext stable/2025.1>:347
msgid ""
"When ``unified_limits_resource_strategy = require``, if a resource in "
"``unified_limits_resource_list`` is requested and has no registered limit "
"set, the quota limit for that resource will be considered to be 0 and all "
"requests to allocate that resource will be rejected for being over quota. "
"Any resource not in the list will be considered to have unlimited quota."
msgstr ""
"``unified_limits_resource_strategy = require``의 경우, ``unified_limits_resource_list``에 있는 리소스가 요청되어 등록된 제한이 none인 경우, 그 리소스의 제한 수는 0으로 간주되며, 그 리소스를 할당할 때는 오버 제한이 된 것으로 간주되어 거부된다. \n"
"``unified_limits_resource_list``에 있는 리소스가 none인 경우, 그 리소스는 제한이 none인 것으로 간주되며, 그 리소스를 할당할 때는 제한이 none인 것으로 간주된다."

#: ../../<reno.sphinxext stable/queens>:1344
msgid ""
"When ``url`` is not used, ``region_name`` no longer defaults to "
"``RegionOne``."
msgstr "url이 사용되지 않으면, region_name은 더 이상 RegionOne에 defaults하지 않는다."

#: ../../<reno.sphinxext stable/2025.2>:480
msgid ""
"When an instance booted from local storage is unshelved, the image ID "
"published to the libvirt domain metadata was the ID for the temporary Glance"
" image generated to store the instance state, instead of the original image "
"as intended. Since the temporary image is removed after the instance is "
"unshelved, this results in an invalid image ID being stored in the libvirt "
"domain metadata. This issue has now been fixed. Affected instances will need"
" to be shutdown, restarted, cold migrated or shelved-and-unshelved to update"
" the metadata."
msgstr ""
"instance가 local storage에서 부팅한 경우, libvirt 도메인 메타데이터에 공유된 image ID는 임시 Glance"
" image ID로, 원래 image ID 대신에 사용되었다. 임시 image는 instance가 unshelved 될 때 제거되므로, "
"libvirt 도메인 메타데이터에 사용된 image ID는 유효하지 않은 ID가 된다. 이 문제는 이미 해결되었다. 영향을 받은 "
"instance는.shutdown, restart, cold migrate 또는 shelved-unshelved를 통해 메타데이터를 "
"업데이트해야 한다."

#: ../../<reno.sphinxext stable/queens>:834
msgid ""
"When cold migrating a server, the ``host`` parameter is available as of "
"microversion 2.56. The target host is checked by the scheduler."
msgstr ""
"chilly migrating a server, the `host` parameter is available as of "
"microversion 2.56. The target host is checked by the scheduler."

#: ../../<reno.sphinxext stable/queens>:770
msgid ""
"When creating a baremetal instance with volumes, the ironic driver will now "
"pass an IP address of an iSCSI initiator to the block storage service "
"because the volume backend may require the IP address for access control. If"
" an IP address is set to an ironic node as a volume connector resource, the "
"address is used. If a node has MAC addresses as volume connector resources, "
"an IP address is retrieved from VIFs associated with the MAC addresses. IPv4"
" addresses are given priority over IPv6 addresses if both are available."
msgstr ""
"baremetal 인스턴스를 볼륨과 함께 만들 때, ironic 드라이버는 볼륨 백엔드가 접근 제어를 위해 IP 주소를 블록 스토리지 "
"서비스에 전달합니다. 볼륨 백엔드는 IP 주소를 필요할 수 있기 때문입니다. 볼륨 कन넥터 리소스로 IP 주소를 설정한 ironic "
"노드에 경우, 주소가 사용됩니다. 노드는 MAC 주소가 볼륨 कन넥터 리소스인 경우, MAC 주소와 관련된 VIFs에 속한 IP 주소를 "
"retrieves합니다. IPv4 주소가 IPv6 주소와 함께 उपलबуществ을 있으면, IPv4 주소가 우선시됩니다."

#: ../../<reno.sphinxext stable/queens>:1028
msgid ""
"When creating a server using a multiattach volume, the API will check if the"
" compute services have all been upgraded to a minimum level of support and "
"will fail with a 409 HTTPConflict response if the computes are not yet "
"upgraded."
msgstr ""
"다중 연결 볼륨을 사용하여 서버를 생성할 때, API는 컴퓨팅 서비스가 최소 지원 수준에 업그레이드되어 있는지 확인하고, 컴퓨팅 서비스가"
" 업그레이드되지 않은 경우 409 HTTPConflict 응답을 반환하여 실패합니다."

#: ../../<reno.sphinxext stable/pike>:1135 stable/pike>:1165
msgid ""
"When enabled, libvirt applies IPTables rules to all interface ports that "
"provide MAC, IP, and ARP spoofing protection."
msgstr ""
"libvirt가 활성화되면, IPTables 규칙을 모든 인터페이스 포트에서 MAC, IP, 및 ARP 스포핑 보호를 제공하는 모든 "
"포트에 적용합니다."

#: ../../<reno.sphinxext stable/pike>:555 stable/queens>:1722
msgid ""
"When forcing a specified destination host during evacuate, the scheduler is "
"bypassed but resource allocations will still be made in the Placement "
"service against the forced destination host. If the resource allocation "
"against the destination host fails, the evacuate operation will fail, "
"regardless of the ``force`` flag being specified in the API. The guest will "
"be unchanged on the source host. For more details, see `bug 1713786`_."
msgstr ""
"force로 지정된 목적지 호스트를 강제로 evacuate 할 때, 스케줄러는 피ypassed becomes but resource "
"할location은 Placement service에 대하여 still 할location이 made되며, 목적지 호스트에 대하여 "
"resource allocation이 실패하면 evacuate operation은 fail하regardless of the `force`"
" flag being specified in the API. guest는 source host에 unchanged remain합니다. "
"For more details, see `bug 1713786_`."

#: ../../<reno.sphinxext stable/pike>:543 stable/queens>:1710
msgid ""
"When forcing a specified destination host during live migration, the "
"scheduler is bypassed but resource allocations will still be made in the "
"Placement service against the forced destination host. If the resource "
"allocation against the destination host fails, the live migration operation "
"will fail, regardless of the ``force`` flag being specified in the API. The "
"guest will be unchanged on the source host. For more details, see `bug "
"1712008`_."
msgstr ""
"live migration 시에 명시된 목적지 호스트를 강제로 사용할 때, 스케줄러는 bypassed becomes but "
"resource 할당은 Placement service 에게 still 할당이 이루어지며, 강제 목적지 호스트에 대한 resource "
"할당이 실패하면 live migration operation will fail, regardless of the `force` flag "
"being specified in the API. The guest will be unchanged on the source host. "
"For more details, see `bug 1712008_`."

#: ../../<reno.sphinxext origin/stable/ocata>:948
msgid ""
"When generating Libvirt XML to attach network interfaces for the `tap`, "
"`ivs`, `iovisor`, `midonet`, and `vrouter` virtual interface types Nova "
"previously generated an empty path attribute to the script element (`<script"
" path=''/>`) of the interface."
msgstr ""
"`tap`, `ivs`, `iovisor`, `midonet`, 및 `vrouter` 가상 인터페이스 타입을 연결하기 위해 Libvirt"
" XML을 생성할 때 Nova는 이전에 스크립트 요소 (`<script path=''/>`)의 attribute 에서 비어 있는 path"
" 속성을 생성했다."

#: ../../<reno.sphinxext stable/2024.1>:161 stable/2024.2>:178
#: stable/2025.1>:151 stable/2025.2>:545
msgid ""
"When live migration fails during pre_live_migration on the destination, "
"during rollback Cinder volumes will now be disconnected from the destination"
" locally instead of remotely over RPC from the source. This should ensure "
"that only connection_info for the destination will be used to disconnect "
"volumes from the destination. See `bug #1899835 "
"<https://bugs.launchpad.net/nova/+bug/1899835>`_ for more details."
msgstr ""
"live migration이 pre_live_migration에서 목적지에서 실패할 때, 목적지에서 로컬로만 volume을 분리할 "
"것이며, 원본에서 RPC를 통해 원본에서만 분리하도록 하였다. 이로 인해 목적지에서만 connection_info를 사용하여 "
"volume을 목적지에서 분리할 수 있게 한다. 더 많은 정보는 "
"<https://bugs.launchpad.net/nova/+bug/1899835>에서 확인할 수 있다."

#: ../../<reno.sphinxext origin/stable/ocata>:1228
msgid ""
"When making connections to Ceph-backed volumes via the Libvirt driver, the "
"auth values (rbd_user, rbd_secret_uuid) are now pulled from the backing "
"cinder.conf rather than nova.conf. The nova.conf values are only used if set"
" and the cinder.conf values are not set, but this fallback support is "
"considered accidental and will be removed in the Nova 16.0.0 Pike release. "
"See the Ceph documentation for `configuring Cinder`_ for RBD auth."
msgstr ""
"Ceph-backed 볼륨에 대한 연결을 Libvirt 드라이버를 통해 할 때, auth 값 (rbd_user, "
"rbd_secret_uuid)은 now backing cinder.conf 대신에 pull됩니다. nova.conf 값은 설정되어 "
"있으면만 사용되고, cinder.conf 값은 설정되지 않아도 but 이 fallback 지원은 부작용으로 간주되어 Nova 16.0.0"
" Pike 릴리스에서 제거됩니다. Ceph 문서에서 RBD auth를 구성하는 방법을 확인하십시오."

#: ../../<reno.sphinxext stable/pike>:1028 stable/queens>:1177
msgid ""
"When such instances were deployed without using a custom resource class, it "
"is possible for the scheduler to try deploying another instance on the same "
"node. It will cause a failure in the compute and a scheduling retry."
msgstr ""
"이러한 인스턴스를 custom resource class를 사용하지 않고 배포할 때, 스케줄러가 동일한 노드에 다른 인스턴스를 배포하는 "
"것을 시도할 수 있습니다. 이것은 컴퓨터와 스케줄링의 실패를 일으키고, 스케줄링의 재시도에 영향을 미칠 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:43 stable/pike>:106
#: stable/queens>:233 stable/rocky>:439 stable/stein>:1335
msgid ""
"When testing whether direct IO is possible on the backing storage for an "
"instance, Nova now uses a block size of 4096 bytes instead of 512 bytes, "
"avoiding issues when the underlying block device has sectors larger than 512"
" bytes. See bug https://launchpad.net/bugs/1801702 for details."
msgstr ""
"Nova에서 인스턴스의 백업 스토리지에 직접 IO가 가능한지 테스트 할 때, Nova는 512 byte보다 4096 byte로 블록 "
"크기를 사용하여, 512 byte보다 큰 섹터가 있는 하드웨어 블록 장치에 문제가 발생할 때의 문제를 피하고 있습니다.  더 자세한 "
"정보는 https://launchpad.net/bugs/1801702 에서 확인할 수 있습니다."

#: ../../<reno.sphinxext stable/2025.1>:457
msgid ""
"When the ``[quota]driver`` configuration option is set to the "
"``UnifiedLimitsDriver``, a limit of ``-1`` in Keystone will now be "
"considered as unlimited and the ``servers`` resource will be considered to "
"be required to have a registered limit set in Keystone because of the values"
" for ``[quota]unified_limits_resource_strategy`` and "
"``unified_limits_resource_strategy``."
msgstr ""
"``[quota]driver`` 설정 옵션을 ``UnifiedLimitsDriver``로 설정하면 Keyston에 ``-1``의 제한은 "
"무제한으로 간주되며 Keyston에 등록된 제한이 ``[quota]unified_limits_resource_strategy``와 "
"``unified_limits_resource_strategy``의 값에 따라 ``servers`` 리소스가 필요할 수 있는지 여부가 "
"결정된다."

#: ../../<reno.sphinxext stable/rocky>:1093
msgid ""
"When the ``placement_database.connection`` setting is omitted the existing "
"settings for the ``api_database`` will be used for hosting placement data."
msgstr ""
"``placement_database.connection`` 설정이 제외되면 ``api_database`` 설정이 현재 설정으로 대체되어"
" 배치 데이터를 호스팅하는 데 사용됩니다."

#: ../../<reno.sphinxext stable/2025.1>:537
msgid ""
"When the default video model for libvirt was updated to virtio in Yoga the "
"default for hosts using spice was not changed. This lead to issues when "
"starting instances on hosts that had spice configured with QXL as the "
"default video model on modern distros. To address this, the default spice "
"video model is now virtio. See https://bugs.launchpad.net/nova/+bug/2097529 "
"for more details."
msgstr ""
"libvirt의 기본 비디오 모델이 Yoga에서 virtio로 업데이트되었을 때, 호스트가 spice를 사용하는 경우 기본 비디오 모델이"
" 변경되지 않았습니다. 이것은 spice가 QXL을 기본 비디오 모델로 사용하고 있는 현대의 배포에서 인스턴스를 시작할 때 문제가 "
"발생했습니다. 이러한 문제를 해결하기 위해, 기본 spice 비디오 모델은 now virtio가 됩니다. 더 많은 정보는 "
"https://bugs.launchpad.net/nova/+bug/2097529에 있습니다."

#: ../../<reno.sphinxext stable/rocky>:37 stable/stein>:71 stable/train>:222
#: stable/ussuri>:233 unmaintained/victoria>:750
msgid ""
"When the driver_cache (default is none) has been configured as neither "
"\"none\" nor \"directsync\", the libvirt driver will ensure the driver_io to"
" be \"threads\" to avoid an instance spawning failure."
msgstr ""
"when the driver_cache (default is none) has been configured as neither "
"\"none\" nor \"directsync\", the libvirt driver will ensure the driver_io to"
" be \"threads\" to avoid an instance spawning failure."

#: ../../<reno.sphinxext stable/rocky>:1793
msgid ""
"When the option is set to True, the console proxy will fall back on the "
"``nova-consoleauth`` service to locate existing console authorizations. The "
"option defaults to False."
msgstr ""
"True일 때, 콘솔 프록시가 존재하는 콘솔 인증을 찾기 위해 `nova-consoleauth` 서비스를 fallback으로 사용합니다."
" 옵션의 디폴트는 False입니다."

#: ../../<reno.sphinxext unmaintained/yoga>:100 unmaintained/zed>:578
msgid ""
"When the server group policy validation upcall is enabled nova will assert "
"that the policy is not violated on move operations and initial instance "
"creation. As noted in `bug 1890244`_, if a server was created in a server "
"group and that group was later deleted the validation upcall would fail due "
"to an uncaught excpetion if the server group was deleted. This prevented "
"evacuate and other move operations form functioning. This has now been fixed"
" and nova will ignore deleted server groups."
msgstr ""
"서버 그룹 정책 유효성 확인(upcall)가 활성화된 경우 노바는 이동 연산과 초기 인스턴스 생성에서 정책이 위반되지 않는다고 "
"주장합니다. `bug 1890244`_에 따르면, 서버가 서버 그룹에 생성되었을 때 그룹이 나중에 삭제되면 유효성 확인(upcall)가 "
"예외가 발생하지 않아 삭제된 그룹을 인지하지 못하여 이동 연산과 다른 연산이 functioning하지 않게 됩니다. 이 문제는 현재 "
"해결되었으며 노바는 삭제된 서버 그룹을 무시합니다."

#: ../../<reno.sphinxext stable/ussuri>:163 unmaintained/victoria>:251
#: unmaintained/wallaby>:568
msgid ""
"When the tempest test coverage was added for resize and cold migrate with "
"neutron ports having QoS minimum bandwidth policy rules we discovered that "
"the cross cell resize code path cannot handle such ports. See bug "
"https://bugs.launchpad.net/nova/+bug/1907522 for details. A fix was "
"implemented that makes sure that Nova falls back to same-cell resize if the "
"server has such ports."
msgstr ""
"tempest test coverage가 resize와 cold migrate에 추가되었을 때, neutron ports가 QoS "
"최소帯width 정책 규칙을 갖는 경우, cross cell resize code path은 이러한 포트를 처리할 수 없다는 것을 "
"발견했습니다. bug https://bugs.launchpad.net/nova/+bug/1907522를 참조하십시오.fix가 "
"implemented되었으며, Nova가 same-cell resize로 fallback하는 것을 보장합니다."

#: ../../<reno.sphinxext unmaintained/victoria>:298 unmaintained/wallaby>:941
msgid ""
"When upgrading compute services from Ussuri to Victoria each by one, the "
"Compute RPC API was pinning to 5.11 (either automatically or by using the "
"specific rpc version in the option) but when rebuilding an instance, a "
"TypeError was raised as an argument was not provided. This error is fixed by"
" `bug 1902925`_."
msgstr ""
"Ussuri에서 Victoria로 Compute 서비스를 업그레이드할 때, 각 단위에서 1을 증가시키면 Compute RPC API는 "
"5.11에 고정되었습니다. (자동으로 또는 특정 rpc 버전을 사용하여) 그러나 인스턴스를 재건할 때, TypeError가 발생했습니다."
" 이 오류는 `bug 1902925`_에 의해 해결되었습니다."

#: ../../<reno.sphinxext stable/train>:891
msgid ""
"When used together, the guest kernel can malfunction with repeated warnings "
"like::"
msgstr "가UEST 커널을 함께 사용할 때, 반복적인 경고와 같은 오류가 발생할 수 있습니다."

#: ../../<reno.sphinxext stable/queens>:1065
msgid ""
"When using XenAPI driver for XenServer, we can support booting instances "
"with a vGPU attached to get better graphics processing capability. In order "
"to use this feature, the operators should specify the enabled vGPU types in "
"the nova compute configuration file with the configuration option - "
"``[devices]/enabled_vgpu_types``. Only the enabled vGPU types can be used by"
" instances."
msgstr ""
"XenAPI 드라이버를 사용하여 XenServer를 사용할 때, vGPU가 인스턴스에 부착된 경우 better graphics "
"processing capability를 얻을 수 있습니다. 이 기능을 사용하려면, operators는 nova compute "
"configuration file에 - ``[devices]/enabled_vgpu_types`` 옵션과 함께 활성화된 vGPU 타입을 "
"지정해야 합니다. 활성화된 vGPU 타입만 인스턴스에 사용할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:284 unmaintained/victoria>:564
msgid ""
"When using file-backed memory, the ``nova-compute`` service will now fail to"
" start if the amount of reserved memory configured using ``[DEFAULT] "
"reserved_host_memory_mb`` is equal to or greater than the total amount of "
"memory configured using ``[libvirt] file_backed_memory``. Where reserved "
"memory is less than the total amount of memory configured, a warning will be"
" raised. This warning will become an error in a future release."
msgstr ""
"``nova-compute`` 서비스를 사용할 때, `reserved_host_memory_mb`를 사용하여 구성된 예약 메모리와 "
"`file_backed_memory`를 사용하여 구성된 총 메모리 사이의 amount가 `reserved_host_memory_mb`가 "
"0이 아닌 경우에 `reserved_host_memory_mb`가 `file_backed_memory`보다 적을 때는 경고가 발생합니다."
" 경고는 향후 릴리스에서 오류로 바뀝니다."

#: ../../<reno.sphinxext origin/stable/ocata>:905
msgid ""
"When using libvirt driver, vrouter VIFs (OpenContrail) now supports "
"multiqueue mode, which allows to scale network performance across number of "
"vCPUs. To use this feature one needs to create instance with more than 1 "
"vCPU from an image with ``hw_vif_multiqueue_enabled`` property set to "
"``true``."
msgstr ""
"libvirt 드라이버를 사용할 때, OpenContrail VIFs (vrouter VIFs)는 현재 여러 큐브 모드 "
"(multiqueue mode)를 지원합니다. 이 모드는 vCPUs의 수에 따라 네트워크 성능을 확장할 수 있습니다. 이 기능을 "
"사용하려면, `hw_vif_multiqueue_enabled` 속성이 `true`로 설정된 이미지에서 1 이상의 vCPU를 가진 "
"인스턴스를 생성해야 합니다."

#: ../../<reno.sphinxext stable/pike>:1569
msgid ""
"When using neutron polling mode with XenAPI driver, booting a VM will "
"timeout because ``nova-compute`` cannot receive network-vif-plugged event. "
"This is because it set vif['id'](i.e. neutron port uuid) to two different "
"OVS ports. One is XenServer VIF, the other is tap device qvo-XXXX, but "
"setting 'nicira-iface-id' to XenServer VIF isn't correct, so deprecate it."
msgstr ""
"네트로恩 pollling 모드와 XenAPI 드라이버를 사용할 때, VM을 부팅하면 `nova-compute`가 네트워크 vif-"
"plugged 이벤트를 받지 못하기 때문에 타임아웃이 발생합니다. 이것은 `nova-compute`가 두 가지 OVS 포트를 설정한 "
"때문입니다. 하나는 XenServer VIF, 다른 하나는 tap device qvo-XXXX입니다. 그러나 'nicira-iface-"
"id'를 XenServer VIF로 설정하는 것은 올바르지 않기 때문에 deprecated합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:841
msgid ""
"When using the *libvirt* compute driver, the **libguestfs** package is now "
"**required** for file injection, if you are supporting that in your cloud "
"(see the ``[libvirt]/inject_partition`` config option)."
msgstr ""
"*libvirt* 컴퓨터 드라이버를 사용할 때, **libguestfs** 패키지는 **필요** 하며, cloud에서 지원하는 경우에 "
"**[libvirt]/inject_partition** config 옵션을 참조하세요."

#: ../../<reno.sphinxext stable/stein>:282
msgid ""
"When using the Placement packaged from Nova, some deployment strategies can "
"lead to the service stalling with error messages similar to::"
msgstr ""
"Nova를 사용하는 경우, Nova의 배치 패키지(Placement packaged)를 사용할 때, 일부 배포 전략은 오류 메시지와 "
"유사한 서비스가 지연되거나 중단될 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:1306
msgid ""
"When using the ``api-paste.ini`` from the release, requests to the versioned"
" discovery endpoints (``/v2.1`` and ``/v2``) no longer require "
"authentication. When using the compute API through certain clients, such as "
"openstacksdk, this eliminates an unnecessary additional query. See `bug "
"1845530 <https://bugs.launchpad.net/nova/+bug/1845530>`_ for details."
msgstr ""
"``api-paste.ini``를 사용할 때, 릴리스에서 제공되는 ``api-paste.ini``를 사용하여 버전화된 discovery "
"엔드포인트 (``/v2.1`` 및 ``/v2``)로의 요청은 더 이상 인증이 필요하지 않습니다. openstacksdk와 같은 특정 "
"클라이언트를 통해 컴퓨트 API를 사용할 때, 불필요한 추가 쿼리를 제거합니다.  자세한 내용은 "
"<https://bugs.launchpad.net/nova/+bug/1845530>을 참조하십시오."

#: ../../<reno.sphinxext stable/ussuri>:1296
msgid ""
"When using the ``api-paste.ini`` from the release, version discovery "
"requests without a trailing slash will no longer receive a 302 redirect to "
"the corresponding URL with a trailing slash (e.g. a request for ``/v2.1`` "
"will no longer redirect to ``/v2.1/``). Instead, such requests will respond "
"with the version discovery document regardless of the presence of the "
"trailing slash. See `bug 1728732 "
"<https://bugs.launchpad.net/nova/+bug/1728732>`_ for details."
msgstr ""
"``api-paste.ini``를 사용할 때, 릴리스에서 사용하는 버전 발견 요청은尾문 슬래ashes가 없는 경우 더 이상 URL의尾문 "
"슬래ashes가 있는 corresponding URL로 302 리다이렉트를 받지 않습니다. (예를 들어, ``/v2.1``를 요청할 경우"
" 더 이상 ``/v2.1/``로 리다이렉트하지 않습니다). 대신, 이러한 요청은尾문 슬래ashes가 있는지 없는지 상관없이 버전 발견 "
"문서를 trả답합니다.  자세한 내용은 <https://bugs.launchpad.net/nova/+bug/1728732>`_을 "
"참조하십시오."

#: ../../<reno.sphinxext stable/ussuri>:611
msgid ""
"When using the libvirt driver, Nova instances will now get a VirtIO-RNG "
"(Random Number Generator) device by default.  This is to ensure guests are "
"not starved of entropy during boot time.  In case you want to *disallow* "
"setting an RNG device for some reason, it can be done by setting the flavor "
"Extra Spec property ``hw_rng:allowed`` to ``False``."
msgstr ""
"libvirt 드라이버를 사용할 때 Nova 인스턴스는 기본적으로 VirtIO-RNG (Random Number Generator) "
"장치를 사용할 것이다.  이는 부트 타임에 guest가 엔트로피를 부족하지 않도록 보장하기 위해이다.  예를 들어, 어떤 이유로 RNG "
"장치를 설정하지 않으려면, hw_rng:allowed 속성 \"Extra Spec\" 속성을 False로 설정하면 할 수 있다."

#: ../../<reno.sphinxext unmaintained/xena>:411
msgid ""
"When using the libvirt virt driver with the QEMU or KVM backends, instances "
"will now be created with the *vmcoreinfo* feature enabled by default. This "
"creates a fw_cfg entry for a guest to store dump details, necessary to "
"process kernel dump with KASLR enabled and providing additional kernel "
"details. For more information, refer to the `libvirt`__ documentation."
msgstr ""
"libvirt virt 드라이버를 QEMU 또는 KVM 백엔드와 함께 사용할 때, 인스턴스는 이제 기본적으로 *vmcoreinfo* "
"기능이 활성화되어 있습니다. 이것은 guest에게 dump details를 저장하기 위해 fw_cfg 엔트리를 생성하고, KASLR가 "
"활성화된 경우에 필요한 kernel dump 처리를 위해 필요합니다. 더 많은 정보를 얻으려면 *libvirt*__ 문서를 참조하십시오."

#: ../../<reno.sphinxext unmaintained/xena>:119 unmaintained/yoga>:196
#: unmaintained/zed>:612
msgid ""
"When vDPA was first introduced move operations were implemented in the code "
"but untested either in a real environment or in functional tests. Due to "
"this gap nova elected to block move operations for instance with vDPA "
"devices. All move operations except for live migration have now been tested "
"and found to indeed work so the API blocks have now been removed and "
"functional tests introduced. Other operations such as suspend and live "
"migration require code changes to support and will be enabled as new "
"features in the future."
msgstr ""
"vdPA가 처음 도입된 때는 move 연산을 코드에 구현했지만 실제 환경이나 기능 테스트에서 테스트되지 않았습니다. 이 격차로 nova는"
" vdPA 장치와 같은 instance에 move 연산을 차단했습니다. live migration 이외의 모든 move 연산이 이제 "
"테스트가 완료되어จริง으로 작동하는 것을 확인했기 때문에 API 블록이 제거되고 기능 테스트가 도입되었습니다. suspend와 live"
" migration과 같은 다른 연산은 현재 코드 변경이 필요하여 지원을 위해 새로운 기능으로 활성화 될 예정입니다."

#: ../../<reno.sphinxext unmaintained/yoga>:677
msgid ""
"When you enable unified limits, those are configured in Keystone against the"
" Nova endpoint, using the names:"
msgstr "unified limits를 활성화하면, Keystone에서 Nova endpoint에 대한 이름으로 구성됩니다."

#: ../../<reno.sphinxext stable/rocky>:1365
msgid "Where \"eth0\" is the interface name related to the physical function."
msgstr "\"eth0\"은 물리적 기능과 관련된 인터페이스 이름입니다."

#: ../../<reno.sphinxext origin/stable/ocata>:111 stable/pike>:176
#: stable/queens>:317 stable/rocky>:1941
msgid ""
"While this is introduced in a backward-compatible way, the default will be "
"changed to ``rule:admin_api`` in a subsequent release. It is advised that "
"you communicate this change to your users before turning on enforcement "
"since it will result in a compute API behavior change."
msgstr ""
"이것은 뒤로 호환 가능한 방식으로 소개되지만, 다음 릴리스에서 디폴트는 ``rule:admin_api`` 로 변경될 것이다. 이 변경 "
"사항을 사용자에게 전달하는 것이แนะนำ되며, enforcement를 활성화할 때까지, 이는 컴퓨팅 API의 행동 변경을 일으킬 수 "
"있다."

#: ../../<reno.sphinxext stable/pike>:979
msgid ""
"Windows guests with PV drivers installed expose devices in a different way "
"to Linux guests with PV drivers. Linux systems will see disk paths under "
"``/sys/devices/``, but Windows guests will see them in the registry, for "
"example ``HKLM\\System\\ControlSet001\\Enum\\SCSIDisk``. These two disks are"
" both on the ``xen`` bus."
msgstr ""
"Windows 게스트가 PV 드라이버를 설치한 경우, PV 드라이버가 설치된 Linux 게스트와는 다른 방식으로 장치를 노출한다. "
"Linux 시스템은 `/sys/devices/` 하위 디스크 경로를 볼 수 있지만, Windows 게스트는 예를 들어 "
"`HKLM\\System\\ControlSet001\\Enum\\SCSIDisk`에서 볼 수 있다. 이 두 개의 디스크는 모두 `xen`"
" 버스에 속한다."

#: ../../<reno.sphinxext unmaintained/yoga>:138 unmaintained/zed>:407
msgid ""
"With QEMU >=2.9 and libvirt >= 4.4.0, libvirt will do the right thing in "
"terms of CPU compatibility checks on the destination host during live "
"migration. Nova satisfies these minimum version requirements by a good "
"margin. So, this workaround provides a way to skip the CPU comparison check "
"on the destination host before migrating a guest, and let libvirt handle it "
"correctly."
msgstr ""
"QEMU >=2.9과 libvirt >= 4.4.0를 사용하면, libvirt는 목적지 호스트에서 live migration 시 CPU "
"호환성 확인에 대해正确한 행동을 취할 수 있습니다. Nova는 이러한 최소 버전 요구 사항을 충족하는 것으로 보인다. 따라서 이 "
"workaround은 목적지 호스트에서 게스트를 mig리ровать 전에 CPU 비교 확인을 skips하고, libvirt가正确하게 "
"처리할 수 있도록 해줍니다."

#: ../../<reno.sphinxext stable/pike>:1924
msgid ""
"With XenAPI driver, we have deprecated bittorrent since '15.0.0', so we "
"decide to remove all bittorrent related files and unit tests."
msgstr ""
"XenAPI 드라이버를 사용하면 '15.0.0'부터 비트토렌트를 비활성화되었습니다. 따라서 모든 비트토렌트 관련 파일과 단위 테스트를 "
"제거하기로 결정했습니다."

#: ../../<reno.sphinxext stable/stein>:983
msgid ""
"With added validations for flavor extra-specs and image properties, the APIs"
" for server create, resize and rebuild will now return 400 exceptions where "
"they did not before due to the extra-specs or properties not being properly "
"formatted or being mutually incompatible."
msgstr ""
"집합으로 추가된.flavor extra-specs 및 image 속성에 대한 유효성 검사, server create, resize 및 "
"rebuild API는 trước에 extra-specs 또는 속성들이 적절히 형식화되지 않았거나 상호 충돌하는 경우에만 400 예외를 "
"반환하지 않았지만 now extra-specs 또는 속성들이 적절히 형식화되지 않았거나 상호 충돌하는 경우에만 400 예외를 반환합니다."

#: ../../<reno.sphinxext stable/ussuri>:446
msgid ""
"With microversion 2.84 the ``GET /servers/{server_id}/os-instance-"
"actions/{request_id}`` API returns a ``details`` parameter for each failed "
"event with a fault message, similar to the server ``fault.message`` "
"parameter in ``GET /servers/{server_id}`` for a server with status "
"``ERROR``."
msgstr ""
"micro 버전 2.84에서 ``GET /servers/{server_id}/os-instance-"
"actions/{request_id}`` API는 오류 메시지와 함께 실패한 이벤트에 대해 ``details`` 매개 변수를 반환합니다."
" 이 매개 변수는 ``GET /servers/{server_id}`` API의 ``fault.message`` 매개 변수와 유사하여 서버"
" 상태가 ``ERROR`` 인 경우에만 반환됩니다."

#: ../../<reno.sphinxext stable/ussuri>:474
msgid ""
"With microversion 2.85 add new API ``PUT /servers/{server_id}/os-"
"volume_attachments/{volume_id}`` which support for specifying "
"``delete_on_termination`` field in the request body to re-config the "
"attached volume whether to delete when the instance is deleted."
msgstr ""
"microversion 2.85에서 새로운 API `PUT /servers/{server_id}/os-"
"volume_attachments/{volume_id}`가 추가되었습니다. 이 API는 `delete_on_termination` 필드를"
" 요청 바디에 지정하여 인스턴스가 삭제될 때 attached volume을 삭제할지 여부를 재 cấu화할 수 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:566
msgid ""
"With microversion 2.85, existing policy ``os-volumes-attachments:update`` is"
" used for updating the resources with the change in its default value from "
"``SYSTEM_ADMIN`` to ``PROJECT_MEMBER_OR_SYSTEM_ADMIN``. New policy ``os-"
"volumes-attachments:swap`` is introduced for swapping the attachment of "
"servers with default to ``SYSTEM_ADMIN``."
msgstr ""
"microversion 2.85에서, 기존 정책 ``os-volumes-attachments:update``는 SYSTEM_ADMIN에서"
" PROJECT_MEMBER_OR_SYSTEM_ADMIN으로의 기본값의 변경에 따라 자원들을 업데이트하는 데 사용됩니다. NEW 정책 "
"``os-volumes-attachments:swap``는 SYSTEM_ADMIN으로 기본값이 있는 서버의 연결을 교체하는 데 "
"사용됩니다."

#: ../../<reno.sphinxext unmaintained/zed>:223
msgid ""
"With microversion 2.92, you can only import a public key and not generate a "
"keypair. You can also use an extended name pattern."
msgstr ""
"microversion 2.92에서, 공공 키만.import할 수 있으며, 키 페어를 생성할 수 없습니다. 또한, 확장 이름 패턴을 "
"사용할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/zed>:388
msgid ""
"With the addition of these features, all instance lifecycle operations are "
"now valid for VMs with VDPA neutron ports."
msgstr "이 기능의 추가로, 모든 인스턴스 라이프 사이클 운영은 이제 VDPA 네트워크 포트를 가진 VM에 대해 유효합니다."

#: ../../<reno.sphinxext stable/2023.2>:112 stable/2024.1>:562
#: unmaintained/2023.1>:153
msgid ""
"With the change from ml2/ovs DHCP agents towards OVN implementation in "
"neutron there is no port with device_owner ``network:dhcp`` anymore. Instead"
" DHCP is provided by ``network:distributed`` port. Fix relies on enable_dhcp"
" provided by neutron-api if no port with ``network:dhcp`` owner is found. "
"See `bug 2055245 <https://bugs.launchpad.net/nova/+bug/2055245>`__ for "
"details."
msgstr ""
"ml2/ovs DHCP_agents와 OVN 구현을 neutron에 적용하면 더 이상 device_owner 속성에 "
"network:dhcp가 있는 포트가 없게 됩니다. 대신 network:distributed 포트를 통해 DHCP 서비스가 제공됩니다. "
"Fix는 neutron-api에서 enable_dhcp를 사용하여 network:dhcp 소유자가 없는 포트가 발견되지 않으면 "
"사용됩니다. 더 자세한 정보는 <https://bugs.launchpad.net/nova/+bug/2055245>에서 확인할 수 "
"있습니다."

#: ../../<reno.sphinxext stable/rocky>:95 stable/stein>:193 stable/train>:433
#: stable/ussuri>:1283
msgid ""
"With the changes introduced to address `bug #1763766`_, Nova now guards "
"against NUMA constraint changes on rebuild. As a result the "
"``NUMATopologyFilter`` is no longer required to run on rebuild since we "
"already know the topology will not change and therefore the existing "
"resource claim is still valid. As such it is now possible to do an in-place "
"rebuild of an instance with a NUMA topology even if the image changes "
"provided the new image does not alter the topology which addresses `bug "
"#1804502`_."
msgstr ""
"`bug #1763766`_에 대한 변경 사항을 적용하면 Nova는 재건 시 NUMA 제약 변경에 대한 보호를 제공합니다. 따라서 "
"`NUMATopologyFilter`는 재건 시 더 이상 필요하지 않으며,Topology가 변경되지 않기 때문에 현재 리소스.claim이"
" 여전히 유효합니다. 따라서,Topology가 변경되지 않는 경우, Image가 변경되더라도 Instance의 NUMA Topology를"
" 재건할 수 있습니다. `bug #1804502`_."

#: ../../<reno.sphinxext stable/train>:1242
msgid ""
"With the introduction of the NUMA-aware live migration feature for the "
"libvirt driver, ``[workarounds]/enable_numa_live_migration`` is deprecated. "
"Once a cell has been fully upgraded to Train, its value is ignored."
msgstr ""
"NUMA-지식 live migration 기능을 libvirt 드라이버에 도입하면 "
"`[workarounds]/enable_numa_live_migration`는弃용된다. Train으로 완전히 업그레이드된 세ลล의 경우,"
" 그 값은 무시된다."

#: ../../<reno.sphinxext stable/2024.2>:237 stable/2024.2>:305
msgid ""
"With the libvirt driver and libvirt version 7.3.0 or newer, mediated devices"
" for vGPUs are now persisted across reboots of a compute host."
msgstr ""
"libvirt 드라이버와 libvirt 버전 7.3.0 이상을 사용하면 현재 vGPU에 대한 매개화 thiết bị는 컴퓨터 호스트의 "
"재부팅을 통해 지속됩니다."

#: ../../<reno.sphinxext stable/train>:819
msgid ""
"With the libvirt driver, live migration now works correctly for instances "
"that have a NUMA topology. Previously, the instance was naively moved to the"
" destination host, without updating any of the underlying NUMA guest to host"
" mappings or the resource usage. With the new NUMA-aware live migration "
"feature, if the instance cannot fit on the destination the live migration "
"will be attempted on an alternate destination if the request is setup to "
"have alternates. If the instance can fit on the destination, the NUMA guest "
"to host mappings will be re-calculated to reflect its new host, and its "
"resource usage updated."
msgstr ""
"libvirt 드라이버를 사용하면 NUMA拓도가 있는 인스턴스에 대한 live migration이正确하게 작동합니다. 이전에, 인스턴스는"
" 단순히 목적지 호스트로 이동했으며, 하위 NUMA 게스트를 호스트로 mapping을 업데이트하거나 리소스 사용량을 업데이트하지 "
"않았습니다. 새로운 NUMA-aware live migration 기능을 사용하면, 목적지에 fits할 수 없다면, live "
"migration은 대체 목적지에 시도됩니다. 이 요청이 대체 목적지를 설정할 경우. 목적지에 fits할 수 있다면, NUMA 게스트를 "
"호스트로 mapping을 재 계산하여 새로운 호스트를 반영하고 리소스 사용량을 업데이트합니다."

#: ../../<reno.sphinxext stable/2023.2>:265
msgid ""
"With this change, on instance reboot, Nova will checks for all volume "
"attachments associated with the instance and verifies their availability in "
"the Cinder database. If attachments are not found they will get deleted from"
" Nova database too."
msgstr ""
"이 변경은 인스턴스 재부팅 시, 인스턴스와 관련된 모든 볼륨에 대한 연결을 확인하고 시더ν 데이터베이스에 Availability를 "
"확인합니다. 연결이 발견되지 않으면 Nova 데이터베이스에서도 삭제됩니다."

#: ../../<reno.sphinxext stable/2024.1>:56 stable/2024.2>:188
#: stable/2025.1>:546 unmaintained/2023.1>:51
msgid ""
"With this change, operators can now resize the instance flavor swap to a "
"smaller swap size, it can be expand and shrunk down to 0 using the same "
"resize API. For more details see: `bug 1552777`_"
msgstr ""
"이 변경은 now 인스턴스 플레인 크기 교체를 더 작은 교체 크기로 크기 조정할 수 있게 하며, same resize API를 사용하여 "
"0으로shrunk down 할 수 있습니다. 더 많은 정보는 `bug 1552777`_를 참조하십시오."

#: ../../<reno.sphinxext unmaintained/yoga>:700
msgid ""
"Work in progress support for Keystone's unified limits can be enabled via "
"``[quota]/driver=nova.quota.UnifiedLimitsDriver``"
msgstr ""
"`Keystone`의統합 제한을 지원하는 작업이 진행 중인 경우, "
"`quotas]/driver=nova.quota.UnifiedLimitsDriver`"

#: ../../<reno.sphinxext stable/queens>:1072
msgid ""
"XenServer automatically detects and groups together identical physical GPUs."
" Although the physical GPUs may support multiple vGPU types, at the moment "
"nova only supports a single vGPU type for each compute node. The operators "
"can run the following CLI commands in XenServer to get the available vGPU "
"types if the host supports vGPU::"
msgstr ""
"XenServer는 동일한 물리적 그래픽 카드(GPU)를 tự động 감지하고 그룹화합니다. 물리적 그래픽 카드는 여러 vGPU 타입을 지원할 수 있지만 현재 nova는 각 컴퓨터 노드에 대해 단일 vGPU 타입만 지원합니다. XenServer에서 호스트가 vGPU를 지원하는지 확인하고 싶다면 다음 CLI 명령을 실행할 수 있습니다. \n"
"\n"
"*   `xenapi-list-vgpu-types` : vGPU 타입을 확인합니다.\n"
"*   `xenapi-list-vgpu-objects` : vGPU 객체를 확인합니다.\n"
"*   `xenapi-list-vgpu-objects -v` : vGPU 객체의 디스플레이를 확인합니다.\n"
"*   `xenapi-list-vgpu-objects -v -a` : vGPU 객체의 디스플레이를 확인합니다.\n"
"*   `xenapi-list-vgpu-objects -v -a -o` : vGPU 객체의 디스플레이를 확인합니다.\n"
"*   `xenapi-list-vgpu-objects -v -a -o -l` : vGPU 객체의 디스플레이를 확인합니다.\n"
"*   `xenapi-list-vgpu-objects -v -a -o -l -o` : vGPU 객체의 디스플레이를 확인합니다.\n"
"*   `xenapi-list-vgpu-objects -v -a -o -l -o -l` : vGPU 객체의 디스플레이를 확인합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1410
msgid ""
"XenServer plugins have been renamed to include a '.py' extension. Code has "
"been included to handle plugins with and without the extension, but this "
"will be removed in the next release. The plugins with the extension should "
"be deployed on all compute nodes to mitigate any upgrade issues."
msgstr ""
"XenServer 플러그인은 '.py' 확장자를 포함하여 이름이 변경되었습니다. 코드가 확장자에 따라 플러그인에 대한 처리를 포함하고 "
"있지만, 다음 릴리스에서 제거됩니다. 확장자가 있는 플러그인은 모든 컴퓨팅 노드에서 배포되어 업그레이드 문제를 방지해야 합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1299
msgid ""
"XenServer users must now set the value of xenserver.ovs_integration_bridge "
"before they can use the system. Previously this had a default of \"xapi1\", "
"which has now been removed, because it is dependent on the environment. The "
"xapi<n> are internal bridges that are incrementally defined from zero and "
"\"xapi1\" may not be the correct bridge. Operators should set this config "
"value to the integration bridge used between all guests and the compute host"
" in their environment."
msgstr ""
"XenServer 사용자들은 now 시스템을 사용할 수 있는지 before xenserver.ovs_integration_bridge의 "
"가치를 설정해야합니다. 이전에 이 had default \"xapi1\"으로, now는 환경에 따라 제거되었습니다. xapi<n>는 "
"internal bridges가 incrementally defined from zero로, \"xapi1\"은 correct "
"bridge가 아니라는 것을 확인해야합니다. Operators는 이 config value를 integration bridge를 사용하는"
" all guests와 compute host의 환경에서 all guests와 compute host의 integration "
"bridge를 사용하는지 확인해야합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:417
msgid "You can `change the default machine type`__ on a compute node safely"
msgstr "`compute 노드에서 기본 머신 타입을 `__변경할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:421
msgid "You can enable `UEFI secure boot`__ for new instances"
msgstr "`UEFI 보안 부트`를 새로운 인스턴스에 활성화할 수 있습니다."

#: ../../<reno.sphinxext stable/pike>:797
msgid ""
"You can enable this feature by setting the ``img_hide_hypervisor_id=true`` "
"property in a Glance image."
msgstr ""
"이 기능을 활성화하려면 Glance 이미지에서 ``img_hide_hypervisor_id=true`` 속성을 설정해야 합니다."

#: ../../<reno.sphinxext stable/2024.2>:248
msgid ""
"You can now require TLS connections for SPICE consoles if you set the "
"``[spice]/require_secure`` configuration option to ``True``."
msgstr ""
"SPICE 콘솔에 TLS 연결을 요구할 수 있습니다. `[spice]/require_secure` 구성 옵션을 `True`로 설정하면 "
"됩니다."

#: ../../<reno.sphinxext stable/2024.2>:279
msgid "You can now use:"
msgstr "You can now use: 집합"

#: ../../<reno.sphinxext origin/stable/ocata>:172 stable/pike>:322
#: stable/queens>:1228
msgid ""
"You can safely remove the AggregateCoreFilter, AggregateRamFilter, and "
"AggregateDiskFilter from your ``[filter_scheduler]enabled_filters`` and you "
"do not need to replace them with any other core/ram/disk filters. The "
"placement query in the FilterScheduler takes care of the core/ram/disk "
"filtering, so CoreFilter, RamFilter, and DiskFilter are redundant."
msgstr ""
"``[filter_scheduler]enabled_filters``에서 AggregateCoreFilter, "
"AggregateRamFilter, AggregateDiskFilter를 안전하게 제거할 수 있습니다. 이 "
"AggregateCoreFilter, AggregateRamFilter, AggregateDiskFilter를 다른 "
"core/ram/disk 필터로 대체할 필요는 없습니다. 필터 스케줄러의 위치查询는 core/ram/disk 필터를 처리하므로 "
"CoreFilter, RamFilter, DiskFilter는 중복됩니다."

#: ../../<reno.sphinxext origin/stable/ocata>:697
msgid ""
"[1] https://blogs.technet.microsoft.com/heyscriptingguy/2016/07/14/passing-"
"through-devices-to-hyper-v-vms-by-using-discrete-device-assignment/ [2] "
"http://docs.openstack.org/admin-guide/compute-pci-passthrough.html"
msgstr ""
"[1] https://blogs.technet.microsoft.com/heyscriptingguy/2016/07/14/passing-through-devices-to-hyper-v-vms-by-using-discrete-device-assignment/\n"
"\n"
"[2] http://docs.openstack.org/admin-guide/compute-pci-passthrough.html"

#: ../../<reno.sphinxext unmaintained/xena>:397
msgid ""
"[1] https://docs.openstack.org/api-ref/compute/#server-usage-audit-log-os-"
"instance-usage-audit-log"
msgstr ""
"[1] https://docs.openstack.org/api-ref/compute/#server-usage-audit-log-os-"
"instance-usage-audit-log"

#: ../../<reno.sphinxext stable/pike>:800
msgid "[1]: http://git.qemu.org/?p=qemu.git;a=commitdiff;h=f522d2a"
msgstr "[1]: http://git.qemu.org/?p=qemu.git;a=commitdiff;h=f522d2a"

#: ../../<reno.sphinxext origin/stable/ocata>:189 stable/pike>:230
msgid ""
"[CVE-2017-18191] Swapping encrypted volumes can lead to data loss and a "
"possible compute host DOS attack."
msgstr ""
"[CVE-2017-18191] 암호화된 볼륨을 교환하면 데이터 손실과 possible compute host DOS 공격에 대한 위험을 "
"발생할 수 있습니다."

#: ../../<reno.sphinxext ../source/mitaka.rst:27 ../source/newton.rst:170
#: origin/stable/ocata>:417 stable/pike>:1741
msgid ""
"[CVE-2017-7214] Failed notification payload is dumped in logs with auth "
"secrets"
msgstr "[CVE-2017-7214] 인증 secret이 포함된 auth notification payload가 로그에 쌓여있다."

#: ../../<reno.sphinxext stable/rocky>:328 stable/stein>:1378
msgid ""
"[`bug 1818295 <https://bugs.launchpad.net/nova/+bug/1818295>`_] Fixes the "
"problem with endpoint lookup in Ironic driver where only public endpoint is "
"possible, which breaks deployments where the controllers have no route to "
"the public network per security requirement. Note that python-ironicclient "
"fix I610836e5038774621690aca88b2aee25670f0262 must also be present to "
"resolve the bug."
msgstr ""
"[`bug 1818295 <https://bugs.launchpad.net/nova/+bug/1818295>`_] 집합 1818295 "
"<https://bugs.launchpad.net/nova/+bug/1818295>을 해결하여 Ironic 드라이버의 엔드포인트 검색에서"
" only 공공 엔드포인트만 가능하다는 문제를 해결합니다. 이 문제는 보안 요구 사항에 따라 컨트롤러가 공공 네트워크에 라우팅할 수 있는"
" 경로가 없기 때문에 배포를 깨트립니다. python-ironicclient의 "
"I610836e5038774621690aca88b2aee25670f0262 fix가 또한 필요하여 문제를 해결합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:112 unmaintained/xena>:71
#: unmaintained/yoga>:609
msgid ""
"[`bug 1958636 <https://bugs.launchpad.net/nova/+bug/1958636>`_] Explicitly "
"check for and enable SMM when firmware requires it. Previously we assumed "
"libvirt would do this for us but this is not true in all cases."
msgstr ""
"[`bug 1958636 <https://bugs.launchpad.net/nova/+bug/1958636>`_] SMM를 명시적으로 "
"확인하고 활성화하여 firmware가 필요할 때만 사용하도록 설정하십시오. 이전에 libvirt가 이를 ourselves를 위해 "
"assumed but 이것은 모든 경우에 true는 아니므로."

#: ../../<reno.sphinxext stable/2023.2>:442 unmaintained/2023.1>:203
msgid ""
"[`bug 1983471 <https://bugs.launchpad.net/nova/+bug/1983471>`_] When "
"offloading a shelved instance, the compute will now remove the binding so "
"instance ports will appear as \"unbound\" in neutron."
msgstr ""
"[`bug 1983471 <https://bugs.launchpad.net/nova/+bug/1983471>`_] nova에서 인스턴스를"
" 제거할 때, 컴퓨터는 now binding을 제거하여 인스턴스 포트가 \"unbound\"로 나타날 수 있습니다."

#: ../../<reno.sphinxext stable/2024.1>:26 stable/2024.2>:26 stable/2025.1>:14
#: stable/2025.2>:499
msgid ""
"[`bug 2117170 <https://bugs.launchpad.net/nova/+bug/2117170>`_] Libvirt "
"driver no longer enables VMCoreInfo device when an instance has memory "
"encryption is enabled, to avoid kernel crash caused by fw_cfg device in "
"guest requiring DMA."
msgstr ""
"[`bug 2117170 <https://bugs.launchpad.net/nova/+bug/2117170>`_] Nova 드라이버에서 "
"VMCoreInfo 장치가 인스턴스에 메모리 암호화가 활성화된 경우 더 이상 활성화되지 않으며, guest에서 fw_cfg 장치가 "
"DMA를 필요로 하는 경우에 발생하는 커널 크래시를 피하기 위해."

#: ../../<reno.sphinxext stable/2025.2>:157
msgid "[oslo_policy] enforce_new_defaults=False enforce_scope=False"
msgstr ""
"[oslo_policy]\n"
" enforce_new_defaults=False\n"
" enforce_scope=False"

#: ../../<reno.sphinxext unmaintained/wallaby>:53 unmaintained/xena>:255
#: unmaintained/yoga>:586
msgid ""
"`Bug #1829479 <https://bugs.launchpad.net/nova/+bug/1829479>`_: Now deleting"
" a nova-compute service removes allocations of successfully evacuated "
"instances. This allows the associated resource provider to be deleted "
"automatically even if the nova-compute service cannot recover after all "
"instances on the node have been successfully evacuated."
msgstr ""
"`Bug #1829479 <https://bugs.launchpad.net/nova/+bug/1829479>`_: 현재 nova-"
"compute 서비스를 삭제하면 성공적으로 추출된 인스턴스의 할당을 삭제합니다. 이로 인해 nova-compute 서비스가 모든 노드에 "
"성공적으로 추출된 인스턴스를 모두 성공적으로 추출한 후 다시 시작할 수 없더라도 연관된 리소스 제공자를 tự động 삭제할 수 "
"있습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:805
msgid ""
"`Bug #1888022 <https://launchpad.net/bugs/1888022>`_: An issue that "
"prevented detach of multi-attached fs-based volumes is resolved."
msgstr ""
"`BUG #1888022 <https://launchpad.net/bugs/1888022>`_: 여러 FS 기반 볼륨을 분리하는 것을 "
"방해하는 문제가 해결되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:39 unmaintained/yoga>:60
#: unmaintained/zed>:564
msgid ""
"`Bug #1941005 <https://bugs.launchpad.net/nova/+bug/1941005>`_ is fixed. "
"During resize Nova now uses the PCI requests from the new flavor to select "
"the destination host."
msgstr ""
"`bug #1941005 <https://bugs.launchpad.net/nova/+bug/1941005>`_은 수정되었습니다. "
"nova는 현재 new flavor의 pci 요청을 사용하여 target host를 선택합니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:73 unmaintained/xena>:32
#: unmaintained/yoga>:245 unmaintained/zed>:532
msgid ""
"`Bug #1970383 <https://bugs.launchpad.net/nova/+bug/1970383>`_: Fixes a "
"permissions error when using the "
"'query_placement_for_routed_network_aggregates' scheduler variable, which "
"caused a traceback on instance creation for non-admin users."
msgstr ""
"`Bug #1970383 <https://bugs.launchpad.net/nova/+bug/1970383>`_: 집합에 라우팅 "
"네트워크를 사용하는 경우 'query_placement_for_routed_network_aggregates' 스케줄러 변수를 사용할 때"
" 권한 오류를修复했습니다. 이로 인해 비-admin 사용자에서 인스턴스 생성 시 트레이스백이 발생했습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:46 unmaintained/wallaby>:80
#: unmaintained/xena>:78 unmaintained/yoga>:169 unmaintained/zed>:548
msgid ""
"`Bug #1978444 <https://bugs.launchpad.net/nova/+bug/1978444>`_: Now nova "
"retries deleting a volume attachment in case Cinder API returns ``504 "
"Gateway Timeout``. Also, ``404 Not Found`` is now ignored and leaves only a "
"warning message."
msgstr ""
"`BUG #1978444 <https://bugs.launchpad.net/nova/+bug/1978444>`_: 현재 nova는 "
"Cinder API가 ``504 Gateway Timeout``를 반환할 때 볼륨 액세서리 삭제를 재시도합니다. 또한, ``404 Not"
" Found``는 현재 무시되고 단지 경고 메시지만 남습니다."

#: ../../<reno.sphinxext unmaintained/xena>:85 unmaintained/yoga>:176
#: unmaintained/zed>:555
msgid ""
"`Bug #1981813 <https://bugs.launchpad.net/nova/+bug/1981813>`_: Now nova "
"detects if the ``vnic_type`` of a bound port has been changed in neutron and"
" leaves an ERROR message in the compute service log as such change on a "
"bound port is not supported. Also the restart of the nova-compute service "
"will not crash any more after such port change. Nova will log an ERROR and "
"skip the  initialization of the instance with such port during the startup."
msgstr ""
"`Bug #1981813 <https://bugs.launchpad.net/nova/+bug/1981813>`_: 현재 nova는 "
"네트워크(Neutron)에서 바인드된 포트의 `vnic_type` 속성이 변경되었을 때, nova가 포트 속성 변경을 감지하고, "
"compute 서비스 로그에 ERROR 메시지를 남기며, 바인드된 포트 속성 변경이 지원되지 않는다는 것을 알게 됩니다. 또한, 포트 "
"속성 변경 후 nova-compute 서비스를 재시작하면 더 이상 오류가 발생하지 않습니다. nova는 ERROR 메시지를 로그하고, "
"포트 속성 변경을 인식하는 인스턴스 초기화 skipping을 수행합니다."

#: ../../<reno.sphinxext unmaintained/zed>:570
msgid ""
"`Bug #1986838 <https://bugs.launchpad.net/nova/+bug/1986838>`_: Nova now "
"correctly schedules an instance that requests multiple PCI devices via "
"multiple PCI aliases in the flavor extra_spec when multiple similar devices "
"are requested but the compute host has only one such device matching with "
"each request individually."
msgstr ""
"`Bug #1986838 <https://bugs.launchpad.net/nova/+bug/1986838>`_: Nova는 flavor"
" extra_spec에서 여러 PCI alias를 통해 여러 PCI 장치를 yêu cầu하는 인스턴스를correctly 스케줄합니다. "
"여러 유사한 장치를 yêu cầu하는 경우, compute host가 각 요청 individually에 매치되는 단일 장치를만 가지고 "
"있지만, Nova는 여러 PCI alias를 통해 여러 PCI 장치를 스케줄합니다."

#: ../../<reno.sphinxext stable/2024.2>:420
msgid ""
"`Bug #2002606`_: Previously, server rescue in stable device mode had a "
"dependency on the original image used to create or rebuild the server. If "
"the original image was deleted from Glance, the server could not be rescued."
" The issue has been fixed by falling back to the instance image metadata if "
"the original image is not found in Glance."
msgstr ""
"`Bug #2002606`_: 이전에, 안정적 장치 모드에서 서버 리스큰은 원본 이미지로 생성하거나 리빌할 때의 의존성이 있었다. 원본 "
"이미지가 글랜스에서 삭제되면 서버가 리스큰할 수 없었다. 이 문제는 원본 이미지가 글랜스에서 찾을 수 없을 때 인스턴스 이미지 "
"메타데이트를 fallback으로 사용하여 해결되었다."

#: ../../<reno.sphinxext stable/2023.2>:448 unmaintained/2023.1>:209
msgid ""
"`Bug #2003991`_: Fixes an issue where quota was not properly enforced during"
" unshelve of a ``SHELVED_OFFLOADED`` server when "
"``[quota]count_usage_from_placement = true`` or ``[quota]driver = "
"nova.quota.UnifiedLimitsDriver`` are configured."
msgstr ""
"`Bug #2003991`_: 집합 #2003991`_: SHELVED_OFFLOADED` 서버의 비용 제한이 제대로 enforce되지 "
"않으며, `[quota]count_usage_from_placement = true` 또는 `[quota]driver = "
"nova.quota.UnifiedLimitsDriver`가 구성된 경우에 대한 문제를修正합니다."

#: ../../<reno.sphinxext stable/2023.2>:421 unmaintained/2023.1>:91
msgid ""
"`Bug #2024258`_: Fixes an issue with performance degradation archiving "
"databases with large numbers of foreign key related records."
msgstr ""
"`Bug #2024258`_: 집합 아rchiving 데이터베이스의 성능 저하를 해결합니다. 이 문제는 foreign key와 관련된 "
"많은 레코드가 있는 데이터베이스에 대한 아rchiving에 대한 성능 저하를 해결합니다."

#: ../../<reno.sphinxext stable/2024.2>:402
msgid ""
"`Bug #2035375 <https://bugs.launchpad.net/nova/+bug/2035375>`_: Fixed "
"leftover NVMe-oF subsystems when disconnecting multiple NVMe-oF volumes on "
"the same host from storage sharing the subsystem for different volumes."
msgstr ""
"`Bug #2035375 <https://bugs.launchpad.net/nova/+bug/2035375>`_: 집합 NVMe-oF "
"subsystem을 여러 NVMe-oF 볼륨을 하나의 호스트에서 분리하여 스토어를 공유하는 볼륨에 대해 분리할 때 남은 오버플로wing "
"NVMe-oF subsystem을修正했다."

#: ../../<reno.sphinxext stable/2023.2>:66 stable/2024.1>:248
#: stable/2024.2>:197 stable/2025.1>:555 unmaintained/2023.1>:60
msgid ""
"`Bug #2091033`_: Fixed calls to libvirt ``listDevices()`` and "
"``listAllDevices()`` from potentially blocking all other greenthreads in "
"``nova-compute``. Under certain circumstances, it was possible for the "
"``nova-compute`` service to freeze with all other greenthreads blocked and "
"unable to perform any other activities including logging. This issue has "
"been fixed by wrapping the libvirt ``listDevices()`` and "
"``listAllDevices()`` calls with ``eventlet.tpool.Proxy``."
msgstr ""
"`Bug #2091033`_: 집합 `libvirt`의 `listDevices()` 및 `listAllDevices()` 함수를 "
"fixed 하며, `nova-compute` 서비스가 모든 다른 greenthread이 블록되어 모든 활동을 수행하지 못하고 로그도 할 "
"수 없는 경우가 발생할 수 있었다. 특정 circumstances에서 `nova-compute` 서비스가 모든 다른 "
"greenthread이 블록되어 모든 활동을 수행하지 못하고 로그도 할 수 없는 경우가 발생할 수 있었다. 이 문제는 `libvirt`의"
" `listDevices()` 및 `listAllDevices()` 함수를 `eventlet.tpool.Proxy`와 함께 감싸서 "
"해결되었다."

#: ../../<reno.sphinxext stable/2024.1>:111 stable/2024.2>:142
#: stable/2025.1>:120 stable/2025.2>:453
msgid ""
"`Bug #2095364`_: Fixed the List Server API and the List Server Detail API "
"500 Internal Server Error issue in v2.96 or later API microversion if one or"
" more instance has no request spec object. One usecase was when cloud user "
"tried to create instance which exceeded their quota, the request does not "
"create instance request spec. Once the no request spec instance is created "
"in cloud user project, the server list API and the list server details API "
"return 500 Internal Server Error for the project until the cloud user "
"deletes the no request spec object instance. After this fix, the v2.96 or "
"later returns `null` at the `pinned_availability_zone` value if not "
"specified."
msgstr ""
"`Bug #2095364`_: `v2.96` 이상 API 마이크로 버전에서, 하나 이상의 인스턴스가 요청 spécifik 오브젝트가 없을"
" 때, `List Server API`와 `List Server Detail API` 500 내부 서버 오류를修复했다. 하나의 사용 "
"사례는 클라우드 사용자가quota를 초과한 인스턴스를 생성하려고 할 때, 요청 spécifik 오브젝트가 없을 때, 인스턴스 생성 요청 "
"spécifik 오브젝트가 없을 때, 클라우드 사용자가 프로젝트에 요청 spécifik 오브젝트를 생성하면, `List Server "
"API`와 `List Server Detail API`는 프로젝트에 500 내부 서버 오류를返す까지는 유지된다. 이修复 후, "
"`v2.96` 이상은 `pinned_availability_zone` 속성에 null을 반환한다."

#: ../../<reno.sphinxext origin/stable/ocata>:495
msgid ""
"`Bug 1665263`_ is fixed. This was a regression where "
"``instance.delete.start`` and ``instance.delete.end`` notifications were not"
" emitted when deleting an instance in ``ERROR`` state due to a failed build."
msgstr ""
"`Bug 1665263`_은 해결되었습니다. 이는 `instance.delete.start` 및 `instance.delete.end` "
"알림이 `ERROR` 상태에서 인스턴스를 삭제할 때 실패한 빌드 때문에 배포되지 않은 경우에 발생하는 반대편의 회전으로, 알림이 전송되지"
" 않았습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:459
msgid ""
"`Bug 1670627`_ is fixed. This bug led to potential over-quota errors after "
"several failed server build attempts, resulting in quota usage to reach the "
"limit even though the servers were deleted."
msgstr ""
"`Bug 1670627`_은 해결되었습니다. 이 버그는 여러 번의 서버 빌드 시도 후에 potencial over-quota 오류가 "
"발생할 수 있는 문제를 일으켰으며, 서버가 삭제되더라도 사용량이 제한량을 달성할 수 있는 문제를 일으켰습니다."

#: ../../<reno.sphinxext ../source/mitaka.rst:29 ../source/newton.rst:172
#: origin/stable/ocata>:419 stable/pike>:1743
msgid "`Bug 1673569 <https://bugs.launchpad.net/nova/+bug/1673569>`_"
msgstr "`bug 1673569 <https://bugs.launchpad.net/nova/+bug/1673569>`"

#: ../../<reno.sphinxext stable/rocky>:205 stable/stein>:1287
msgid ""
"`Bug 1675791`_ has been fixed by granting image membership access to "
"snapshot images when the owner of the server is not performing the "
"snapshot/backup/shelve operation on the server. For example, an admin "
"shelves a user's server and the user needs to unshelve the server so the "
"user needs access to the shelved snapshot image."
msgstr ""
"`Bug 1675791`_은 서버 소유자가 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 서버 "
"소유자가 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 서버 소유자가 스냅샘/백업/셰이브 연산을"
" 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 서버 소유자가 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 "
"스냅샘/백업/셰이브 연산을 수행하지 않는 서버 소유자가 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지"
" 않는 서버 소유자가 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 "
"연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 "
"스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 "
"경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 "
"않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 수행하지 않는 경우 스냅샘/백업/셰이브 연산을 "
"수행하지 않는 경우 스냅샘"

#: ../../<reno.sphinxext origin/stable/ocata>:192 stable/pike>:233
msgid "`Bug 1739593 <https://bugs.launchpad.net/nova/+bug/1739593>`_"
msgstr "`bug 1739593 <https://bugs.launchpad.net/nova/+bug/1739593>`"

#: ../../<reno.sphinxext stable/pike>:43 stable/queens>:113 stable/rocky>:231
#: stable/stein>:351 stable/train>:1337
msgid ""
"`Bug 1811726`_ is fixed by deleting the resource provider (in placement) "
"associated with each compute node record managed by a ``nova-compute`` "
"service when that service is deleted via the ``DELETE /os-"
"services/{service_id}`` API. This is particularly important for compute "
"services managing ironic baremetal nodes."
msgstr ""
"`Bug 1811726`_은 `nova-compute` 서비스가 삭제되면 `DELETE /os-services/{service_id}` "
"API를 사용하여 삭제되는 경우 각 컴퓨터 노드 레코드가 관리하는 리소스 제공자 (placement)에 대한 연관성을 삭제하는 "
"것으로修正됩니다. 특히, 이ironic baremetal 노드들을 관리하는 컴퓨터 서비스에 대한 경우 특히 중요합니다."

#: ../../<reno.sphinxext stable/train>:392 stable/ussuri>:1188
msgid ""
"`Bug 1845986`_ has been fixed by adding iommu driver when the following "
"metadata options are used with AMD SEV:"
msgstr ""
"`Bug 1845986`_ 집합은 AMD SEV와 다음 메타데이터 옵션을 사용할 때 iommu 드라이버를 추가하여修复되었습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:350 unmaintained/xena>:667
msgid ""
"`Bug 1851545 <https://bugs.launchpad.net/nova/+bug/1851545>`_, wherein "
"unshelving an instance with SRIOV Neutron ports did not update the port "
"binding's ``pci_slot`` and could cause libvirt PCI conflicts, has been "
"fixed."
msgstr ""
"`Bug 1851545 <https://bugs.launchpad.net/nova/+bug/1851545>`_, 집합 1851545 "
"에서, SRIOV Neutron 포트를 사용하여 인스턴스를 비어주면 포트 결합의 `pci_slot` 속성이 업데이트되지 않아, "
"libvirt PCI 충돌이 발생할 수 있습니다. 이 문제는 해결되었습니다."

#: ../../<reno.sphinxext unmaintained/xena>:272 unmaintained/yoga>:630
msgid ""
"`Bug 1950657 <https://bugs.launchpad.net/nova/+bug/1950657>`_, fixing "
"behavior when nova-compute wouldn't retry image download when gets \"Corrupt"
" image download\" error from glanceclient and has num_retries config option "
"set."
msgstr ""
"`Bug 1950657 <https://bugs.launchpad.net/nova/+bug/1950657>`_, 집합 1950657의 "
"<https://bugs.launchpad.net/nova/+bug/1950657>`_, nova-compute가 \"Corrupt "
"image download\" 오류를 받고 num_retries 설정 옵션을 사용할 때, 이미지 tải를 재시도하지 않는다."

#: ../../<reno.sphinxext origin/stable/ocata>:1170
msgid ""
"`HTTP Bad Request 400` will be returned for the filters/sort keys which are "
"on joined tables or internal data model attributes. They would previously "
"cause a `HTTP Server Internal Error 500`, namely:"
msgstr ""
"`HTTP Bad Request 400`는 연합 테이블 또는 내부 데이터 모델 속성에 있는 필터/정렬 키에 대한 반환을 의미합니다. "
"이전에는 `HTTP Server Internal Error 500`로 반환되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:346
msgid ""
"`Image pre-caching support to compute hosts`__ using ``os-aggregates`` API "
"information, allowing some distributed edge cases and preemptive image "
"caching for instance creation."
msgstr ""
"`이미지 전처리 지원을 컴퓨팅 호스트`__ using ``os-aggregates`` API 정보, 일부 분산 edge case와 "
"인스턴스 생성 시 전처리된 이미지 캐싱을 허용합니다."

#: ../../<reno.sphinxext stable/train>:1416
msgid ""
"`Numbered request groups`_ can be defined in the flavor extra_spec but they "
"can come from other sources as well (e.g. neutron ports). If there is more "
"than one numbered request group in the allocation candidate query and the "
"flavor does not specify any group policy then the query will fail in "
"placement as group_policy is mandatory in this case. Nova previously printed"
" a warning to the scheduler logs but let the request fail. However the "
"creator of the flavor cannot know if the flavor later on will be used in a "
"boot request that has other numbered request groups. So nova will start "
"defaulting the group_policy to 'none' which means that the resource "
"providers fulfilling the numbered request groups can overlap. Nova will only"
" default the group_policy if it is not provided in the flavor extra_spec, "
"and there is more than one numbered request group present in the final "
"request, and the flavor only provided one or zero of such groups."
msgstr ""
"`집합 수신 그룹`는 `flavor`의 `extra_spec`에서 정의할 수 있지만 다른 nguồn에서도 오를 수 있다 (예: "
"`neutron` 포트). `flavor`가 `group_policy`를 정의하지 않으면 `allocation`에서 "
"`group_policy`가 필수적이므로 `placement`에서 실패할 것이다. `Nova`는 이전에 스케줄러 로그에 경고를 출력했지만"
" 요청을 실패했다. 그러나 `flavor`의 창조자에게 `flavor`가 나중에 `boot` 요청에 다른 `집합 수신 그룹`이 포함될 "
"가능성이 있는지 알 수 없기 때문에 `Nova`는 `group_policy`를 기본으로 `'none'`로 설정한다. 이는 `집합 수신 "
"그룹`이 겹치기 가능성이 있다. `Nova`는 `group_policy`가 `flavor`의 `extra_spec`에서 제공되지 않으며,"
" `final` 요청에서 `group_policy`가 여러 개일 때만 기본으로 설정한다. 또한 `flavor`가 "
"`group_policy`를 제공하지 않으며, `group_policy`가 1개 또는 0개만 제공할 때이다."

#: ../../<reno.sphinxext ../source/newton.rst:77 origin/stable/ocata>:334
#: stable/pike>:453 stable/queens>:1629
msgid "`OSSA-2017-005`_: Nova Filter Scheduler bypass through rebuild action"
msgstr "`OSSA-2017-005`_: Nova Filter Scheduler bypass through rebuild action"

#: ../../<reno.sphinxext stable/pike>:386 stable/queens>:1642
msgid ""
"`OSSA-2017-006`_: Nova FilterScheduler doubles resource allocations during "
"rebuild with new image (CVE-2017-17051)"
msgstr ""
"`OSSA-2017-006`_: Nova FilterScheduler는 새로운 이미지로 재건 시 자원 할당을 두 배로 증가시킵니다. "
"(CVE-2017-17051)"

#: ../../<reno.sphinxext origin/stable/ocata>:14 stable/pike>:14
#: stable/queens>:84 stable/rocky>:176 stable/stein>:322 stable/train>:1268
msgid ""
"`OSSA-2019-003`_: Nova Server Resource Faults Leak External Exception "
"Details (CVE-2019-14433)"
msgstr ""
"`OSSA-2019-003`_: Nova Server Resource Faults Leak External Exception "
"Details (CVE-2019-14433)"

#: ../../<reno.sphinxext unmaintained/2023.1>:285
msgid ""
"`PCI devices can now be scheduled "
"<https://docs.openstack.org/nova/latest/admin/pci-passthrough.html#pci-"
"tracking-in-placement>`_ by Nova using the Placement API on a opt-in basis. "
"This will help the nova-scheduler service to better schedule flavors that "
"use PCI (non-Neutron related) resources, will generate less reschedules if "
"an instance cannot be created on a candidate and will help the nova-"
"scheduler to not miss valid candidates if the list was too large."
msgstr ""
"`PCI 장치들은 now PCI Tracking in Placement에 따라 "
"<https://docs.openstack.org/nova/latest/admin/pci-passthrough.html#pci-"
"tracking-in-placement>`_ Nova가 opt-in 기반으로 Placement API를 사용하여 scheduling할 수"
" 있습니다. 이것은 nova-scheduler 서비스가 PCI(Neutron과 관련이 없는) 리소스를 사용하는 플레버를 더 잘 "
"scheduling할 수 있도록 도와주고, 인스턴스가 후보에 생성되지 않을 경우 재cheduling이 적은 것을 생성하고, 후보 목록이 "
"너무 큰 경우 nova-scheduler가 유효한 후보를 misses하지 않도록 도와줍니다."

#: ../../<reno.sphinxext unmaintained/2023.1>:304
msgid ""
"`SPICE consoles <https://docs.openstack.org/nova/latest/admin/remote-"
"console-access.html#spice-console>`_ can now be configured with compression "
"settings which include choices of the compression algorithm and the "
"compression mode."
msgstr ""
"`SPICE 콘솔 <https://docs.openstack.org/nova/latest/admin/remote-console-"
"access.html#spice-console>`_은 현재 압축 설정을 포함한 압축 알고리즘과 압축 모드를 선택할 수 있습니다."

#: ../../<reno.sphinxext unmaintained/xena>:303
msgid ""
"`Support for accelerators`__ in Nova servers has been improved. Now Cyborg-"
"managed SmartNICs can be attached as SR-IOV devices."
msgstr ""
"`가속화기 지원`__ Nova 서버에서 개선되었습니다. 현재 시버borg- 관리된 스마트 NICs가 SR-IOV 장치로 연결될 수 "
"있습니다."

#: ../../<reno.sphinxext unmaintained/wallaby>:397
msgid ""
"`Support for accelerators`__ in Nova servers has been improved. Now shelving"
" and unshelving instances using Cyborg accelerators is supported."
msgstr ""
"`가속화기 지원`__ Nova 서버에서 개선되었다. 현재 Cyborg 가속화기를 사용하여 인스턴스를 보관하고 해보관하는 지원이 제공된다."

#: ../../<reno.sphinxext unmaintained/zed>:197
msgid ""
"`Virtual IOMMU devices`__  can now be created and attached to an instance "
"when running on a x86 host and using the libvirt driver."
msgstr ""
"`가상 I/O MMU 장치`  __  현재 x86 호스트에서 libvirt 드라이버를 사용하여 실행 중인 인스턴스에 attached 할 "
"수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:835
msgid ""
"``?resources=$RESOURCE_CLASS_NAME:$AMOUNT,$RESOURCE_CLASS_NAME:$AMOUNT``"
msgstr "집합 = $RESOURCE_CLASS_NAME:$AMOUNT,$RESOURCE_CLASS_NAME:$AMOUNT"

#: ../../<reno.sphinxext stable/2024.2>:251
msgid ""
"``AggregateMultitenancyIsolation`` scheduler filter now supports multiple "
"aggregate keys prefixed by ``filter_tenant_id`` which removes the limitation"
" on the number of tenants an aggregate of hosts can relate to."
msgstr ""
"집합 다중tenant 분리  스케줄러 필터는 현재 \"filter_tenant_id\"으로 prefix된 여러 집합 키를 지원합니다. "
"이것은 호스트 집합에 대한 tenant의 수에 대한 제한을 제거합니다."

#: ../../<reno.sphinxext stable/stein>:591
msgid "``COMPUTE_DEVICE_TAGGING``"
msgstr "COMPUTE_DEVICE_TAGGING"

#: ../../<reno.sphinxext stable/stein>:592
msgid "``COMPUTE_NET_ATTACH_INTERFACE_WITH_TAG``"
msgstr "COMPUTE_NET_ATTACH_INTERFACE_WITH_TAG"

#: ../../<reno.sphinxext stable/stein>:590
msgid "``COMPUTE_NET_ATTACH_INTERFACE``"
msgstr "COMPUTE_NET_ATTACH_INTERFACE → 컴퓨팅 네트워크 연결 인터페이스"

#: ../../<reno.sphinxext stable/stein>:596
msgid "``COMPUTE_TRUSTED_CERTS``"
msgstr "COMPUTE_TRUSTED_CERTS"

#: ../../<reno.sphinxext stable/stein>:593
msgid "``COMPUTE_VOLUME_ATTACH_WITH_TAG``"
msgstr "COMPUTE_VOLUME_ATTACH_WITH_TAG"

#: ../../<reno.sphinxext stable/stein>:594
msgid "``COMPUTE_VOLUME_EXTEND``"
msgstr "COMPUTE_VOLUME_EXTEND"

#: ../../<reno.sphinxext stable/stein>:595
msgid "``COMPUTE_VOLUME_MULTI_ATTACH``"
msgstr "COMPUTE_VOLUME_MULTI_ATTACH"

#: ../../<reno.sphinxext stable/train>:1083
msgid "``DELETE /os-cells/{cell_id}``"
msgstr "`{cell_id}`"

#: ../../<reno.sphinxext stable/rocky>:1487
msgid "``DELETE /os-floating-ip-dns/{domain}/entries/{name}``"
msgstr "``/os-floating-ip-dns/{domain}/entries/{name}``"

#: ../../<reno.sphinxext stable/rocky>:1483
msgid "``DELETE /os-floating-ip-dns/{domain}``"
msgstr "``집합 /os-floating-ip-dns/{domain}``"

#: ../../<reno.sphinxext stable/ussuri>:866
msgid "``DELETE /os-networks``"
msgstr "``집합 /os-networks``"

#: ../../<reno.sphinxext stable/ussuri>:864
msgid "``DELETE /os-security-group-default-rules/{id}``"
msgstr "`DELETE /os-security-group-default-rules/{id}`"

#: ../../<reno.sphinxext stable/ussuri>:873
msgid "``DELETE /os-tenant-networks``"
msgstr "``집합 /os-tenant-networks``"

#: ../../<reno.sphinxext stable/ussuri>:844
msgid "``DELETE /servers/{server_id}/consoles/{console_id}``"
msgstr ""
"`DELETE /servers/{server_id}/consoles/{console_id}`\n"
"\n"
"`DELETE /servers/{server_id}/consoles/{console_id}`\n"
"\n"
"`집합 /servers/{server_id}/consoles/{console_id}`"

#: ../../<reno.sphinxext stable/queens>:857 stable/rocky>:609
msgid "``GET /flavors/detail``"
msgstr "``집합 /flavors/detail``"

#: ../../<reno.sphinxext stable/queens>:858 stable/rocky>:610
msgid "``GET /flavors/{flavor_id}``"
msgstr "``GET /flavors/{flavor_id}``"

#: ../../<reno.sphinxext stable/queens>:856
msgid "``GET /flavors``"
msgstr "``집합 /flavors``"

#: ../../<reno.sphinxext stable/train>:1077
msgid "``GET /os-cells/capacities``"
msgstr "``GET /os-cells/capacities``"

#: ../../<reno.sphinxext stable/train>:1078
msgid "``GET /os-cells/detail``"
msgstr "``집합 /os-cells/detail``"

#: ../../<reno.sphinxext stable/train>:1079
msgid "``GET /os-cells/info``"
msgstr "``집합 /os-cells/info``"

#: ../../<reno.sphinxext stable/train>:1084
msgid "``GET /os-cells/{cell_id}/capacities``"
msgstr "``GET /os-cells/{cell_id}/집합``"

#: ../../<reno.sphinxext stable/train>:1081
msgid "``GET /os-cells/{cell_id}``"
msgstr "``GET /os-cells/{cell_id}``"

#: ../../<reno.sphinxext stable/train>:1075
msgid "``GET /os-cells``"
msgstr "``집합 /os-cells``"

#: ../../<reno.sphinxext stable/rocky>:1474
msgid "``GET /os-fixed-ips/{fixed_ip}``"
msgstr "``GET /os-fixed-ips/{fixed_ip}``"

#: ../../<reno.sphinxext stable/rocky>:1484
msgid "``GET /os-floating-ip-dns/{domain}/entries/{ip}``"
msgstr ""
"``GET /os-floating-ip-dns/{domain}/entries/{ip}`` \n"
"\n"
"``{domain}``를 `{도메인}`으로, `{ip}`를 `{IP}`로 mapping합니다.\n"
"\n"
"``GET /os-floating-ip-dns/{도메인}/entries/{IP}`"

#: ../../<reno.sphinxext stable/rocky>:1485
msgid "``GET /os-floating-ip-dns/{domain}/entries/{name}``"
msgstr ""
"`GET /os-floating-ip-dns/{domain}/entries/{name}`\n"
"\n"
"`집합 floating IP DNS/{도메인}/entries/{이름}`"

#: ../../<reno.sphinxext stable/rocky>:1481
msgid "``GET /os-floating-ip-dns``"
msgstr ""
"``GET /os-floating-ip-dns`` \n"
"\n"
"``GET /os-floating-ip-dns``"

#: ../../<reno.sphinxext stable/rocky>:1478
msgid "``GET /os-floating-ips-bulk/{host_name}``"
msgstr ""
"``GET /os-floating-ips-bulk/{host_name}`` \n"
"\n"
"``GET /os-float-ips-bulk/{host_name}``"

#: ../../<reno.sphinxext stable/rocky>:1477
msgid "``GET /os-floating-ips-bulk``"
msgstr "``GET /os-floating-ips-bulk``"

#: ../../<reno.sphinxext stable/rocky>:1472
msgid "``GET /os-fping/{server_id}``"
msgstr "`GET /os-fping/{server_id}`"

#: ../../<reno.sphinxext stable/rocky>:1471
msgid "``GET /os-fping``"
msgstr "``집합 /os-fping``"

#: ../../<reno.sphinxext stable/ussuri>:469
msgid "``GET /os-migrations?project_id=011ee9f4-8f16-4c38-8633-a254d420fd54``"
msgstr ""
"``GET /os-migrations?project_id=집합 011ee9f4-8f16-4c38-8633-a254d420fd54``"

#: ../../<reno.sphinxext stable/ussuri>:470
msgid ""
"``GET /os-"
"migrations?user_id=ef9d34b4-45d0-4530-871b-3fb535988394&project_id=011ee9f4-8f16-4c38-8633-a254d420fd54``"
msgstr ""
"``GET /os-migrations?user_id=집합 "
"ef9d34b4-45d0-4530-871b-3fb535988394&project_id=집합 "
"011ee9f4-8f16-4c38-8633-a254d420fd54``"

#: ../../<reno.sphinxext stable/ussuri>:468
msgid "``GET /os-migrations?user_id=ef9d34b4-45d0-4530-871b-3fb535988394``"
msgstr ""
"``GET /os-migrations?user_id=집합 ef9d34b4-45d0-4530-871b-3fb535988394``"

#: ../../<reno.sphinxext stable/stein>:771 stable/ussuri>:460
msgid "``GET /os-migrations``"
msgstr "``GET /os-migrations``"

#: ../../<reno.sphinxext stable/queens>:744
msgid ""
"``GET /os-migrations``, ``GET "
"/servers/{server_id}/migrations/{migration_id}`` and ``GET "
"/servers/{server_id}/migrations`` will now return a uuid value in addition "
"to the migrations id in the response."
msgstr ""
"GET /os-migrations`, `GET /servers/{server_id}/migrations/{migration_id}` 및 "
"`GET /servers/{server_id}/migrations`는 현재 응답에서 마이그레이션 ID와 함께 uuid 값을 추가로 "
"반환합니다."

#: ../../<reno.sphinxext stable/ussuri>:893
msgid "``GET /os-quota-class-sets/{id}``"
msgstr "``GET /os-quota-class-sets/{id}``"

#: ../../<reno.sphinxext stable/ussuri>:890
msgid "``GET /os-quota-sets/{project_id}/defaults``"
msgstr ""
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota-sets/{project_id}/defaults`를 `{os-quota-sets}/{project_id}/defaults`로 바꿔서, \n"
"``GET /os-quota-sets/{project_id}/defaults`` \n"
"\n"
"``{project_id}``를 `{project_id}`로, `/os-quota"

#: ../../<reno.sphinxext stable/ussuri>:891
msgid "``GET /os-quota-sets/{project_id}/detail``"
msgstr "``GET /os-quota-sets/{project_id}/ detail``"

#: ../../<reno.sphinxext stable/ussuri>:889
msgid "``GET /os-quota-sets/{project_id}``"
msgstr "``GET /os-quota-sets/{project_id}``"

#: ../../<reno.sphinxext stable/ussuri>:888
msgid "``GET /os-quota-sets``"
msgstr "``집합 /os-quota-sets``"

#: ../../<reno.sphinxext stable/ussuri>:863
msgid "``GET /os-security-group-default-rules/{id}``"
msgstr "``GET /os-segurity-group-default-rules/{id}``"

#: ../../<reno.sphinxext stable/ussuri>:861
msgid "``GET /os-security-group-default-rules``"
msgstr "``GET /os-사ecurityGroup-기본ルール``"

#: ../../<reno.sphinxext stable/rocky>:1322 stable/stein>:769
#: stable/train>:633
msgid "``GET /servers/detail``"
msgstr "``GET /서버/ detail``"

#: ../../<reno.sphinxext stable/ussuri>:524
msgid "``GET /servers/detail`` if using API microversion >= 2.16"
msgstr "``GET /servers/detail`` if using API microversion >= 2.16"

#: ../../<reno.sphinxext stable/ussuri>:843
msgid "``GET /servers/{server_id}/consoles/{console_id}``"
msgstr "`GET /servers/{server_id}/consoles/{console_id}`"

#: ../../<reno.sphinxext stable/ussuri>:842
msgid "``GET /servers/{server_id}/consoles``"
msgstr ""
"``GET /servers/{server_id}/consoles`` \n"
"\n"
"``GET /서버/{서버_아이드}/컨SOLE``"

#: ../../<reno.sphinxext stable/ussuri>:462
msgid "``GET /servers/{server_id}/migrations/{migration_id}``"
msgstr ""
"`GET /servers/{server_id}/migrations/{migration_id}`\n"
"\n"
"`GET /서버/{서버_아이디}/이동{이동_아이디}`"

#: ../../<reno.sphinxext stable/ussuri>:461
msgid "``GET /servers/{server_id}/migrations``"
msgstr "``GET /서버/{서버_아이디}/이동``"

#: ../../<reno.sphinxext stable/stein>:770
msgid "``GET /servers/{server_id}/os-instance-actions``"
msgstr "`GET /servers/{server_id}/os-instance-actions`"

#: ../../<reno.sphinxext stable/rocky>:1473
msgid "``GET /servers/{server_id}/os-virtual-interfaces``"
msgstr ""
"``GET /servers/{server_id}/os-virtual-interfaces`` \n"
"\n"
"``{server_id}``를 `{server_id}`로 유지하고, `/os-virtual-interfaces`를 `{os-virtual-interfaces}`로 바꾸어 `GET /servers/{server_id}/{os-virtual-interfaces}`로 바꾸어 translation을 완성합니다."

#: ../../<reno.sphinxext stable/train>:729
msgid "``GET /servers/{server_id}/os-volume_attachments/{volume_id}``"
msgstr ""
"`GET /servers/{server_id}/os-volume_attachments/{volume_id}`\n"
"\n"
"`GET /서버/{서버_아이드}/os-volume_attachments/{volume_id}`"

#: ../../<reno.sphinxext stable/train>:728
msgid "``GET /servers/{server_id}/os-volume_attachments``"
msgstr "`GET /servers/{server_id}/os-volume_attachments`"

#: ../../<reno.sphinxext stable/train>:556
msgid "``GET /servers/{server_id}/topology``"
msgstr ""
"``GET /servers/{server_id}/topology`` \n"
"\n"
"``GET /서버/{서버_아이드}/topology``"

#: ../../<reno.sphinxext stable/rocky>:1323 stable/stein>:711
msgid "``GET /servers/{server_id}``"
msgstr "`GET /servers/{server_id}`"

#: ../../<reno.sphinxext stable/ussuri>:523
msgid "``GET /servers/{server_id}`` if using API microversion >= 2.16"
msgstr "`GET /servers/{server_id}` if using API microversion >= 2.16"

#: ../../<reno.sphinxext stable/stein>:768
msgid "``GET /servers``"
msgstr "``GET /servers``"

#: ../../<reno.sphinxext stable/train>:632
msgid "``GET servers/{server_id}``"
msgstr "``GET servers/{server_id}``"

#: ../../<reno.sphinxext stable/2024.1>:126 stable/2024.2>:157
#: stable/2025.1>:102 stable/2025.2>:492
msgid ""
"``Nova`` now strictly enforces that only ``cinder`` can call the ``update "
"volume attachment`` aka ``swap volume`` api. This is part of addressing a "
"security hardening gap identified as part of bug: "
"https://bugs.launchpad.net/nova/+bug/2112187"
msgstr ""
"``nova``은 현재 ``cinder``만이 ``volume attachment``을 ``update``하거나 ``swap``하는 "
"API를 호출할 수 있도록 제한하고 있습니다. 이는 보안 강화의 결함을 해결하기 위한 일환으로 identified 한 버그: "
"https://bugs.launchpad.net/nova/+bug/2112187"

#: ../../<reno.sphinxext stable/queens>:859 stable/rocky>:611
msgid "``POST /flavors``"
msgstr "``집합 /flavors``"

#: ../../<reno.sphinxext stable/ussuri>:553
msgid "``POST /os-aggregates/{aggregate_id}/images``"
msgstr ""
"``POST /os-aggregates/{aggregate_id}/images`` \n"
"\n"
"``os-aggregates/{aggregate_id}/images``"

#: ../../<reno.sphinxext stable/train>:1080
msgid "``POST /os-cells/sync_instances``"
msgstr "``POST /os-cells/sync_instances``"

#: ../../<reno.sphinxext stable/train>:1076
msgid "``POST /os-cells``"
msgstr "``/os-cells``"

#: ../../<reno.sphinxext stable/rocky>:1475
msgid "``POST /os-fixed-ips/{fixed_ip}/action (reserve)``"
msgstr ""
"``POST /os-fixed-ips/{fixed_ip}/action (reserve)`` \n"
"\n"
"``POST /os-fixed-ips/{fixed_ip}/action (집합)``"

#: ../../<reno.sphinxext stable/rocky>:1476
msgid "``POST /os-fixed-ips/{fixed_ip}/action (unreserve)``"
msgstr ""
"``POST /os-fixed-ips/{fixed_ip}/action (unreserve)`` \n"
"\n"
"``POST /os-fixed-ips/{fixed_ip}/action (unreserve)`` \n"
"\n"
"``POST /os-fixed-ips/{fixed_ip}/action (unreserve)`` \n"
"\n"
"``POST /os-fixed-ips/{fixed_ip}/action (unreserve)``"

#: ../../<reno.sphinxext stable/rocky>:1479
msgid "``POST /os-floating-ips-bulk``"
msgstr "``POST /os-floating-ips-bulk``"

#: ../../<reno.sphinxext stable/ussuri>:867
msgid "``POST /os-networks/add``"
msgstr "``POST /os-networks/add``"

#: ../../<reno.sphinxext stable/ussuri>:868
msgid "``POST /os-networks/{id} (associate_host)``"
msgstr "``POST /os-networks/{id} (연결호스트)``"

#: ../../<reno.sphinxext stable/ussuri>:869
msgid "``POST /os-networks/{id} (disassociate)``"
msgstr "``POST /os-networks/{id} (해제)``"

#: ../../<reno.sphinxext stable/ussuri>:870
msgid "``POST /os-networks/{id} (disassociate_host)``"
msgstr ""
"``POST /os-networks/{id} (disassociate_host)`` \n"
"\n"
"``/os-networks/{id} (disassociate_host)``"

#: ../../<reno.sphinxext stable/ussuri>:871
msgid "``POST /os-networks/{id} (disassociate_project)``"
msgstr ""
"``POST /os-networks/{id} (disassociate_project)`` \n"
"\n"
"``/os-networks/{id} (disassociate_project)``"

#: ../../<reno.sphinxext stable/ussuri>:865
msgid "``POST /os-networks``"
msgstr "``/os-networks``"

#: ../../<reno.sphinxext stable/ussuri>:862
msgid "``POST /os-security-group-default-rules``"
msgstr "``POST /os-ecurity-group-기본규칙``"

#: ../../<reno.sphinxext stable/ussuri>:872
msgid "``POST /os-tenant-networks``"
msgstr "``/os-tenant-networks``"

#: ../../<reno.sphinxext stable/rocky>:1310 stable/rocky>:1325
#: stable/stein>:713
msgid "``POST /servers/{server_id}/action (rebuild)``"
msgstr ""
"`POST /servers/{server_id}/action (rebuild)` \n"
"\n"
"`POST` / `서버/{서버_아이드}/액션 (rebuild)`"

#: ../../<reno.sphinxext stable/ussuri>:526
msgid ""
"``POST /servers/{server_id}/action`` (rebuild) if using API microversion >= "
"2.75"
msgstr ""
"``POST /servers/{server_id}/action`` (rebuild) if using API microversion >= 2.75`` \n"
"\n"
"집합 /servers/{server_id}/action`` (rebuild) if using API microversion >= 2.75"

#: ../../<reno.sphinxext stable/train>:634
msgid "``POST /servers/{server_id}/action`` where the action is rebuild"
msgstr ""
"`POST /servers/{server_id}/action`에서 `action`는 `rebuild`로 translation은 `POST"
" /servers/{server_id}/action`에서 `action`는 `rebuild`로 translation은 `집합`로 "
"translation은 `집합`로 translation은 `집합`로"

#: ../../<reno.sphinxext stable/ussuri>:841
msgid "``POST /servers/{server_id}/consoles``"
msgstr "`POST /servers/{server_id}/consoles`"

#: ../../<reno.sphinxext stable/train>:727
msgid "``POST /servers/{server_id}/os-volume_attachments``"
msgstr "`POST /servers/{server_id}/os-volume_attachments`"

#: ../../<reno.sphinxext stable/rocky>:1309
msgid "``POST /servers``"
msgstr "`/servers`"

#: ../../<reno.sphinxext stable/queens>:860 stable/rocky>:612
msgid "``PUT /flavors/{flavor_id}``"
msgstr "``PUT /flavors/{flavor_id}``"

#: ../../<reno.sphinxext stable/rocky>:315 stable/stein>:1280
msgid ""
"``PUT /os-aggregates/{aggregate_id}`` and ``POST /os-"
"aggregates/{aggregate_id}/action`` (for set_metadata action) will now return"
" HTTP 400 for availability zone renaming if the hosts of the aggregate have "
"any instances."
msgstr ""
"``PUT /os-aggregates/{aggregate_id}``와 ``POST /os-"
"aggregates/{aggregate_id}/action`` (set_metadata action을 위해)가 now "
"availability zone renaming을 위해 400 HTTP를 trả return 할 때, aggregate의 hosts가 "
"any instance를 가지고 있다면."

#: ../../<reno.sphinxext stable/train>:1082
msgid "``PUT /os-cells/{cell_id}``"
msgstr "``PUT /os-cells/{cell_id}``"

#: ../../<reno.sphinxext stable/rocky>:1486
msgid "``PUT /os-floating-ip-dns/{domain}/entries/{name}``"
msgstr ""
"``PUT /os-floating-ip-dns/{domain}/entries/{name}`` \n"
"\n"
"``os-floating-ip-dns/{domain}/entries/{name}``"

#: ../../<reno.sphinxext stable/rocky>:1482
msgid "``PUT /os-floating-ip-dns/{domain}``"
msgstr "``PUT /os-floating-ip-dns/{domain}``"

#: ../../<reno.sphinxext stable/rocky>:1480
msgid "``PUT /os-floating-ips-bulk/delete``"
msgstr ""
"``PUT /os-floating-ips-bulk/delete`` \n"
"\n"
"``/os-floating-ips-bulk/delete``"

#: ../../<reno.sphinxext stable/ussuri>:894
msgid "``PUT /os-quota-class-sets/{id}``"
msgstr "`PUT /os-quota-class-sets/{id}`"

#: ../../<reno.sphinxext stable/ussuri>:892
msgid "``PUT /os-quota-sets/{project_id}``"
msgstr "``PUT /os-quota-sets/{project_id}``"

#: ../../<reno.sphinxext stable/rocky>:1324 stable/stein>:712
msgid "``PUT /servers/{server_id}``"
msgstr "`PUT /servers/{server_id}`"

#: ../../<reno.sphinxext stable/ussuri>:525
msgid "``PUT /servers/{server_id}`` if using API microversion >= 2.75"
msgstr "`PUT /servers/{server_id}` if using API microversion >= 2.75"

#: ../../<reno.sphinxext stable/train>:635
msgid "``PUT servers/{server_id}``"
msgstr "``집합 servers/{server_id}``"

#: ../../<reno.sphinxext unmaintained/victoria>:358
msgid ""
"``VMWare`` virt driver is now supported again in Victoria after being "
"deprecated during the Ussuri release, as testing issues have been addressed."
msgstr ""
"`VMWare` virt 드라이버는 Ussuri 릴리스 시 deprecated 된 후 다시 지원되었습니다. 테스트 문제가 해결되었습니다."

#: ../../<reno.sphinxext unmaintained/victoria>:355
msgid ""
"``XenAPI`` virt driver has been removed, including the related configuration"
" options."
msgstr "XenAPI virt 드라이버가 제거되었으며 관련 구성 옵션도 제거되었다."

#: ../../<reno.sphinxext stable/ussuri>:923 stable/ussuri>:992
msgid "``[DEFAULT] allow_same_net_traffic``"
msgstr "[DEFAULT] same_net_traffic 허용"

#: ../../<reno.sphinxext stable/ussuri>:949
msgid "``[DEFAULT] auto_assign_floating_ip``"
msgstr "[DEFAULT] float_ip_할당"

#: ../../<reno.sphinxext stable/ussuri>:936
msgid "``[DEFAULT] cnt_vpn_clients``"
msgstr "cnt_vpn_clients"

#: ../../<reno.sphinxext stable/2025.2>:361
msgid "``[DEFAULT] compute_link_prefix``"
msgstr "[DEFAULT] 아키텍처 연결前缀"

#: ../../<reno.sphinxext stable/2025.2>:351
msgid "``[DEFAULT] config_drive_skip_versions``"
msgstr "[DEFAULT] config_drive_skip_versions"

#: ../../<reno.sphinxext stable/ussuri>:938
msgid "``[DEFAULT] create_unique_mac_address_attempts``"
msgstr "[DEFAULT] 고유 MAC 주소 생성 시도"

#: ../../<reno.sphinxext stable/ussuri>:953
msgid "``[DEFAULT] default_floating_pool``"
msgstr "[DEFAULT] default_floating_pool"

#: ../../<reno.sphinxext stable/ussuri>:993
msgid "``[DEFAULT] defer_iptables_apply``"
msgstr "[DEFAULT] 집합 이프테bles 적용 지연"

#: ../../<reno.sphinxext stable/2025.2>:358 stable/ussuri>:947
msgid "``[DEFAULT] dhcp_domain``"
msgstr "``[DEFAULT] dhcp_domain``"

#: ../../<reno.sphinxext stable/ussuri>:972
msgid "``[DEFAULT] dhcp_lease_time``"
msgstr "[기본] dhcp_lease_time"

#: ../../<reno.sphinxext stable/ussuri>:970
msgid "``[DEFAULT] dhcpbridge_flagfile``"
msgstr "[DEFAULT] dhcpbridge_flagfile집합"

#: ../../<reno.sphinxext stable/ussuri>:971
msgid "``[DEFAULT] dhcpbridge``"
msgstr "[DEFAULT] dhcpbridge"

#: ../../<reno.sphinxext stable/ussuri>:981
msgid "``[DEFAULT] dmz_cidr``"
msgstr "[DEFAULT] dmz_cidr집합"

#: ../../<reno.sphinxext stable/ussuri>:973
msgid "``[DEFAULT] dns_server``"
msgstr "[DEFAULT] 집합 서버"

#: ../../<reno.sphinxext stable/ussuri>:946
msgid "``[DEFAULT] dns_update_periodic_interval``"
msgstr "[DEFAULT] dns_update_periodic_interval → [DEFAULT] DNS更新 주기 간격"

#: ../../<reno.sphinxext stable/ussuri>:975
msgid "``[DEFAULT] dnsmasq_config_file``"
msgstr "[DEFAULT] dnsmasq_config_file집합"

#: ../../<reno.sphinxext stable/ussuri>:976
msgid "``[DEFAULT] ebtables_exec_attempts``"
msgstr "[DEFAULT] ebtables_exec_attempts집합"

#: ../../<reno.sphinxext stable/ussuri>:977
msgid "``[DEFAULT] ebtables_retry_interval``"
msgstr "[DEFAULT] ebtables_retry_interval → [DEFAULT] ebtables_retry_interval"

#: ../../<reno.sphinxext stable/2025.2>:364
msgid "``[DEFAULT] enable_instance_password``"
msgstr "``[DEFAULT] 인스턴스 패스워드 사용``"

#: ../../<reno.sphinxext stable/ussuri>:978
msgid "``[DEFAULT] fake_network``"
msgstr "[DEFAULT] 집합"

#: ../../<reno.sphinxext stable/ussuri>:922
msgid "``[DEFAULT] firewall_driver``"
msgstr "[DEFAULT]防火walls 드라이버"

#: ../../<reno.sphinxext stable/ussuri>:937
msgid "``[DEFAULT] fixed_ip_disassociate_timeout``"
msgstr ""
"[DEFAULT] fixed_ip_disassociate_timeout → [DEFAULT] "
"fixed_ip_disassociate_timeout"

#: ../../<reno.sphinxext stable/ussuri>:933
msgid "``[DEFAULT] fixed_range_v6``"
msgstr "``[DEFAULT] 고정 IP v6``"

#: ../../<reno.sphinxext stable/ussuri>:926
msgid "``[DEFAULT] flat_interface``"
msgstr "[DEFAULT] flat_interface"

#: ../../<reno.sphinxext stable/ussuri>:924
msgid "``[DEFAULT] flat_network_bridge``"
msgstr "[DEFAULT] flat_network_bridge"

#: ../../<reno.sphinxext stable/ussuri>:925
msgid "``[DEFAULT] flat_network_dns``"
msgstr "[DEFAULT] flat_network_dns → flat_network_dns집합"

#: ../../<reno.sphinxext stable/ussuri>:950
msgid "``[DEFAULT] floating_ip_dns_manager``"
msgstr ""
"``[DEFAULT] floating_ip_dns_manager`` \n"
"\n"
"``floating_ip_dns_manager``"

#: ../../<reno.sphinxext stable/ussuri>:944
msgid "``[DEFAULT] force_dhcp_release``"
msgstr "[DEFAULT] force_dhcp_release"

#: ../../<reno.sphinxext stable/ussuri>:982
msgid "``[DEFAULT] force_snat_range``"
msgstr "[DEFAULT] force_snat_range"

#: ../../<reno.sphinxext stable/ussuri>:986
msgid "``[DEFAULT] forward_bridge_interface``"
msgstr "[DEFAULT] 전환桥接 인터페이스"

#: ../../<reno.sphinxext stable/ussuri>:935
msgid "``[DEFAULT] gateway_v6``"
msgstr "[DEFAULT] 게이트웨이 V6"

#: ../../<reno.sphinxext stable/ussuri>:934
msgid "``[DEFAULT] gateway``"
msgstr "[DEFAULT] 게이트웨이"

#: ../../<reno.sphinxext stable/2025.2>:362
msgid "``[DEFAULT] glance_link_prefix``"
msgstr "[DEFAULT] glance_link_prefix"

#: ../../<reno.sphinxext stable/ussuri>:952
msgid "``[DEFAULT] instance_dns_domain``"
msgstr "``[DEFAULT] 인스턴스 DNS 도메인``"

#: ../../<reno.sphinxext stable/ussuri>:951
msgid "``[DEFAULT] instance_dns_manager``"
msgstr "[DEFAULT] 인스턴스 DNS 관리자"

#: ../../<reno.sphinxext stable/ussuri>:958
msgid "``[DEFAULT] iptables_bottom_regex``"
msgstr "[DEFAULT] iptables_bottom_regex → [DEFAULT] iptables_bottom_regex"

#: ../../<reno.sphinxext stable/ussuri>:959
msgid "``[DEFAULT] iptables_drop_action``"
msgstr "[DEFAULT] 집합 drop_action"

#: ../../<reno.sphinxext stable/ussuri>:957
msgid "``[DEFAULT] iptables_top_regex``"
msgstr "[DEFAULT] 집합 iptables_top_regex"

#: ../../<reno.sphinxext stable/ussuri>:954
msgid "``[DEFAULT] ipv6_backend``"
msgstr "[DEFAULT] ipv6_backend → [DEFAULT] ipv6 아키텍처"

#: ../../<reno.sphinxext stable/ussuri>:940
msgid "``[DEFAULT] l3_lib``"
msgstr "l3_lib"

#: ../../<reno.sphinxext stable/ussuri>:965
msgid "``[DEFAULT] ldap_dns_base_dn``"
msgstr "ldap_dns_base_dn → ldap: DNS 기본 DN"

#: ../../<reno.sphinxext stable/ussuri>:962
msgid "``[DEFAULT] ldap_dns_password``"
msgstr "ldap_dns_password"

#: ../../<reno.sphinxext stable/ussuri>:964
msgid "``[DEFAULT] ldap_dns_servers``"
msgstr "[DEFAULT] ldap_dns_servers집합"

#: ../../<reno.sphinxext stable/ussuri>:968
msgid "``[DEFAULT] ldap_dns_soa_expiry``"
msgstr "ldap_dns_soa_expiry"

#: ../../<reno.sphinxext stable/ussuri>:963
msgid "``[DEFAULT] ldap_dns_soa_hostmaster``"
msgstr "ldap_dns_soa_hostmaster"

#: ../../<reno.sphinxext stable/ussuri>:969
msgid "``[DEFAULT] ldap_dns_soa_minimum``"
msgstr "ldap_dns_soa_minimum"

#: ../../<reno.sphinxext stable/ussuri>:966
msgid "``[DEFAULT] ldap_dns_soa_refresh``"
msgstr "[DEFAULT] ldap_dns_soa_refresh → ldap_dns_soa_refresh"

#: ../../<reno.sphinxext stable/ussuri>:967
msgid "``[DEFAULT] ldap_dns_soa_retry``"
msgstr "ldap_dns_soa_retry"

#: ../../<reno.sphinxext stable/ussuri>:960
msgid "``[DEFAULT] ldap_dns_url``"
msgstr "``[DEFAULT] 집합 DNS URL``"

#: ../../<reno.sphinxext stable/ussuri>:961
msgid "``[DEFAULT] ldap_dns_user``"
msgstr "[DEFAULT] 집합 dns_user"

#: ../../<reno.sphinxext stable/ussuri>:983
msgid "``[DEFAULT] linuxnet_interface_driver``"
msgstr "[DEFAULT] linuxnet_interface_driver → [DEFAULT] linuxnet 인터페이스 드라이버"

#: ../../<reno.sphinxext stable/ussuri>:984
msgid "``[DEFAULT] linuxnet_ovs_integration_bridge``"
msgstr "linuxnet_ovs_integration_bridge"

#: ../../<reno.sphinxext unmaintained/xena>:532
msgid ""
"``[DEFAULT] max_instances_per_host`` (now ``[scheduler] "
"max_instances_per_host``)"
msgstr ""
"``[DEFAULT] max_instances_per_host`` (현재 ``[scheduler] "
"max_instances_per_host``)"

#: ../../<reno.sphinxext unmaintained/xena>:530
msgid ""
"``[DEFAULT] max_io_ops_per_host`` (now ``[scheduler] max_io_ops_per_host``)"
msgstr ""
"``[DEFAULT] max_io_ops_per_host`` (이제 ``[scheduler] max_io_ops_per_host`` )"

#: ../../<reno.sphinxext stable/2025.2>:360
msgid "``[DEFAULT] max_limit``"
msgstr "``[DEFAULT] 집합 limits``"

#: ../../<reno.sphinxext stable/2025.2>:357
msgid "``[DEFAULT] metadata_cache_expiration``"
msgstr "[DEFAULT] metadata_cache_expiration"

#: ../../<reno.sphinxext stable/ussuri>:955
msgid "``[DEFAULT] metadata_host``"
msgstr "[DEFAULT] metadata_host → [DEFAULT] metadata_host"

#: ../../<reno.sphinxext stable/ussuri>:956
msgid "``[DEFAULT] metadata_port``"
msgstr "[DEFAULT] metadata_port → [DEFAULT] metadata_port"

#: ../../<reno.sphinxext stable/ussuri>:943
msgid "``[DEFAULT] multi_host``"
msgstr "[DEFAULT] 집합 호스트"

#: ../../<reno.sphinxext stable/ussuri>:941
msgid "``[DEFAULT] network_driver``"
msgstr "[DEFAULT] 네트워크 드라이버"

#: ../../<reno.sphinxext stable/ussuri>:942
msgid "``[DEFAULT] network_manager``"
msgstr "[DEFAULT] 네트워크 관리자"

#: ../../<reno.sphinxext stable/ussuri>:932
msgid "``[DEFAULT] network_size``"
msgstr "[기본] 네트워크 크기"

#: ../../<reno.sphinxext stable/ussuri>:988
msgid "``[DEFAULT] networks_path``"
msgstr "[DEFAULT] 네트워크 경로"

#: ../../<reno.sphinxext stable/ussuri>:929
msgid "``[DEFAULT] num_networks``"
msgstr "``[DEFAULT] 집합 수``"

#: ../../<reno.sphinxext stable/ussuri>:987
msgid "``[DEFAULT] ovs_vsctl_timeout``"
msgstr "[DEFAULT] ovss_vsctl_timeout"

#: ../../<reno.sphinxext stable/ussuri>:989
msgid "``[DEFAULT] public_interface``"
msgstr "``[DEFAULT] 공공 인터페이스``"

#: ../../<reno.sphinxext stable/ussuri>:990
msgid "``[DEFAULT] routing_source_ip``"
msgstr "``[DEFAULT] 집합 IP``"

#: ../../<reno.sphinxext unmaintained/xena>:536
msgid ""
"``[DEFAULT] scheduler_available_filters`` (now ``[scheduler] "
"available_filters``)"
msgstr ""
"``[DEFAULT] scheduler_available_filters`` (이전 ``[scheduler] "
"available_filters``)"

#: ../../<reno.sphinxext unmaintained/xena>:528
msgid ""
"``[DEFAULT] scheduler_host_subset_size`` (now ``[scheduler] "
"host_subset_size``)"
msgstr "``[DEFAULT] 스케줄러 호스트 집합 크기`` (현재 ``[scheduler] 호스트 집합 크기``)"

#: ../../<reno.sphinxext unmaintained/xena>:527
msgid ""
"``[DEFAULT] scheduler_max_attempts`` (now ``[scheduler] max_attempts``)"
msgstr ""
"``[DEFAULT] scheduler_max_attempts`` (이전 ``[scheduler] max_attempts``)"

#: ../../<reno.sphinxext unmaintained/xena>:534
msgid ""
"``[DEFAULT] scheduler_tracks_instance_changes`` (now ``[scheduler] "
"track_instance_changes``)"
msgstr "[DEFAULT] 스케줄러 트랙스 인스턴스 변경"

#: ../../<reno.sphinxext stable/ussuri>:980
msgid "``[DEFAULT] send_arp_for_ha_count``"
msgstr "``[DEFAULT] HA 카운트에 ARP를 보냄``"

#: ../../<reno.sphinxext stable/ussuri>:979
msgid "``[DEFAULT] send_arp_for_ha``"
msgstr "``[DEFAULT] HA에 ARP를 보냄``"

#: ../../<reno.sphinxext stable/ussuri>:994
msgid "``[DEFAULT] share_dhcp_address``"
msgstr "[기본] share_dhcp_address"

#: ../../<reno.sphinxext stable/ussuri>:939
msgid "``[DEFAULT] teardown_unused_network_gateway``"
msgstr "[DEFAULT] 네트워크 사용하지 않는 게이트웨이 제거"

#: ../../<reno.sphinxext stable/ussuri>:945
msgid "``[DEFAULT] update_dns_entries``"
msgstr "[DEFAULT] dns_entries 업데이트"

#: ../../<reno.sphinxext stable/ussuri>:991
msgid "``[DEFAULT] use_ipv6``"
msgstr "``[DEFAULT] IPv6 사용``"

#: ../../<reno.sphinxext stable/ussuri>:974
msgid "``[DEFAULT] use_network_dns_servers``"
msgstr "``[DEFAULT] 네트워크 DNS 서버 사용``"

#: ../../<reno.sphinxext stable/2025.2>:363
msgid "``[DEFAULT] use_neutron_default_nets``"
msgstr "``[DEFAULT] 집합 사용``"

#: ../../<reno.sphinxext stable/ussuri>:948
msgid "``[DEFAULT] use_neutron``"
msgstr "``[DEFAULT] 집합``"

#: ../../<reno.sphinxext stable/ussuri>:985
msgid "``[DEFAULT] use_single_default_gateway``"
msgstr "``[DEFAULT] 사용 단일 게이트웨이``"

#: ../../<reno.sphinxext stable/2025.2>:355
msgid "``[DEFAULT] vendordata_dynamic_connect_timeout``"
msgstr "[DEFAULT] 파트너 데이터 동적 연결 시간 초과"

#: ../../<reno.sphinxext stable/2025.2>:356
msgid "``[DEFAULT] vendordata_dynamic_read_timeout``"
msgstr "[DEFAULT] 파트너 데이터 동적 읽기 타임아웃"

#: ../../<reno.sphinxext stable/2025.2>:354
msgid "``[DEFAULT] vendordata_dynamic_ssl_certfile``"
msgstr "``[DEFAULT] 파트너 데이터 동적 SSL 자격 증명서 파일``"

#: ../../<reno.sphinxext stable/2025.2>:353
msgid "``[DEFAULT] vendordata_dynamic_targets``"
msgstr "[DEFAULT] vendordata_dynamic_targets"

#: ../../<reno.sphinxext stable/2025.2>:359
msgid "``[DEFAULT] vendordata_jsonfile_path``"
msgstr "``[DEFAULT] 파트너 데이터 JSON 파일 경로``"

#: ../../<reno.sphinxext stable/2025.2>:352
msgid "``[DEFAULT] vendordata_providers``"
msgstr "[DEFAULT] vendeurdata_providers"

#: ../../<reno.sphinxext stable/ussuri>:927
msgid "``[DEFAULT] vlan_interface``"
msgstr "[DEFAULT] vlan_interface"

#: ../../<reno.sphinxext stable/ussuri>:928
msgid "``[DEFAULT] vlan_start``"
msgstr "[DEFAULT] vlan_start"

#: ../../<reno.sphinxext stable/ussuri>:930
msgid "``[DEFAULT] vpn_ip``"
msgstr "``[DEFAULT] 집합``"

#: ../../<reno.sphinxext stable/ussuri>:931
msgid "``[DEFAULT] vpn_start``"
msgstr "[DEFAULT] 집합 시작"

#: ../../<reno.sphinxext stable/queens>:209 stable/rocky>:354
#: stable/stein>:1404
msgid "``[DEFAULT]/compute_driver = libvirt``"
msgstr "[DEFAULT]/compute_driver = 집합"

#: ../../<reno.sphinxext stable/ussuri>:1143
msgid "``[DEFAULT]/default_schedule_zone=None``"
msgstr ""
"``[DEFAULT]/default_schedule_zone=None`` \n"
"\n"
"``[DEFAULT]/default_schedule_zone=None``"

#: ../../<reno.sphinxext stable/queens>:1574 stable/rocky>:1702
msgid "``[DEFAULT]/monkey_patch_modules``"
msgstr "[DEFAULT]/monkey_patch_modules"

#: ../../<reno.sphinxext stable/queens>:1573 stable/rocky>:1701
msgid "``[DEFAULT]/monkey_patch``"
msgstr "[DEFAULT]/monkey_patch"

#: ../../<reno.sphinxext stable/ussuri>:1097
msgid "``[DEFAULT]image_cache_manager_interval``"
msgstr "[DEFAULT]이미지 캐시 관리자 간격"

#: ../../<reno.sphinxext stable/ussuri>:1099
msgid "``[DEFAULT]image_cache_subdirectory_name``"
msgstr "[DEFAULT]이미지 캐시 서브 디렉터 이름"

#: ../../<reno.sphinxext stable/ussuri>:1101
msgid "``[DEFAULT]remove_unused_base_images``"
msgstr "[DEFAULT]집합 비용less base images"

#: ../../<reno.sphinxext stable/ussuri>:1103
msgid "``[DEFAULT]remove_unused_original_minimum_age_seconds``"
msgstr "[DEFAULT]집합 사용하지 않은 원본 최소อาย자 초단"

#: ../../<reno.sphinxext stable/rocky>:1491
msgid "``[api]/fping_path``"
msgstr "[API]/fping_path"

#: ../../<reno.sphinxext stable/queens>:1293
msgid "``[cinder]/catalog_info`` - Already defaults to Cinder v3"
msgstr "``[cinder]/catalog_info`` - 이미 시더 v3 버전으로 기본적으로 설정되어 있습니다."

#: ../../<reno.sphinxext stable/ussuri>:1142
msgid "``[cinder]/cross_az_attach=False``"
msgstr ""
"``[cinder]/cross_az_attach=False`` \n"
"\n"
"``[cinder] / cross_az_attach=False``"

#: ../../<reno.sphinxext stable/queens>:1294
msgid "``[cinder]/endpoint_template`` - Not used by default."
msgstr "[cinder]/endpoint_template - 기본적으로 사용되지 않습니다."

#: ../../<reno.sphinxext stable/2025.1>:383
msgid "``[compute]heal_instance_info_cache_interval`` now defaults to -1."
msgstr ""
"[compute] heal_instance_info_cache_interval \n"
"\n"
"이 현재 -1로 디폴트로 설정되었습니다."

#: ../../<reno.sphinxext stable/2024.1>:511
msgid "``[hyperv] config_drive_cdrom``"
msgstr "[하이퍼바이저] config_drive_cdrom"

#: ../../<reno.sphinxext stable/2024.1>:512
msgid "``[hyperv] config_drive_inject_password``"
msgstr "[하이퍼바이저] config_drive_inject_password"

#: ../../<reno.sphinxext stable/2024.1>:500
msgid "``[hyperv] dynamic_memory_ratio``"
msgstr "[하이퍼바이저] dynamic_memory_ratio"

#: ../../<reno.sphinxext stable/2024.1>:501
msgid "``[hyperv] enable_instance_metrics_collection``"
msgstr "[하이퍼바이저] enable_instance_metrics_collection"

#: ../../<reno.sphinxext stable/2024.1>:515
msgid "``[hyperv] enable_remotefx``"
msgstr "[하이퍼바이저] enable_remotefx"

#: ../../<reno.sphinxext stable/2024.1>:502
msgid "``[hyperv] instances_path_share``"
msgstr "[하이퍼바이저] 인스턴스_path_share"

#: ../../<reno.sphinxext stable/2024.1>:517
msgid "``[hyperv] iscsi_initiator_list``"
msgstr "[하이퍼바이저]는csi_initiator_list"

#: ../../<reno.sphinxext stable/2024.1>:503
msgid "``[hyperv] limit_cpu_features``"
msgstr "[하이퍼바이저] limit_cpu_features"

#: ../../<reno.sphinxext stable/2024.1>:504
msgid "``[hyperv] mounted_disk_query_retry_count``"
msgstr "[하이퍼바이저] mounted_disk_query_retry_count"

#: ../../<reno.sphinxext stable/2024.1>:505
msgid "``[hyperv] mounted_disk_query_retry_interval``"
msgstr "[하이퍼바이저] mounted_disk_query_retry_interval"

#: ../../<reno.sphinxext stable/2024.1>:506
msgid "``[hyperv] power_state_check_timeframe``"
msgstr "[하이퍼바이저] power_state_check_timeframe"

#: ../../<reno.sphinxext stable/2024.1>:507
msgid "``[hyperv] power_state_event_polling_interval``"
msgstr "[하이퍼바이저] power_state_event_polling_interval"

#: ../../<reno.sphinxext stable/2024.1>:508
msgid "``[hyperv] qemu_img_cmd``"
msgstr "[하이퍼바이저] qemu_img_cmd"

#: ../../<reno.sphinxext stable/2024.1>:516
msgid "``[hyperv] use_multipath_io``"
msgstr "[하이퍼바이저] use_multipath_io"

#: ../../<reno.sphinxext stable/2024.1>:513
msgid "``[hyperv] volume_attach_retry_count``"
msgstr "[하이퍼바이저] volume_attach_retry_count"

#: ../../<reno.sphinxext stable/2024.1>:514
msgid "``[hyperv] volume_attach_retry_interval``"
msgstr "[하이퍼바이저] volume_attach_retry_interval"

#: ../../<reno.sphinxext stable/2024.1>:509
msgid "``[hyperv] vswitch_name``"
msgstr "[하이퍼바이저] vswitch_name"

#: ../../<reno.sphinxext stable/2024.1>:510
msgid "``[hyperv] wait_soft_reboot_seconds``"
msgstr "[하이퍼바이저] wait_soft_reboot_seconds"

#: ../../<reno.sphinxext stable/ussuri>:1098
msgid "``[image_cache]manager_interval``"
msgstr "[이미지 캐시] 매니저 интер벌"

#: ../../<reno.sphinxext stable/ussuri>:1102
msgid "``[image_cache]remove_unused_base_images``"
msgstr "[이미지 캐시] 비용-free base images를 제거하십시오"

#: ../../<reno.sphinxext stable/ussuri>:1104
msgid "``[image_cache]remove_unused_original_minimum_age_seconds``"
msgstr "[이미지 캐시] unused_original_minimum_age_seconds"

#: ../../<reno.sphinxext stable/ussuri>:1106
msgid "``[image_cache]remove_unused_resized_minimum_age_seconds``"
msgstr "[이미지 캐시] unused_resized_minimum_age_seconds"

#: ../../<reno.sphinxext stable/ussuri>:1100
msgid "``[image_cache]subdirectory_name``"
msgstr "[이미지 캐시]subdirectory_name"

#: ../../<reno.sphinxext stable/rocky>:927
msgid ""
"``[libvirt]/file_backed_memory`` specifies the available capacity in MiB for"
" file backed memory, at the directory configured for ``memory_backing_dir`` "
"in libvirt's ``qemu.conf``. When enabled, the libvirt driver will report the"
" configured value for the total memory capacity of the node, and will report"
" used memory as the sum of all configured guest memory."
msgstr ""
"[libvirt]/file_backed_memory specifies the available capacity in MiB for "
"file backed memory, at the directory configured for memory_backing_dir in "
"libvirt's qemu.conf. When enabled, the libvirt driver will report the "
"configured value for the total memory capacity of the node, and will report "
"used memory as the sum of all configured guest memory."

#: ../../<reno.sphinxext stable/queens>:210 stable/rocky>:355
#: stable/stein>:1405
msgid "``[libvirt]/images_type = rbd``"
msgstr "[libvirt]/이미지 타입 = rbd"

#: ../../<reno.sphinxext origin/stable/ocata>:1441 stable/pike>:1516
msgid ""
"``[libvirt]/live_migration_progress_timeout`` has been deprecated as this "
"feature has been found not to work. See bug 1644248 for more details."
msgstr ""
"[libvirt]/live_migration_progress_timeout은 이 기능이 작동하지 않는다는 것을 발견하여弃용되었습니다. 더"
" 많은 정보는 bug 1644248를 참조하십시오."

#: ../../<reno.sphinxext unmaintained/wallaby>:866
msgid ""
"``[libvirt]/virt_type`` config option values other than ``kvm`` or ``qemu`` "
"may be impacted, like ``lxc``, where libguestfs was not previously required."
msgstr ""
"[libvirt]/virt_type config 옵션의 다른 giá치 이외에 `kvm` 또는 `qemu` 이외의 giá치가 영향을 받을 "
"수 있으며, `lxc`와 같은 경우, libguestfs가 이전에 필요하지 않았기 때문에."

#: ../../<reno.sphinxext stable/ussuri>:1105
msgid "``[libvirt]remove_unused_resized_minimum_age_seconds``"
msgstr "[libvirt] unused_resized_minimum_age_seconds"

#: ../../<reno.sphinxext stable/2023.2>:294
msgid "``[libvirt]tb_cache_size``"
msgstr "[libvirt] tb_cache_size"

#: ../../<reno.sphinxext stable/queens>:1584 stable/rocky>:1703
msgid "``[notifications]/default_publisher_id``"
msgstr ""
"``[notifications]/default_publisher_id`` \n"
"\n"
"``[알림]/default_publisher_id``"

#: ../../<reno.sphinxext stable/2024.1>:518
msgid "``[rdp] enabled``"
msgstr "[rdp] 활성화"

#: ../../<reno.sphinxext stable/2024.1>:519
msgid "``[rdp] html5_proxy_base_url``"
msgstr ""
"``[rdp] html5_proxy_base_url`` \n"
"\n"
"``[rdp] html5_proxy_base_url`` \n"
"\n"
"``[rdp] html5_proxy_base_url`` \n"
"\n"
"``[rdp] html5_proxy_base_url``"

#: ../../<reno.sphinxext stable/rocky>:1845
msgid "``[spice] keymap``"
msgstr "[스피스] 키맵"

#: ../../<reno.sphinxext stable/ussuri>:836
msgid "``[upgrade_levels] console``"
msgstr "``[업그레이드 수준] console``"

#: ../../<reno.sphinxext stable/train>:1160
msgid "``[upgrade_levels] consoleauth``"
msgstr "[업그레이드 수준] consoleauth"

#: ../../<reno.sphinxext stable/ussuri>:995
msgid "``[upgrade_levels] network``"
msgstr "``[업그레이드 수준] 네트워크``"

#: ../../<reno.sphinxext stable/ussuri>:996
msgid "``[vmware] vlan_interface``"
msgstr "[vmware] vlan_interface → [vmware] vlan 인터페이스"

#: ../../<reno.sphinxext stable/rocky>:1859
msgid "``[vmware] vnc_keymap``"
msgstr "[vmware] vnc_keymap"

#: ../../<reno.sphinxext stable/rocky>:1844
msgid "``[vnc] keymap``"
msgstr "``[vnc] 키맵``"

#: ../../<reno.sphinxext stable/ussuri>:1007
msgid "``[vnc] xvpvncproxy_base_url``"
msgstr ""
"``[vnc] xvpvncproxy_base_url`` \n"
"\n"
"``[vnc] xvpvncproxy_base_url``"

#: ../../<reno.sphinxext stable/ussuri>:1008
msgid "``[vnc] xvpvncproxy_host``"
msgstr "``[vnc] xvpvncproxy_host``"

#: ../../<reno.sphinxext stable/ussuri>:1009
msgid "``[vnc] xvpvncproxy_port``"
msgstr ""
"``[vnc] xvpvncproxy_port`` \n"
"\n"
"``[vnc] xvpvncproxy_port`` \n"
"\n"
"``[vnc] xvpvncproxy_port``"

#: ../../<reno.sphinxext stable/queens>:1467
msgid "``[vnc]vncserver_listen`` (now ``[vnc]server_listen``)"
msgstr "``[vnc]서버_리스트닝``"

#: ../../<reno.sphinxext stable/queens>:1468
msgid ""
"``[vnc]vncserver_proxyclient_address`` (now "
"``[vnc]server_proxyclient_address``)"
msgstr "``[아치]아치서버_proxy클라이언트 주소``"

#: ../../<reno.sphinxext stable/train>:1161
msgid "``[workarounds] enable_consoleauth``"
msgstr "[workarounds] enable_consoleauth"

#: ../../<reno.sphinxext unmaintained/wallaby>:221 unmaintained/xena>:464
msgid "``[workarounds]libvirt_disable_apic``"
msgstr "[workarounds]libvirt_disable_apic"

#: ../../<reno.sphinxext unmaintained/victoria>:673
msgid "``[xenserver] agent_path``"
msgstr "[xenserver].agent_path"

#: ../../<reno.sphinxext unmaintained/victoria>:672
msgid "``[xenserver] agent_resetnetwork_timeout``"
msgstr "[xenserver] 네트워크 리셋 시간 초과"

#: ../../<reno.sphinxext unmaintained/victoria>:670
msgid "``[xenserver] agent_timeout``"
msgstr "[xenserver].agent_timeout"

#: ../../<reno.sphinxext unmaintained/victoria>:671
msgid "``[xenserver] agent_version_timeout``"
msgstr "[xenserver].agent_version_timeout"

#: ../../<reno.sphinxext unmaintained/victoria>:681
msgid "``[xenserver] block_device_creation_timeout``"
msgstr "[xenserver] 블록 디스크 생성 시간 초과"

#: ../../<reno.sphinxext unmaintained/victoria>:678
msgid "``[xenserver] cache_images``"
msgstr "[xenserver] 캐시 이미지"

#: ../../<reno.sphinxext unmaintained/victoria>:693
msgid "``[xenserver] check_host``"
msgstr "[xenserver] host 확인"

#: ../../<reno.sphinxext unmaintained/victoria>:677
msgid "``[xenserver] connection_concurrent``"
msgstr "[xenserver] 접속_동시"

#: ../../<reno.sphinxext unmaintained/victoria>:691
msgid "``[xenserver] connection_password``"
msgstr "[xenserver] 접속 시 mật khẩu"

#: ../../<reno.sphinxext unmaintained/victoria>:689
msgid "``[xenserver] connection_url``"
msgstr "[xenserver] 접속 URL"

#: ../../<reno.sphinxext unmaintained/victoria>:690
msgid "``[xenserver] connection_username``"
msgstr "[xenserver] 접속 사용자 이름"

#: ../../<reno.sphinxext unmaintained/victoria>:705
msgid "``[xenserver] console_public_hostname``"
msgstr ""
"[xenserver] console_public_hostname → [xenserver] console_public_hostname"

#: ../../<reno.sphinxext unmaintained/victoria>:680
msgid "``[xenserver] default_os_type``"
msgstr "[xenserver] 기본 운영 체제 유형"

#: ../../<reno.sphinxext unmaintained/victoria>:674
msgid "``[xenserver] disable_agent``"
msgstr "[xenserver] 집합 비활성화"

#: ../../<reno.sphinxext unmaintained/victoria>:679
msgid "``[xenserver] image_compression_level``"
msgstr "[xenserver] image_compression_level"

#: ../../<reno.sphinxext unmaintained/victoria>:701
msgid "``[xenserver] image_handler``"
msgstr "[xenserver] image_handler → [xenserver] image_handler"

#: ../../<reno.sphinxext unmaintained/victoria>:700
msgid "``[xenserver] image_upload_handler``"
msgstr "[xenserver] image_upload_handler → [xenserver] image_upload_handler"

#: ../../<reno.sphinxext unmaintained/victoria>:698
msgid "``[xenserver] independent_compute``"
msgstr "[xenserver] 독립적 컴퓨팅"

#: ../../<reno.sphinxext unmaintained/victoria>:702
msgid "``[xenserver] introduce_vdi_retry_wait``"
msgstr "[xenserver] introduce_vdi_retry_wait"

#: ../../<reno.sphinxext unmaintained/victoria>:687
msgid "``[xenserver] ipxe_boot_menu_url``"
msgstr "[xenserver] ipxe_boot_menu_url → [xenserver] ipxe 부트 메뉴 URL"

#: ../../<reno.sphinxext unmaintained/victoria>:688
msgid "``[xenserver] ipxe_mkisofs_cmd``"
msgstr "[xenserver] ipxe_mkisofs_cmd → [xenserver] ipxe_mkisofs 명령"

#: ../../<reno.sphinxext unmaintained/victoria>:686
msgid "``[xenserver] ipxe_network_name``"
msgstr "[xenserver] IPXE 네트워크 이름"

#: ../../<reno.sphinxext unmaintained/victoria>:676
msgid "``[xenserver] login_timeout``"
msgstr "[xenserver] 로그인 시간 초과"

#: ../../<reno.sphinxext unmaintained/victoria>:682
msgid "``[xenserver] max_kernel_ramdisk_size``"
msgstr ""
"[xenserver] max_kernel_ramdisk_size → [xenserver] max_kernel_ramdisk_size"

#: ../../<reno.sphinxext unmaintained/victoria>:685
msgid "``[xenserver] num_vbd_unplug_retries``"
msgstr ""
"[xenserver] num_vbd_unplug_retries → [xenserver] num_vbd_unplug_retries"

#: ../../<reno.sphinxext unmaintained/victoria>:703
msgid "``[xenserver] ovs_integration_bridge``"
msgstr ""
"[xenserver] ovs_integration_bridge \n"
"\n"
"* xenserver : XenServer\n"
"* ovs_integration_bridge : ovs integration bridge"

#: ../../<reno.sphinxext unmaintained/victoria>:699
msgid "``[xenserver] running_timeout``"
msgstr "[xenserver] running_timeout → [xenserver] running_timeout"

#: ../../<reno.sphinxext unmaintained/victoria>:684
msgid "``[xenserver] sparse_copy``"
msgstr "[xenserver] 집합 복사"

#: ../../<reno.sphinxext unmaintained/victoria>:695
msgid "``[xenserver] sr_base_path``"
msgstr "[xenserver] sr_base_path → [xenserver] sr_base_path"

#: ../../<reno.sphinxext unmaintained/victoria>:683
msgid "``[xenserver] sr_matching_filter``"
msgstr "[xenserver] sr_matching_filter → [xenserver] sr_집합 매칭 필터"

#: ../../<reno.sphinxext unmaintained/victoria>:696
msgid "``[xenserver] target_host``"
msgstr "[xenserver] 집합 호스트"

#: ../../<reno.sphinxext unmaintained/victoria>:697
msgid "``[xenserver] target_port``"
msgstr "[xenserver] target_port → [xenserver] target_port"

#: ../../<reno.sphinxext unmaintained/victoria>:675
msgid "``[xenserver] use_agent_default``"
msgstr "[xenserver] use_agent_default"

#: ../../<reno.sphinxext unmaintained/victoria>:704
msgid "``[xenserver] use_join_force``"
msgstr "[xenserver] use_join_force"

#: ../../<reno.sphinxext unmaintained/victoria>:694
msgid "``[xenserver] vhd_coalesce_max_attempts``"
msgstr ""
"[xenserver] vhd_coalesce_max_attempts → [xenserver] "
"vhd_coalesce_max_attempts"

#: ../../<reno.sphinxext unmaintained/victoria>:692
msgid "``[xenserver] vhd_coalesce_poll_interval``"
msgstr ""
"[xenserver] vhd_coalesce_poll_interval → [xenserver] "
"vhd_coalesce_poll_interval"

#: ../../<reno.sphinxext stable/ussuri>:1010
msgid "``[xvp] console_xvp_conf_template``"
msgstr "[xvp] console_xvp_conf_template"

#: ../../<reno.sphinxext stable/ussuri>:1011
msgid "``[xvp] console_xvp_conf``"
msgstr "[xvp] console_xvp_conf"

#: ../../<reno.sphinxext stable/ussuri>:1012
msgid "``[xvp] console_xvp_log``"
msgstr "[xvp] console_xvp_log"

#: ../../<reno.sphinxext stable/ussuri>:1013
msgid "``[xvp] console_xvp_multiplex_port``"
msgstr "[xvp] console_xvp_multiplex_port"

#: ../../<reno.sphinxext stable/ussuri>:1014
msgid "``[xvp] console_xvp_pid``"
msgstr "[xvp] console_xvp_pid"

#: ../../<reno.sphinxext stable/ussuri>:503
msgid "``accel``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:1426
msgid "``account``"
msgstr "집합"

#: ../../<reno.sphinxext stable/2025.2>:81
msgid "``admin``"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/zed>:322
msgid "``admin`` (Legacy admin)"
msgstr "``기상 관리자``"

#: ../../<reno.sphinxext stable/queens>:1429
msgid "``agent``"
msgstr "집합"

#: ../../<reno.sphinxext stable/rocky>:1022
msgid "``aggregate.updatemetadata``"
msgstr "집합.updatemetadata"

#: ../../<reno.sphinxext stable/rocky>:1023
msgid "``aggregate.updateprop``"
msgstr "집합.updateprop"

#: ../../<reno.sphinxext origin/stable/ocata>:1066
msgid "``aggregate_image_properties_isolation_namespace``"
msgstr "집합이미지 속성 분리 네임 스페이스"

#: ../../<reno.sphinxext origin/stable/ocata>:1067
msgid "``aggregate_image_properties_isolation_separator``"
msgstr "집합이미지 속성 분리.separator"

#: ../../<reno.sphinxext stable/ussuri>:504
msgid "``aggregate_instance_extra_specs``"
msgstr "집합 인스턴스 추가 속성"

#: ../../<reno.sphinxext origin/stable/ocata>:1006 stable/rocky>:1667
msgid "``allow_instance_snapshots``"
msgstr "allow_instance_snapshots"

#: ../../<reno.sphinxext stable/pike>:1565
msgid "``allow_same_net_traffic``"
msgstr "allow_same_net_traffic"

#: ../../<reno.sphinxext stable/pike>:1087
msgid "``api_endpoint`` in the ``ironic`` group"
msgstr "``ironic`` 그룹의 ``api_endpoint``"

#: ../../<reno.sphinxext stable/queens>:840
msgid "``attach_interface``"
msgstr "``인터페이스 연결``"

#: ../../<reno.sphinxext stable/queens>:842
msgid "``attach_volume``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:1114
msgid "``auth_schemes``"
msgstr "``auth_schemes``"

#: ../../<reno.sphinxext origin/stable/ocata>:993
msgid "``auth_strategy``"
msgstr "auth_strategy"

#: ../../<reno.sphinxext origin/stable/ocata>:1473
msgid "``auto_assign_floating_ip``"
msgstr "``가용 IP 할당``"

#: ../../<reno.sphinxext stable/train>:1101
msgid "``bandwidth_update_interval``"
msgstr "집합更新주기"

#: ../../<reno.sphinxext origin/stable/ocata>:1312
msgid "``barbican.catalog_info``"
msgstr "``barbican.catalog_info``"

#: ../../<reno.sphinxext origin/stable/ocata>:1313
msgid "``barbican.endpoint_template``"
msgstr "``barbican.endpoint_template``"

#: ../../<reno.sphinxext origin/stable/ocata>:1314
msgid "``barbican.os_region_name``"
msgstr "``barbican.os_region_name`` → ``barbican.os_region_name``"

#: ../../<reno.sphinxext origin/stable/ocata>:1054
msgid ""
"``baremetal_scheduler_default_filters`` (now ``baremetal_enabled_filters``)"
msgstr "baremetal_scheduler_default_filters (현재 baremetal_enabled_filters)"

#: ../../<reno.sphinxext stable/rocky>:1644
msgid "``ca_file``"
msgstr "``ca_file``"

#: ../../<reno.sphinxext stable/pike>:1197
msgid "``ca_file`` now called ``cafile``"
msgstr "``ca_file``은 ``cafile``로 이름이 변경되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1648
msgid "``ca_path``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1097
msgid "``call_timeout``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1096 stable/ussuri>:505
msgid "``capabilities``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1099
msgid "``cell_type``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1116
msgid "``cells_config``"
msgstr "집합 구성"

#: ../../<reno.sphinxext stable/train>:1088
msgid "``cells_scheduler_filter:DifferentCellFilter``"
msgstr "`세ลล 스케줄러 필터:DiferentCellFilter`"

#: ../../<reno.sphinxext stable/train>:1089
msgid "``cells_scheduler_filter:TargetCellFilter``"
msgstr "`세ลล 스케줄러 필터: 타겟 세ลล 필터`"

#: ../../<reno.sphinxext stable/train>:1121
msgid "``cells``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1198
msgid "``cert_file`` now called ``certfile``"
msgstr "``cert_file``이 ``certfile``로 이름이 변경되었습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1352
msgid "``cert_manager``"
msgstr "집합 관리자"

#: ../../<reno.sphinxext stable/rocky>:1909
msgid ""
"``cert`` - The ``nova-cert`` service was removed in the 16.0.0 Pike release "
"so this option is no longer used."
msgstr ""
"``cert`` - nova-cert 서비스는 16.0.0 피크 릴리즈에서 제거되었습니다. 따라서 이 옵션은 더 이상 사용되지 않습니다."

#: ../../<reno.sphinxext unmaintained/yoga>:682
msgid "``class:MEMORY_MB``"
msgstr "``클래스: 메모리 MB``"

#: ../../<reno.sphinxext unmaintained/yoga>:680
msgid "``class:VCPU``"
msgstr "``클래스: VCPU``"

#: ../../<reno.sphinxext origin/stable/ocata>:1461
msgid "``cnt_vpn_clients``"
msgstr "cnt_vpn_clients → cnt_vpn_customers"

#: ../../<reno.sphinxext stable/train>:570
msgid "``compute:server:topology:host:index``"
msgstr "``집합:서버:아키텍처:호스트:인덱스``"

#: ../../<reno.sphinxext stable/train>:565
msgid "``compute:server:topology:index``"
msgstr "``집합:서버:아키텍처:인덱스``"

#: ../../<reno.sphinxext origin/stable/ocata>:1004
msgid "``compute_link_prefix`` (was ``osapi_compute_link_prefix``)"
msgstr "``compute_link_prefix`` (was ``osapi_compute_link_prefix``)"

#: ../../<reno.sphinxext origin/stable/ocata>:1349
msgid "``compute_manager``"
msgstr "`compute_manager`"

#: ../../<reno.sphinxext stable/pike>:1363
msgid "``compute_topic``"
msgstr "``아키텍처``"

#: ../../<reno.sphinxext origin/stable/ocata>:1354
msgid "``conductor.manager``"
msgstr "conductor.manager → conductor.manager"

#: ../../<reno.sphinxext origin/stable/ocata>:995
msgid "``config_drive_skip_versions``"
msgstr "`config_drive_skip_versions`"

#: ../../<reno.sphinxext origin/stable/ocata>:1350
msgid "``console_manager``"
msgstr "`console_manager`"

#: ../../<reno.sphinxext origin/stable/ocata>:1212
msgid ""
"``console_public_hostname`` console options under the ``DEFAULT`` group have"
" been moved to the ``xenserver`` group."
msgstr ""
"``console_public_hostname`` console options under the ``DEFAULT`` group가 "
"``xenserver`` group로 이동되었습니다."

#: ../../<reno.sphinxext stable/pike>:1364
msgid "``console_topic``"
msgstr "`console_topic`"

#: ../../<reno.sphinxext origin/stable/ocata>:1351
msgid "``consoleauth_manager``"
msgstr "``consoleauth_manager``"

#: ../../<reno.sphinxext stable/pike>:1365
msgid "``consoleauth_topic``"
msgstr "``consoleauth_topic``"

#: ../../<reno.sphinxext stable/rocky>:1911
msgid ""
"``consoleauth`` - The ``nova-consoleauth`` service was deprecated in the "
"18.0.0 Rocky release and will be removed in an upcoming release."
msgstr ""
"`consoleauth` - `nova-consoleauth` 서비스는 18.0.0 로키 릴리스에서弃용되었으며 향후 릴리스에서 제거될 "
"예정입니다."

#: ../../<reno.sphinxext stable/queens>:848
msgid "``createBackup``"
msgstr "집합 복사"

#: ../../<reno.sphinxext stable/queens>:849
msgid "``createImage``"
msgstr "`createImage`"

#: ../../<reno.sphinxext origin/stable/ocata>:1463
msgid "``create_unique_mac_address_attempts``"
msgstr "집합 MAC 주소 생성 시도"

#: ../../<reno.sphinxext stable/rocky>:1646
msgid "``crl_file``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1115
msgid "``db_check_interval``"
msgstr "db_check_interval"

#: ../../<reno.sphinxext stable/pike>:1524
msgid ""
"``default_floating_pool`` (neutron users should use the "
"``neutron.default_floating_pool``)"
msgstr ""
"``default_floating_pool`` (네트로恩 사용자는 ``neutron.default_floating_pool``를 사용해야"
" 함)"

#: ../../<reno.sphinxext origin/stable/ocata>:1222
msgid "``default_level`` (was ``default_notification_level``)"
msgstr "default_level (was default_notification_level)"

#: ../../<reno.sphinxext origin/stable/ocata>:1223
msgid "``default_publisher_id``"
msgstr "집합 ID"

#: ../../<reno.sphinxext stable/stein>:1270
msgid "``defer_iptables_apply``"
msgstr ""
"``defer_iptables_apply`` \n"
"\n"
"``defer_iptables_apply`` \n"
"\n"
"``defer_iptables_apply``"

#: ../../<reno.sphinxext stable/queens>:841
msgid "``detach_interface``"
msgstr "`` detach_interface``"

#: ../../<reno.sphinxext stable/queens>:843
msgid "``detach_volume``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1471
msgid "``dhcp_domain``"
msgstr "dhcp_domain"

#: ../../<reno.sphinxext stable/pike>:1545
msgid "``dhcp_lease_time``"
msgstr "dhcp_lease_time → dhcp_lease_time"

#: ../../<reno.sphinxext stable/pike>:1543
msgid "``dhcpbridge_flagfile``"
msgstr "dhcpbridge_flagfile"

#: ../../<reno.sphinxext stable/pike>:1544
msgid "``dhcpbridge``"
msgstr "dhcpbridge"

#: ../../<reno.sphinxext stable/rocky>:1401
msgid "``direct_vhd``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1059
msgid "``disk_weight_multipler``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1554
msgid "``dmz_cidr``"
msgstr "집합 CIDR"

#: ../../<reno.sphinxext stable/pike>:1546
msgid "``dns_server``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1470
msgid "``dns_update_periodic_interval``"
msgstr "집합 주기"

#: ../../<reno.sphinxext stable/pike>:1548
msgid "``dnsmasq_config_file``"
msgstr "dnsmasq_config_file"

#: ../../<reno.sphinxext origin/stable/ocata>:1318
msgid ""
"``driver`` configuration option has been removed from ``cells`` group. There"
" is only one possible driver for cells (CellsRPCDriver), which makes this "
"option redundant."
msgstr ""
"드라이버 설정 옵션을 `cells` 그룹에서 제거했습니다. `cells`는 단일 드라이버만 사용할 수 있기 때문에 이 옵션은 비용이 "
"지출되지 않습니다. (CellsRPCDriver)"

#: ../../<reno.sphinxext stable/pike>:1549
msgid "``ebtables_exec_attempts``"
msgstr "``ebtables_exec_attempts`` → 집합: ebtables_exec_attempts"

#: ../../<reno.sphinxext stable/pike>:1550
msgid "``ebtables_retry_interval``"
msgstr "``ebtables_retry_interval``"

#: ../../<reno.sphinxext origin/stable/ocata>:1011
msgid "``enable_instance_password``"
msgstr "``인스턴스 사용 시 암호``"

#: ../../<reno.sphinxext stable/ussuri>:898
msgid "``enable_network_quota``"
msgstr "집합 제한"

#: ../../<reno.sphinxext unmaintained/xena>:592
msgid ""
"``enabled_vgpu_types`` is now deprecated in favour of ``enabled_mdev_types``"
msgstr "`enabled_vgpu_types`는 `enabled_mdev_types`로 대체되었습니다."

#: ../../<reno.sphinxext stable/train>:1094
msgid "``enabled``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1551
msgid "``fake_network``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1328
msgid ""
"``fatal_exception_format_errors`` configuration option has been removed, as "
"it was only used for internal testing."
msgstr ""
"`fatal_exception_format_errors` 설정 옵션은 내부 테스트를 위해만 사용되었습니다. 따라서 이 옵션은 "
"제거되었습니다."

#: ../../<reno.sphinxext stable/queens>:917
msgid "``file_event_handler_interval``"
msgstr "``집합``"

#: ../../<reno.sphinxext stable/queens>:916
msgid "``file_event_handler``"
msgstr "`file_event_handler`"

#: ../../<reno.sphinxext stable/pike>:1527
msgid "``firewall_driver``"
msgstr "``firewall_driver`` → ``防화 tường lửa 드라이버``"

#: ../../<reno.sphinxext origin/stable/ocata>:1462
msgid "``fixed_ip_disassociate_timeout``"
msgstr "집합: fixed_ip_disassociate_timeout"

#: ../../<reno.sphinxext origin/stable/ocata>:1481 stable/pike>:1763
msgid "``fixed_ips``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1458
msgid "``fixed_range_v6``"
msgstr "집합_v6"

#: ../../<reno.sphinxext origin/stable/ocata>:1451
msgid "``flat_interface``"
msgstr "``flat_interface``"

#: ../../<reno.sphinxext origin/stable/ocata>:1449
msgid "``flat_network_bridge``"
msgstr "집합 네트워크 브리지"

#: ../../<reno.sphinxext origin/stable/ocata>:1450
msgid "``flat_network_dns``"
msgstr "집합 네트워크 DNS"

#: ../../<reno.sphinxext origin/stable/ocata>:1474
msgid "``floating_ip_dns_manager``"
msgstr "``floating_ip_dns_manager``"

#: ../../<reno.sphinxext origin/stable/ocata>:1480 stable/pike>:1764
msgid "``floating_ips``"
msgstr "``floating_ips``"

#: ../../<reno.sphinxext stable/ussuri>:906
msgid "``floating``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1468
msgid "``force_dhcp_release``"
msgstr "집합 해제"

#: ../../<reno.sphinxext stable/pike>:1555
msgid "``force_snat_range``"
msgstr "``집합 Snat 범위``"

#: ../../<reno.sphinxext stable/pike>:1559
msgid "``forward_bridge_interface``"
msgstr "``forward_bridge_interface``"

#: ../../<reno.sphinxext origin/stable/ocata>:1008
msgid "``fping_path``"
msgstr "``fping_path``"

#: ../../<reno.sphinxext origin/stable/ocata>:1460
msgid "``gateway_v6``"
msgstr "``게이트웨이 V6``"

#: ../../<reno.sphinxext origin/stable/ocata>:1459
msgid "``gateway``"
msgstr "게이트웨이"

#: ../../<reno.sphinxext origin/stable/ocata>:1005
msgid "``glance_link_prefix`` (was ``osapi_glance_link_prefix``)"
msgstr "``glance_link_prefix`` (was ``osapi_glance_link_prefix``)"

#: ../../<reno.sphinxext stable/pike>:1324
msgid "``glusterfs_mount_point_base``"
msgstr "``집합FS 마운트 포인트 기본``"

#: ../../<reno.sphinxext origin/stable/ocata>:1007
msgid ""
"``hide_server_address_states`` (was ``osapi_hide_server_address_states``)"
msgstr "`hide_server_address_states` (was `osapi_hide_server_address_states`)"

#: ../../<reno.sphinxext stable/pike>:305 stable/queens>:1204
msgid ""
"``host-passthrough`` mode will completely break live migration, *unless* all"
" the Compute nodes (running libvirtd) have *identical* CPUs."
msgstr ""
"host-passthrough 모드가 완전히 live migration을 완전히 파괴할 수는 *만약* 모든 Compute 노드 "
"(libvirtd를 실행하는 노드)가 *identical* CPU를 가지고 있으면 *만약*입니다."

#: ../../<reno.sphinxext stable/queens>:1428
msgid "``host``"
msgstr "호스트"

#: ../../<reno.sphinxext stable/pike>:1089
msgid "``html5_proxy_base_url`` in the ``rdp`` group"
msgstr "``html5_proxy_base_url`` in the ``rdp`` group"

#: ../../<reno.sphinxext unmaintained/yoga>:714
msgid "``hw_cdrom_bus``"
msgstr "`hw_cdrom_bus`"

#: ../../<reno.sphinxext unmaintained/yoga>:715
msgid "``hw_disk_bus``"
msgstr "`hw_disk_bus`"

#: ../../<reno.sphinxext unmaintained/yoga>:716
msgid "``hw_input_bus``"
msgstr "``hw_input_bus``"

#: ../../<reno.sphinxext unmaintained/yoga>:717
msgid "``hw_pointer_model``"
msgstr "`hw_pointer_model`"

#: ../../<reno.sphinxext stable/train>:889
msgid "``hw_qemu_guest_agent=yes``"
msgstr ""
"``hw_qemu_guest_agent=yes`` \n"
"\n"
"``hw_qemu_guest_agent=yes``"

#: ../../<reno.sphinxext stable/ussuri>:507
msgid "``hw_rng``"
msgstr "hw_rng"

#: ../../<reno.sphinxext stable/train>:395 stable/train>:886
#: stable/ussuri>:1191
msgid ""
"``hw_scsi_model=virtio-scsi`` and either ``hw_disk_bus=scsi`` or "
"``hw_cdrom_bus=scsi``"
msgstr ""
"``hw_scsi_model=집합virtio-scsi`` and either ``hw_disk_bus=SCSI`` or "
"``hw_cdrom_bus=SCSI``"

#: ../../<reno.sphinxext stable/train>:397 stable/train>:888
#: stable/ussuri>:1193
msgid "``hw_video_model=virtio``"
msgstr "``hw_video_model=집합``"

#: ../../<reno.sphinxext unmaintained/yoga>:718
msgid "``hw_video_model``"
msgstr "``hw_video_model``"

#: ../../<reno.sphinxext stable/ussuri>:508
msgid "``hw_video``"
msgstr "``hw_video``"

#: ../../<reno.sphinxext unmaintained/yoga>:719
msgid "``hw_vif_model``"
msgstr "``hw_vif_model``"

#: ../../<reno.sphinxext stable/ussuri>:506
msgid "``hw``"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/2023.1>:329
msgid "``image_compression``"
msgstr "집합 압축"

#: ../../<reno.sphinxext stable/pike>:1386
msgid "``image_file_url.FS.id``"
msgstr "``이미지 파일 URL.FS.id``"

#: ../../<reno.sphinxext stable/pike>:1387
msgid "``image_file_url.FS.mountpoint``"
msgstr "`이미지 파일 URL.FS.mountpoint`"

#: ../../<reno.sphinxext stable/pike>:1385
msgid "``image_file_url.filesystems``"
msgstr "`이미지 파일 URL.filesystems`"

#: ../../<reno.sphinxext stable/stein>:493
msgid "``instance.create.end``"
msgstr "``인스턴스.create.end``"

#: ../../<reno.sphinxext stable/stein>:494
msgid "``instance.create.error``"
msgstr "``인스턴스 생성 오류``"

#: ../../<reno.sphinxext stable/stein>:492
msgid "``instance.create.start``"
msgstr "`인스턴스.create.start`"

#: ../../<reno.sphinxext stable/pike>:663
msgid "``instance.create``"
msgstr "`인스턴스.create`"

#: ../../<reno.sphinxext stable/pike>:664
msgid "``instance.delete``"
msgstr "`인스턴스 삭제`"

#: ../../<reno.sphinxext stable/rocky>:1024
msgid "``instance.exists``"
msgstr "`instance.exists`"

#: ../../<reno.sphinxext stable/rocky>:1026
msgid "``instance.live.migration.force.complete``"
msgstr "``인스턴스.live.migration.force.complete``"

#: ../../<reno.sphinxext stable/rocky>:1025
msgid "``instance.live_migration._post``"
msgstr "`인스턴스.live_migration._post`"

#: ../../<reno.sphinxext stable/rocky>:1027
msgid "``instance.live_migration.post.dest``"
msgstr "``인스턴스.live_migration.post.dest``"

#: ../../<reno.sphinxext stable/rocky>:1028
msgid "``instance.live_migration.rollback.dest``"
msgstr "``인스턴스.live_migration.rollback.dest``"

#: ../../<reno.sphinxext stable/pike>:666
msgid "``instance.pause``"
msgstr "`instance.pause`"

#: ../../<reno.sphinxext stable/rocky>:1029
msgid "``instance.rebuild.scheduled``"
msgstr "`인스턴스.rebuild.scheduled`"

#: ../../<reno.sphinxext stable/pike>:665
msgid "``instance.resize``"
msgstr "`인스턴스.리ไซ즈`"

#: ../../<reno.sphinxext origin/stable/ocata>:76 stable/pike>:1860
msgid ""
"``instance.shutdown.end`` versioned notification will have an empty "
"``ip_addresses`` field since the network resources associated with the "
"instance are deallocated before this notification is sent, which is actually"
" more accurate. Consumers should rely on the instance.shutdown.start "
"notification if they need the network information for the instance when it "
"is being deleted."
msgstr ""
"``instance.shutdown.end`` 버전의 알림은 인스턴스의 네트워크 리소스를 할당하지 않은 상태에서 알림이 전송되기 때문에 "
"``ip_addresses`` 필드는 비어 있을 것이고, 실제로 더แม่น진다. 인스턴스가 삭제되는 동안 네트워크 정보가 필요할 때는 "
"``instance.shutdown.start`` 알림을 사용해야 한다."

#: ../../<reno.sphinxext origin/stable/ocata>:1476
msgid "``instance_dns_domain``"
msgstr "``인스턴스 DNS 도메인``"

#: ../../<reno.sphinxext origin/stable/ocata>:1475
msgid "``instance_dns_manager``"
msgstr "``인스턴스 DNS 매니저``"

#: ../../<reno.sphinxext stable/train>:1107
msgid "``instance_update_num_instances``"
msgstr "`instance_update_num_instances`"

#: ../../<reno.sphinxext stable/train>:1102
msgid "``instance_update_sync_database_limit``"
msgstr "`인스턴스 업데이트 및 동기화 데이터베이스 제한`"

#: ../../<reno.sphinxext stable/train>:1106
msgid "``instance_updated_at_threshold``"
msgstr "``인스턴스 업데이트 시점 임계치``"

#: ../../<reno.sphinxext stable/train>:1122
msgid "``intercell``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1060
msgid "``io_ops_weight_multipler``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1531
msgid "``iptables_bottom_regex``"
msgstr "iptables_bottom_regex"

#: ../../<reno.sphinxext stable/pike>:1532
msgid "``iptables_drop_action``"
msgstr "iptables_drop_action → iptables drop 액션"

#: ../../<reno.sphinxext stable/pike>:1530
msgid "``iptables_top_regex``"
msgstr "`iptables_top_regex`"

#: ../../<reno.sphinxext stable/pike>:1526
msgid "``ipv6_backend``"
msgstr "``아키텍처``"

#: ../../<reno.sphinxext origin/stable/ocata>:1064
msgid "``isolated_hosts``"
msgstr "집합 호스트"

#: ../../<reno.sphinxext origin/stable/ocata>:1063
msgid "``isolated_images``"
msgstr "집합이미지"

#: ../../<reno.sphinxext unmaintained/2023.1>:330
msgid "``jpeg_compression``"
msgstr "`jpeg_compression`"

#: ../../<reno.sphinxext stable/rocky>:1645
msgid "``key_file``"
msgstr "`키 파일`"

#: ../../<reno.sphinxext stable/pike>:1199
msgid "``key_file`` now called ``keyfile``"
msgstr "``key_file``이 ``keyfile``로 이름이 변경되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1647
msgid "``keys_path``"
msgstr "``키 경로``"

#: ../../<reno.sphinxext origin/stable/ocata>:1465
msgid "``l3_lib``"
msgstr "``l3_lib``"

#: ../../<reno.sphinxext stable/pike>:1538
msgid "``ldap_dns_base_dn``"
msgstr "``집합 DNS 기본 DN``"

#: ../../<reno.sphinxext stable/pike>:1535
msgid "``ldap_dns_password``"
msgstr "집합 DNS 계정"

#: ../../<reno.sphinxext stable/pike>:1537
msgid "``ldap_dns_servers``"
msgstr "집합 DNS서버"

#: ../../<reno.sphinxext stable/pike>:1541
msgid "``ldap_dns_soa_expiry``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1536
msgid "``ldap_dns_soa_hostmaster``"
msgstr "ldap_dns_soa_hostmaster → ldap-dns-soa-hostmaster"

#: ../../<reno.sphinxext stable/pike>:1542
msgid "``ldap_dns_soa_minimum``"
msgstr "ldap_dns_soa_minimum → ldap-dns-soa-최소"

#: ../../<reno.sphinxext stable/pike>:1539
msgid "``ldap_dns_soa_refresh``"
msgstr "ldap_dns_soa_refresh"

#: ../../<reno.sphinxext stable/pike>:1540
msgid "``ldap_dns_soa_retry``"
msgstr "ldap_dns_soa_retry"

#: ../../<reno.sphinxext stable/pike>:1533
msgid "``ldap_dns_url``"
msgstr "``집합 DNS URL``"

#: ../../<reno.sphinxext stable/pike>:1534
msgid "``ldap_dns_user``"
msgstr "집합_dns_user"

#: ../../<reno.sphinxext stable/queens>:1005
msgid "``legacy`` (must-have-if-available) (default)"
msgstr "``기상``"

#: ../../<reno.sphinxext unmaintained/2023.1>:563 unmaintained/xena>:139
#: unmaintained/yoga>:216 unmaintained/zed>:167
msgid ""
"``libvirt.libvirtError: internal error: migration was active, but no RAM "
"info was set``"
msgstr "libvirt.libvirtError: 내부 오류: 이식이 활성화되었지만 RAM 정보가 설정되지 않았습니다."

#: ../../<reno.sphinxext stable/pike>:1391
msgid ""
"``libvirt.num_iscsi_scan_tries`` option has been renamed to "
"``libvirt.num_volume_scan_tries``, as the previous name was suggesting that "
"this option only concerns devices connected using iSCSI interface. It also "
"concerns devices connected using fibrechannel, scaleio and disco."
msgstr ""
"`libvirt.num_iscsi_scan_tries` 옵션은 이전 이름이 iSCSI 인터페이스를 사용하여 연결된 장치에만関心이 있었던 "
"것처럼 보이던 옵션을 `libvirt.num_volume_scan_tries`로 이름이 변경되었다. 그러나 이 옵션은 iSCSI "
"인터페이스를 사용하여 연결된 장치뿐만 아니라 fibrechannel, scaleio 및 disco를 사용하여 연결된 장치에도関心이 있다."

#: ../../<reno.sphinxext stable/pike>:1556
msgid "``linuxnet_interface_driver``"
msgstr "`linuxnet_interface_driver`"

#: ../../<reno.sphinxext stable/pike>:1557
msgid "``linuxnet_ovs_integration_bridge``"
msgstr "``linuxnet_ovs_integration_bridge``"

#: ../../<reno.sphinxext stable/pike>:1188
msgid "``live_migration_downtime_delay`` with minimum value 10"
msgstr "집합 live_migration_downtime_delay의 최소值은 10입니다."

#: ../../<reno.sphinxext stable/pike>:1187
msgid "``live_migration_downtime_steps`` with minimum value 3"
msgstr "집합 live_migration_downtime_steps (min 3 )"

#: ../../<reno.sphinxext stable/pike>:1186
msgid "``live_migration_downtime`` with minimum value 100"
msgstr "집합 live_migration_downtime에 최소值 100"

#: ../../<reno.sphinxext stable/queens>:845
msgid "``lock``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:915
msgid "``log_dir``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:1427
msgid "``logs``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1275 stable/stein>:1193
msgid "``max_age``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1108
msgid "``max_hop_count``"
msgstr "집합 수"

#: ../../<reno.sphinxext origin/stable/ocata>:1003
msgid "``max_limit`` (was ``osapi_max_limit``)"
msgstr "``max_limit`` (이전 ``osapi_max_limit``)"

#: ../../<reno.sphinxext origin/stable/ocata>:1001
msgid "``metadata_cache_expiration``"
msgstr "집합 캐시-expiration"

#: ../../<reno.sphinxext stable/pike>:1528
msgid "``metadata_host``"
msgstr "``metadata_host``"

#: ../../<reno.sphinxext origin/stable/ocata>:1348
msgid "``metadata_manager``"
msgstr "집합 관리자"

#: ../../<reno.sphinxext stable/pike>:1529
msgid "``metadata_port``"
msgstr "집합포트"

#: ../../<reno.sphinxext stable/rocky>:1030
msgid "``metrics.update``"
msgstr "`metrics.update`"

#: ../../<reno.sphinxext stable/pike>:1088
msgid "``mksproxy_base_url`` in the ``mks`` group"
msgstr "``mksproxy_base_url`` in the ``mks`` group"

#: ../../<reno.sphinxext origin/stable/ocata>:1467
msgid "``multi_host``"
msgstr "집합"

#: ../../<reno.sphinxext stable/rocky>:1680
msgid "``multi_instance_display_name_template``"
msgstr "``집합 이름 템플릿``"

#: ../../<reno.sphinxext stable/train>:1100
msgid "``mute_child_interval``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1103
msgid "``mute_weight_multiplier``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1095
msgid "``name``"
msgstr "`name`"

#: ../../<reno.sphinxext origin/stable/ocata>:1466
msgid "``network_driver``"
msgstr "네트워크 드라이버"

#: ../../<reno.sphinxext stable/rocky>:1872
msgid "``network_manager``"
msgstr "네트워크 관리자"

#: ../../<reno.sphinxext origin/stable/ocata>:1457
msgid "``network_size``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1367
msgid "``network_topic``"
msgstr "네트워크 주제"

#: ../../<reno.sphinxext stable/ussuri>:905
msgid "``network``"
msgstr "네트워크"

#: ../../<reno.sphinxext stable/rocky>:1907
msgid ""
"``network`` - The ``nova-network`` service was deprecated in the 14.0.0 "
"Newton release and will be removed in an upcoming release."
msgstr ""
"``네트워크`` - nova-network 서비스는 Newton 14.0.0 릴리스에서弃기되었으며 향후 릴리스에서 제거될 예정입니다."

#: ../../<reno.sphinxext stable/pike>:1561
msgid "``networks_path``"
msgstr "집합 경로"

#: ../../<reno.sphinxext stable/pike>:1767
msgid "``networks``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1010
msgid "``neutron_default_tenant_id``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1224
msgid "``notification_format``"
msgstr "집합 형식"

#: ../../<reno.sphinxext origin/stable/ocata>:1221
msgid "``notify_on_api_faults`` (was ``notify_api_faults``)"
msgstr "``API 오류에 대한 알림``"

#: ../../<reno.sphinxext origin/stable/ocata>:1220
msgid "``notify_on_state_change``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1393
msgid "``nova-manage account scrub``"
msgstr "nova-manage account scrub → nova-manage 계정 집합 청소"

#: ../../<reno.sphinxext stable/train>:1061
msgid "``nova-manage api_db sync``"
msgstr "nova-manage api_db sync"

#: ../../<reno.sphinxext stable/train>:1060
msgid "``nova-manage db sync``"
msgstr "nova-manage db sync"

#: ../../<reno.sphinxext origin/stable/ocata>:1394
msgid "``nova-manage fixed *``"
msgstr ""
"``nova-manage fixed *`` \n"
"\n"
"* \"nova-manage\" → \"nova-manage\"\n"
"* \"fixed\" → \"정상\"\n"
"* \"*\" → \"*\""

#: ../../<reno.sphinxext unmaintained/wallaby>:659
msgid "``nova-manage libvirt get_machine_type``"
msgstr ""
"nova-manage libvirt get_machine_type → nova-manage libvirt get_machine_type"

#: ../../<reno.sphinxext unmaintained/wallaby>:688
msgid "``nova-manage libvirt list_unset_machine_type``"
msgstr ""
"nova-manage libvirt list_unset_machine_type\n"
"\n"
"*   nova-manage : nova 관리 명령어\n"
"*   libvirt : virtio virtualization API\n"
"*   list : 목록\n"
"*   unset_machine_type : máychine 유형 제거"

#: ../../<reno.sphinxext unmaintained/wallaby>:664
msgid "``nova-manage libvirt set_machine_type``"
msgstr "nova-manage libvirt set_machine_type"

#: ../../<reno.sphinxext origin/stable/ocata>:1395
msgid "``nova-manage project scrub``"
msgstr "nova-manage 프로젝트 집합 청소"

#: ../../<reno.sphinxext origin/stable/ocata>:1396
msgid "``nova-manage vpn *``"
msgstr "nova-manage vpn *"

#: ../../<reno.sphinxext stable/queens>:1409
msgid ""
"``nova.keymgr.conf_key_mgr.ConfKeyManager`` still remains, but the "
"``fixed_key`` configuration options should be moved to the ``[key_manager]``"
" section"
msgstr ""
"nova.keymgr.conf_key_mgr.ConfKeyManager이 여전히 남아 있지만, fixed_key 설정 옵션은 "
"[key_manager] 섹션으로 이동되어야 합니다."

#: ../../<reno.sphinxext stable/rocky>:1758
msgid "``nova.quota.DbQuotaDriver``"
msgstr "nova.quota.DbQuotaDriver → nova.quota.집합구조자"

#: ../../<reno.sphinxext stable/rocky>:1759
msgid "``nova.quota.NoopQuotaDriver``"
msgstr "nova.quota.집합. NoopQuotaDriver"

#: ../../<reno.sphinxext stable/queens>:1392
msgid "``null_kernel``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1454
msgid "``num_networks``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1105
msgid "``offset_weight_multiplier``"
msgstr "집합 가중치"

#: ../../<reno.sphinxext stable/2024.1>:525
msgid "``os:monitors``"
msgstr "os:monitors → os:監控"

#: ../../<reno.sphinxext stable/2024.1>:524
msgid "``os:resolution``"
msgstr "os:resolution → os:집합"

#: ../../<reno.sphinxext stable/2024.1>:526
msgid "``os:vram``"
msgstr "os:vram → os:VRAM"

#: ../../<reno.sphinxext stable/pike>:1650
msgid "``os_compute_api:flavors``"
msgstr ""
"``os_compute_api:flavors`` \n"
"\n"
"``os_compute_api:flavors`` \n"
"\n"
"``os_compute_api:flavors``"

#: ../../<reno.sphinxext stable/queens>:1515
msgid "``os_compute_api:image-size``"
msgstr ""
"``os_compute_api:image-size`` \n"
"\n"
"``os_compute_api:image-size`` \n"
"\n"
"``os_compute_api:image-size``"

#: ../../<reno.sphinxext stable/ussuri>:755
msgid "``os_compute_api:limits:other_project``"
msgstr ""
"``os_compute_api:limits:other_project`` \n"
"\n"
"``os_compute_api:limits:other_project`` \n"
"\n"
"``os_compute_api:limits:other_project``"

#: ../../<reno.sphinxext stable/ussuri>:700
msgid "``os_compute_api:os-agents:create``"
msgstr "os_compute_api:os-agents:create"

#: ../../<reno.sphinxext stable/ussuri>:702
msgid "``os_compute_api:os-agents:delete``"
msgstr "os_compute_api:os-agents:delete"

#: ../../<reno.sphinxext stable/ussuri>:703
msgid "``os_compute_api:os-agents:list``"
msgstr "os_compute_api:os-agents:list"

#: ../../<reno.sphinxext stable/ussuri>:701
msgid "``os_compute_api:os-agents:update``"
msgstr "os_compute_api:os-agents:update"

#: ../../<reno.sphinxext stable/ussuri>:1043
msgid "``os_compute_api:os-agents``"
msgstr "os_compute_api:os-agents → os_compute_api:os-agents"

#: ../../<reno.sphinxext stable/ussuri>:698
msgid "``os_compute_api:os-agents`` is made granular to"
msgstr "os_compute_api:os-agents``은 집합으로 구성되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:707
msgid "``os_compute_api:os-attach-interfaces:create``"
msgstr "os_compute_api:os-attach-interfaces:create"

#: ../../<reno.sphinxext stable/ussuri>:708
msgid "``os_compute_api:os-attach-interfaces:delete``"
msgstr "os_compute_api:os-attach-interfaces:delete"

#: ../../<reno.sphinxext stable/ussuri>:710
msgid "``os_compute_api:os-attach-interfaces:list``"
msgstr "os_compute_api:os-attach-interfaces:list"

#: ../../<reno.sphinxext stable/ussuri>:709
msgid "``os_compute_api:os-attach-interfaces:show``"
msgstr ""
"os_compute_api:os-attach-interfaces:show → os_compute_api:os-attach-"
"interfaces:show"

#: ../../<reno.sphinxext stable/ussuri>:1044
msgid "``os_compute_api:os-attach-interfaces``"
msgstr "os_compute_api:os-attach-interfaces → os_compute_api:os-연결-인터페이스"

#: ../../<reno.sphinxext stable/ussuri>:705
msgid "``os_compute_api:os-attach-interfaces`` is made granular to"
msgstr "os_compute_api:os-attach-interfaces``은 집합으로 만들어졌습니다."

#: ../../<reno.sphinxext stable/queens>:1500
msgid "``os_compute_api:os-config-drive``"
msgstr "os_compute_api:os-config-drive"

#: ../../<reno.sphinxext stable/ussuri>:850
msgid "``os_compute_api:os-consoles:create``"
msgstr "os_compute_api:os-consoles:create"

#: ../../<reno.sphinxext stable/ussuri>:851
msgid "``os_compute_api:os-consoles:delete``"
msgstr "os_compute_api:os-consoles:delete"

#: ../../<reno.sphinxext stable/ussuri>:849
msgid "``os_compute_api:os-consoles:index``"
msgstr "os_compute_api:os-consoles:index"

#: ../../<reno.sphinxext stable/ussuri>:852
msgid "``os_compute_api:os-consoles:show``"
msgstr "os_compute_api:os-consoles:show"

#: ../../<reno.sphinxext stable/ussuri>:715
msgid "``os_compute_api:os-deferred-delete:force``"
msgstr ""
"os_compute_api:os-deferred-delete:force → os_compute_api:os-deferred-"
"delete:force"

#: ../../<reno.sphinxext stable/ussuri>:714
msgid "``os_compute_api:os-deferred-delete:restore``"
msgstr "os_compute_api:os-deferred-delete:restore"

#: ../../<reno.sphinxext stable/ussuri>:1045
msgid "``os_compute_api:os-deferred-delete``"
msgstr "os_compute_api:os-deferred-delete"

#: ../../<reno.sphinxext stable/ussuri>:712
msgid "``os_compute_api:os-deferred-delete`` is made granular to"
msgstr "os_compute_api:os-deferred-delete``은 집합적이게"

#: ../../<reno.sphinxext stable/queens>:1501
msgid "``os_compute_api:os-extended-availability-zone``"
msgstr "os_compute_api:os-extended-가용 구역"

#: ../../<reno.sphinxext stable/queens>:1502
msgid "``os_compute_api:os-extended-status``"
msgstr "os_compute_api:os-extended-status"

#: ../../<reno.sphinxext stable/queens>:1503
msgid "``os_compute_api:os-extended-volumes``"
msgstr "os_compute_api:os-extended-volumes"

#: ../../<reno.sphinxext stable/queens>:1511
msgid "``os_compute_api:os-flavor-access`` (only from ``/flavors`` APIs)"
msgstr "os_compute_api:os-flavor-access (only from /flavors APIs)"

#: ../../<reno.sphinxext stable/pike>:762 stable/stein>:1171
msgid "``os_compute_api:os-flavor-manage:create``"
msgstr "os_compute_api:os-flavor-manage:create"

#: ../../<reno.sphinxext stable/pike>:763 stable/stein>:1172
msgid "``os_compute_api:os-flavor-manage:delete``"
msgstr ""
"os_compute_api:os-flavor-manage:delete → os_compute_api:os-flavor-manage:삭제"

#: ../../<reno.sphinxext stable/queens>:1510
msgid "``os_compute_api:os-flavor-rxtx``"
msgstr "os_compute_api:os-flavor-rxtx"

#: ../../<reno.sphinxext stable/ussuri>:720
msgid "``os_compute_api:os-hypervisors:list-detail``"
msgstr "os_compute_api:os-hypervisors:list-detail"

#: ../../<reno.sphinxext stable/ussuri>:719
msgid "``os_compute_api:os-hypervisors:list``"
msgstr "os_compute_api:os-hypervisors:list"

#: ../../<reno.sphinxext stable/ussuri>:724
msgid "``os_compute_api:os-hypervisors:search``"
msgstr ""
"os_compute_api:os-hypervisors:search → os_compute_api:os-hypervisors:search"

#: ../../<reno.sphinxext stable/ussuri>:725
msgid "``os_compute_api:os-hypervisors:servers``"
msgstr ""
"os_compute_api:os-hypervisors:servers → os_compute_api:os-hypervisors:서버"

#: ../../<reno.sphinxext stable/ussuri>:722
msgid "``os_compute_api:os-hypervisors:show``"
msgstr ""
"os_compute_api:os-hypervisors:show → os_compute_api:os-hypervisors:show"

#: ../../<reno.sphinxext stable/ussuri>:721
msgid "``os_compute_api:os-hypervisors:statistics``"
msgstr ""
"os_compute_api:os-hypervisors:statistics → os_compute_api:os-hypervisors:집합"

#: ../../<reno.sphinxext stable/ussuri>:723
msgid "``os_compute_api:os-hypervisors:uptime``"
msgstr ""
"os_compute_api:os-hypervisors:uptime\n"
"\n"
"* os_compute_api: \n"
"  + os- \n"
"    - os- : \n"
"      - os- : \n"
"        - os- : \n"
"          - os- : \n"
"            - os- : \n"
"              - os- : \n"
"                - os- : \n"
"                  - os- : \n"
"                    - os- : \n"
"                      - os- : \n"
"                        - os- : \n"
"                          - os- : \n"
"                            - os- : \n"
"                              - os- : \n"
"                                - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
"                  - os- : \n"
"                    - os- : \n"
"                      - os- : \n"
"                        - os- : \n"
"                          - os- : \n"
"                            - os- : \n"
"                              - os- : \n"
"                                - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
"                - os- : \n"
"                  - os- : \n"
"                    - os- : \n"
"                      - os- : \n"
"                        - os- : \n"
"                          - os- : \n"
"                            - os- : \n"
"                              - os- : \n"
"                                - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os- : \n"
" - os"

#: ../../<reno.sphinxext stable/ussuri>:1046
msgid "``os_compute_api:os-hypervisors``"
msgstr "os_compute_api:os-hypervisors"

#: ../../<reno.sphinxext stable/ussuri>:717
msgid "``os_compute_api:os-hypervisors`` is made granular to"
msgstr "os_compute_api:os-hypervisors``는 집합으로 구성되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:740
msgid "``os_compute_api:os-instance-actions:list``"
msgstr "os_compute_api:os-instance-actions:list"

#: ../../<reno.sphinxext stable/ussuri>:741
msgid "``os_compute_api:os-instance-actions:show``"
msgstr ""
"os_compute_api:os-instance-actions:show\n"
"\n"
"```\n"
"os_compute_api:os-instance-actions:show``는 다음으로 번역됩니다.\n"
"\n"
"os_compute_api:os-instance-actions:show\n"
"\n"
"```\n"
"os_compute_api:os-instance-actions:show``는 os_compute_api API의 os-instance-actions에 대한 show action을 나타냅니다."

#: ../../<reno.sphinxext stable/ussuri>:1049
msgid "``os_compute_api:os-instance-actions``"
msgstr "os_compute_api:os-instance-actions → os_compute_api:os-인스턴스-액션"

#: ../../<reno.sphinxext stable/ussuri>:738
msgid "``os_compute_api:os-instance-actions`` is made granular to"
msgstr "os_compute_api:os-instance-actions``은 집합으로"

#: ../../<reno.sphinxext stable/ussuri>:735
msgid "``os_compute_api:os-instance-usage-audit-log:list``"
msgstr "os_compute_api:os-instance-usage-audit-log:list"

#: ../../<reno.sphinxext stable/ussuri>:736
msgid "``os_compute_api:os-instance-usage-audit-log:show``"
msgstr ""
"os_compute_api:os-instance-usage-audit-log:show → os_compute_api:os-"
"instance-usage-audit-log:show"

#: ../../<reno.sphinxext stable/ussuri>:1048
msgid "``os_compute_api:os-instance-usage-audit-log``"
msgstr ""
"os_compute_api:os-instance-usage-audit-log → os_compute_api:os-인스턴스-사용량-사용기록"

#: ../../<reno.sphinxext stable/ussuri>:733
msgid "``os_compute_api:os-instance-usage-audit-log`` is made granular to"
msgstr "os_compute_api:os-instance-usage-audit-log은 집합으로"

#: ../../<reno.sphinxext stable/queens>:1504
msgid "``os_compute_api:os-keypairs``"
msgstr "os_compute_api:os-keypairs"

#: ../../<reno.sphinxext stable/2025.2>:290
msgid "``os_compute_api:os-migrate-server:migrate_live:host``"
msgstr ""
"os_compute_api:os-migrate-server:migrate_live:host → os_compute_api:os-"
"migrate-server:migrate_live:호스트"

#: ../../<reno.sphinxext stable/2025.2>:118
msgid ""
"``os_compute_api:os-migrate-server:migrate_live:host`` (live migrate server "
"to a specific host)"
msgstr ""
"os_compute_api:os-migrate-server:migrate_live:host → os_compute_api:os-"
"migrate-server:migrate_live:집합"

#: ../../<reno.sphinxext stable/2025.2>:284
msgid "``os_compute_api:os-migrate-server:migrate_live``"
msgstr ""
"os_compute_api:os-migrate-server:migrate_live → os_compute_api:os-migrate-"
"server:집합 live"

#: ../../<reno.sphinxext stable/2025.2>:93 stable/2025.2>:112
msgid ""
"``os_compute_api:os-migrate-server:migrate_live`` (live migrate server "
"without specifying host)"
msgstr "os_compute_api:os-migrate-server:migrate_live"

#: ../../<reno.sphinxext stable/2025.2>:91
msgid ""
"``os_compute_api:os-migrate-server:migrate`` (Cold migrate a server without "
"specifying a host)"
msgstr ""
"os_compute_api:os-migrate-server:migrate (cold migrate server without "
"specifying host)"

#: ../../<reno.sphinxext stable/2025.2>:292
msgid "``os_compute_api:os-migrations:index:all_projects``"
msgstr "os_compute_api:os-migrations:index:all_projects"

#: ../../<reno.sphinxext stable/2025.2>:146
msgid ""
"``os_compute_api:os-migrations:index:all_projects`` (List migrations for all"
" or cross projects)"
msgstr "os_compute_api:os-migrations:index:all_projects"

#: ../../<reno.sphinxext stable/2025.2>:293
msgid "``os_compute_api:os-migrations:index:host``"
msgstr "os_compute_api:os-migrations:index:host"

#: ../../<reno.sphinxext stable/2025.2>:149
msgid ""
"``os_compute_api:os-migrations:index:host`` (List migrations with host info)"
msgstr "os_compute_api:os-migrations:index:host"

#: ../../<reno.sphinxext stable/2025.2>:286
msgid "``os_compute_api:os-migrations:index``"
msgstr "os_compute_api:os-migrations:index"

#: ../../<reno.sphinxext stable/2025.2>:95 stable/2025.2>:140
msgid ""
"``os_compute_api:os-migrations:index`` (List migrations without host info)"
msgstr "os_compute_api:os-migrations:index (집합 이중 마이그레이션: 호스트 정보가 없는 경우)"

#: ../../<reno.sphinxext stable/ussuri>:879
msgid "``os_compute_api:os-networks-associate``"
msgstr "os_compute_api:os-networks-associate"

#: ../../<reno.sphinxext stable/ussuri>:878
msgid "``os_compute_api:os-networks``"
msgstr "os_compute_api:os-networks"

#: ../../<reno.sphinxext stable/ussuri>:750
msgid "``os_compute_api:os-rescue``"
msgstr "os_compute_api:os-rescue"

#: ../../<reno.sphinxext stable/ussuri>:748
msgid "``os_compute_api:os-rescue`` is made granular to"
msgstr "os_compute_api:os-rescue는 집합으로"

#: ../../<reno.sphinxext stable/ussuri>:877
msgid "``os_compute_api:os-security-group-default-rules``"
msgstr "os_compute_api:os-시큐리티 그룹 디폴트 규칙"

#: ../../<reno.sphinxext stable/ussuri>:729
msgid "``os_compute_api:os-security-groups:add``"
msgstr "``os_compute_api:os-security-groups:추가``"

#: ../../<reno.sphinxext stable/ussuri>:731
msgid "``os_compute_api:os-security-groups:list``"
msgstr "os_compute_api:os-시큐리티 그룹:리스트"

#: ../../<reno.sphinxext stable/ussuri>:730
msgid "``os_compute_api:os-security-groups:remove``"
msgstr "os_compute_api:os-security-groups:remove"

#: ../../<reno.sphinxext stable/ussuri>:1047
msgid "``os_compute_api:os-security-groups``"
msgstr "os_compute_api:os-안전 그룹"

#: ../../<reno.sphinxext stable/queens>:1506
msgid "``os_compute_api:os-security-groups`` (only from ``/servers`` APIs)"
msgstr "``os_compute_api:os-security-groups`` (만ly /servers APIs에서)"

#: ../../<reno.sphinxext stable/ussuri>:727
msgid "``os_compute_api:os-security-groups`` is made granular to"
msgstr "os_compute_api:os-security-groups``은 집합으로"

#: ../../<reno.sphinxext stable/pike>:1649
msgid "``os_compute_api:os-server-groups``"
msgstr "os_compute_api:os-server-groups"

#: ../../<reno.sphinxext stable/ussuri>:746
msgid "``os_compute_api:os-server-password:clear``"
msgstr ""
"os_compute_api:os-server-password:clear → os_compute_api:os-server-"
"password:clear"

#: ../../<reno.sphinxext stable/ussuri>:745
msgid "``os_compute_api:os-server-password:show``"
msgstr ""
"os_compute_api:os-server-password:show → os_compute_api:os-server-"
"password:show"

#: ../../<reno.sphinxext stable/ussuri>:1050
msgid "``os_compute_api:os-server-password``"
msgstr "os_compute_api:os-server-password → os_compute_api:os-server-비밀번호"

#: ../../<reno.sphinxext stable/ussuri>:743
msgid "``os_compute_api:os-server-password`` is made granular to"
msgstr "os_compute_api:os-server-password은 집합으로"

#: ../../<reno.sphinxext stable/queens>:1505
msgid "``os_compute_api:os-server-usage``"
msgstr ""
"os_compute_api:os-server-usage\n"
"\n"
"* os_compute_api: \n"
"  + glossary mapping: \"os_compute_api\" → \"OS 컴퓨터 API\"\n"
"* os-server-usage: \n"
"  + glossary mapping: \"server\" → \"서버\", \"usage\" → \"사용\"\n"
"  + combined translation: \"서버 사용\""

#: ../../<reno.sphinxext stable/ussuri>:761
msgid "``os_compute_api:os-services:delete``"
msgstr "os_compute_api:os-services:delete"

#: ../../<reno.sphinxext stable/ussuri>:759
msgid "``os_compute_api:os-services:list``"
msgstr "os_compute_api:os-services:집합"

#: ../../<reno.sphinxext stable/ussuri>:760
msgid "``os_compute_api:os-services:update``"
msgstr "os_compute_api:os-services:update"

#: ../../<reno.sphinxext stable/ussuri>:1052
msgid "``os_compute_api:os-services``"
msgstr ""
"``os_compute_api:os-services`` \n"
"\n"
"``os_compute_api:os-services`` \n"
"\n"
"``os_compute_api:os 서비스``"

#: ../../<reno.sphinxext stable/ussuri>:757
msgid "``os_compute_api:os-services`` is made granular to"
msgstr "os_compute_api:os-services 집합은 granular로"

#: ../../<reno.sphinxext stable/ussuri>:751
msgid "``os_compute_api:os-unrescue``"
msgstr "os_compute_api:os-unrescue"

#: ../../<reno.sphinxext stable/ussuri>:1051
msgid "``os_compute_api:os-used-limits``"
msgstr "os_compute_api:os-used-limits → os_compute_api:os-사용한 제한"

#: ../../<reno.sphinxext stable/ussuri>:753
msgid "``os_compute_api:os-used-limits`` is renamed to"
msgstr "os_compute_api:os-used-limits → os_compute_api:os-used-limits"

#: ../../<reno.sphinxext stable/rocky>:1316
msgid "``os_compute_api:servers:create:trusted_certs``"
msgstr ""
"os_compute_api:servers:create:trusted_certs → os_compute_api:서버:생성:신뢰할 수 있는 "
"자격증"

#: ../../<reno.sphinxext stable/2025.2>:285
msgid "``os_compute_api:servers:migrations::index``"
msgstr ""
"os_compute_api:servers:migrations::index → os_compute_api:서버:migration:인덱스"

#: ../../<reno.sphinxext stable/2025.2>:101
msgid ""
"``os_compute_api:servers:migrations:delete`` (Delete(Abort) an in-progress "
"live migration)"
msgstr "os_compute_api:servers:migrations:delete"

#: ../../<reno.sphinxext stable/2025.2>:99
msgid ""
"``os_compute_api:servers:migrations:force_complete`` (Force an in-progress "
"live migration for a given server)"
msgstr "`os_compute_api:servers:migrations:force_complete`"

#: ../../<reno.sphinxext stable/2025.2>:291
msgid "``os_compute_api:servers:migrations:index:host``"
msgstr ""
"os_compute_api:servers:migrations:index:host → "
"os_compute_api:서버:migration:인덱스:호스트"

#: ../../<reno.sphinxext stable/2025.2>:132
msgid ""
"``os_compute_api:servers:migrations:index:host`` (Lists in-progress live "
"migrations for a given server with host info)"
msgstr ""
"os_compute_api:servers:migrations:index:host → "
"os_compute_api:servers:migrations:index:호스트"

#: ../../<reno.sphinxext stable/2025.2>:126
msgid "``os_compute_api:servers:migrations:index`` (Lists in-progress live"
msgstr ""
"os_compute_api:servers:migrations:index → os_compute_api:서버:migration:인덱스"

#: ../../<reno.sphinxext stable/2025.2>:97
msgid ""
"``os_compute_api:servers:migrations:index`` (Lists in-progress live "
"migrations for a given server without host info)"
msgstr ""
"os_compute_api:servers:migrations:index → os_compute_api:서버:migration:index"

#: ../../<reno.sphinxext stable/rocky>:1317
msgid "``os_compute_api:servers:rebuild:trusted_certs``"
msgstr ""
"os_compute_api:servers:rebuild:trusted_certs → "
"os_compute_api:서버:rebuild:trusted_certs"

#: ../../<reno.sphinxext stable/rocky>:1691
msgid "``os_interface`` (use ``valid_interfaces`` instead)"
msgstr "``os_interface`` (사용 ``valid_interfaces`` 대신)"

#: ../../<reno.sphinxext stable/queens>:1619
msgid "``os_interface`` is deprecated in favor of ``valid_interfaces``"
msgstr "os_interface는 valid_interfaces에 의해 대체되어弃되었습니다."

#: ../../<reno.sphinxext stable/rocky>:1690
msgid "``os_region_name`` (use ``region_name`` instead)"
msgstr "``region_name``"

#: ../../<reno.sphinxext stable/queens>:1618
msgid "``os_region_name`` is deprecated in favor of ``region_name``"
msgstr "os_region_name은 region_name에 의해 대체되었습니다."

#: ../../<reno.sphinxext stable/ussuri>:509
msgid "``os``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1560
msgid "``ovs_vsctl_timeout``"
msgstr "ovs_vsctl_timeout"

#: ../../<reno.sphinxext stable/ussuri>:510
msgid "``pci_passthrough``"
msgstr "pci_passthrough"

#: ../../<reno.sphinxext unmaintained/2023.1>:332
msgid "``playback_compression``"
msgstr "집합 압축"

#: ../../<reno.sphinxext stable/ussuri>:511
msgid "``powervm``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:1006
msgid "``preferred`` (nice-to-have)"
msgstr "집합 (선택 사항)"

#: ../../<reno.sphinxext unmaintained/yoga>:354
msgid "``project admin``"
msgstr "``project admin``"

#: ../../<reno.sphinxext unmaintained/yoga>:355 unmaintained/zed>:323
msgid "``project member``"
msgstr "project 멤버"

#: ../../<reno.sphinxext unmaintained/yoga>:356 unmaintained/zed>:324
msgid "``project reader``"
msgstr "``project reader``"

#: ../../<reno.sphinxext stable/rocky>:1651
msgid "``project_cert_subject``"
msgstr "``project_cert_subject``"

#: ../../<reno.sphinxext stable/2025.2>:82
msgid "``project_manager``"
msgstr "project_manager → 프로젝트 관리자"

#: ../../<reno.sphinxext stable/2025.2>:83
msgid "``project_member``"
msgstr "project_member"

#: ../../<reno.sphinxext stable/2025.2>:84
msgid "``project_reader``"
msgstr "``project_reader``"

#: ../../<reno.sphinxext stable/queens>:1425
msgid "``project``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1562
msgid "``public_interface``"
msgstr "``공공 인터페이스``"

#: ../../<reno.sphinxext stable/pike>:1325
msgid "``qemu_allowed_storage_drivers``"
msgstr "qemu_allowed_storage_drivers"

#: ../../<reno.sphinxext origin/stable/ocata>:1260
msgid "``quota_cores`` (now ``cores``)"
msgstr "집합 코어"

#: ../../<reno.sphinxext origin/stable/ocata>:1276
msgid "``quota_driver`` (now ``driver``)"
msgstr "quota_driver (현재 driver)"

#: ../../<reno.sphinxext origin/stable/ocata>:1263
msgid "``quota_fixed_ips`` (now ``fixed_ips``)"
msgstr "집합fixed_ips"

#: ../../<reno.sphinxext origin/stable/ocata>:1262
msgid "``quota_floating_ips`` (now ``floating_ips``)"
msgstr "``집합浮动 IP``"

#: ../../<reno.sphinxext origin/stable/ocata>:1266
msgid ""
"``quota_injected_file_content_bytes`` (now ``injected_file_content_bytes``)"
msgstr "``집합 추가한 파일 내용 바이트``"

#: ../../<reno.sphinxext origin/stable/ocata>:1267
msgid ""
"``quota_injected_file_path_length`` (now ``injected_file_path_length``)"
msgstr "``quotas`` (now ``injected_file_path_length``)"

#: ../../<reno.sphinxext origin/stable/ocata>:1265
msgid "``quota_injected_files`` (now ``injected_files``)"
msgstr "집합: injected_files"

#: ../../<reno.sphinxext origin/stable/ocata>:1259
msgid "``quota_instances`` (now ``instances``)"
msgstr "집합 인스턴스"

#: ../../<reno.sphinxext origin/stable/ocata>:1270
msgid "``quota_key_pairs`` (now ``key_pairs``)"
msgstr "``quota_key_pairs`` (현재 ``key_pairs``)"

#: ../../<reno.sphinxext origin/stable/ocata>:1264
msgid "``quota_metadata_items`` (now ``metadata_items``)"
msgstr "집합 metadata_items"

#: ../../<reno.sphinxext stable/ussuri>:899
msgid "``quota_networks``"
msgstr "집합 네트워크"

#: ../../<reno.sphinxext origin/stable/ocata>:1261
msgid "``quota_ram`` (now ``ram``)"
msgstr "집합(ram)"

#: ../../<reno.sphinxext origin/stable/ocata>:1269
msgid "``quota_security_group_rules`` (now ``security_group_rules``)"
msgstr "``집합 보안 그룹 규칙``"

#: ../../<reno.sphinxext origin/stable/ocata>:1268
msgid "``quota_security_groups`` (now ``security_groups``)"
msgstr "집합 보안 그룹"

#: ../../<reno.sphinxext origin/stable/ocata>:1272
msgid "``quota_server_group_members`` (now ``server_group_members``)"
msgstr "집합서버 그룹 멤버"

#: ../../<reno.sphinxext origin/stable/ocata>:1271
msgid "``quota_server_groups`` (now ``server_groups``)"
msgstr "집합 서버 그룹"

#: ../../<reno.sphinxext stable/queens>:1423 stable/ussuri>:512
msgid "``quota``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1058 stable/train>:1104
msgid "``ram_weight_multiplier``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:1439
msgid "``remap_vbd_dev_prefix``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:1438
msgid "``remap_vbd_dev``"
msgstr "집합 remap_vbd_dev"

#: ../../<reno.sphinxext stable/queens>:1004
msgid "``required`` (must-have)"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1273 stable/stein>:1191
msgid "``reservation_expire``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1098
msgid "``reserve_percent``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1932
msgid ""
"``reserve_volume`` call was added to the boot from volume scenario. In case "
"a failure occurs while building the instance, the instance goes into ERROR "
"state while the volume stays in ``attaching`` state. The volume state will "
"be set back to ``available`` when the instance gets deleted."
msgstr ""
"``reserve_volume`` call이 부트에서 볼륨 스цен어리오에 추가되었다. 인스턴스가 생성 중에 오류가 발생할 경우, "
"인스턴스는 ERROR 상태로 이동하지만 볼륨은 ``attaching`` 상태에 유지된다. 인스턴스가 삭제될 때까지 볼륨 상태는 "
"``available``로 설정된다."

#: ../../<reno.sphinxext stable/ussuri>:513
msgid "``resources`` (including ``_{group}`` suffixes)"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1065
msgid "``restrict_isolated_hosts_to_isolated_images``"
msgstr "``집합_선택된ホスト를 isolated 이미지로 선택``"

#: ../../<reno.sphinxext stable/pike>:1563
msgid "``routing_source_ip``"
msgstr "집합 IP"

#: ../../<reno.sphinxext stable/train>:1110
msgid "``rpc_driver_queue_base``"
msgstr "``집합 드라이버 Queue``"

#: ../../<reno.sphinxext stable/pike>:1347
msgid "``scality_sofs_config``"
msgstr "``scalability_sofs_config``"

#: ../../<reno.sphinxext stable/pike>:1348
msgid "``scality_sofs_mount_point``"
msgstr "``집합``"

#: ../../<reno.sphinxext origin/stable/ocata>:1052
msgid "``scheduler_available_filters`` (now ``available_filters``)"
msgstr "``가용 필터``"

#: ../../<reno.sphinxext origin/stable/ocata>:1053
msgid "``scheduler_default_filters`` (now ``enabled_filters``)"
msgstr "``default 필터``"

#: ../../<reno.sphinxext origin/stable/ocata>:1043
msgid "``scheduler_driver_task_period`` (now ``periodic_task_interval``)"
msgstr "``시드러driverタスクペリ오드`` (현재는 ``peridic_task_interval``)"

#: ../../<reno.sphinxext origin/stable/ocata>:1041
msgid "``scheduler_driver`` (now ``driver``)"
msgstr "``스케줄드 드라이버`` (현재 ``드라이버``)"

#: ../../<reno.sphinxext stable/train>:1111
msgid "``scheduler_filter_classes``"
msgstr "`스케줄러 필터 클래스`"

#: ../../<reno.sphinxext origin/stable/ocata>:1042
msgid "``scheduler_host_manager`` (now ``host_manager``)"
msgstr "`scheduler_host_manager` (현재 `host_manager`)"

#: ../../<reno.sphinxext origin/stable/ocata>:1049
msgid "``scheduler_host_subset_size`` (now ``host_subset_size``)"
msgstr "집합 호스트 크기"

#: ../../<reno.sphinxext origin/stable/ocata>:1353
msgid "``scheduler_manager``"
msgstr "집합 관리자"

#: ../../<reno.sphinxext origin/stable/ocata>:1044
msgid "``scheduler_max_attempts`` (now ``max_attempts``)"
msgstr "``max_attempts``"

#: ../../<reno.sphinxext origin/stable/ocata>:1050
msgid "``scheduler_max_instances_per_host`` (now ``max_instances_per_host``)"
msgstr "`scheduler_max_instances_per_host` (현재 `max_instances_per_host`)"

#: ../../<reno.sphinxext stable/train>:1113
msgid "``scheduler_retries``"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1114
msgid "``scheduler_retry_delay``"
msgstr "집합 지연"

#: ../../<reno.sphinxext stable/pike>:1366
msgid "``scheduler_topic``"
msgstr "집합 주제"

#: ../../<reno.sphinxext origin/stable/ocata>:1051
msgid "``scheduler_tracks_instance_changes`` (now ``track_instance_changes``)"
msgstr "``인스턴스 트랙킹``"

#: ../../<reno.sphinxext origin/stable/ocata>:1056
msgid "``scheduler_use_baremetal_filters`` (now ``use_baremetal_filters``)"
msgstr "``baremetalFilters``"

#: ../../<reno.sphinxext stable/train>:1112
msgid "``scheduler_weight_classes``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1057
msgid "``scheduler_weight_classes`` (now ``weight_classes``)"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:1109
msgid "``scheduler``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1483 stable/pike>:1766
msgid "``security_group_rules``"
msgstr "집합 규칙"

#: ../../<reno.sphinxext origin/stable/ocata>:1482 stable/pike>:1765
msgid "``security_groups``"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:1553
msgid "``send_arp_for_ha_count``"
msgstr "집합 ARP를 보냄"

#: ../../<reno.sphinxext stable/pike>:1552
msgid "``send_arp_for_ha``"
msgstr "집합 ARP gửi"

#: ../../<reno.sphinxext stable/pike>:1090
msgid "``serial_port_proxy_uri`` in the ``vmware`` group"
msgstr "``vmware`` 그룹의 ``serial_port_proxy_uri``"

#: ../../<reno.sphinxext stable/pike>:1758 unmaintained/yoga>:689
msgid "``server_group_members``"
msgstr "집합 구성원"

#: ../../<reno.sphinxext stable/pike>:1757 unmaintained/yoga>:688
msgid "``server_groups``"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/yoga>:685
msgid "``server_injected_file_content_bytes``"
msgstr "``서버에 추가한 파일 내용 바이트``"

#: ../../<reno.sphinxext unmaintained/yoga>:686
msgid "``server_injected_file_path_bytes``"
msgstr "``서버에 추가한 파일 경로(바이트)`"

#: ../../<reno.sphinxext unmaintained/yoga>:684
msgid "``server_injected_files``"
msgstr "``서버에 추가된 파일``"

#: ../../<reno.sphinxext unmaintained/yoga>:687
msgid "``server_key_pairs``"
msgstr "`서버 키 쌍`"

#: ../../<reno.sphinxext unmaintained/yoga>:683
msgid "``server_metadata_items``"
msgstr "집합"

#: ../../<reno.sphinxext stable/rocky>:1031
msgid "``servergroup.addmember``"
msgstr "servergroup.addmember"

#: ../../<reno.sphinxext unmaintained/yoga>:681
msgid "``servers``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:1424
msgid "``shell``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:847
msgid "``shelveOffload``"
msgstr "집합오프로드"

#: ../../<reno.sphinxext origin/stable/ocata>:1061
msgid "``soft_affinity_weight_multiplier``"
msgstr "집합 가중치ulti플리저"

#: ../../<reno.sphinxext origin/stable/ocata>:1062
msgid "``soft_anti_affinity_weight_multiplier``"
msgstr "집합 반대성 가중치 곱"

#: ../../<reno.sphinxext unmaintained/2023.1>:333
msgid "``streaming_mode``"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:844
msgid "``swap_volume``"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/yoga>:353
msgid "``system admin``"
msgstr "시스템 관리자"

#: ../../<reno.sphinxext origin/stable/ocata>:1464
msgid "``teardown_unused_network_gateway``"
msgstr "teardown_unused_network_gateway"

#: ../../<reno.sphinxext origin/stable/ocata>:1530
msgid "``torrent_base_url``"
msgstr "torrent_base_url → torrent_base_url"

#: ../../<reno.sphinxext origin/stable/ocata>:1536
msgid "``torrent_download_stall_cutoff``"
msgstr "torrent_download_stall_cutoff → torrent_download_stall_cutoff"

#: ../../<reno.sphinxext origin/stable/ocata>:1535
msgid "``torrent_listen_port_end``"
msgstr "torrent_listen_port_end → torrent_listen_port_end"

#: ../../<reno.sphinxext origin/stable/ocata>:1534
msgid "``torrent_listen_port_start``"
msgstr "torrent_listen_port_start → torrent_listen_port_start"

#: ../../<reno.sphinxext origin/stable/ocata>:1533
msgid "``torrent_max_last_accessed``"
msgstr "torrent_max_last_accessed"

#: ../../<reno.sphinxext origin/stable/ocata>:1537
msgid "``torrent_max_seeder_processes_per_host``"
msgstr ""
"torrent_max_seeder_processes_per_host → "
"torrent_max_seeder_processes_per_host"

#: ../../<reno.sphinxext origin/stable/ocata>:1531
msgid "``torrent_seed_chance``"
msgstr "torrent_seed_chance → torrent_seed_chance (집합)"

#: ../../<reno.sphinxext origin/stable/ocata>:1532
msgid "``torrent_seed_duration``"
msgstr "torrent_seed_duration → torrent_seed_duration"

#: ../../<reno.sphinxext stable/ussuri>:514
msgid "``trait`` (including ``_{group}`` suffixes)"
msgstr "``특성`` (이하 ``_{group}`` 접미사 포함)"

#: ../../<reno.sphinxext stable/rocky>:880
msgid ""
"``trait`` keys for a given group are optional.  That is, you may specify "
"``resources42:XXX`` without a corresponding ``trait42:YYY``. However, the "
"reverse (specifying ``trait42:YYY`` without ``resources42:XXX``) will result"
" in an error."
msgstr ""
"trait의 키를 특정 그룹에 대해 선택적으로 사용할 수 있습니다. 즉, `resources42:XXX`를 `trait42:YYY`와 "
"함께 사용하지 않아도 됩니다. 그러나 반대로 `trait42:YYY`를 `resources42:XXX`와 함께 사용하지 않으면 오류가 "
"발생합니다."

#: ../../<reno.sphinxext stable/queens>:846
msgid "``unlock``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1274 stable/stein>:1192
msgid "``until_refresh``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1469
msgid "``update_dns_entries``"
msgstr "집합 수정"

#: ../../<reno.sphinxext origin/stable/ocata>:994
msgid "``use_forwarded_for``"
msgstr "``forwarded_for``"

#: ../../<reno.sphinxext origin/stable/ocata>:1290
msgid ""
"``use_glance_v1`` option was removed due to plans to remove Glance V1 "
"support during Ocata development."
msgstr "`use_glance_v1` 옵션은 Ocata 개발 시 Glance V1 지원을 제거하기 위해 제거되었습니다."

#: ../../<reno.sphinxext stable/pike>:1564
msgid "``use_ipv6``"
msgstr "``IPv6``"

#: ../../<reno.sphinxext stable/pike>:1547
msgid "``use_network_dns_servers``"
msgstr "``네트워크 DNS 서버``"

#: ../../<reno.sphinxext origin/stable/ocata>:1009
msgid "``use_neutron_default_nets``"
msgstr "``neutron 기본 네트워크``"

#: ../../<reno.sphinxext origin/stable/ocata>:1472
msgid "``use_neutron``"
msgstr "``neutron``"

#: ../../<reno.sphinxext stable/rocky>:1649
msgid "``use_project_ca``"
msgstr "``project CA``"

#: ../../<reno.sphinxext stable/pike>:1558
msgid "``use_single_default_gateway``"
msgstr "``single_default_gateway``"

#: ../../<reno.sphinxext stable/rocky>:1650
msgid "``user_cert_subject``"
msgstr "``사용자 자격 증명서 주체``"

#: ../../<reno.sphinxext stable/rocky>:1407
msgid "``vdi_local_dev``"
msgstr "`vdi_local_dev`"

#: ../../<reno.sphinxext stable/rocky>:1415
msgid "``vdi_remote_stream``"
msgstr "`vdi_remote_stream`"

#: ../../<reno.sphinxext stable/queens>:1117
msgid "``vencrypt_ca_certs``"
msgstr "``vencrypt_ca_certs``"

#: ../../<reno.sphinxext stable/queens>:1116
msgid "``vencrypt_client_cert``"
msgstr "``vencrypt_client_cert``"

#: ../../<reno.sphinxext stable/queens>:1115
msgid "``vencrypt_client_key``"
msgstr "``vencrypt_client_key``"

#: ../../<reno.sphinxext origin/stable/ocata>:999
msgid "``vendordata_dynamic_connect_timeout``"
msgstr "``가입자 데이터 동적 연결 타임아웃``"

#: ../../<reno.sphinxext origin/stable/ocata>:1000
msgid "``vendordata_dynamic_read_timeout``"
msgstr "``제공 데이터 동적 읽기 타임아웃``"

#: ../../<reno.sphinxext origin/stable/ocata>:998
msgid "``vendordata_dynamic_ssl_certfile``"
msgstr "``제공 데이터 동적 SSL 자격 증명서``"

#: ../../<reno.sphinxext origin/stable/ocata>:997
msgid "``vendordata_dynamic_targets``"
msgstr "``가입자 데이터 동적 목표``"

#: ../../<reno.sphinxext origin/stable/ocata>:1002
msgid "``vendordata_jsonfile_path``"
msgstr "``제공 데이터 JSON 파일 경로``"

#: ../../<reno.sphinxext origin/stable/ocata>:996
msgid "``vendordata_providers``"
msgstr "``가입자 데이터 제공자``"

#: ../../<reno.sphinxext origin/stable/ocata>:1452
msgid "``vlan_interface``"
msgstr "vlan 인터페이스"

#: ../../<reno.sphinxext origin/stable/ocata>:1453
msgid "``vlan_start``"
msgstr "vlan_start"

#: ../../<reno.sphinxext stable/ussuri>:515
msgid "``vmware``"
msgstr "``아키텍처``"

#: ../../<reno.sphinxext unmaintained/yoga>:547
msgid ""
"``vncserver_listen`` opt removed, now we use only server_listen to bind vnc "
"address opt."
msgstr ""
"``vncserver_listen`` 옵션 제거되었으며 현재만 ``server_listen`` 옵션을 사용하여 VNC 주소에 "
"바인딩합니다."

#: ../../<reno.sphinxext unmaintained/yoga>:549
msgid ""
"``vncserver_proxyclient_address`` opt removed, now we use only "
"server_proxyclient_address opt."
msgstr ""
"`vncserver_proxyclient_address` 옵션을 제거하고 현재는 `server_proxyclient_address` "
"옵션만 사용합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1455
msgid "``vpn_ip``"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1456
msgid "``vpn_start``"
msgstr "``vpn_start``"

#: ../../<reno.sphinxext unmaintained/2023.1>:331
msgid "``zlib_compression``"
msgstr "집합 압축"

#: ../../<reno.sphinxext stable/pike>:1580
msgid "`account`"
msgstr "`계정`"

#: ../../<reno.sphinxext stable/pike>:1589
msgid "`agent`"
msgstr "`agent` → ` एजент`"

#: ../../<reno.sphinxext unmaintained/wallaby>:920
msgid ""
"`bug 1882521`_ has now been resolved by increasing the incremental and max "
"sleep times between device detach attempts. This works around some undefined"
" QEMU behaviour documented in `bug 1894804`_ where overlapping "
"``device_del`` requests would cancel the initial call leading to a situation"
" where the device was never detached fully."
msgstr ""
"`bug 1882521`_은 현재 장치 분리 시의递增 및 최대 잠금 시간을 증가시켜 해결되었다. 이 방법은 `bug 1894804`_에서"
" 설명된 일부 QEMU 행동을 해결한다. 이 행동은 `device_del` 요청이 겹치면 초기 호출을 취소하여 장치가 완전히 분리되지 "
"않는 상황을 일으킨다."

#: ../../<reno.sphinxext unmaintained/victoria>:489
msgid ""
"`bug 1894804`_ documents a known device detachment issue with QEMU ``4.2.0``"
" as shipped by the Focal ``20.04`` Ubuntu release. This can lead to the "
"failure to detach devices from the underlying libvirt domain of an instance "
"as QEMU never emits the correct ``DEVICE_DELETED`` event to libvirt. This in"
" turn leaves the device attached within libvirt and OpenStack Nova while it "
"has been detached from the underlying QEMU process. Subsequent attempts to "
"detach the device will also fail as it is no longer found within the QEMU "
"process."
msgstr ""
"`bug 1894804`_은 Focal `20.04` Ubuntu 릴리스에서 배포된 QEMU `4.2.0`에서 알려진 장치 분리 문제를 "
"설명합니다. 이 문제는 QEMU가 libvirt의 underlying domain의 장치를 분리할 수 없게 되며, libvirt의 "
"DEVICE_DELETED 이벤트를正确하게.emit하지 않기 때문입니다. 이로 인해 libvirt와 OpenStack Nova의 "
"OpenStack Nova에서 장치를 분리한 후에도 장치가 QEMU 프로세스에서 더 이상 찾을 수 없기 때문에 subsequent "
"attempts를 시도할 때도 실패합니다."

#: ../../<reno.sphinxext stable/pike>:1595
msgid "`host`"
msgstr "`호스트`"

#: ../../<reno.sphinxext stable/pike>:1600
msgid "`log`"
msgstr "`로그`"

#: ../../<reno.sphinxext stable/pike>:1607
msgid "`project`"
msgstr "`project`"

#: ../../<reno.sphinxext stable/pike>:1612
msgid "`shell`"
msgstr "`shell`"

#: ../../<reno.sphinxext unmaintained/wallaby>:425
msgid "`vDPA (vHost data path acceleration)`__ usage is now possible"
msgstr "`vDPA (vHost data path acceleration)`__ 사용은 이제 가능합니다"

#: ../../<reno.sphinxext origin/stable/ocata>:1081 origin/stable/ocata>:1136
msgid "access_ip_v4"
msgstr "집합_v4"

#: ../../<reno.sphinxext origin/stable/ocata>:1082 origin/stable/ocata>:1137
msgid "access_ip_v6"
msgstr "access_ip_v6 → access_ip_v6"

#: ../../<reno.sphinxext origin/stable/ocata>:1338
msgid "admin_password"
msgstr "admin_password → admin_password"

#: ../../<reno.sphinxext origin/stable/ocata>:1340
msgid "admin_tenant_name"
msgstr "admin_tenant_name → 관리자 tenant 이름"

#: ../../<reno.sphinxext origin/stable/ocata>:1339
msgid "admin_url"
msgstr "admin_url → 관리자 URL"

#: ../../<reno.sphinxext origin/stable/ocata>:1337
msgid "admin_usernale"
msgstr "admin_usernale → admin_usernale"

#: ../../<reno.sphinxext stable/queens>:886
msgid "aggregate.add_host"
msgstr "집합.add_host"

#: ../../<reno.sphinxext origin/stable/ocata>:753
msgid "aggregate.create"
msgstr "집합.create"

#: ../../<reno.sphinxext origin/stable/ocata>:754
msgid "aggregate.delete"
msgstr "집합. 삭제"

#: ../../<reno.sphinxext stable/queens>:887
msgid "aggregate.remove_host"
msgstr "집합.remove_host"

#: ../../<reno.sphinxext origin/stable/ocata>:1083
msgid "all_tenants"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/2023.1>:518 unmaintained/zed>:145
msgid ""
"apache mod_wsgi does not support passing commandline arguments to the wsgi "
"application that it hosts. As a result when the nova api or metadata api "
"where run under mod_wsgi it was not posible to use multiple config files or "
"non-default file names i.e. nova-api.conf This has been adressed by the "
"intoduction of a new, optional, envionment varible ``OS_NOVA_CONFIG_FILES``."
" ``OS_NOVA_CONFIG_FILES`` is a ``;`` seperated list fo file path relitive to"
" ``OS_NOVA_CONFIG_DIR``. When unset the default ``api-paste.ini`` and "
"``nova.conf`` will be used form ``/etc/nova``. This is supported for the "
"nova api and nova metadata wsgi applications."
msgstr ""
"Apache mod_wsgi는 WSGI 애플리케이션을 호스팅하는 동안 명령줄 매개 변수를 전달하지 않는다. 이로 인해 nova API "
"또는 metadata API가 mod_wsgi에서 실행되면, 여러 configuration 파일 또는 기본 파일 이름을 사용할 수 없다는"
" 문제가 발생했다. 이 문제는 nova API와 nova metadata WSGI 애플리케이션을 지원하기 위해 new, optional,"
" environment variable `OS_NOVA_CONFIG_FILES`가 도입되었다. `OS_NOVA_CONFIG_FILES`는"
" `;`로 구분된 파일 경로의 relative path가 포함된 파일 목록이다. 이 environment variable가 설정되지 "
"않으면, 기본적으로 `/etc/nova`에 있는 `api-paste.ini`와 `nova.conf`가 사용된다."

#: ../../<reno.sphinxext origin/stable/ocata>:1084 origin/stable/ocata>:1138
msgid "auto_disk_config"
msgstr "auto_disk_config"

#: ../../<reno.sphinxext origin/stable/ocata>:1085 origin/stable/ocata>:1139
#: stable/ussuri>:432
msgid "availability_zone"
msgstr "가용 구역"

#: ../../<reno.sphinxext stable/stein>:1105
msgid ""
"because of the atomic allocation claims made during scheduling by the "
"``filter_scheduler`` driver, it is safe [1]_ to run multiple scheduler "
"workers and scale horizontally"
msgstr ""
"원자적 할당을 통해 스케줄링 시에 ``filter_scheduler`` 드라이버가 제시한 주장으로 인해, 여러 스케줄러 워커를 동시에 "
"실행하고 가로 확장하는 것은 안전하다. [1]"

#: ../../<reno.sphinxext origin/stable/ocata>:1174
msgid "block_device_mapping"
msgstr "집합 매핑"

#: ../../<reno.sphinxext origin/stable/ocata>:1495
msgid "cells.topic"
msgstr "세ลล스.topik"

#: ../../<reno.sphinxext origin/stable/ocata>:1087
msgid "changes-since"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1496
msgid "compute_topic"
msgstr "`compute_topic`"

#: ../../<reno.sphinxext origin/stable/ocata>:1497
msgid "conductor.topic"
msgstr "conductor.topic → conductor.topic"

#: ../../<reno.sphinxext origin/stable/ocata>:1086 origin/stable/ocata>:1140
#: stable/ussuri>:433
msgid "config_drive"
msgstr "드라이브 구성"

#: ../../<reno.sphinxext origin/stable/ocata>:1498
msgid "console_topic"
msgstr "`console_topic`"

#: ../../<reno.sphinxext origin/stable/ocata>:1499
msgid "consoleauth_topic"
msgstr "`consoleauth_topic`"

#: ../../<reno.sphinxext origin/stable/ocata>:1088 origin/stable/ocata>:1141
#: stable/ussuri>:435
msgid "created_at"
msgstr "created_at"

#: ../../<reno.sphinxext origin/stable/ocata>:1089
msgid "deleted"
msgstr "삭제"

#: ../../<reno.sphinxext origin/stable/ocata>:1090
msgid "description"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1091 origin/stable/ocata>:1142
msgid "display_description"
msgstr "집합 설명"

#: ../../<reno.sphinxext origin/stable/ocata>:1092 origin/stable/ocata>:1143
msgid "display_name"
msgstr "display_name"

#: ../../<reno.sphinxext unmaintained/zed>:358
msgid "evmc"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1093
msgid "flavor"
msgstr "flavor → 맛"

#: ../../<reno.sphinxext unmaintained/zed>:354
msgid "frequencies"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1094 origin/stable/ocata>:1144
msgid "host"
msgstr "호스트"

#: ../../<reno.sphinxext origin/stable/ocata>:1095 origin/stable/ocata>:1145
msgid "hostname"
msgstr "`hostname`"

#: ../../<reno.sphinxext origin/stable/ocata>:789
msgid "http://docs.openstack.org/developer/nova/cells.html"
msgstr "http://docs.openstack.org/developer/nova/cells.html"

#: ../../<reno.sphinxext unmaintained/wallaby>:850
msgid ""
"http://lists.openstack.org/pipermail/openstack-"
"dev/2014-September/046764.html"
msgstr ""
"http://lists.openstack.org/pipermail/openstack-"
"dev/2014-September/046764.html"

#: ../../<reno.sphinxext unmaintained/wallaby>:852
msgid ""
"http://lists.openstack.org/pipermail/openstack-dev/2016-July/098703.html"
msgstr "http://lists.openstack.org/pipermail/openstack-dev/2016-7/098703.html"

#: ../../<reno.sphinxext unmaintained/wallaby>:854
msgid ""
"http://lists.openstack.org/pipermail/openstack-dev/2016-November/107233.html"
msgstr ""
"http://lists.openstack.org/pipermail/openstack-dev/2016-November/107233.html"

#: ../../<reno.sphinxext ../source/newton.rst:20 origin/stable/ocata>:268
#: stable/pike>:336
msgid "https://bugs.launchpad.net/nova/+bug/1738094"
msgstr "https://bugs.launchpad.net/nova/+bug/1738094"

#: ../../<reno.sphinxext stable/stein>:272
msgid "https://bugs.launchpad.net/nova/+bug/1825584"
msgstr "https://bugs.launchpad.net/nova/+bug/1825584"

#: ../../<reno.sphinxext stable/stein>:273
msgid "https://bugs.launchpad.net/nova/+bug/1829062"
msgstr "https://bugs.launchpad.net/nova/+bug/1829062"

#: ../../<reno.sphinxext stable/ussuri>:1118
msgid "https://bugs.launchpad.net/nova/+bug/1863009"
msgstr "https://bugs.launchpad.net/nova/+bug/1863009"

#: ../../<reno.sphinxext stable/ussuri>:1120
msgid "https://bugs.launchpad.net/nova/+bug/1867840"
msgstr "https://bugs.launchpad.net/nova/+bug/1867840"

#: ../../<reno.sphinxext stable/ussuri>:1119
msgid "https://bugs.launchpad.net/nova/+bug/1869396"
msgstr "https://bugs.launchpad.net/nova/+bug/1869396"

#: ../../<reno.sphinxext stable/ussuri>:1123
msgid "https://bugs.launchpad.net/nova/+bug/1869543"
msgstr "https://bugs.launchpad.net/nova/+bug/1869543"

#: ../../<reno.sphinxext stable/ussuri>:1121
msgid "https://bugs.launchpad.net/nova/+bug/1869791"
msgstr "https://bugs.launchpad.net/nova/+bug/1869791"

#: ../../<reno.sphinxext stable/ussuri>:1122
msgid "https://bugs.launchpad.net/nova/+bug/1869841"
msgstr "https://bugs.launchpad.net/nova/+bug/1869841"

#: ../../<reno.sphinxext stable/ussuri>:1131
msgid "https://bugs.launchpad.net/nova/+bug/1870226"
msgstr "https://bugs.launchpad.net/nova/+bug/1870226"

#: ../../<reno.sphinxext stable/ussuri>:1128
msgid "https://bugs.launchpad.net/nova/+bug/1870484"
msgstr "https://bugs.launchpad.net/nova/+bug/1870484"

#: ../../<reno.sphinxext stable/ussuri>:1126
msgid "https://bugs.launchpad.net/nova/+bug/1870488"
msgstr "https://bugs.launchpad.net/nova/+bug/1870488"

#: ../../<reno.sphinxext stable/ussuri>:1127
msgid "https://bugs.launchpad.net/nova/+bug/1870872"
msgstr "https://bugs.launchpad.net/nova/+bug/1870872"

#: ../../<reno.sphinxext stable/ussuri>:1129
msgid "https://bugs.launchpad.net/nova/+bug/1870881"
msgstr "https://bugs.launchpad.net/nova/+bug/1870881"

#: ../../<reno.sphinxext stable/ussuri>:1124
msgid "https://bugs.launchpad.net/nova/+bug/1870883"
msgstr "https://bugs.launchpad.net/nova/+bug/1870883"

#: ../../<reno.sphinxext stable/ussuri>:1125
msgid "https://bugs.launchpad.net/nova/+bug/1871287"
msgstr "https://bugs.launchpad.net/nova/+bug/1871287"

#: ../../<reno.sphinxext stable/ussuri>:1130
msgid "https://bugs.launchpad.net/nova/+bug/1871665"
msgstr "https://bugs.launchpad.net/nova/+bug/1871665"

#: ../../<reno.sphinxext stable/queens>:1161
msgid "https://bugs.launchpad.net/python-glanceclient/+bug/1707995"
msgstr "https://bugs.launchpad.net/python-glanceclient/+bug/1707995"

#: ../../<reno.sphinxext stable/rocky>:225 stable/stein>:1307
msgid "https://developer.openstack.org/api-ref/image/v2/index.html#sharing"
msgstr ""
"https://developer.openstack.org/api-ref/image/v2/index.html#sharing\n"
"\n"
"집합 openstack.org의 API Ref의 Image API의 Sharing Section을 참조하십시오."

#: ../../<reno.sphinxext stable/rocky>:834
msgid ""
"https://docs.openstack.org/cinder/latest/admin/blockstorage-basic-volume-"
"qos.html"
msgstr ""
"https://docs.openstack.org/cinder/latest/admin/blockstorage-basic-volume-qos.html\n"
"\n"
"* blockstorage-basic-volume-qos\n"
"* basic volume QoS"

#: ../../<reno.sphinxext stable/queens>:1567
msgid ""
"https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html#scheduling-based-on-resource-classes"
msgstr ""
"https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html#scheduling-based-on-resource-classes"

#: ../../<reno.sphinxext stable/rocky>:1743
msgid ""
"https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html#scheduling-resource-classes"
msgstr ""
"https://docs.openstack.org/ironic/latest/install/configure-nova-"
"flavors.html#scheduling-resource-classes"

#: ../../<reno.sphinxext stable/rocky>:411
msgid ""
"https://docs.openstack.org/ironic/rocky/install/configure-nova-flavors.html"
msgstr ""
"https://docs.openstack.org/ironic/rocky/install/configure-nova-flavors.html\n"
"\n"
"https://docs.openstack.org/ironic/rocky/install/configure-nova-flavors.html"

#: ../../<reno.sphinxext stable/rocky>:1043
msgid ""
"https://docs.openstack.org/nova/latest/admin/configuration/schedulers.html#aggregates-"
"in-placement"
msgstr ""
"https://docs.openstack.org/nova/latest/admin/configuration/schedulers.html#집합-"
"in-placement"

#: ../../<reno.sphinxext stable/rocky>:944
msgid "https://docs.openstack.org/nova/latest/admin/file-backed-memory.html"
msgstr "https://docs.openstack.org/nova/latest/admin/파일 기반 메모리"

#: ../../<reno.sphinxext unmaintained/yoga>:421
msgid ""
"https://docs.openstack.org/nova/latest/admin/hw-machine-type.html#device-"
"bus-and-model-image-properties"
msgstr ""
"https://docs.openstack.org/nova/latest/admin/hw-machine-type.html#device-"
"bus-and-model-image-속성"

#: ../../<reno.sphinxext stable/rocky>:1054 stable/rocky>:2098
#: stable/stein>:1117
msgid "https://docs.openstack.org/nova/latest/cli/nova-manage.html#placement"
msgstr ""
"https://docs.openstack.org/nova/latest/cli/nova-manage.html#placement\n"
"\n"
"placement"

#: ../../<reno.sphinxext stable/rocky>:707
msgid ""
"https://docs.openstack.org/nova/latest/configuration/index.html#placement-"
"policy"
msgstr ""
"https://docs.openstack.org/nova/latest/configuration/index.html#placement-policy\n"
"\n"
"집합 정책"

#: ../../<reno.sphinxext stable/stein>:1123
msgid ""
"https://docs.openstack.org/nova/latest/contributor/policies.html#out-of-"
"tree-support"
msgstr ""
"https://docs.openstack.org/nova/latest/Contributor%20policies.html#out-of-"
"tree-support"

#: ../../<reno.sphinxext unmaintained/yoga>:440
msgid "https://libvirt.org/drvnodedev.html#VPDCap"
msgstr ""
"https://libvirt.org/drvnodedev.html#VPDCap\n"
"\n"
"*libvirt.org/drvnodedev.html#VPDCap*를 한국어로 번역하면 다음과 같습니다.\n"
"\n"
"https://libvirt.org/drvnodedev.html#VPDCap"

#: ../../<reno.sphinxext unmaintained/yoga>:439
msgid "https://libvirt.org/news.html#v7-9-0-2021-11-01"
msgstr ""
"https://libvirt.org/news.html#v7-9-0-2021-11-01\n"
"\n"
"*libvirt*의 *v7.9.0* 버전이 2021년 11월 1일에 출시되었습니다."

#: ../../<reno.sphinxext stable/queens>:1850
msgid ""
"https://specs.openstack.org/openstack/nova-"
"specs/specs/queens/approved/cinder-new-attach-apis.html"
msgstr ""
"https://specs.openstack.org/openstack/nova-"
"specs/specs/queens/approved/cinder-new-attach-apis.html"

#: ../../<reno.sphinxext origin/stable/ocata>:1096
msgid "image"
msgstr "이미지"

#: ../../<reno.sphinxext unmaintained/yoga>:336
msgid ""
"image meta now includes the ``hw_emulation_architecture`` property. This "
"allows an operator to define their emulated cpu architecture for an image, "
"and nova will deploy accordingly."
msgstr ""
"이미지 메타 now 포함 ``hw_emulation_architecture`` 속성. 이것은 운영자가 이미지에 대한 simulatored"
" cpu 아키텍처를 정의할 수 있게 해주고, nova는 accordingly 배포합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1097 origin/stable/ocata>:1146
msgid "image_ref"
msgstr "image_ref"

#: ../../<reno.sphinxext origin/stable/ocata>:1175
msgid "info_cache"
msgstr "info_cache"

#: ../../<reno.sphinxext stable/stein>:899
msgid "initial_cpu_allocation_ratio with default value 16.0"
msgstr "initial_cpu_allocation_ratio의 기본값은 16.0입니다."

#: ../../<reno.sphinxext stable/stein>:901
msgid "initial_disk_allocation_ratio with default value 1.0"
msgstr "초기 디스크 할당 비율 (initial_disk_allocation_ratio) - 기본값은 1.0"

#: ../../<reno.sphinxext stable/stein>:900
msgid "initial_ram_allocation_ratio with default value 1.5"
msgstr "initial_ram_allocation_ratio의 기본값은 1.5"

#: ../../<reno.sphinxext origin/stable/ocata>:755
msgid "instance.create"
msgstr "인스턴스.create"

#: ../../<reno.sphinxext stable/queens>:888
msgid "instance.evacuate"
msgstr "instance.evacuate → 인스턴스.이동"

#: ../../<reno.sphinxext origin/stable/ocata>:756
msgid "instance.finish_resize"
msgstr "인스턴스.finish_resize"

#: ../../<reno.sphinxext stable/queens>:889
msgid "instance.interface_attach"
msgstr "인스턴스.인터페이스.연결"

#: ../../<reno.sphinxext stable/queens>:890
msgid "instance.interface_detach"
msgstr "인스턴스.인터페이스_detach"

#: ../../<reno.sphinxext stable/queens>:891
msgid "instance.live_migration_abort"
msgstr "인스턴스.live_migration_abort"

#: ../../<reno.sphinxext stable/queens>:892
msgid "instance.live_migration_pre"
msgstr "instance.live_migration_pre → 인스턴스 → live_migration_pre"

#: ../../<reno.sphinxext origin/stable/ocata>:757
msgid "instance.power_off"
msgstr "인스턴스.파워오프"

#: ../../<reno.sphinxext stable/queens>:893
msgid "instance.rescue"
msgstr "instance.rescue → 인스턴스.재생"

#: ../../<reno.sphinxext stable/queens>:897
msgid "instance.resize.error"
msgstr "인스턴스 리ไซ즈 오류"

#: ../../<reno.sphinxext stable/queens>:894
msgid "instance.resize_confirm"
msgstr "인스턴스. 리사이즈 확인"

#: ../../<reno.sphinxext stable/queens>:895
msgid "instance.resize_prep"
msgstr "인스턴스. 리ไซ즈. 프리프"

#: ../../<reno.sphinxext stable/queens>:896
msgid "instance.resize_revert"
msgstr "인스턴스. 리サイズ 복원"

#: ../../<reno.sphinxext origin/stable/ocata>:758
msgid "instance.resume"
msgstr "인스턴스. 재개"

#: ../../<reno.sphinxext origin/stable/ocata>:759
msgid "instance.shelve_offload"
msgstr "instance.shelve_offload → 인스턴스. 셰일 오프로드"

#: ../../<reno.sphinxext origin/stable/ocata>:760
msgid "instance.shutdown"
msgstr "인스턴스..shutdown"

#: ../../<reno.sphinxext origin/stable/ocata>:761
msgid "instance.snapshot"
msgstr "인스턴스 스냅샷"

#: ../../<reno.sphinxext stable/queens>:898
msgid "instance.trigger_crash_dump"
msgstr "인스턴스. 트리거. 크래시. 댄프"

#: ../../<reno.sphinxext origin/stable/ocata>:762
msgid "instance.unpause"
msgstr "인스턴스.unpause"

#: ../../<reno.sphinxext stable/queens>:899
msgid "instance.unrescue"
msgstr "인스턴스. 언리서스"

#: ../../<reno.sphinxext origin/stable/ocata>:763
msgid "instance.unshelve"
msgstr "instance.unshelve"

#: ../../<reno.sphinxext stable/pike>:962
msgid "instance.volume_attach.end"
msgstr "instance.volume_attach.end → 인스턴스.卷 연결. 끝"

#: ../../<reno.sphinxext stable/pike>:963
msgid "instance.volume_attach.error"
msgstr "instance.volume_attach.error → 인스턴스.卷 연결 오류"

#: ../../<reno.sphinxext stable/pike>:961
msgid "instance.volume_attach.start"
msgstr "인스턴스. volume_attach. start"

#: ../../<reno.sphinxext stable/pike>:965
msgid "instance.volume_detach.end"
msgstr "instance.volume_detach.end → 인스턴스. volume_detach.end"

#: ../../<reno.sphinxext stable/pike>:964
msgid "instance.volume_detach.start"
msgstr "인스턴스. volume_detach.start"

#: ../../<reno.sphinxext origin/stable/ocata>:587
msgid "instance.volume_swap.end"
msgstr "instance.volume_swap.end → 인스턴스. volume_swap. end"

#: ../../<reno.sphinxext origin/stable/ocata>:588
msgid "instance.volume_swap.error"
msgstr "인스턴스. volume_swap.error"

#: ../../<reno.sphinxext origin/stable/ocata>:586
msgid "instance.volume_swap.start"
msgstr "인스턴스. volume_swap. start"

#: ../../<reno.sphinxext origin/stable/ocata>:1147
msgid "instance_type_id"
msgstr "인스턴스 타입 ID"

#: ../../<reno.sphinxext origin/stable/ocata>:1098
msgid "ip"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1099
msgid "ip6"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/zed>:357
msgid "ipi"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1100 origin/stable/ocata>:1148
msgid "kernel_id"
msgstr "kernel_id → 커널 ID"

#: ../../<reno.sphinxext origin/stable/ocata>:1101 origin/stable/ocata>:1149
#: stable/ussuri>:434
msgid "key_name"
msgstr "key_name"

#: ../../<reno.sphinxext stable/queens>:900
msgid "keypair.delete"
msgstr "keypair.delete → keypair.delete"

#: ../../<reno.sphinxext stable/queens>:901
msgid "keypair.import"
msgstr "keypair.import"

#: ../../<reno.sphinxext origin/stable/ocata>:1102 origin/stable/ocata>:1150
msgid "launch_index"
msgstr "launch_index → launch_index"

#: ../../<reno.sphinxext origin/stable/ocata>:1103 origin/stable/ocata>:1151
#: stable/ussuri>:436
msgid "launched_at"
msgstr "launched_at → 시작 시간"

#: ../../<reno.sphinxext stable/rocky>:718
msgid "libvirt: add support for virtio-net rx/tx queue sizes"
msgstr "libvirt: virtio-net rx/tx queue size 지원을 추가합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1104
msgid "limit"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1105 origin/stable/ocata>:1152
msgid "locked_by"
msgstr "locked_by"

#: ../../<reno.sphinxext origin/stable/ocata>:1106
msgid "marker"
msgstr "마커"

#: ../../<reno.sphinxext origin/stable/ocata>:1176
msgid "metadata"
msgstr "집합"

#: ../../<reno.sphinxext stable/2025.2>:127
msgid "migrations for a given server)"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1107
msgid "name"
msgstr "이름"

#: ../../<reno.sphinxext origin/stable/ocata>:1500
msgid "network_topic"
msgstr "네트워크 주제"

#: ../../<reno.sphinxext stable/rocky>:1529
msgid ""
"noVNC 1.0.0 introduced a breaking change in the URLs used to access the "
"console. Previously, the ``vnc_auto.html`` path was used but it is now "
"necessary to use the ``vnc_lite.html`` path. When noVNC is updated to 1.0.0,"
" ``[vnc] novncproxy_base_url`` configuration value must be updated on each "
"compute node to reflect this change."
msgstr ""
"noVNC 1.0.0에서 콘솔에 대한 접근을 위해 URL을 사용하는 변경점이 도입되었다. 이전에는 ``vnc_auto.html`` 경로가"
" 사용되었지만 현재 ``vnc_lite.html`` 경로를 사용해야 한다. noVNC를 1.0.0로 업데이트할 때, ``[vnc] "
"novncproxy_base_url`` 설정 값은 각 컴퓨터 노드에서 업데이트되어야 하는데, 이 변경점을 반영해야 한다."

#: ../../<reno.sphinxext origin/stable/ocata>:1108 origin/stable/ocata>:1153
msgid "node"
msgstr "노드"

#: ../../<reno.sphinxext origin/stable/ocata>:1109
msgid "not-tags          (available in 2.26+)"
msgstr "not-tags          (2.26+에서 사용 가능)"

#: ../../<reno.sphinxext origin/stable/ocata>:1110
msgid "not-tags-any      (available in 2.26+)"
msgstr "not-tags-any      (2.26+에서 사용 가능)"

#: ../../<reno.sphinxext origin/stable/ocata>:1402
msgid "nova-manage vm list"
msgstr "nova-manage vm list"

#: ../../<reno.sphinxext origin/stable/ocata>:857
msgid ""
"nova-scheduler process is now calling the placement API in order to get a "
"list of valid destinations before calling the filters. That works only if "
"all your compute nodes are fully upgraded to Ocata. If some nodes are not "
"upgraded, the scheduler will still lookup from the DB instead which is less "
"performant."
msgstr ""
"nova-scheduler 프로세스는 현재 placement API를 호출하여 유효한 대상들을 확인하기 위해 filters를 호출하기 "
"전에 대상 목록을 얻는 것을 시작했습니다. 이것은 모든 컴퓨터 노드가 Ocata로 완전히 업그레이드된 경우만 작동합니다. 일부 노드가 "
"업그레이드되지 않은 경우, 스케줄러는 DB에서 검색하는 것을 계속합니다. 이것은 성능이 더 낮습니다."

#: ../../<reno.sphinxext stable/stein>:1156
msgid "os_compute_api:image-size"
msgstr "os_compute_api:집합 size"

#: ../../<reno.sphinxext stable/2025.2>:301 stable/2025.2>:380
msgid "os_compute_api:os-assisted-volume-snapshots:create"
msgstr ""
"os_compute_api:os-assisted-volume-snapshots:create \n"
"\n"
"(이.translate는 glossary에 정의된 단어를 exact하게 따라 번역합니다. )"

#: ../../<reno.sphinxext stable/2025.2>:302 stable/2025.2>:381
msgid "os_compute_api:os-assisted-volume-snapshots:delete"
msgstr ""
"os_compute_api:os-assisted-volume-snapshots:delete → os_compute_api:os-"
"assisted-volume-snapshots:delete"

#: ../../<reno.sphinxext stable/stein>:1141
msgid "os_compute_api:os-config-drive"
msgstr "os_compute_api:os-config-drive"

#: ../../<reno.sphinxext stable/stein>:1142
msgid "os_compute_api:os-extended-availability-zone"
msgstr "os_compute_api:os-extended-가용 구역"

#: ../../<reno.sphinxext stable/stein>:1143
msgid "os_compute_api:os-extended-status"
msgstr "os_compute_api:os-extended-status → os_compute_api:os-extended-status"

#: ../../<reno.sphinxext stable/stein>:1144
msgid "os_compute_api:os-extended-volumes"
msgstr "os_compute_api:os-extended-volumes"

#: ../../<reno.sphinxext stable/stein>:1152
msgid "os_compute_api:os-flavor-access (only from /flavors APIs)"
msgstr "os_compute_api:os-flavor-access (만 /flavors API에서만)"

#: ../../<reno.sphinxext stable/stein>:1151
msgid "os_compute_api:os-flavor-rxtx"
msgstr ""
"os_compute_api:os-flavor-rxtx \n"
"\n"
"* os_compute_api: \n"
"  + os- \n"
"    - os: \n"
"      - os: \n"
"        - os: \n"
"          - os: \n"
"            - os: \n"
"              - os: \n"
"                - os: \n"
"                  - os: \n"
"                    - os: \n"
"                      - os: \n"
"                        - os: \n"
"                          - os: \n"
"                            - os: \n"
"                              - os: \n"
"                                - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
"                  - os: \n"
"                    - os: \n"
"                      - os: \n"
"                        - os: \n"
"                          - os: \n"
"                            - os: \n"
"                              - os: \n"
"                                - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
"                - os: \n"
"                  - os: \n"
"                    - os: \n"
"                      - os: \n"
"                        - os: \n"
"                          - os: \n"
"                            - os: \n"
"                              - os: \n"
"                                - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
"                    - os: \n"
"                      - os: \n"
"                        - os: \n"
"                          - os: \n"
"                            - os: \n"
"                              - os: \n"
"                                - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os: \n"
" - os:"

#: ../../<reno.sphinxext stable/stein>:1145
msgid "os_compute_api:os-keypairs"
msgstr "os_compute_api:os-keypairs"

#: ../../<reno.sphinxext stable/stein>:1147
msgid "os_compute_api:os-security-groups (only from /servers APIs)"
msgstr "os_compute_api:os-security-groups (만 /servers API에서만)"

#: ../../<reno.sphinxext stable/2025.2>:303 stable/2025.2>:382
msgid "os_compute_api:os-server-external-events:create"
msgstr "os_compute_api:os-server-external-events:create"

#: ../../<reno.sphinxext stable/stein>:1146
msgid "os_compute_api:os-server-usage"
msgstr "os_compute_api:os-server-usage → os_compute_api:os-서버-사용"

#: ../../<reno.sphinxext stable/2025.2>:304 stable/2025.2>:383
msgid "os_compute_api:os-volumes-attachments:swap"
msgstr ""
"os_compute_api:os-volumes-attachments:swap → os_compute_api:os-volumes-"
"attachments:집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1033
msgid "pci_alias (now pci.alias)"
msgstr "pci_alias (현재 pci.alias)"

#: ../../<reno.sphinxext origin/stable/ocata>:1177
msgid "pci_devices"
msgstr "pci_devices → pci 장치"

#: ../../<reno.sphinxext origin/stable/ocata>:1034
msgid "pci_passthrough_whitelist (now pci.passthrough_whitelist)"
msgstr "pci_passthrough_whitelist (현재 pci.passthrough_whitelist)"

#: ../../<reno.sphinxext origin/stable/ocata>:1111 origin/stable/ocata>:1154
#: stable/ussuri>:438
msgid "power_state"
msgstr "집합"

#: ../../<reno.sphinxext stable/pike>:283 stable/queens>:1804
msgid ""
"prevent swap_volume action if the instance is in state SUSPENDED, STOPPED or"
" SOFT_DELETED. A conflict (409) will be raised now as previously it used to "
"fail silently."
msgstr ""
"suspend 스위치 action을 volume 스위치에 적용하지 않도록 if instance는 state SUSPENDED, "
"STOPPED 또는 SOFT_DELETED 상태에 있으면. 현재는 이전에 fail silently 하던 conflict (409)가 "
"발생합니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1112 origin/stable/ocata>:1155
#: stable/ussuri>:441
msgid "progress"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1113 origin/stable/ocata>:1156
msgid "project_id"
msgstr "project_id"

#: ../../<reno.sphinxext origin/stable/ocata>:611
msgid "quota:disk_total_bytes_sec"
msgstr "집합:디스크 총 바이트 초"

#: ../../<reno.sphinxext origin/stable/ocata>:612
msgid ""
"quota:disk_total_iops_sec - those are normalized IOPS, thus each IO request "
"is accounted for as 1 normalized IO if the size of the request is less than "
"or equal to a predefined base size (8KB)."
msgstr ""
"quota:disk_total_iops_sec - 이들은 정상화된 IOPS를 의미하며, 각 IO 요청이 요청 크기가 8KB 이하인 경우 "
"1정상화된 IO로 계산된다."

#: ../../<reno.sphinxext stable/rocky>:824
msgid "quota:read_bytes_sec_max"
msgstr "quota:read_bytes_sec_max → 할당: 읽기 바이트 초과 시간"

#: ../../<reno.sphinxext stable/rocky>:827
msgid "quota:read_iops_sec_max"
msgstr "quotas:집합:read_iops_sec_max"

#: ../../<reno.sphinxext stable/rocky>:830
msgid "quota:size_iops_sec"
msgstr "집합: size_iops_sec"

#: ../../<reno.sphinxext stable/rocky>:826
msgid "quota:total_bytes_sec_max"
msgstr "집합:total_bytes_sec_max"

#: ../../<reno.sphinxext stable/rocky>:829
msgid "quota:total_iops_sec_max"
msgstr "집합:total_iops_sec_max"

#: ../../<reno.sphinxext stable/rocky>:825
msgid "quota:write_bytes_sec_max"
msgstr "집합:write_bytes_sec_max"

#: ../../<reno.sphinxext stable/rocky>:828
msgid "quota:write_iops_sec_max"
msgstr "집합:write_iops_sec_max"

#: ../../<reno.sphinxext origin/stable/ocata>:1114 origin/stable/ocata>:1157
msgid "ramdisk_id"
msgstr "ramdisk_id"

#: ../../<reno.sphinxext unmaintained/zed>:355
msgid "reenlightenment"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/zed>:362
msgid "relaxed"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1115
msgid "reservation_id"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/zed>:353
msgid "reset"
msgstr "집합"

#: ../../<reno.sphinxext stable/stein>:1102
msgid ""
"resource claims (allocations) are made atomically during scheduling to "
"alleviate the potential for racing to concurrently build servers on the same"
" compute host which could lead to failures"
msgstr ""
"집합 (리소스 주장)가 원자적으로 scheduling 시에 만들어지며, 동시적으로 same compute host에서 same "
"server를 build할 수 있는 racing의 가능성이 줄어들어 fails를 줄일 수 있습니다."

#: ../../<reno.sphinxext origin/stable/ocata>:1116 origin/stable/ocata>:1158
msgid "root_device_name"
msgstr "집합 이름"

#: ../../<reno.sphinxext unmaintained/zed>:351
msgid "runtime"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1501
msgid "scheduler_topic"
msgstr "스케줄러 주제"

#: ../../<reno.sphinxext origin/stable/ocata>:1178
msgid "security_groups"
msgstr "집합"

#: ../../<reno.sphinxext stable/queens>:902
msgid "server_group.create"
msgstr "서버 그룹 생성"

#: ../../<reno.sphinxext stable/queens>:903
msgid "server_group.delete"
msgstr "서버 그룹 삭제"

#: ../../<reno.sphinxext origin/stable/ocata>:1179
msgid "services"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1117
msgid "sort_dir"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1118
msgid "sort_key"
msgstr "집합"

#: ../../<reno.sphinxext unmaintained/zed>:364
msgid "spinlocks retries"
msgstr "집합 retry"

#: ../../<reno.sphinxext origin/stable/ocata>:1119
msgid "status"
msgstr "상태"

#: ../../<reno.sphinxext unmaintained/zed>:352
msgid "synic"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1180
msgid "system_metadata"
msgstr "시스템 메타데이트"

#: ../../<reno.sphinxext origin/stable/ocata>:1120
msgid "tags              (available in 2.26+)"
msgstr "tags              (2.26+에서 사용 가능)"

#: ../../<reno.sphinxext origin/stable/ocata>:1121
msgid "tags-any          (available in 2.26+)"
msgstr "tags-any          (2.26+에서 사용 가능)"

#: ../../<reno.sphinxext origin/stable/ocata>:1122 origin/stable/ocata>:1159
#: stable/ussuri>:439
msgid "task_state"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:1123
msgid "tenant_id"
msgstr "tenant_id → tenant_id"

#: ../../<reno.sphinxext origin/stable/ocata>:1124 origin/stable/ocata>:1160
#: stable/ussuri>:437
msgid "terminated_at"
msgstr "terminated_at → 종료 시간"

#: ../../<reno.sphinxext stable/pike>:736
msgid "this change (1.5)"
msgstr "이 변경 (1.5)"

#: ../../<reno.sphinxext unmaintained/zed>:356
msgid "tlbflush"
msgstr "tlbflush"

#: ../../<reno.sphinxext origin/stable/ocata>:619
msgid "total_bytes_sec"
msgstr "집합"

#: ../../<reno.sphinxext origin/stable/ocata>:620
msgid "total_iops_sec - normalized IOPS"
msgstr "total_iops_sec - 집합 IOPS"

#: ../../<reno.sphinxext stable/rocky>:848
msgid "trait:HW_CPU_X86_AVX2=forbidden"
msgstr "trait: HW_CPU_X86_AVX2= forbidden"

#: ../../<reno.sphinxext stable/queens>:980
msgid "trait:HW_CPU_X86_AVX2=required"
msgstr "trait: HW_CPU_X86_AVX2=required"

#: ../../<reno.sphinxext stable/rocky>:849
msgid "trait:STORAGE_DISK_SSD=forbidden"
msgstr "trait:STORAGE_DISK_SSD=집합"

#: ../../<reno.sphinxext stable/queens>:981
msgid "trait:STORAGE_DISK_SSD=required"
msgstr "trait:집합 디스크 SSD=필수"

#: ../../<reno.sphinxext origin/stable/ocata>:1161
msgid "updated_at"
msgstr "업데이트 일자"

#: ../../<reno.sphinxext stable/pike>:738
msgid "updates this resource provider"
msgstr "집합을 업데이트합니다"

#: ../../<reno.sphinxext origin/stable/ocata>:1125 origin/stable/ocata>:1162
#: stable/ussuri>:442
msgid "user_id"
msgstr "user_id → user_id"

#: ../../<reno.sphinxext origin/stable/ocata>:1126 origin/stable/ocata>:1163
msgid "uuid"
msgstr "uuid"

#: ../../<reno.sphinxext unmaintained/zed>:369
msgid ""
"vDPA support was first introduced in the 23.0.0 (Wallaby) release with "
"limited instance lifecycle operations. Nova now supports all instance "
"lifecycle operations including suspend, attach/detach and hot-plug live "
"migration."
msgstr ""
"vDPA 지원은 23.0.0 (Wallaby) 릴리스에서 제한된 인스턴스 라이프 사이클 운영을 포함한 최초로 도입되었다. Nova는 현재"
" 모든 인스턴스 라이프 사이클 운영을 포함한暂停, 연결/분리 및.hot-플그 live migration을 지원한다."

#: ../../<reno.sphinxext unmaintained/zed>:363
msgid "vapic"
msgstr "vapic"

#: ../../<reno.sphinxext unmaintained/zed>:365
msgid "vendor_id spoofing"
msgstr "vendor_id spoofing → vendor_id 스포핑"

#: ../../<reno.sphinxext origin/stable/ocata>:1127 origin/stable/ocata>:1164
#: stable/ussuri>:440
msgid "vm_state"
msgstr "vm_state"

#: ../../<reno.sphinxext unmaintained/yoga>:544
msgid ""
"vnc-related config options were deprecated in Pike release and now has been "
"removed:"
msgstr "vnc 관련 설정 옵션은 피크 릴리즈에서弃용되었으며 현재 제거되었습니다."

#: ../../<reno.sphinxext unmaintained/zed>:350
msgid "vpindex"
msgstr "집합"

#: ../../<reno.sphinxext stable/train>:23 stable/ussuri>:69
#: unmaintained/victoria>:163 unmaintained/wallaby>:323 unmaintained/xena>:625
msgid "which if visited, will redirect a user to example.com."
msgstr ""
"which if visited, will redirect a user to example.com.\n"
"\n"
"* \"which\" → \"어떤\"\n"
"* \"if\" → \"만약\"\n"
"* \"visited\" → \"방문\"\n"
"* \"will\" → \"will\"\n"
"* \"redirect\" → \"이동\"\n"
"* \"a user\" → \"사용자\"\n"
"* \"to\" → \"로\"\n"
"* \"example.com\" → \"example.com\""

#: ../../<reno.sphinxext stable/ussuri>:555
msgid "which is controlled by the policy ``compute:aggregates:images`` rule."
msgstr "`compute:aggregates:images` 정책에 의해 제어되는もの입니다."

#: ../source/2023.1.rst:3
msgid "2023.1 Series Release Notes"
msgstr "2023.1 시리즈 릴리스 노트"

#: ../source/2023.2.rst:3
msgid "2023.2 Series Release Notes"
msgstr "2023.2 시리즈 릴리스 노트"

#: ../source/2024.1.rst:3
msgid "2024.1 Series Release Notes"
msgstr "2024.1 시리즈 릴리스 노트"

#: ../source/2024.2.rst:3
msgid "2024.2 Series Release Notes"
msgstr "2024.2 시리즈 릴리스 노트"

#: ../source/2025.1.rst:3
msgid "2025.1 Series Release Notes"
msgstr "2025.1 시리즈 릴리스 노트"

#: ../source/2025.2.rst:3
msgid "2025.2 Series Release Notes"
msgstr "2025.2 시리즈 릴리스 노트"

#: ../source/index.rst:3
msgid "Nova Release Notes"
msgstr "노바 릴리스 노트"

#: ../source/liberty.rst:3
msgid "Liberty Series Release Notes"
msgstr "자유 시리즈 릴리스 노트"

#: ../source/liberty.rst:8
msgid "12.0.5"
msgstr "12.0.5"

#: ../source/liberty.rst:17
msgid ""
"The qemu-img tool now has resource limits applied which prevent it from "
"using more than 1GB of address space or more than 2 seconds of CPU time. "
"This provides protection against denial of service attacks from maliciously "
"crafted or corrupted disk images. oslo.concurrency>=2.6.1 is required for "
"this fix."
msgstr ""
"qemu-img 도구는 현재 1GB의 주소 공간 또는 2초의 CPU 시간을 초과하지 않도록 리소스 제한이 적용되어 있습니다. 이것은 "
"악성으로 제작된 또는 오류가 있는 디스크 이미지에 대한 도전의 거부 공격에 대한 보호를 제공합니다. "
"oslo.concurrency>=2.6.1이 필요합니다."

#: ../source/liberty.rst:23
msgid "12.0.4"
msgstr "12.0.4"

#: ../source/liberty.rst:32 ../source/mitaka.rst:98 ../source/newton.rst:670
msgid ""
"The ``record`` configuration option for the console proxy services (like "
"VNC, serial, spice) is changed from boolean to string. It specifies the "
"filename that will be used for recording websocket frames."
msgstr ""
"``record`` 설정 옵션은 콘솔 프록시 서비스 (VNC, serial, spice와 같은) 에서 boolean에서 string으로 "
"변경되었습니다. 이 설정은 웹소켓 프레임의 이름을 기록할 filename을 지정합니다."

#: ../source/liberty.rst:42 ../source/mitaka.rst:687 ../source/newton.rst:1108
msgid ""
"When plugging virtual interfaces of type vhost-user the MTU value will not "
"be applied to the interface by nova. vhost-user ports exist only in "
"userspace and are not backed by kernel netdevs, for this reason it is not "
"possible to set the mtu on a vhost-user interface using standard tools such "
"as ifconfig or ip link."
msgstr ""
"vhost-user 인터페이스의 MTU 값은 nova에 의해 적용되지 않습니다. vhost-user 포트는 사용자 스페이스에서만 "
"존재하며, 커널의 네트워크 데브 (netdev)가 backing하지 않기 때문에, ifconfig 또는 ip link과 같은 표준 도구를"
" 사용하여 vhost-user 인터페이스의 MTU 값을 설정할 수 없습니다."

#: ../source/liberty.rst:48
msgid "12.0.3"
msgstr "12.0.3"

#: ../source/liberty.rst:57 ../source/mitaka.rst:670
msgid ""
"[OSSA 2016-007] Host data leak during resize/migrate for raw-backed "
"instances  (CVE-2016-2140)"
msgstr ""
"[OSSA 2016-007] 호스트 데이터泄漏(Host data leak) - raw-backed 인스턴스(Instance)에서 "
"resize/migrate 시 (CVE-2016-2140)"

#: ../source/liberty.rst:59 ../source/mitaka.rst:672
msgid "`Bug 1548450 <https://bugs.launchpad.net/nova/+bug/1548450>`_"
msgstr "`bugs.launchpad.net/nova/+bug/1548450`"

#: ../source/liberty.rst:60 ../source/mitaka.rst:673
msgid ""
"`Announcement <http://lists.openstack.org/pipermail/openstack-"
"announce/2016-March/001009.html>`__"
msgstr ""
"`공고 <http://lists.openstack.org/pipermail/openstack-"
"announce/2016-March/001009.html>`"

#: ../source/liberty.rst:66
msgid "12.0.1"
msgstr "12.0.1"

#: ../source/liberty.rst:75
msgid "The 12.0.1 release contains fixes for two security issues."
msgstr "12.0.1 버전은 두 보안 문제를修正한 버전입니다."

#: ../source/liberty.rst:85 ../source/mitaka.rst:660
msgid "[OSSA 2016-001] Nova host data leak through snapshot (CVE-2015-7548)"
msgstr "[OSSA 2016-001] Nova 호스트 데이터泄露 durch Snapshot (CVE-2015-7548)"

#: ../source/liberty.rst:87 ../source/mitaka.rst:662
msgid "`Bug 1524274 <https://bugs.launchpad.net/nova/+bug/1524274>`_"
msgstr "`bug 1524274 <https://bugs.launchpad.net/nova/+bug/1524274>`_"

#: ../source/liberty.rst:88 ../source/mitaka.rst:663
msgid ""
"`Announcement <http://lists.openstack.org/pipermail/openstack-"
"announce/2016-January/000911.html>`__"
msgstr ""
"`공고 <http://lists.openstack.org/pipermail/openstack-"
"announce/2016-January/000911.html>`"

#: ../source/liberty.rst:90 ../source/mitaka.rst:665
msgid ""
"[OSSA 2016-002] Xen connection password leak in logs via StorageError "
"(CVE-2015-8749)"
msgstr ""
"[OSSA 2016-002] Xen connection password 유출 (StorageError를 통해 로그에) "
"CVE-2015-8749"

#: ../source/liberty.rst:92 ../source/mitaka.rst:667
msgid "`Bug 1516765 <https://bugs.launchpad.net/nova/+bug/1516765>`_"
msgstr "`bugs.launchpad.net/nova/+bug/1516765`"

#: ../source/liberty.rst:93 ../source/mitaka.rst:668
msgid ""
"`Announcement <http://lists.openstack.org/pipermail/openstack-"
"announce/2016-January/000916.html>`__"
msgstr ""
"`공고 <http://lists.openstack.org/pipermail/openstack-"
"announce/2016-January/000916.html>`"

#: ../source/liberty.rst:103
msgid ""
"Fixes a bug where Nova services won't recover after a temporary DB "
"connection issue, when service group DB driver is used together with local "
"conductor, as the driver only handles RPC timeout errors."
msgstr ""
"Nova 서비스가 임시 DB 연결 문제로 recover하지 못하는 버그를修复합니다. 이 버그는 서비스 그룹 DB 드라이버가 로컬 컨더더와"
" 함께 사용되는 경우에 발생합니다. 드라이버는 오직 RPC 타임아웃 오류만 처리합니다."

#: ../source/liberty.rst:107
msgid "For more info see https://bugs.launchpad.net/nova/+bug/1505471"
msgstr "https://bugs.launchpad.net/nova/+bug/1505471"

#: ../source/liberty.rst:111
msgid ""
"Fixes a bug where Nova services won't recover after a temporary DB / MQ "
"connection issue, when service group DB driver is used together with remote "
"conductor, as the driver only handles RPC timeout errors and does not "
"account for other types of errors (e.g. wrapped DB errors on the remote "
"conductor transported over RPC)"
msgstr ""
"Nova 서비스가 임시 DB/MQ 연결 문제로 recover하지 못하는 버그를修复합니다. 이 버그는 서비스 그룹 DB 드라이버가 원격 "
"컨더더와 함께 사용되면 발생합니다. 드라이버는 RPC 타임아웃 오류만 처리하고 다른 오류 종류(예: 원격 컨더더에서 전송된 DB "
"오류가wrapped)로 처리하지 않기 때문입니다."

#: ../source/liberty.rst:117
msgid "For more info see https://bugs.launchpad.net/nova/+bug/1517926"
msgstr "https://bugs.launchpad.net/nova/+bug/1517926"

#: ../source/liberty.rst:127
msgid "Start using reno to manage release notes."
msgstr "reno를 사용하여 릴리스 노트를 관리하기 시작해라."

#: ../source/mitaka.rst:3
msgid "Mitaka Series Release Notes"
msgstr "미타카 시리즈 릴리스 노트"

#: ../source/mitaka.rst:8
msgid "13.1.4"
msgstr "13.1.4"

#: ../source/mitaka.rst:35
msgid "13.1.3"
msgstr "13.1.3"

#: ../source/mitaka.rst:56
msgid "13.1.2"
msgstr "13.1.2"

#: ../source/mitaka.rst:65 ../source/newton.rst:1085
msgid ""
"Fixed bug #1579706: \"Listing nova instances with invalid status raises 500 "
"InternalServerError for admin user\". Now passing an invalid status as a "
"filter will return an empty list. A subsequent patch will then correct this "
"to raise a 400 Bad Request when an invalid status is received."
msgstr ""
"정상화된 버그 #1579706: \"유효하지 않은 상태로 nova 인스턴스를 listing 할 때, admin 사용자에게 500 "
"InternalServerError가 발생한다\". 현재 유효하지 않은 상태를 필터로 passing 할 경우, 비어있는 목록이 반환된다."
" 이후의 패치에서는 유효하지 않은 상태가 수신되는 경우 400 Bad Request를 raises 할 것이다."

#: ../source/mitaka.rst:74
msgid "13.1.1"
msgstr "13.1.1"

#: ../source/mitaka.rst:83 ../source/newton.rst:616
msgid ""
"When using Neutron extension 'port_security' and booting an instance on a "
"network with 'port_security_enabled=False' the Nova API response says there "
"is a 'default' security group attached to the instance which is incorrect. "
"However when listing security groups for the instance there are none listed,"
" which is correct. The API response will be fixed separately with a "
"microversion."
msgstr ""
"Neutron 확장 'port_security'를 사용하여 네트워크에 인스턴스를 부팅하고 "
"'port_security_enabled=False'인 경우 Nova API의 응답은 인스턴스에 'default' 보안 그룹이 부착되어 "
"있는 것으로 말할 수 있지만 오류입니다. 그러나 인스턴스에 대한 보안 그룹 목록을 liệtrich는 경우에는 none이 나와 있습니다. "
"이것은 correct입니다. API 응답은 separately microversion과 함께 fixing되었습니다."

#: ../source/mitaka.rst:89
msgid "13.1.0"
msgstr "13.1.0"

#: ../source/mitaka.rst:108 ../source/newton.rst:1071
msgid ""
"The qemu-img tool now has resource limits applied which prevent it from "
"using more than 1GB of address space or more than 2 seconds of CPU time. "
"This provides protection against denial of service attacks from maliciously "
"crafted or corrupted disk images."
msgstr ""
"qemu-img 도구는 현재 리소스 제한이 적용되어 address space를 1GB 이상 사용하거나 CPU 시간을 2초 이상 사용하지 "
"않도록 제한되었습니다. 이것은 악성으로 제작된 또는 오류가 있는 디스크 이미지에서 발생하는 서비스 거부 공격에 대한 보호를 제공합니다."

#: ../source/mitaka.rst:114
msgid "13.0.0"
msgstr "집합"

#: ../source/mitaka.rst:139
msgid ""
"Nova 13.0.0 release is including a lot of new features and bugfixes. It can "
"be extremely hard to mention all the changes we introduced during that "
"release but we beg you to read at least the upgrade section which describes "
"the required modifications that you need to do for upgrading your cloud from"
" 12.0.0 (Liberty) to 13.0.0 (Mitaka)."
msgstr ""
"노바 13.0.0 버전은 많은 새로운 기능과 버그fix를 포함하고 있습니다. 노바 12.0.0 (리버티)에서 13.0.0 (미타카)로 "
"클라우드를 업그레이드하는 경우에 대한 필요한 수정 사항을 설명하는 업그레이드 섹션을至少 읽어보세요."

#: ../source/mitaka.rst:145
msgid ""
"That said, a few major changes are worth to notice here. This is not an "
"exhaustive list of things to notice, rather just important things you need "
"to know :"
msgstr ""
"그럼에도 불구하고, 몇 가지 주요 변경 사항이 여기서 주목할 가치가 있습니다. 이것은 모든 것을 포함하는 목록이 아니라, 주목할 가치 "
"있는 것들을 알려드릴 뿐입니다."

#: ../source/mitaka.rst:149
msgid "Latest API microversion supported for Mitaka is v2.25"
msgstr "마이타카 버전에서 지원되는 가장 최근의 API 마이크로 버전은 v2.25입니다."

#: ../source/mitaka.rst:150
msgid "Nova now requires a second database (called 'API DB')."
msgstr "Nova에서 두 번째 데이터베이스 (API DB라고 함)를 필요로합니다."

#: ../source/mitaka.rst:151
msgid ""
"A new nova-manage script allows you to perform all online DB migrations once"
" you upgrade your cloud"
msgstr ""
"nova-manage 스크립트가 새로운 버전이 나와서 now 가 사용할 수 있는 새로운 DB-migration 스크립트가 나왔습니다."

#: ../source/mitaka.rst:153
msgid "EC2 API support is fully removed."
msgstr "EC2 API 지원은 완전히 제거되었습니다."

#: ../source/mitaka.rst:175
msgid ""
"Enables NUMA topology reporting on PowerPC architecture from the libvirt "
"driver in Nova but with a caveat as mentioned below. NUMA cell affinity and "
"dedicated cpu pinning code assumes that the host operating system is exposed"
" to threads. PowerPC based hosts use core based scheduling for processes. "
"Due to this, the cores on the PowerPC architecture are treated as threads. "
"Since cores are always less than or equal to the threads on a system, this "
"leads to non-optimal resource usage while pinning. This feature is supported"
" from libvirt version 1.2.19 for PowerPC."
msgstr ""
"NUMA topology reporting를 PowerPC 아키텍처에서 libvirt 드라이버에서 활성화합니다. 그러나 아래에 설명된 "
"caveatem을 제외하면, NUMA 세ลล์ 연관성과 고유 CPU 고정 코드는 호스트 운영 체제가 스레드에 노출되어 있는 것을 "
"가정합니다. PowerPC 기반 호스트는 프로세스에 대한 코어 기반 스케줄링을 사용합니다. 이로 인해 PowerPC 아키텍처의 코어는 "
"스레드와 동일하게 처리됩니다. 시스템에서 코어는 항상 스레드보다 또는เท่าก습니다. 따라서 이로 인해 고정을 할 때 비용이 비대칭적이게"
" 됩니다. 이 기능은 PowerPC에서 libvirt 1.2.19 버전부터 지원됩니다."

#: ../source/mitaka.rst:188
msgid ""
"A new REST API to cancel an ongoing live migration has been added in "
"microversion 2.24. Initially this operation will only work with the libvirt "
"virt driver."
msgstr ""
"다음은 microversion 2.24에서 live migration을 중단하는 새로운 REST API입니다. 이 연산은 초기에 "
"libvirt virt 드라이버와만 작동합니다."

#: ../source/mitaka.rst:192
msgid ""
"It is possible to call attach and detach volume API operations for instances"
" which are in shelved and shelved_offloaded state. For an instance in "
"shelved_offloaded state Nova will set to None the value for the device_name "
"field, the right value for that field will be set once the instance will be "
"unshelved as it will be managed by a specific compute manager."
msgstr ""
"집합 API 연산을 통해 인스턴스에 volume을.attach 또는 detach할 수 있습니다. 인스턴스가 "
"shelved_offloaded 상태인 경우, Nova는 device_name 필드의 value를 None으로 설정합니다. 이 필드의 "
"correct value는 인스턴스가 unshelved 될 때, specific compute manager에 의해 관리되는 경우에만 "
"설정됩니다."

#: ../source/mitaka.rst:196
msgid ""
"It is possible to block live migrate instances with additional cinder "
"volumes attached. This requires libvirt version to be >=1.2.17 and does not "
"work when live_migration_tunnelled is set to True."
msgstr ""
"live migrate 인스턴스를 추가로 cinder 볼륨을 부착하여 live migrate 할 수 있다. 이 경우 libvirt 버전이"
" 1.2.17 이상이어야 하며, live_migration_tunnelled가 True로 설정된 경우에는 작동하지 않는다."

#: ../source/mitaka.rst:200
msgid ""
"Project-id and user-id are now also returned in the return data of os-"
"server-groups APIs. In order to use this new feature, user have to contain "
"the header of request microversion v2.13 in the API request."
msgstr ""
"Project-id와 user-id는 현재 os-server-groups API의 리턴 데이터에 추가로 반환됩니다. 이 새로운 기능을 "
"사용하기 위해서는 API 요청의 헤더에 microversion v2.13의 요청 헤더가 포함되어 있어야 합니다."

#: ../source/mitaka.rst:208
msgid "Add support for enabling uefi boot with libvirt."
msgstr "UEFI 부트를 활성화하는 libvirt를 지원하도록 추가합니다."

#: ../source/mitaka.rst:212
msgid ""
"A new host_status attribute for servers/detail and servers/{server_id}. In "
"order to use this new feature, user have to contain the header of request "
"microversion v2.16 in the API request. A new policy "
"``os_compute_api:servers:show:host_status`` added to enable the feature. By "
"default, this is only exposed to cloud administrators."
msgstr ""
"새 호스트 상태 속성 (host_status)가 서버/세부 사항과 서버/{server_id}에 추가되었다. 이 새로운 기능을 사용하려면,"
" 사용자는 API 요청의 헤더에 microversion v2.16의 헤더를 포함해야 한다. 새로운 정책 "
"os_compute_api:servers:show:host_status가 추가되어 이 기능을 활성화할 수 있다. 기본적으로, 이 기능은 "
"클라우드 관리자에게만 노출된다."

#: ../source/mitaka.rst:220
msgid ""
"A new server action trigger_crash_dump has been added to the REST API in "
"microversion 2.17."
msgstr ""
"`trigger_crash_dump`를 REST API의 `microversion 2.17`에서 추가된 새로운 서버 액션입니다."

#: ../source/mitaka.rst:224
msgid ""
"When RBD is used for ephemeral disks and image storage, make snapshot use "
"Ceph directly, and update Glance with the new location. In case of failure, "
"it will gracefully fallback to the \"generic\" snapshot method.  This "
"requires changing the typical permissions for the Nova Ceph user (if using "
"authx) to allow writing to the pool where vm images are stored, and it also "
"requires configuring Glance to provide a v2 endpoint with direct_url support"
" enabled (there are security implications to doing this). See "
"http://docs.ceph.com/docs/master/rbd/rbd-openstack/ for more information on "
"configuring OpenStack with RBD."
msgstr ""
"RBD를 사용하여 임시 디스크와 이미지 스토리지에 대해 스냅샷을 Ceph에서 직접 사용하고, 새로운 위치를 Glance에 업데이트합니다. 실패 시에는 \"기본\" 스냅샷 방법으로 유연하게 fallback합니다. 이에 대한 권한 변경은 Nova Ceph 사용자 (authx를 사용하는 경우)에서 일반적인 권한을 변경하여 vm 이미지 저장 pool에 쓰기 허용을 허용해야 하며, 또한 Glance를 v2 엔드포인트와 직접 URL 지원이 활성화된 것을 제공해야 합니다. (이것은 보안 implications에 대한 더 많은 정보가 http://docs.ceph.com/docs/master/rbd/rbd-openstack/에서 제공됩니다.)\n"
"\n"
"- RBD를 사용하여 임시 디스크와 이미지 스토리지에 대해 스냅샷을 Ceph에서 직접 사용하고, 새로운 위치를 Glance에 업데이트합니다.\n"
"- 실패 시에는 \"기본\" 스냅샷 방법으로 유연하게 fallback합니다.\n"
"- 이에 대한 권한 변경은 Nova Ceph 사용자 (authx를 사용하는 경우)에서 일반적인 권한을 변경하여 vm 이미지 저장 pool에 쓰기 허용을 허용해야 하며, 또한 Glance를 v2 엔드포인트와 직접 URL 지원이 활성화된 것을 제공해야 합니다.\n"
"- 더 많은 정보는 http://docs.ceph.com/docs/master/rbd/rbd-openstack/에서 제공됩니다."

#: ../source/mitaka.rst:228
msgid ""
"A new option \"live_migration_inbound_addr\" has been added in the "
"configuration file, set None as default value. If this option is present in "
"pre_migration_data, the ip address/hostname provided will be used instead of"
" the migration target compute node's hostname as the uri for live migration,"
" if it's None, then the mechanism remains as it is before."
msgstr ""
"live_migration_inbound_addr 옵션은 구성 파일에 추가되었으며, 기본값은 None입니다. 이 옵션이 "
"pre_migration_data에 존재한다면, 이 IP 주소/호스트가 live_migration의 URI로 사용됩니다. 이 경우, 이 "
"IP 주소/호스트가 None이면, 이전과 동일한 Mechanism이 유지됩니다."

#: ../source/mitaka.rst:238
msgid ""
"Added support for CPU thread policies, which can be used to control how the "
"libvirt virt driver places guests with respect to CPU SMT \"threads\". These"
" are provided as instance and image metadata options, 'hw:cpu_thread_policy'"
" and 'hw_cpu_thread_policy' respectively, and provide an additional level of"
" control over CPU pinning policy, when compared to the existing CPU policy "
"feature. These changes were introduced in commits '83cd67c' and 'aaaba4a'."
msgstr ""
"CPU 스레드 정책을 추가하여, libvirt virt 드라이버가 CPU SMT \"스레드\"에 대한 게스트를 제어하는 방법을 제어할 수"
" 있습니다. 이러한 정책은 인스턴스 및 이미지 메타데이터 옵션으로 제공됩니다. 'hw:cpu_thread_policy'와 "
"'hw_cpu_thread_policy' respectively. 이러한 정책은 현재 CPU 정책 기능과 비교하여 CPU pinning "
"정책에 추가적인 제어 수준을 제공합니다. 이러한 변경 사항은 '83cd67c'와 'aaaba4a'의 커밋에서 도입되었습니다."

#: ../source/mitaka.rst:242
msgid ""
"Add support for enabling discard support for block devices with libvirt. "
"This will be enabled for Cinder volume attachments that specify support for "
"the feature in their connection properties. This requires support to be "
"present in the version of libvirt (v1.0.6+) and qemu (v1.6.0+) used along "
"with the configured virtual drivers for the instance. The virtio-blk driver "
"does not support this functionality."
msgstr ""
"libvirt에 블록 장치에 대한 버리기 지원을 추가합니다. 이 기능은 Cinder 볼륨 연결이 특성 지원을 지정하는 연결 속성에 의해 "
"활성화됩니다. 이 기능이 활성화되기 위해서는 libvirt 버전 (v1.0.6+) 및 qemu 버전 (v1.6.0+)가 사용되며 구성된 "
"가상 드라이버와 함께 사용되는 버전이 support를 포함해야 합니다. virtio-blk 드라이버는 이 기능을 지원하지 않습니다."

#: ../source/mitaka.rst:246
msgid ""
"A new ``auto`` value for the configuration option ``upgrade_levels.compute``"
" is accepted, that allows automatic determination of the compute service "
"version to use for RPC communication. By default, we still use the newest "
"version if not set in the config, a specific version if asked, and only do "
"this automatic behavior if 'auto' is configured. When 'auto' is used, "
"sending a SIGHUP to the service will cause the value to be re-calculated. "
"Thus, after an upgrade is complete, sending SIGHUP to all services will "
"cause them to start sending messages compliant with the newer RPC version."
msgstr ""
"``auto`` 가용성으로 ``upgrade_levels.compute`` 설정 옵션에 새로운 ``auto`` 가치가 수용되며, 이 "
"가지는 컴퓨터 서비스 버전을 사용하여 RPC 통신을 위한 tự động 결정을 허용합니다. 기본적으로, 설정이 없으면 새로운 버전을 "
"사용하고, 특정 버전이 요청되면, 이 tự động화된 행동을 사용하지 않습니다. 'auto'가 구성된 경우에만. 'auto'가 "
"사용되면, 서비스에 SIGHUP를 보냈을 때, 가치가 재 계산됩니다. 업그레이드가 완료되면, 모든 서비스에 SIGHUP를 보냈을 때, "
"새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 "
"보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 "
"메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC "
"버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운"
" RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 "
"때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를"
" 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는"
" 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC 버전에 맞는 메시지를 보냈을 때, 새로운 RPC "
"버전에 맞는 메시지를 보"

#: ../source/mitaka.rst:258
msgid "Libvirt driver in Nova now supports Cinder DISCO volume driver."
msgstr "Libvirt 드라이버는 Nova에서 현재 Cinder DISCO 볼륨 드라이버를 지원합니다."

#: ../source/mitaka.rst:262
msgid ""
"A disk space scheduling filter is now available, which prefers compute nodes"
" with the most available disk space.  By default, free disk space is given "
"equal importance to available RAM.  To increase the priority of free disk "
"space in scheduling, increase the disk_weight_multiplier option."
msgstr ""
"디스크 공간 scheduling 필터가 현재 उपलब져 있으며, 가장 많은 디스크 공간이 있는 컴퓨터 노드가 선호됩니다.  기본적으로, "
"디스크 공간이 비어 있는 디스크 공간이 RAM에 대한 사용 가능한 공간과 동일한 중요도를 가집니다.  디스크 공간의 우선순위를 "
"증가시키고자 할 때, 디스크_weight_multiplier 옵션을 증가시킵니다."

#: ../source/mitaka.rst:266
msgid ""
"A new REST API to force live migration to complete has been added in "
"microversion 2.22."
msgstr ""
"microversion 2.22에서 새로운 REST API가 추가되어 live migration을 완료하는 데 강제로 live "
"migration을 사용할 수 있습니다."

#: ../source/mitaka.rst:270
msgid ""
"The os-instance-actions methods now read actions from deleted instances. "
"This means that 'GET /v2.1/{tenant-id}/servers/{server-id}/os-instance-"
"actions' and 'GET /v2.1/{tenant-id}/servers/{server-id}/os-instance-"
"actions/{req-id}' will return instance-action items even if the instance "
"corresponding to '{server-id}' has been deleted."
msgstr ""
"os-인스턴스-액션 메소드의 현재 기능은 삭제된 인스턴스에서 액션을 읽는 것을 의미합니다. 이로 인해 'GET /v2.1/{tenant-"
"id}/servers/{server-id}/os-instance-actions' 및 'GET /v2.1/{tenant-"
"id}/servers/{server-id}/os-instance-actions/{req-id}'가 '{server-id}'에 해당하는 "
"인스턴스가 삭제된 경우에도 인스턴스-액션 아이템을 반환합니다."

#: ../source/mitaka.rst:274
msgid ""
"When booting an instance, its sanitized 'hostname' attribute is now used to "
"populate the 'dns_name' attribute of the Neutron ports the instance is "
"attached to. This functionality enables the Neutron internal DNS service to "
"know the ports by the instance's hostname. As a consequence, commands like "
"'hostname -f' will work as expected when executed in the instance. When a "
"port's network has a non-blank 'dns_domain' attribute, the port's 'dns_name'"
" combined with the network's 'dns_domain' will be published by Neutron in an"
" external DNS as a service like Designate. As a consequence, the instance's "
"hostname is published in the external DNS as a service. This functionality "
"is added to Nova when the 'DNS Integration' extension is enabled in Neutron."
" The publication of 'dns_name' and 'dns_domain' combinations to an external "
"DNS as a service additionally requires the configuration of the appropriate "
"driver in Neutron. When the 'Port Binding' extension is also enabled in "
"Neutron, the publication of a 'dns_name' and 'dns_domain' combination to the"
" external DNS as a service will require one additional update operation when"
" Nova allocates the port during the instance boot. This may have a "
"noticeable impact on the performance of the boot process."
msgstr ""
"인스턴스를 부트로면, 인스턴스의 '호스트네임' 속성은 now 'dns_name' 속성을 가득 채우기 위해 사용됩니다. 이 기능은 "
"Neutron 내부 DNS 서비스가 인스턴스의 호스트네임으로 인스턴스를 식별할 수 있게 합니다. 이로 인해 'hostname -f'와 "
"같은 명령은 인스턴스에서 실행 시에 예상대로 작동합니다. 네트워크의 'dns_domain' 속성이 비어 있지 않은 경우, Neutron은"
" Designate와 같은 서비스와 함께 외부 DNS에 'dns_name'과 'dns_domain'을 공유합니다. 이로 인해 인스턴스의 "
"호스트네임은 외부 DNS에 서비스로 공유됩니다. 'DNS Integration' 확장 기능이 Neutron에서 활성화된 경우, "
"'dns_name'과 'dns_domain'의 결합을 외부 DNS에 서비스로 공유하는 기능이 Nova에 추가됩니다. 'Port "
"Binding' 확장 기능이 Neutron에서 활성화된 경우, Nova가 인스턴스 부트 시 포트를 할당할 때, 'dns_name'과 "
"'dns_domain'의 결합을 외부 DNS에 서비스로 공유하는 것은 추가로 1회 업데이트를 필요로 합니다. 이로 인해 부트 프로세스의 "
"성능에显著한 영향을 줄 수 있습니다."

#: ../source/mitaka.rst:278
msgid ""
"The libvirt driver now has a live_migration_tunnelled configuration option "
"which should be used where the VIR_MIGRATE_TUNNELLED flag would previously "
"have been set or unset in the live_migration_flag and block_migration_flag "
"configuration options."
msgstr ""
"libvirt 드라이버는 jetzt live_migration_tunnelled 구성 옵션을 가지고 있으며, "
"VIR_MIGRATE_TUNNELLED 플래그가 이전에 live_migration_flag와 block_migration_flag 구성 "
"옵션에 설정되거나 비어있었을 때 사용해야 합니다."

#: ../source/mitaka.rst:282
msgid ""
"For the libvirt driver, by default hardware properties will be retrieved "
"from the Glance image and if such haven't been provided, it will use a "
"libosinfo database to get those values. If users want to force a specific "
"guest OS ID for the image, they can now use a new glance image property "
"``os_distro`` (eg. ``--property os_distro=fedora21``). In order to use the "
"libosinfo database, you need to separately install the related native "
"package provided for your operating system distribution."
msgstr ""
"libvirt 드라이버에 대해, 기본적으로 하드웨어 속성은 Glance 이미지에서 가져오고, 그 경우에 이미 제공되지 않은 경우 "
"libosinfo 데이터베이스를 통해 그 값을 얻습니다. 사용자가 이미지에 특정 게스트 OS ID를 강제로 사용하고 싶다면, 새로운 "
"Glance 이미지 속성 ``os_distro`` (예: ``--property os_distro=fedora21``) 를 사용할 수 "
"있습니다. libosinfo 데이터베이스를 사용하려면, 사용하는 운영 체제 배포판에 대한 관련 native 패키지를 separately "
"설치해야 합니다."

#: ../source/mitaka.rst:286
msgid ""
"Add support for allowing Neutron to specify the bridge name for the OVS, "
"Linux Bridge, and vhost-user VIF types."
msgstr ""
"네트워크(Neutron)에서 OVS, 리눅스 브리지(Linux Bridge), vhost-user VIF 유형에 대한 브리지 이름을 "
"지정할 수 있도록 지원을 추가합니다."

#: ../source/mitaka.rst:290
msgid ""
"Added a ``nova-manage db online_data_migrations`` command for forcing online"
" data migrations, which will run all registered migrations for the release, "
"instead of there being a separate command for each logical data migration. "
"Operators need to make sure all data is migrated before upgrading to the "
"next release, and the new command provides a unified interface for doing it."
msgstr ""
"``nova-manage db online_data_migrations`` 명령을 추가하여 온라인 데이터 전환을 강제하는 명령을 "
"추가했습니다. 이 명령은 릴리스에 등록된 모든 전환을 실행합니다. 대신, 논리적 데이터 전환에 대한 별도의 명령이 필요하지 않습니다. "
"업그레이드를 위해 모든 데이터가 전환되어야 하는 운영자는 이 명령이 제공하는 유니파이ED 인터페이스를 사용하여 전환을 수행할 수 "
"있습니다."

#: ../source/mitaka.rst:294
msgid ""
"Provides API 2.18, which makes the use of project_ids in API urls optional."
msgstr "API 2.18을 제공합니다. 이 API는 프로젝트 ID를 API URL에 사용하는 것을 선택적으로 허용합니다."

#: ../source/mitaka.rst:298
msgid ""
"Libvirt with Virtuozzo virtualisation type now supports snapshot operations"
msgstr "Libvirt와 Virtuozzo virtualization type를 사용하는 경우 snapshots 연산이 지원됩니다."

#: ../source/mitaka.rst:302
msgid ""
"Remove ``onSharedStorage`` parameter from server's evacuate action in "
"microversion 2.14. Nova will automatically detect if the instance is on "
"shared storage. Also adminPass is removed from the response body which makes"
" the response body empty. The user can get the password with the server's "
"os-server-password action."
msgstr ""
"``onSharedStorage`` 파라미터를 마이크로 버전 2.14에서 서버의 evacuate 액션에서 제거합니다. 노바는 공유 "
"스토리지에 있는 인스턴스를 tự độngตรวจ출합니다. 또한, adminPass는 응답 바디에서 제거되며, 응답 바디는 비어 있습니다."
" 사용자는 서버의 os-server-password 액션을 통해 패스워드를 얻을 수 있습니다."

#: ../source/mitaka.rst:306
msgid ""
"Add two new list/show API for server-migration. The list API will return the"
" in progress live migratons information of a server. The show API will "
"return a specified in progress live migration of a server. This has been "
"added in microversion 2.23."
msgstr ""
"서버 이식에 새로운 두 개의 목록/แสดง API를 추가한다. 목록 API는 현재 진행 중인 live 이식 정보를 반환한다. 표시 "
"API는 특정한 현재 진행 중인 live 이식 정보를 반환한다. 이 기능은 마이크로 버전 2.23에 추가되었다."

#: ../source/mitaka.rst:314
msgid ""
"A new service.status versioned notification has been introduced. When the "
"status of the Service object is changed nova will send a new service.update "
"notification with versioned payload according to bp versioned-notification-"
"api. The new notification is documented in "
"http://docs.openstack.org/developer/nova/notifications.html"
msgstr ""
"새로운 서비스 상태 버전화通知가 도입되었다. 서비스 오브젝트의 상태가 변경되면 nova는 서비스 업데이트通知를 보낸다. 이通知는 bp "
"버전화通知 API에 따라 버전화된ayload를 포함하여 new service.update通知이다. 새로운通知은 "
"http://docs.openstack.org/developer/nova/notifications.html에 설명되어 있다."

#: ../source/mitaka.rst:318
msgid ""
"Two new policies soft-affinty and soft-anti-affinity have been implemented "
"for the server-group feature of Nova. This means that POST  "
"/v2.1/{tenant_id}/os-server-groups API resource now accepts 'soft-affinity' "
"and 'soft-anti-affinity' as value of the 'policies' key of the request body."
msgstr ""
"다음은 Nova의 server-group 기능에 대한 두 가지 새로운 정책 soft-affinity와 soft-anti-affinity가"
" implementation되었습니다. 이것은 의미가 POST  /v2.1/{tenant_id}/os-server-groups API "
"리소스를 now 'soft-affinity'와 'soft-anti-affinity'를 'policies' key의 request "
"body의 value로 수용합니다."

#: ../source/mitaka.rst:322
msgid ""
"In Nova Compute API microversion 2.19, you can specify a \"description\" "
"attribute when creating, rebuilding, or updating a server instance.  This "
"description can be retrieved by getting server details, or list details for "
"servers. Refer to the Nova Compute API documentation for more information. "
"Note that the description attribute existed in prior Nova versions, but was "
"set to the server name by Nova, and was not visible to the user.  So, "
"servers you created with microversions prior to 2.19 will return the "
"description equals the name on server details microversion 2.19."
msgstr ""
"Nova Compute API microversion 2.19에서, 서버 인스턴스를 생성하거나, 재생성하거나, 업데이트할 때, "
"\"description\" 속성을 지정할 수 있습니다. 이 설명은 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 "
"retrieves하거나, 서버รายละเอียด을 retrieves하거나, 서버รายละเอียด을 retrieves하거나, "
"서버รายละเอียด을 retrieves"

#: ../source/mitaka.rst:328
msgid ""
"As part of refactoring the notification interface of Nova a new config "
"option 'notification_format' has been added to specifies which notification "
"format shall be used by nova. The possible values are 'unversioned' (e.g. "
"legacy), 'versioned', 'both'. The default value is 'both'. The new versioned"
" notifications are documented in "
"http://docs.openstack.org/developer/nova/notifications.html"
msgstr ""
"nova通知 인터페이스의 리파이터링을 위해 새로운 구성 옵션 'notification_format'가 추가되었다. 이 구성 옵션은 "
"nova가 사용할 notification format를 지정한다. possible values는 'unversioned' (예: "
"legacy), 'versioned', 'both'이다. 기본값은 'both'이다. 새로운 versioned通知은 "
"http://docs.openstack.org/developer/nova/notifications.html 에서 설명되어 있다."

#: ../source/mitaka.rst:332
msgid ""
"For the VMware driver, the flavor extra specs for quotas has been extended "
"to support:"
msgstr "VMware 드라이버에 대해,_quota에 대한 flavor extra specs가 확장되어 다음과 같은 지원을 제공합니다."

#: ../source/mitaka.rst:335
msgid ""
"quota:cpu_limit - The cpu of a virtual machine will not exceed this limit, "
"even if there are available resources. This is typically used to ensure a "
"consistent performance of virtual machines independent of available "
"resources. Units are MHz."
msgstr ""
"quota:cpu_limit - 가상 머신의 CPU는 가상 머신이 사용 가능한 자원에 따라도 제한이 없는 경우에도 이 제한이 적용되며, "
"가상 머신의 안정적인 성능을 보장하는 데 사용됩니다. 단위는 MHz입니다."

#: ../source/mitaka.rst:339
msgid "quota:cpu_reservation - guaranteed minimum reservation (MHz)"
msgstr "quota:cpu_reservation - 보장된 최소 예약 (MHz)"

#: ../source/mitaka.rst:340
msgid ""
"quota:cpu_shares_level - the allocation level. This can be 'custom', 'high',"
" 'normal' or 'low'."
msgstr ""
"quota:cpu_shares_level - 할당 수준. 이는 'custom', 'high', 'normal' 또는 'low'의 중 "
"하나가 될 수 있습니다."

#: ../source/mitaka.rst:342
msgid ""
"quota:cpu_shares_share - in the event that 'custom' is used, this is the "
"number of shares."
msgstr "quota:cpu_shares_share - 'custom'을 사용할 경우, 이는 공유 수를 나타낸다."

#: ../source/mitaka.rst:344
msgid ""
"quota:memory_limit - The memory utilization of a virtual machine will not "
"exceed this limit, even if there are available resources. This is typically "
"used to ensure a consistent performance of virtual machines independent of "
"available resources. Units are MB."
msgstr ""
"quota:memory_limit - 가상 머신의 메모리 사용은 이 제한을 초과하지 않으며, readily available "
"resources가 있더라도. 이 제한은 일반적으로 가상 머신의-consistent performance을 보장하기 위해 사용됩니다. "
"단位는 MB입니다."

#: ../source/mitaka.rst:348
msgid "quota:memory_reservation - guaranteed minimum reservation (MB)"
msgstr "quota:memory_reservation - 보장된 최소 예약 (MB)"

#: ../source/mitaka.rst:349
msgid ""
"quota:memory_shares_level - the allocation level. This can be 'custom', "
"'high', 'normal' or 'low'."
msgstr ""
"quota:memory_shares_level - 메모리 공유 수준. 이 수준은 'custom', 'high', 'normal' 또는 "
"'low' 가 possible."

#: ../source/mitaka.rst:351
msgid ""
"quota:memory_shares_share - in the event that 'custom' is used, this is the "
"number of shares."
msgstr "quota:memory_shares_share - 'custom'를 사용할 경우, 이는 공유의 수를 나타낸다."

#: ../source/mitaka.rst:353
msgid ""
"quota:disk_io_limit - The I/O utilization of a virtual machine will not "
"exceed this limit. The unit is number of I/O per second."
msgstr "quota:disk_io_limit - 디스크 I/O 사용량은 이 제한 below 1초당 I/O의 수를 초과하지 않습니다."

#: ../source/mitaka.rst:355
msgid ""
"quota:disk_io_reservation - Reservation control is used to provide "
"guaranteed allocation in terms of IOPS"
msgstr ""
"quota:disk_io_reservation - 디스크 I/O 예약은 IOPS에 대한 보장된 할당을 제공하기 위해 사용되는 예약 "
"제어입니다."

#: ../source/mitaka.rst:357
msgid ""
"quota:disk_io_shares_level - the allocation level. This can be 'custom', "
"'high', 'normal' or 'low'."
msgstr ""
"quota:disk_io_shares_level - 할당 수준. 이는 'custom', 'high', 'normal' 또는 'low'의 "
"중 하나일 수 있습니다."

#: ../source/mitaka.rst:359
msgid ""
"quota:disk_io_shares_share - in the event that 'custom' is used, this is the"
" number of shares."
msgstr "quota:disk_io_shares_share - 'custom'을 사용할 경우, 이는 공유의 수를 나타낸다."

#: ../source/mitaka.rst:361
msgid ""
"quota:vif_limit - The bandwidth limit for the virtual network adapter. The "
"utilization of the virtual network adapter will not exceed this limit, even "
"if there are available resources. Units in Mbits/sec."
msgstr ""
"quota:vif_limit - 가상 네트워크 어댑터의-bandwidth 제한. 가상 네트워크 어댑터의 사용은 이 제한을 초과하지 "
"않으며, readily available resources가 있더라도. 단位는 Mbits/sec."

#: ../source/mitaka.rst:364
msgid ""
"quota:vif_reservation - Amount of network bandwidth that is guaranteed to "
"the virtual network adapter. If utilization is less than reservation, the "
"resource can be used by other virtual network adapters. Reservation is not "
"allowed to exceed the value of limit if limit is set. Units in Mbits/sec."
msgstr ""
"quota:vif_reservation - 네트워크 밸런스-bandwidth가 보장된 가상 네트워크 어드apter의 양. "
"utilization이 reservation보다 낮을 때, 다른 가상 네트워크 어드apter가 사용할 수 있습니다. limit이 설정된 "
"경우, limit의值를 초과할 수 없습니다. 단位는 Mbits/sec입니다."

#: ../source/mitaka.rst:369
msgid ""
"quota:vif_shares_level - the allocation level. This can be 'custom', 'high',"
" 'normal' or 'low'."
msgstr ""
"quota:vif_shares_level - 할당 수준. 이는 'custom', 'high', 'normal' 또는 'low'의 한가운데"
" 하나를 의미합니다."

#: ../source/mitaka.rst:371
msgid ""
"quota:vif_shares_share - in the event that 'custom' is used, this is the "
"number of shares."
msgstr "quota:vif_shares_share - 'custom'를 사용할 경우, 이는 공유의 수를 나타낸다."

#: ../source/mitaka.rst:382
msgid ""
"All noVNC proxy configuration options have been added to the 'vnc' group. "
"They should no longer be included in the 'DEFAULT' group."
msgstr ""
"모든 noVNC 프록시 구성 옵션은 'vnc' 그룹에 추가되었으며, 더 이상 'DEFAULT' 그룹에 포함되지 않도록 할 수 있습니다."

#: ../source/mitaka.rst:386
msgid ""
"All VNC XVP configuration options have been added to the 'vnc' group. They "
"should no longer be included in the 'DEFAULT' group."
msgstr ""
"모든 VNC XVP 구성 옵션은 'vnc' 그룹에 추가되었으며, 더 이상 'DEFAULT' 그룹에 포함되지 않도록 해야 합니다."

#: ../source/mitaka.rst:390
msgid ""
"Upon first startup of the scheduler service in Mitaka, all defined "
"aggregates will have UUIDs generated and saved back to the database. If you "
"have a significant number of aggregates, this may delay scheduler start as "
"that work is completed, but it should be minor for most deployments."
msgstr ""
"Mitaka에서 스케줄러 서비스의 첫 시작 시, 모든 정의된 집합은 UUID를 생성하고 데이터베이스에 저장합니다. 만약 집합의 수가 "
"크면, 스케줄러 시작에 지연이 발생할 수 있지만, 일반적인 배포에서는 이가 소중하지 않습니다."

#: ../source/mitaka.rst:394
msgid ""
"During an upgrade to Mitaka, operators must create and initialize a database"
" for the API service. Configure this in [api_database]/connection, and then "
"run ``nova-manage api_db sync``"
msgstr ""
"Mitaka 업그레이드를 위해, API 서비스에 대한 데이터베이스를 생성하고 초기화해야 합니다. 이 데이터베이스를 설정하려면 "
"[api_database]/connection에 구성하고, 그 다음 `nova-manage api_db sync` 명령을 실행합니다."

#: ../source/mitaka.rst:398
msgid ""
"We can not use microversion 2.25 to do live-migration during upgrade, nova-"
"api will raise bad request if there is still old compute node in the "
"cluster."
msgstr ""
"집합 2.25를 사용하여 live-migration을 live-upgrade 시에 수행할 수 없습니다. nova-api는 클러스터에 "
"여전히 стар Compute 노드가 있는 경우에 bad request를 raises합니다."

#: ../source/mitaka.rst:402
msgid ""
"The option ``scheduler_driver`` is now changed to use entrypoint instead of "
"full class path. Set one of the entrypoints under the namespace "
"'nova.scheduler.driver' in 'setup.cfg'. Its default value is 'host_manager'."
" The full class path style is still supported in current release. But it is "
"not recommended because class path can be changed and this support will be "
"dropped in the next major release."
msgstr ""
"``scheduler_driver`` 옵션은 now entrypoint 대신 full class path를 사용합니다. "
"entrypoint를 'nova.scheduler.driver' 네임스페이스 아래 하나를 설정하세요. 'setup.cfg'에. 기본값은 "
"'host_manager'입니다. 현재 릴리스에서는 full class path 스타일을 지원합니다. 그러나 class path가 변경될"
" 수 있기 때문에 이 지원은 다음 주요 릴리스에서 제거됩니다."

#: ../source/mitaka.rst:411
msgid ""
"The option ``scheduler_host_manager`` is now changed to use entrypoint "
"instead of full class path. Set one of the entrypoints under the namespace "
"'nova.scheduler.host_manager' in 'setup.cfg'. Its default value is "
"'host_manager'. The full class path style is still supported in current "
"release. But it is not recommended because class path can be changed and "
"this support will be dropped in the next major release."
msgstr ""
"`scheduler_host_manager` 옵션은 현재 full class path 대신 entrypoint를 사용하도록 "
"변경되었습니다. `nova.scheduler.host_manager` 네임스페이스 아래의 one of entrypoint를 "
"`setup.cfg`에 설정하세요. 기본값은 `host_manager`입니다. 현재 릴리스에서는 full class path 스타일을 "
"지원합니다. 그러나 class path가 변경될 수 있기 때문에 이 지원은 다음 주요 릴리스에서 제거될 예정입니다."

#: ../source/mitaka.rst:420
msgid ""
"The local conductor mode is now deprecated and may be removed as early as "
"the 14.0.0 release. If you are using local conductor mode, plan on deploying"
" remote conductor by the time you upgrade to the 14.0.0 release."
msgstr ""
"지방 컨트롤러 모드가 현재 deprecated되어 14.0.0 릴리스부터 제거될 수 있습니다. 지방 컨트롤러 모드를 사용하고 있으시다면,"
" 14.0.0 릴리스로 업그레이드할 때까지 원격 컨트롤러를 배포하도록 계획해 주세요."

#: ../source/mitaka.rst:427
msgid ""
"The Extensible Resource Tracker is deprecated and will be removed in the "
"14.0.0 release. If you use this functionality and have custom resources that"
" are managed by the Extensible Resource Tracker, please contact the Nova "
"development team by posting to the openstack-dev mailing list. There is no "
"future planned support for the tracking of custom resources."
msgstr ""
"Extensible Resource Tracker는 14.0.0 릴리스에서弃용되며 제거될 예정입니다. 이 기능을 사용하고 "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource "
"Tracker가 관리하는 custom 리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom "
"리소스를 사용하는 경우, Extensible Resource Tracker가 관리하는 custom 리소스를 사용하는 경우, "
"Extensible"

#: ../source/mitaka.rst:436
msgid ""
"For Liberty compute nodes, the disk_allocation_ratio works as before, you "
"must set it on the scheduler if you want to change it. For Mitaka compute "
"nodes, the disk_allocation_ratio set on the compute nodes will be used only "
"if the configuration is not set on the scheduler. This is to allow, for "
"backwards compatibility, the ability to still override the disk allocation "
"ratio by setting the configuration on the scheduler node. In Newton, we plan"
" to remove the ability to set the disk allocation ratio on the scheduler, at"
" which point the compute nodes will always define the disk allocation ratio,"
" and pass that up to the scheduler. None of this changes the default disk "
"allocation ratio of 1.0. This matches the behaviour of the RAM and CPU "
"allocation ratios."
msgstr ""
"리버티 컴퓨터 노드에 대해 디스크 할당 비율은 이전과 같이 작동합니다. 디스크 할당 비율을 변경하고 싶다면 스케줄러에 설정해야 합니다. "
"미타카 컴퓨터 노드에 대해 디스크 할당 비율이 스케줄러에 설정된 경우가 아니라면, 디스크 할당 비율이 설정된 컴퓨터 노드만 사용됩니다. "
"이는 이전의 반 backward compatibility를 위해, 디스크 할당 비율을 스케줄러 노드에 설정하여 오버라이드 할 수 있는 "
"능력을 유지하기 위해입니다. 뉴턴에서, 디스크 할당 비율을 스케줄러에 설정하는 능력을 제거하고자 합니다. 이때, 컴퓨터 노드는 항상 "
"디스크 할당 비율을 정의하고, 스케줄러에 전달합니다. 이는 디스크 할당 비율의 기본값인 1.0를 변경하지 않습니다. 이는 RAM 및 "
"CPU 할당 비율의 행동과 일치합니다."

#: ../source/mitaka.rst:440
msgid ""
"(Only if you do continuous deployment) "
"1337890ace918fa2555046c01c8624be014ce2d8 drops support for an instance major"
" version, which means that you must have deployed at least commit "
"713d8cb0777afb9fe4f665b9a40cac894b04aacb before deploying this one."
msgstr ""
"(만약 지속적 배포를 사용한다면) 1337890ace918fa2555046c01c8624be014ce2d8가 인스턴스 주요 버전을 "
"지원하지 않게 되면, 이는 최소한 713d8cb0777afb9fe4f665b9a40cac894b04aacb 커밋을 배포한 후에 이 배포를"
" 진행해야 하는ことを 의미합니다."

#: ../source/mitaka.rst:447
msgid "nova now requires ebtables 2.0.10 or later"
msgstr "nova 현재 ebtables 2.0.10 또는 이후 버전이 필요합니다."

#: ../source/mitaka.rst:451
msgid "nova recommends libvirt 1.2.11 or later"
msgstr "nova는 libvirt 1.2.11 또는 이후 버전을 추천합니다."

#: ../source/mitaka.rst:455
msgid ""
"Filters internal interface changed using now the RequestSpec NovaObject "
"instead of an old filter_properties dictionary. In case you run out-of-tree "
"filters, you need to modify the host_passes() method to accept a new "
"RequestSpec object and modify the filter internals to use that new object. "
"You can see other in-tree filters for getting the logic or ask for help in "
"#openstack-nova IRC channel."
msgstr ""
"Filters internal interface가 현재 RequestSpec NovaObject를 사용하여 old "
"filter_properties dictionary 대신에 변경되었다. 내부 필터를 사용하는 경우, tree outside 필터가 "
"사용되는 경우, host_passes() 메소드를 수정하여 새로운 RequestSpec object를 수용하고, 새로운 object를 "
"사용하여 internal 필터를 수정해야 한다. 다른 in-tree 필터를 사용하여 로직을 얻거나, #openstack-nova IRC "
"channel에서 도움을 요청할 수 있다."

#: ../source/mitaka.rst:464
msgid ""
"The ``force_config_drive`` configuration option provided an ``always`` value"
" which was deprecated in the previous release. That ``always`` value is now "
"no longer accepted and deployments using that value have to change it to "
"``True`` before upgrading."
msgstr ""
"``force_config_drive`` 설정 옵션은 이전 버전에서弃해진 ``always`` 값이었습니다. 현재는 ``always`` "
"값이 더 이상 수용되지 않으며, 이전 버전에서 사용하는 ``always`` 값은 ``True``로 변경되어 업그레이드 시에 변경해야 "
"합니다."

#: ../source/mitaka.rst:471
msgid ""
"Support for Windows / Hyper-V Server 2008 R2 has been deprecated in Liberty "
"(12.0.0) and it is no longer supported in Mitaka (13.0.0). If you have "
"compute nodes running that version, please consider moving the running "
"instances to other compute nodes before upgrading those to Mitaka."
msgstr ""
"Windows / Hyper-V Server 2008 R2의 지원은 Liberty (12.0.0)에서 비활성화되었으며, Mitaka "
"(13.0.0)에서 더 이상 지원되지 않습니다. 만약 Compute 노드가 2008 R2 버전을-running하는 인스턴스를 가지고 "
"있다면, Mitaka로 업그레이드하기 전에 다른 Compute 노드에 인스턴스를 옮겨야 합니다."

#: ../source/mitaka.rst:475
msgid ""
"The libvirt driver will now correct unsafe and invalid values for the "
"live_migration_flag and block_migration_flag configuration options. The "
"live_migration_flag must not contain VIR_MIGRATE_SHARED_INC but "
"block_migration_flag must contain it. Both options must contain the "
"VIR_MIGRATE_PEER2PEER, except when using the 'xen' virt type this flag is "
"not supported. Both flags must contain the VIR_MIGRATE_UNDEFINE_SOURCE flag "
"and not contain the VIR_MIGRATE_PERSIST_DEST flag."
msgstr ""
"libvirt 드라이버는 현재 live_migration_flag 및 block_migration_flag 설정 옵션의 불안정하고 비效한"
" 값들을 수정합니다. live_migration_flag에는 VIR_MIGRATE_SHARED_INC가 포함되지 않지만 "
"block_migration_flag에는 포함되어야 합니다. 두 옵션 모두 VIR_MIGRATE_PEER2PEER를 포함해야 합니다. "
"'xen' virt 타입을 사용할 때는 이 플래그가 지원되지 않습니다. 두 플래그 모두 VIR_MIGRATE_UNDEFINE_SOURCE"
" 플래그를 포함해야 하며 VIR_MIGRATE_PERSIST_DEST 플래그를 포함하지 않아야 합니다."

#: ../source/mitaka.rst:479
msgid ""
"The libvirt driver has changed the default value of the 'live_migration_uri'"
" flag, that now is dependent on the 'virt_type'. The old default "
"'qemu+tcp://%s/system' now is adjusted for each of the configured "
"hypervisors. For Xen this will be 'xenmigr://%s/system', for kvm/qemu this "
"will be 'qemu+tcp://%s/system'."
msgstr ""
"libvirt 드라이버는 'live_migration_uri' 플래그의 기본값이 'virt_type'에 의존하게 changed되었습니다."
" 이로 인해 'qemu+tcp://%s/system'가 old default로 사용되었습니다. 그러나 now는 각 구성된 하이퍼바이저에 "
"따라 조정됩니다. Xen의 경우 'xenmigr://%s/system'로, kvm/qemu의 경우 "
"'qemu+tcp://%s/system'로 조정됩니다."

#: ../source/mitaka.rst:483
msgid ""
"The minimum required libvirt is now version 0.10.2. The minimum libvirt for "
"the N release has been set to 1.2.1."
msgstr ""
"minimum required libvirt은 현재 0.10.2 버전입니다. N 릴리즈에 대한 minimum libvirt은 1.2.1 "
"버전으로 설정되었습니다."

#: ../source/mitaka.rst:487
msgid ""
"In order to make project_id optional in urls, we must constrain the set of "
"allowed values for project_id in our urls. This defaults to a regex of "
"``[0-9a-f\\-]+``, which will match hex uuids (with / without dashes), and "
"integers. This covers all known project_id formats in the wild. If your site"
" uses other values for project_id, you can set a site specific validation "
"with ``project_id_regex`` config variable."
msgstr ""
"project_id를 url에 옵셔널로 сделать 위해서는 url에 허용되는 project_id의 집합을 제한해야 합니다. 이 "
"defaults는 url에 project_id의 allowed value를 제한하는 regular expression의 형태입니다. 이 "
"regular expression은 ``[0-9a-f\\-]+``로, 이 hex uuids (with / without dashes), "
"integers를 일치시킵니다. 이 regular expression은 모든 알려진 project_id 형식을ครอบ집니다. 그러나 "
"आपक site가 다른 value를 project_id로 사용한다면, site-specific validation을 "
"``project_id_regex`` config variable로 설정할 수 있습니다."

#: ../source/mitaka.rst:492
msgid ""
"The old neutron communication options that were slated for removal in Mitaka"
" are no longer available. This means that going forward communication to "
"neutron will need to be configured using auth plugins."
msgstr ""
"이전의 마ит카에서 제거할 planned neutron communication 옵션은 더 이상 사용할 수 없습니다. 이에 따라, "
"neutron을 위해 communicate를 계속하기 위해서는 auth 플러그인으로 구성해야 합니다."

#: ../source/mitaka.rst:496
msgid ""
"All code and tests for Nova's EC2 and ObjectStore API support which was "
"deprecated in Kilo "
"(https://wiki.openstack.org/wiki/ReleaseNotes/Kilo#Upgrade_Notes_2) has been"
" completely removed in Mitaka. This has been replaced by the new ec2-api "
"project (http://opendev.org/openstack/ec2-api/)."
msgstr ""
"nova의 EC2 및 ObjectStore API에 대한 모든 코드 및 테스트는 Kilo에서弃용된 support를 완전히 제거되었다. "
"(https://wiki.openstack.org/wiki/ReleaseNotes/Kilo#Upgrade_Notes_2) 이에 대체된 "
"것은 새로운 ec2-api 프로젝트 (http://opendev.org/openstack/ec2-api/) 이다."

#: ../source/mitaka.rst:502
msgid ""
"Some installation tools (such as ``packstack``) hardcode the value of "
"``enabled_apis`` in your nova.conf. While the defaults for ``enabled_apis`` "
"dropped ``ec2`` as a value, if that is hard coded in your nova.conf, you "
"will need to remove it before restarting Nova's API server, or it will not "
"start."
msgstr ""
"다음은 원문입니다.\n"
"\n"
"Some installation tools (such as ``packstack``) hardcode the value of ``enabled_apis`` in your nova.conf. While the defaults for ``enabled_apis`` dropped ``ec2`` as a value, if that is hard coded in your nova.conf, you will need to remove it before restarting Nova's API server, or it will not start.\n"
"\n"
"다음은 한국어 번역입니다.\n"
"\n"
"다음은 일부 설치 도구 (예를 들어 ``packstack``)가 ``enabled_apis``의 값이-hardcode 된 경우에 ``nova.conf``에 있습니다. ``enabled_apis``의 기본값은 ``ec2``를 값으로 drop했습니다. 그러나 ``nova.conf``에 ``ec2``가 hard-coded 된 경우, Nova API 서버를 재시작할 때까지 ``ec2``를 제거해야 합니다. 그렇지 않으면 시작하지 않을 것입니다."

#: ../source/mitaka.rst:510
msgid ""
"The commit with change-id Idd4bbbe8eea68b9e538fa1567efd304e9115a02a requires"
" that the nova_api database is setup and Nova is configured to use it.  "
"Instructions on doing that are provided below."
msgstr ""
"Idd4bbbe8eea68b9e538fa1567efd304e9115a02a의 커밋은 nova_api 데이터베이스가 설정되어야 하며, "
"노바가 이를 사용하도록 구성되어야 한다는 것을 요구합니다.  그에 대한 지침은 아래에 제공됩니다."

#: ../source/mitaka.rst:514
msgid ""
"Nova now requires that two databases are available and configured.  The "
"existing nova database needs no changes, but a new nova_api database needs "
"to be setup.  It is configured and managed very similarly to the nova "
"database.  A new connection string configuration option is available in the "
"api_database group.  An example::"
msgstr ""
"nova now requires that two databases are available and configured.  The "
"existing nova database needs no changes, but a new nova_api database needs "
"to be setup.  It is configured and managed very similarly to the nova "
"database.  A new connection string configuration option is available in the "
"api_database group.  An example: 집합 now requires that two databases are "
"available and configured.  The existing 집합 database needs no changes, but a "
"new nova_api 집합 database needs to be setup.  It is configured and managed "
"very similarly to the 집합 database.  A new connection string configuration "
"option is available in the api_database group.  An example: 집합 now requires "
"that two databases are available and configured.  The existing 집합 database "
"needs no changes, but a new nova_api 집합 database needs to be setup.  It is "
"configured and managed very similarly to the 집합 database.  A new connection "
"string configuration option is available in the api_database group.  An "
"example: 집합 now requires that two databases are available and configured.  "
"The existing 집합 database needs no changes, but a new nova_api 집합 database "
"needs to be setup.  It is configured and managed very similarly to the 집합 "
"database.  A new connection string configuration option is available in the "
"api_database group.  An example: 집합 now requires that two databases are "
"available and configured.  The existing 집합 database needs no changes, but a "
"new nova_api 집합 database needs to be setup.  It is configured and managed "
"very similarly to the 집합 database.  A new connection string configuration "
"option is available in the api_database group.  An example: 집합 now requires "
"that two databases are available and configured.  The existing 집합 database "
"needs no changes, but a new nova_api 집합 database needs to be setup.  It is "
"configured and managed very similarly to the 집합 database.  A new connection "
"string configuration option is available in the api_database group.  An "
"example: 집합 now requires that two databases are available and configured.  "
"The existing 집합 database needs no changes, but a new nova_api 집합 database "
"needs to be setup.  It is configured and managed very similarly to the 집합 "
"database.  A new connection string configuration option is available in the "
"api_database group.  An example: 집합 now requires that two databases are "
"available and configured.  The existing 집합 database needs no changes, but a "
"new nova_api 집합 database needs to be setup.  It is configured and managed"

#: ../source/mitaka.rst:523
msgid ""
"And a new nova-manage command has been added to manage db migrations for "
"this database.  \"nova-manage api_db sync\" and \"nova-manage api_db "
"version\" are available and function like the parallel \"nova-manage db "
"...\" version."
msgstr ""
"nova-manage api_db sync와 nova-manage api_db version을 사용하여 이 데이터베이스의 DB "
"마이그레이션을 관리할 수 있습니다.  \"nova-manage api_db sync\"와 \"nova-manage api_db "
"version\"은 \"nova-manage db ...\" 버전과 마찬가지로 동시적으로 작동합니다."

#: ../source/mitaka.rst:529
msgid ""
"A new ``use_neutron`` option is introduced which replaces the obtuse "
"``network_api_class`` option. This defaults to 'False' to match existing "
"defaults, however if ``network_api_class`` is set to the known Neutron value"
" Neutron networking will still be used as before."
msgstr ""
"``neutron`` 사용을 위한 새로운 옵션 ``use_neutron``가 도입되었습니다. 이 옵션은 "
"``network_api_class`` 옵션을 대체하여 사용됩니다. 이 옵션은 기본적으로 'False'로 설정되지만, "
"``network_api_class``가 Neutron의 알려진 값으로 설정되면 이전과 같이 네트워크 사용이 계속됩니다."

#: ../source/mitaka.rst:533
msgid ""
"The FilterScheduler is now including disabled hosts. Make sure you include "
"the ComputeFilter in the ``scheduler_default_filters`` config option to "
"avoid placing instances on disabled hosts."
msgstr ""
"FilterScheduler는 현재 비활성화된 호스트를 포함합니다. 비활성화된 호스트에 인스턴스를 배치하지 않도록 "
"scheduler_default_filters의 config 옵션에 ComputeFilter를 포함시켜 주세요."

#: ../source/mitaka.rst:537
msgid ""
"Upgrade the rootwrap configuration for the compute service, so that patches "
"requiring new rootwrap configuration can be tested with grenade."
msgstr ""
"rootwrap 구성의 컴퓨터 서비스를 업그레이드하여, 새로운 rootwrap 구성이 필요한 패치가 grenade와 함께 테스트할 수 "
"있도록 하십시오."

#: ../source/mitaka.rst:541
msgid ""
"For backward compatible support the setting "
"``CONF.vmware.integration_bridge`` needs to be set when using the Neutron "
"NSX|MH plugin. The default value has been set to ``None``."
msgstr ""
"뒤로wardsly compatible한 지원을 위해, Neutron NSX|MH 플러그인을 사용할 때는 "
"CONF.vmware.integration_bridge 설정이 필요합니다. 기본값은 None입니다."

#: ../source/mitaka.rst:545
msgid ""
"XenServer hypervisor type has been changed from ``xen`` to ``XenServer``. It"
" could impact your aggregate metadata or your flavor extra specs if you "
"provide only the former."
msgstr ""
"XenServer 하이퍼바이저 유형이 ``xen``에서 ``XenServer``로 변경되었습니다. 이것은 आपक이 집합 메타데이터 또는 "
"flavor의 추가 속성에 영향을 미칠 수 있습니다. 만약만 전형적인 ``xen``만 제공한다면."

#: ../source/mitaka.rst:549
msgid ""
"The glance xenserver plugin has been bumped to version 1.3 which includes "
"new interfaces for referencing glance servers by url. All dom0 will need to "
"be upgraded with this plugin before upgrading the nova code."
msgstr ""
"가 Nzense xenserver 플러그인은 버전 1.3으로 업데이트되어 URL로 참조하는 glance 서버의 새로운 인터페이스를 "
"포함합니다. 모든 dom0는 nova 코드를 업그레이드하기 전에 이 플러그인으로 업그레이드해야 합니다."

#: ../source/mitaka.rst:559
msgid ""
"It is now deprecated to use [glance] api_servers without a protocol scheme "
"(http / https). This is required to support urls throughout the system. "
"Update any api_servers list with fully qualified https / http urls."
msgstr ""
"이제 [glance] api_servers를 http / https 프로토콜 sche-mu (http / https)가 없는 경우 "
"사용하는 것은 더 이상 depreciated입니다. 이는 시스템의 모든 url을 지원하기 위해 필요합니다. api_servers의 목록을"
" 모두 완전한 https / http url로 업데이트하세요."

#: ../source/mitaka.rst:563
msgid ""
"The conductor.manager configuration option is now deprecated and will be "
"removed."
msgstr "conductor.manager 구성 옵션은 현재 deprecated되어 제거될 예정입니다."

#: ../source/mitaka.rst:567
msgid ""
"Deprecate ``compute_stats_class`` config option. This allowed loading an "
"alternate implementation for collecting statistics for the local compute "
"host. Deployments that felt the need to use this facility are encoraged to "
"propose additions upstream so we can create a stable and supported interface"
" here."
msgstr ""
"``compute_stats_class`` 설정 옵션을 비상대합니다. 이 설정 옵션은 로컬 컴퓨터 호스트에 통계를 수집하는 대체 구현을 "
"로드할 수 있도록 허용했습니다. 이 시설을 사용해야 하는 배포가 필요하다면 이 시설을 사용하는 것을 khuyến진합니다. 이 시설을 "
"사용하는 배포는 이 시설을 사용하는 배포를 사용하는 것을 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고"
" 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 "
"제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 "
"배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 "
"사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 "
"시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 "
"제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 "
"배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 "
"사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 "
"시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포를 제외하고 이 시설을 사용하는 배포"

#: ../source/mitaka.rst:571
msgid ""
"Deprecate the ``db_driver`` config option. Previously this let you replace "
"our SQLAlchemy database layer with your own. This approach is deprecated. "
"Deployments that felt the need to use the facility are encourage to work "
"with upstream Nova to address db driver concerns in the main SQLAlchemy code"
" paths."
msgstr ""
"``db_driver`` 설정 옵션을 비활성화하십시오. 이전에 이 옵션은 우리의 SQLAlchemy 데이터베이스 계층을 당신의 "
"eigenen 계층으로 대체할 수있었습니다. 이 접근 방식은 비활성화되었습니다. 이 시설을 사용해야 하는 배포가 필요하다면, 우리의 메인"
" SQLAlchemy 코드 경로에서 db driver에 대한 우려를 해결하기 위해 upstream Nova와 함께 작업하도록 "
"khuyến진합니다."

#: ../source/mitaka.rst:575
msgid ""
"The host, port, and protocol options in the [glance] configuration section "
"are deprecated, and will be removed in the N release. The api_servers value "
"should be used instead."
msgstr ""
"호스트, 포트, 및 프로토콜 옵션은 [glance] 구성 섹션에서 비상대적이며, N 릴리스에서 제거될 예정입니다. api_servers "
"가용性는 대신 사용해야 합니다."

#: ../source/mitaka.rst:579
msgid ""
"Deprecate the use of nova.hooks. This facility used to let arbitrary out of "
"tree code be executed around certain internal actions, but is unsuitable for"
" having a well maintained API. Anyone using this facility should bring "
"forward their use cases in the Newton cycle as nova-specs."
msgstr ""
"nova.hooks를 사용하지 않도록 depreciate 하십시오. 이 시설은 이전에 특정 내부 액션 주변에 arbitrarly tree"
" code를 실행시킬 수있었습니다. 그러나 잘 유지되는 API를 위해 적합하지 않습니다. nova.hooks를 사용하는 사람들은 뉴턴 "
"주기에서 nova-specs를 사용하여 사용 사례를 전달해야합니다."

#: ../source/mitaka.rst:583
msgid ""
"Nova used to support the concept that ``service managers`` were replaceable "
"components. There are many config options where you can replace a manager by"
" specifying a new class. This concept is deprecated in Mitaka as are the "
"following config options."
msgstr ""
"노바는 이전에 \" 서비스 관리자 \"가 대체 가능한 구성 요소로 간주되었다. 많은 구성 옵션들이 있습니다. 이러한 구성 옵션을 사용하여"
" 새로운 클래스를 지정하여 관리자를 대체할 수 있습니다. 이러한 개념은 미타카에서弃기되었다. 다음 구성 옵션도弃기되었다."

#: ../source/mitaka.rst:588
msgid "[cells] manager"
msgstr "[세ลล스] 관리자"

#: ../source/mitaka.rst:589
msgid "metadata_manager"
msgstr "metadata_manager → metadata 매니저"

#: ../source/mitaka.rst:590
msgid "compute_manager"
msgstr "`compute_manager`"

#: ../source/mitaka.rst:591
msgid "console_manager"
msgstr "`console_manager`"

#: ../source/mitaka.rst:592
msgid "consoleauth_manager"
msgstr "`consoleauth_manager`"

#: ../source/mitaka.rst:593
msgid "cert_manager"
msgstr "`cert_manager`"

#: ../source/mitaka.rst:594
msgid "scheduler_manager"
msgstr "스케줄러 매니저"

#: ../source/mitaka.rst:596
msgid ""
"Many of these will be removed in Newton. Users of these options are "
"encouraged to work with Nova upstream on any features missing in the default"
" implementations that are needed."
msgstr ""
"Newton에서 많은 이 기능은 제거될 것이다. Newton에서 사용할 수 있는 이 기능은 Nova upstream와 함께 작업하여 기본"
" 구현에서 missing한 기능을 추가하는 것을 권장한다."

#: ../source/mitaka.rst:602
msgid ""
"Deprecate ``security_group_api`` configuration option. The current values "
"are ``nova`` and ``neutron``. In future the correct security_group_api "
"option will be chosen based on the value of ``use_neutron`` which provides a"
" more coherent user experience."
msgstr ""
"``security_group_api`` 설정 옵션을 비활성화합니다. 현재의 값은 ``nova``와 ``neutron``입니다. 미래에 "
"``use_neutron`` 값에 따라 ``security_group_api`` 옵션의正确한 값이 선택되며 사용자 경험은 더 일관된 "
"것으로 제공됩니다."

#: ../source/mitaka.rst:606
msgid ""
"Deprecate the ``vendordata_driver`` config option. This allowed creating a "
"different class loader for defining vendordata metadata. The default driver "
"loads from a json file that can be arbitrarily specified, so is still quite "
"flexible. Deployments that felt the need to use this facility are encoraged "
"to propose additions upstream so we can create a stable and supported "
"interface here."
msgstr ""
"``vendordata_driver`` 설정 옵션을 비활성화합니다. 이 설정 옵션은 vendordata 메타데이터를 정의하는 class "
"loader를 다른 class loader로 만들 수 있게 해주었습니다. 기본적으로는 json 파일에서 데이터를 로드할 수 있는 "
"flexibility가 있습니다. 그러나, 이 설정 옵션을 사용하는 배포가 필요하다면, 이 기능을 upstream에 추가하는 것을 "
"khuyến진합니다. 이렇게 하면 안정적이고 지원 가능한 인터페이스를 만들 수 있습니다."

#: ../source/mitaka.rst:610
msgid ""
"The configuration option ``api_version`` in the ``ironic`` group was marked "
"as deprecated and will be removed in the future. The only possible value for"
" that configuration was \"1\" (because Ironic only has 1 API version) and "
"the Ironic team came to an agreement that setting the API version via "
"configuration option should not be supported anymore. As the Ironic driver "
"in Nova requests the Ironic v1.8 API, that means that Nova 13.0.0 "
"(\"Mitaka\") requires Ironic 4.0.0 (\"Liberty\") or newer if you want to use"
" the Ironic driver."
msgstr ""
"`ironic` 그룹의 `api_version` 설정 옵션은弃용되었으며 향후에 제거될 예정입니다. 이 설정 옵션의 유일한可能한 값은 "
"\"1\" (Ironic이 1개의 API 버전만 가지고 있기 때문)이며 Ironic 팀은 설정 옵션을 통해 API 버전을 설정하는 것을 "
"더 이상 지원하지 않기로 합의했습니다. Ironic 드라이버는 Nova에서 Ironic v1.8 API를 요청하는 것이므로 Nova "
"13.0.0 (\"Mitaka\")은 Ironic 4.0.0 (\"Liberty\") 또는 newer를 사용하고 싶다면 Ironic "
"드라이버를 사용해야 합니다."

#: ../source/mitaka.rst:614
msgid ""
"The libvirt live_migration_flag and block_migration_flag config options are "
"deprecated. These options gave too fine grained control over the flags used "
"and, in some cases, misconfigurations could have dangerous side effects. "
"Please note the availability of a new live_migration_tunnelled configuration"
" option."
msgstr ""
"libvirt live_migration_flag 및 block_migration_flag config 옵션은弃용되었습니다. 이 옵션은 "
"fine-grained control을 제공했지만, 일부 경우에는 misconfiguration이 위험한 side effects를 일으킬"
" 수 있었습니다. 새로운 live_migration_tunnelled configuration 옵션의 Availability를 "
"확인하세요."

#: ../source/mitaka.rst:623
msgid ""
"The ``network_device_mtu`` option in Nova is deprecated for removal since "
"network MTU should be specified when creating the network with nova-network."
" With Neutron networks, the MTU value comes from the ``segment_mtu`` "
"configuration option in Neutron."
msgstr ""
"``네트워크 장치 MTU`` 옵션은 노바에서 비활성화되어 제거할 수 있는 것으로, 네트워크 MTU가 노바-네트워크와 함께 네트워크를 "
"생성할 때 명시되어야 합니다. 네트론 네트워크의 경우, `segment_mtu` 구성 옵션의值가 네트론의 MTU를 제공합니다."

#: ../source/mitaka.rst:627
msgid ""
"The old top-level resource ``/os-migrations`` is deprecated, it won't be "
"extended anymore. And migration_type for /os-migrations, also add ref link "
"to the /servers/{uuid}/migrations/{id} for it when the migration is an in-"
"progress live-migration. This has been added in microversion 2.23."
msgstr ""
"이전의 고급 리소스 `/os-migrations`는 더 이상 확장되지 않으며, 더 이상 사용되지 않습니다. 또한 `/os-"
"migrations`의 `migration_type`를 추가하고, `/servers/{uuid}/migrations/{id}`에 ref "
"link을 추가합니다. 이 기능은 마이크로 버전 2.23에서 추가되었습니다."

#: ../source/mitaka.rst:631
msgid ""
"Deprecate ``volume_api_class`` and ``network_api_class`` config options. We "
"only have one sensible backend for either of these. These options will be "
"removed and turned into constants in Newton."
msgstr ""
"``volume_api_class`` 및 ``network_api_class``를 비활성화하고 ``volume_api_class`` 및 ``network_api_class``를 제거하고 Newton에서 상수로 바꾸는 것을 고려합니다. \n"
"\n"
"이 두 가지 옵션은 각각 하나의 합리적인 백엔드만 가지고 있습니다. \n"
"\n"
"이 두 가지 옵션은 제거되고 Newton에서 상수로 바뀌게 됩니다."

#: ../source/mitaka.rst:635
msgid ""
"Option ``memcached_servers`` is deprecated in Mitaka. Operators should use "
"oslo.cache configuration instead. Specifically ``enabled`` option under "
"[cache] section should be set to True and the url(s) for the memcached "
"servers should be in [cache]/memcache_servers option."
msgstr ""
"`memcached_servers` 옵션은 미타카에서弃용되었습니다. 사용자들은 oslo.cache 구성으로 대체해야 합니다. 특히 "
"[캐시] 섹션의 `enabled` 옵션을 True로 설정하고 메모리 캐시 서버의 URL(s)가 [캐시]/memcache_servers "
"옵션에 위치해야 합니다."

#: ../source/mitaka.rst:639
msgid "The Zookeeper Service Group driver has been removed."
msgstr "Zookeeper Service Group 드라이버가 제거되었습니다."

#: ../source/mitaka.rst:641
msgid ""
"The driver has no known users and is not actively maintained. A warning log "
"message about the driver's state was added for the Kilo release. Also, "
"evzookeeper library that the driver depends on is unmaintained and "
"`incompatible with recent eventlet releases`_."
msgstr ""
"드라이버는 알려진 사용자가 없고 활성적으로 유지 관리되지 않는다. Kilo 릴리즈에서 드라이버의 상태에 대한 경고 로그 메시지가 "
"추가되었다. 또한 드라이버가 의존하는 evzookeeper 라이브러리도 유지 관리되지 않으며 ` recent eventlet 릴리즈와 "
"호환되지 않는다`_."

#: ../source/mitaka.rst:646
msgid ""
"A future release of Nova will `use the Tooz library to track service "
"liveliness`_, and Tooz supports Zookeeper."
msgstr ""
"nova의 미래 릴리스는 `Tooz 라이브러리`를 사용하여 서비스의 생명력 tracking을 할 것이며, Tooz는 Zookeeper를 "
"지원한다."

#: ../source/mitaka.rst:683
msgid ""
"In a race condition if base image is deleted by ImageCacheManager while "
"imagebackend is copying the image to instance path, then the instance goes "
"in to error state. In this case when libvirt has changed the base file "
"ownership to libvirt-qemu while imagebackend is copying the image, then we "
"get permission denied error on updating the file access time using os.utime."
" Fixed this issue by updating the base file access time with root user "
"privileges using 'touch' command."
msgstr ""
"집합 조건에서 base image가 ImageCacheManager에 의해 삭제되면 imagebackend이 instance path에 "
"이미지 복사 중이면, instance는 오류 상태에 들어간다. 이 경우 libvirt가 base file의 소유권을 libvirt-"
"qemu로 변경하고 imagebackend가 이미지 복사 중이면, file access time을 업데이트하는 os.utime을 사용할 "
"때 permission denied 오류가 발생한다. 이 문제를 해결하기 위해 root 사용자 권한으로 'touch' 명령을 사용하여 "
"base file access time을 업데이트했다."

#: ../source/mitaka.rst:697
msgid "Conductor RPC API no longer supports v2.x."
msgstr "Conductor RPC API는 더 이상 v2.x를 지원하지 않습니다."

#: ../source/mitaka.rst:701
msgid ""
"The service subcommand of nova-manage is deprecated. Use the nova service-* "
"commands from python-novaclient instead or the os-services REST resource. "
"The service subcommand will be removed in the 14.0 release."
msgstr ""
"nova-manage의 서비스 서브комmando는弃用되었습니다. python-novaclient에서 nova service-* 명령어를"
" 사용하거나 os-services REST 리소스를 사용하십시오. 14.0 릴리스에서 서비스 서브комmando가 제거되었습니다."

#: ../source/mitaka.rst:705
msgid ""
"The Neutron network MTU value is now used when plugging virtual interfaces "
"in nova-compute. If the value is 0, which is the default value for the "
"``segment_mtu`` configuration option in Neutron before Mitaka, then the "
"(deprecated) ``network_device_mtu`` configuration option in Nova is used, "
"which defaults to not setting an MTU value."
msgstr ""
"네트워크 MTU 가치가 현재 nova-compute 에서 가상 인터페이스를 연결할 때 사용됩니다. 가치가 0 인 경우, 이전 미타카 "
"시절의 Neutron 에서 사용된 ``segment_mtu`` 구성 옵션의 디폴트 가치와 동일합니다. 이 경우, (Deprecated )"
" 네트워크 장치 MTU 구성 옵션의 네오아에서 사용됩니다. 이 구성 옵션은 디폴트로 MTU 가치를 설정하지 않습니다."

#: ../source/mitaka.rst:709
msgid ""
"The sample policy file shipped with Nova contained many policies set to "
"\"\"(allow all) which was not the proper default for many of those checks. "
"It was also a source of confusion as some people thought \"\" meant to use "
"the default rule. These empty policies have been updated to be explicit in "
"all cases. Many of them were changed to match the default rule of "
"\"admin_or_owner\" which is a more restrictive policy check but does not "
"change the restrictiveness of the API calls overall because there are "
"similar checks in the database already. This does not affect any existing "
"deployment, just the sample file included for use by new deployments."
msgstr ""
"집합 정책 파일이 Nova와 함께 배포된 샘플 정책 파일에는 많은 정책이 \"\"(허용 모든)으로 설정되어 있었는데, 이것은 많은 체크의"
" 기본 정책이 아니었다. 또한, 일부 사람들은 \"\"를 기본 규칙을 사용하도록 사용하도록 생각했다. 이러한 비어 있는 정책은 모든 "
"경우에 명확하게 명시되도록 업데이트되었다. 많은 них이 \"admin_or_owner\"라는 기본 규칙의 정책을 일치시키는 것으로 "
"변경되었다. 이것은 \"admin_or_owner\"라는 정책은 API 호출의 제한성에 영향을 미치지 않지만 데이터베이스에도 유사한 "
"체크가 존재한다는 점을 고려하여 더 제한적인 정책 체크이다. 이것은 현재 배포에 영향을 미치지 않지만 새로운 배포를 위해 사용된 샘플 "
"파일에만 영향을 미친다."

#: ../source/mitaka.rst:715
msgid ""
"Nova's EC2 API support which was deprecated in Kilo "
"(https://wiki.openstack.org/wiki/ReleaseNotes/Kilo#Upgrade_Notes_2) is "
"removed from Mitaka. This has been replaced by the new ec2-api project "
"(http://opendev.org/openstack/ec2-api/)."
msgstr ""
"노바의 EC2 API 지원이 Kilo "
"(https://wiki.openstack.org/wiki/ReleaseNotes/Kilo#Upgrade_Notes_2)에서弃용되었으며,"
" Mitaka에서 제거되었다. 이에 대체된 것은 새로운 ec2-api 프로젝트 "
"(http://opendev.org/openstack/ec2-api/)이다."

#: ../source/newton.rst:3
msgid "Newton Series Release Notes"
msgstr "뉴턴 시리즈 릴리스 노트"

#: ../source/newton.rst:8
msgid "14.1.0"
msgstr "14.1.0"

#: ../source/newton.rst:68
msgid "14.0.10"
msgstr "집합"

#: ../source/newton.rst:84
msgid "The fix is in the ``nova-api`` and ``nova-conductor`` services."
msgstr "``nova-api`` 및 ``nova-conductor`` 서비스에 있는fix이 있습니다."

#: ../source/newton.rst:92
msgid "14.0.7"
msgstr "14.0.7"

#: ../source/newton.rst:120
msgid ""
"The ``nova-manage cell_v2 simple_cell_setup`` command now creates the "
"default cell0 database connection using the ``[database]`` connection "
"configuration option rather than the ``[api_database]`` connection. The "
"cell0 database schema is the ``main`` database, i.e. the ``instances`` "
"table, rather than the ``api`` database schema. In other words, the cell0 "
"database would be called something like ``nova_cell0`` rather than "
"``nova_api_cell0``."
msgstr ""
"nova-manage cell_v2 simple_cell_setup 명령은 현재 [database] 연결 설정 옵션을 사용하여 "
"[api_database] 연결 대신 default cell0 데이터베이스 연결을 생성합니다. cell0 데이터베이스 스키마는 main "
"데이터베이스, 즉 instances 테이블이기 때문에 api 데이터베이스 스키마와는 달리 cell0 데이터베이스는 nova_cell0과 "
"같은 이름으로 불리우게 됩니다."

#: ../source/newton.rst:132
msgid "14.0.5"
msgstr "14.0.5"

#: ../source/newton.rst:151
msgid ""
"The live-migration progress timeout controlled by the configuration option "
"``[libvirt]/live_migration_progress_timeout`` has been discovered to "
"frequently cause live-migrations to fail with a progress timeout error, even"
" though the live-migration is still making good progress. To minimize "
"problems caused by these checks we recommend setting the value to 0, which "
"means do not trigger a timeout.  (This has been made the default in Ocata "
"and Pike.) To modify when a live-migration will fail with a timeout error, "
"please now look at ``[libvirt]/live_migration_completion_timeout`` and "
"``[libvirt]/live_migration_downtime``."
msgstr ""
"live-migration progress timeout controlled by the configuration option "
"``[libvirt]/live_migration_progress_timeout``가 discoveryd에 의해 자주 fail live-"
"migration을 cause하고, live-migration이 still good progress를 making하고, progress "
"timeout error와 함께 fail하는 것을 발견했다. 이러한 체크를 최소화하기 위해, 이 value를 0으로 설정하는 것을 "
"추천한다. (Ocata와 Pike에서 기본으로 설정되었다.) live-migration이 fail할 때 timeout error가 "
"발생하는 것을 modify하려면, now ``[libvirt]/live_migration_completion_timeout``와 "
"``[libvirt]/live_migration_downtime``를 확인하십시오."

#: ../source/newton.rst:178
msgid "14.0.4"
msgstr "14.0.4"

#: ../source/newton.rst:187
msgid ""
"When generating Libvirt XML to attach network interfaces for the ``tap``, "
"``ivs``, ``iovisor``, ``midonet``, and ``vrouter`` virtual interface types "
"Nova previously generated an empty path attribute to the script element "
"(``<script path=''/>``) of the interface."
msgstr ""
"``tap``, ``ivs``, ``iovisor``, ``midonet``, 및 ``vrouter`` 가상 인터페이스 유형을 연결하기 "
"위해 Libvirt XML을 생성할 때, Nova는 이전에 스크립트 요소 (``<script path=''/>``) 의 attribute"
" 에서 비어 있는 path 속성을 생성했습니다."

#: ../source/newton.rst:199
msgid ""
"Additionally, where virtual machines already exist that were created using "
"earlier versions of Libvirt interactions with these virtual machines via "
"Nova or other utilities (e.g. ``virsh``) may result in similar errors."
msgstr ""
"또한, 이전 버전의 Libvirt와의 상호 작용으로 생성된 이미지가 이미지가 존재할 경우, Nova 또는 다른 utility(예를 들어 "
"`virsh`)를 통해 이미지가 작동하는 경우에 similar errors가 발생할 수 있습니다."

#: ../source/newton.rst:232
msgid "14.0.2"
msgstr "14.0.2"

#: ../source/newton.rst:241
msgid ""
"A new database schema migration is included in this release to fix `bug "
"1635446 <https://bugs.launchpad.net/nova/+bug/1635446>`_."
msgstr ""
"`1635446` 버그를修正하기 위해 이 릴리즈에 새로운 데이터베이스 스키마 전환이 포함됩니다. "
"<https://bugs.launchpad.net/nova/+bug/1635446>"

#: ../source/newton.rst:251
msgid ""
"Use of the newly introduced optional placement RESTful API in Newton "
"requires WebOb>=1.6.0. This requirement was not reflected prior to the "
"release of Newton in requirements.txt with the lower limit being set to "
"WebOb>=1.2.3."
msgstr ""
"Newton에서 새로운 옵셔널 배치 RESTful API를 사용하려면 WebOb>=1.6.0을 사용해야 합니다. 이 요구 사항은 "
"Newton의 릴리스 이전에 requirements.txt에 반영되지 않았으며, lower limit은 WebOb>=1.2.3으로 "
"설정되었습니다."

#: ../source/newton.rst:261
msgid ""
"Contains database schema migration "
"``021_build_requests_instance_mediumtext`` which increases the size of the "
"``build_requests.instance`` column on MySQL backends. This is needed to "
"create new instances which have very large ``user_data`` fields."
msgstr ""
"`021_build_requests_instance_mediumtext` 데이터베이스 스키마 전환에 포함되어 있으며 MySQL 백엔드의 "
"`build_requests.instance` стол định의 크기를 증가시킵니다. 이로 인해 매우 큰 `user_data` 필드를 "
"가진 새로운 인스턴스를 생성할 수 있습니다."

#: ../source/newton.rst:270
msgid "14.0.1"
msgstr "14.0.1"

#: ../source/newton.rst:279
msgid ""
"Nova 14.0.0 release is including a lot of new features and bugfixes. It can "
"be extremely hard to mention all the changes we introduced during that "
"release but we beg you to read at least the upgrade section which describes "
"the required modifications that you need to do for upgrading your cloud from"
" 13.0.0 (Mitaka) to 14.0.0 (Newton). That said, a few major changes are "
"worth to notice here. This is not an exhaustive list of things to notice, "
"rather just important things you need to know :"
msgstr ""
"nova 14.0.0 린스은 많은 새로운 기능과 버그fix를 포함하고 있습니다. 이 린스에서 도입된 많은 변경 사항을 모두 언급할 수는 "
"어렵습니다. 따라서 13.0.0 (Mitaka)에서 14.0.0 (Newton)으로 클라우드를 업그레이드하는 데 필요한 수정 사항을 "
"설명하는 업그레이드 섹션을至少 읽어보세요. 그러나, 이 섹션은 모든 변경 사항을 포함하지 않습니다. 대신, 이 섹션에서 언급하는 변경 "
"사항은 중요합니다."

#: ../source/newton.rst:282
msgid "Latest API microversion supported for Newton is v2.38"
msgstr "최근 Newton에서 지원하는 API 마이크로 버전은 v2.38입니다."

#: ../source/newton.rst:283
msgid ""
"Nova now provides a new placement RESTful API endpoint that is for the "
"moment optional where Nova compute nodes use it for providing resources. For"
" the moment, the nova-scheduler is not using it but we plan to check the "
"placement resources for Ocata. In case you plan to rolling-upgrade the "
"compute nodes between Newton and Ocata, please look in the notes below how "
"to use the new placement API."
msgstr ""
"nova now provides a new placement RESTful API endpoint that is for the moment optional where Nova compute nodes use it for providing resources. For the moment, the nova-scheduler is not using it but we plan to check the placement resources for Ocata. In case you plan to rolling-upgrade the compute nodes between Newton and Ocata, please look in the notes below how to use the new placement API.\n"
"\n"
"* \"Nova\" → \"nova\"\n"
"* \"placement\" → \"배치\"\n"
"* \"RESTful API endpoint\" → \"RESTful API endpoint\"\n"
"* \"optional\" → \"선택적\"\n"
"* \"compute nodes\" → \"compute 노드\"\n"
"* \"providing resources\" → \"자원 제공\"\n"
"* \"nova-scheduler\" → \"nova-scheduler\"\n"
"* \"Ocata\" → \"Ocata\"\n"
"* \"rolling-upgrade\" → \"롤링 업그레이드\"\n"
"* \"compute nodes\" → \"compute 노드\"\n"
"* \"Newton\" → \"Newton\"\n"
"* \"notes\" → \"노트\"\n"
"* \"placement API\" → \"배치 API\""

#: ../source/newton.rst:289
msgid ""
"Cells V2 now supports booting instances for one cell v2 only. We plan to add"
" a multi-cell support for Ocata. You can prepare for Ocata now by creating a"
" cellv2 now using the nova-manage related commands, but configuring Cells V2"
" is still fully optional for this cycle."
msgstr ""
"Cells V2 V2 현재는 하나의 Cell V2만 부트 인스턴스를 지원합니다. Ocata를 지원하기 위해 multi-cell "
"support를 추가할 계획입니다. Ocata를 지원하기 위해 Cell V2를 now 만드는 nova-manage 관련 명령을 사용할 수"
" 있습니다. 그러나 이 시기에는 Cells V2를 구성하는 것은 여전히 선택적입니다."

#: ../source/newton.rst:293
msgid "Nova is now using Glance v2 API for getting image resources."
msgstr "Nova는 현재 Glance v2 API를 사용하여 이미지 리소스를 얻고 있습니다."

#: ../source/newton.rst:294
msgid ""
"API microversions 2.36 and above now deprecate the REST resources in Nova "
"used to proxy calls to other service type APIs (eg. /os-volumes). We'll "
"still supporting those until we raise our minimum API version to 2.36 which "
"is not planned yet (we're supporting v2.1 as of now) but you're encouraged "
"to stop using those resources and rather calling the other services that "
"provide those natively."
msgstr ""
"API microversions 2.36 이상은 현재 Nova에서 사용된 REST 리소스를 사용하여 다른 서비스 타입 API에 호출을 "
"proxy하는 것을 비고합니다. (예: /os-volumes). API microversions 2.36 이상은 더 이상 사용되지 "
"않습니다. 그러나 2.36 이상 API 버전을 최소화할 때까지 그 사용을 지속합니다. (현재 2.1을 지원하고 있습니다. 그러나 그 "
"계획은yet이ません). 그러나 사용을 중단하고 nativelly 제공하는 다른 서비스를 호출하는 것을 khuyến진합니다."

#: ../source/newton.rst:305
msgid "14.0.0"
msgstr "집합"

#: ../source/newton.rst:314
msgid ""
"Add perf event support for libvirt driver. This can be done by adding new "
"configure option 'enabled_perf_events' in libvirt section of nova.conf. This"
" feature requires libvirt>=2.0.0."
msgstr ""
"perf 이벤트 지원을 libvirt 드라이버에 추가합니다. 이는 nova.conf의 libvirt 섹션에 new configure "
"option 'enabled_perf_events'를 추가하는 것을 통해 수행할 수 있습니다. 이 기능은 libvirt>=2.0.0을 "
"사용할 수 있습니다."

#: ../source/newton.rst:318
msgid ""
"Starting from REST API microversion 2.34 pre-live-migration checks are "
"performed asynchronously. ``instance-actions`` should be used for getting "
"information about the checks results. New approach allows to reduce rpc "
"timeouts amount, as previous workflow was fully blocking and checks before "
"live-migration make blocking rpc request to both source and destination "
"compute node."
msgstr ""
"REST API microversion 2.34부터, live-migration 전의 확인은 비동기적으로 수행됩니다. "
"\"instance-actions\"를 사용하여 확인 결과에 대한 정보를 얻을 수 있습니다. 새로운 접근 방식은 이전 workflow가 "
"완전히 블록하는 것을 줄여주고, live-migration 전의 확인은 source와 destination compute node에 모두"
" 블록 RPC 요청을 발생시킵니다."

#: ../source/newton.rst:322
msgid ""
"New configuration option live_migration_permit_auto_converge has been added "
"to allow hypervisor to throttle down CPU of an instance during live "
"migration in case of a slow progress due to high ratio of dirty pages. "
"Requires libvirt>=1.2.3 and QEMU>=1.6.0."
msgstr ""
"live_migration_permit_auto_converge 옵션을 통해 live_migration 시, 하이퍼바이저가 dirty "
"page의 비율이 높은 경우 속도가 느린 경우에 인스턴스의 CPU를 제한할 수 있습니다. libvirt>=1.2.3 및 "
"QEMU>=1.6.0를 사용할 수 있습니다."

#: ../source/newton.rst:326
msgid ""
"New configuration option live_migration_permit_post_copy has been added to "
"start live migrations in a way that allows nova to switch an on-going live "
"migration to post-copy mode. Requires libvirt>=1.3.3 and QEMU>=2.5.0. If "
"post copy is permitted and version requirements are met it also changes "
"behaviour of 'live_migration_force_complete', so that it switches on-going "
"live migration to post-copy mode instead of pausing an instance during live "
"migration."
msgstr ""
"새한 구성 옵션 live_migration_permit_post_copy 가 live_migration을 시작하는 방법을 위해 nova가"
" ongoing live migration을 post-copy 모드에 전환할 수 있도록 추가되었다. libvirt>=1.3.3 및 "
"QEMU>=2.5.0를 사용할 수 있다. post copy가 허용되고 버전 요구 사항이 충족되면 "
"live_migration_force_complete의 행동도 변경된다. 이로 인해 ongoing live migration을 post-"
"copy 모드에 전환하는 대신 live migration 동안 인스턴스를 일시화하지 않는다."

#: ../source/newton.rst:330
msgid ""
"Fix os-console-auth-tokens API to return connection info for all types of "
"tokens, not just RDP."
msgstr ""
"os-console-auth-tokens API를 RDP만 아니라 모든 tipo의 토큰에 대한 연결 정보를 반환하도록 수정하세요."

#: ../source/newton.rst:334
msgid "Hyper-V RemoteFX feature."
msgstr "Hyper-V 리모트FX 기능"

#: ../source/newton.rst:336
msgid ""
"Microsoft RemoteFX enhances the visual experience in RDP connections, "
"including providing access to virtualized instances of a physical GPU to "
"multiple guests running on Hyper-V."
msgstr ""
"마이크로소프트 리모트FX는 RDP 연결에서 시각적 경험을 개선하고, 가상화된 인스턴스를 물리적 GPU의 가상화된 버전에 접근할 수 있는 "
"여러 게스트가 Hyper-V에서 실행되는 것을 포함하여 가상화된 그래픽을 제공합니다."

#: ../source/newton.rst:340
msgid ""
"In order to use RemoteFX in Hyper-V 2012 R2, one or more DirectX 11 capable "
"display adapters must be present and the RDS-Virtualization server feature "
"must be installed."
msgstr ""
"다음과 같은 영어 텍스트를 한국어로 번역합니다.\n"
"\n"
"Hyper-V 2012 R2에서 RemoteFX를 사용하려면 DirectX 11에 대한 적합한 디스플레이 어댑터가 하나 이상이 필요하고 RDS-Virtualization 서버 기능이 설치되어 있어야 합니다."

#: ../source/newton.rst:344
msgid ""
"To enable this feature, the following config option must be set in the "
"Hyper-V compute node's 'nova.conf' file::"
msgstr "이 기능을 활성화하려면 다음 구성 옵션을 Hyper-V 컴퓨터 노드의 'nova.conf' 파일에 설정해야합니다."

#: ../source/newton.rst:350
msgid ""
"To create instances with RemoteFX capabilities, the following flavor extra "
"specs must be used:"
msgstr "다음과 같은 플레버를 사용하여 리모트เอ프엑스 기능을 가진 인스턴스를 생성해야 합니다."

#: ../source/newton.rst:353
msgid ""
"**os:resolution**. Guest VM screen resolution size. Acceptable values::"
msgstr "**os:resolution**. 게스트 VM 화면 해상도 크기. 수용 가능한 값::"

#: ../source/newton.rst:357
msgid "'3840x2160' is only available on Windows / Hyper-V Server 2016."
msgstr "'3840x2160'는 Windows / Hyper-V Server 2016에서만 사용 가능합니다."

#: ../source/newton.rst:359
msgid "**os:monitors**. Guest VM number of monitors. Acceptable values::"
msgstr "**os:monitors**. 게스트 VM의 모니터 수. 수용 가능한 값::"

#: ../source/newton.rst:364
msgid ""
"**os:vram**. Guest VM VRAM amount. Only available on Windows / Hyper-V "
"Server 2016. Acceptable values::"
msgstr ""
"**os:vram**. 게스트 VM의 VRAM 양. Windows / Hyper-V Server 2016에서만 사용 가능합니다. 수용 "
"가능한 값은 다음과 같습니다."

#: ../source/newton.rst:369
msgid "There are a few considerations that needs to be kept in mind:"
msgstr "집합이 필요하다."

#: ../source/newton.rst:371
msgid "Not all guests support RemoteFX capabilities."
msgstr "not all guests support RemoteFX capabilities."

#: ../source/newton.rst:372
msgid ""
"Windows / Hyper-V Server 2012 R2 does not support Generation 2 VMs with "
"RemoteFX capabilities."
msgstr ""
"Windows / Hyper-V Server 2012 R2는 Generation 2 VMs에 RemoteFX 기능을 지원하지 않는다."

#: ../source/newton.rst:374
msgid ""
"Per resolution, there is a maximum amount of monitors that can be added. The"
" limits are as follows:"
msgstr "Per resolution, 집합의 최대 수를 초과할 수 있는 모니터의 수를 제한합니다. 제한은 다음과 같습니다."

#: ../source/newton.rst:377
msgid "For Windows / Hyper-V Server 2012 R2::"
msgstr "Windows / Hyper-V Server 2012 R2"

#: ../source/newton.rst:385
msgid "For Windows / Hyper-V Server 2016::"
msgstr "Windows / Hyper-V Server 2016"

#: ../source/newton.rst:396
msgid ""
"Microversion v2.26 allows to create/update/delete simple string tags. They "
"can be used for filtering servers by these tags."
msgstr ""
"Microversion v2.26은 단순한 문자열 태그를 생성/수정/삭제할 수 있습니다. 이러한 태그는 이 태그를 사용하여 서버를 "
"필터링할 수 있습니다."

#: ../source/newton.rst:400
msgid ""
"Added microversion v2.35 that adds pagination support for keypairs with the "
"help of new optional parameters 'limit' and 'marker' which were added to GET"
" /os-keypairs request."
msgstr ""
"microversion v2.35이 keypairs에 pagination 지원을 추가하는 데 도움을 주는 새로운 옵션 매개 변수 "
"'limit'와 'marker'를 추가한 GET /os-keypairs 요청에 추가되었다."

#: ../source/newton.rst:404
msgid ""
"Added microversion v2.28 from which hypervisor's 'cpu_info' field returned "
"as JSON object by sending GET /v2.1/os-hypervisors/{hypervisor_id} request."
msgstr ""
"microversion v2.28을 추가하여 하이퍼바이저의 'cpu_info' 필드는 GET /v2.1/os-"
"hypervisors/{hypervisor_id} 요청을 통해 JSON 对象로 반환되었습니다."

#: ../source/newton.rst:408
msgid ""
"Virtuozzo Storage is available as a volume backend in libvirt virtualization"
" driver."
msgstr "Virtuozzo Storage는 libvirt 가상화 드라이버에서 볼륨 백엔드로 사용할 수 있습니다."

#: ../source/newton.rst:411
msgid "Only qcow2/raw volume format supported, but not ploop."
msgstr "qcow2/raw 형식만 지원되지만 ploop은 지원되지 않습니다."

#: ../source/newton.rst:415
msgid "Virtuozzo ploop disks can be resized now during \"nova resize\"."
msgstr "Virtuozzo ploop 디스크는 현재 \"nova resize\" 시에 크기 조정이 가능합니다."

#: ../source/newton.rst:419
msgid "Virtuozzo instances with ploop disks now support the rescue operation"
msgstr "Virtuozzo 인스턴스와 ploop 디스크가 현재 구호 작동을 지원합니다."

#: ../source/newton.rst:423
msgid ""
"A new nova-manage command has been added to discover any new hosts that are "
"added to a cell. If a deployment has migrated to cellsv2 using either the "
"simple_cell_setup or the map_cell0/map_cell_and_hosts/map_instances combo "
"then anytime a new host is added to a cell this new \"nova-manage cell_v2 "
"discover_hosts\" needs to be run before instances can be booted on that "
"host. If multiple hosts are added at one time the command only needs to be "
"run one time to discover all of them. This command should be run from an API"
" host, or a host that is configured to use the nova_api database. Please "
"note that adding a host to a cell and not running this command could lead to"
" build failures/reschedules if that host is selected by the scheduler. The "
"discover_hosts command is necessary to route requests to the host but is not"
" necessary in order for the scheduler to be aware of the host. It is advised"
" that nova-compute hosts are configured with \"enable_new_services=False\" "
"in order to avoid failures before the hosts have been discovered."
msgstr ""
"nova-manage cell_v2 discover_hosts 명령이 추가되어 세ลล에 추가된 새로운 호스트를 발견할 수 있습니다. "
"세ลลsv2를 사용하여 배포가 이동되었을 때, simple_cell_setup 또는 "
"map_cell0/map_cell_and_hosts/map_instances의 조합을 사용하여, 새로운 호스트가 세ลล에 추가되면 "
"\"nova-manage cell_v2 discover_hosts\" 명령이 반드시 실행되어 인스턴스가 호스트에 부팅되기 전에 실행되어야"
" 합니다. 여러 호스트를 동시에 추가할 경우, 명령은 단일 번호로만 실행되어 모든 호스트를 발견할 수 있습니다. 이 명령은 API 호스트"
" 또는 nova_api 데이터베이스를 사용하는 호스트에서 실행해야 합니다. 추가된 호스트가 세ลล에 추가되지 않으면, 호스트가 스케줄러에"
" 선택되면 build fails/reschedule가 발생할 수 있습니다. 호스트에 요청을 라우팅할 수 있도록 discover_hosts"
" 명령이 필요합니다. 스케줄러가 호스트를 biết하기 위해 필요하지 않습니다. nova-compute 호스트는 "
"\"enable_new_services=False\"로 구성되어야 fail-free로 호스트가 발견되지 않은 경우에 fail-free로 "
"작동하지 않도록 주의해야 합니다."

#: ../source/newton.rst:428
msgid ""
"On evacuate actions, the default behaviour when providing a host in the "
"request body changed. Now, instead of bypassing the scheduler when asking "
"for a destination, it will instead call it with the requested destination to"
" make sure the proposed host is accepted by all the filters and the original"
" request. In case the administrator doesn't want to call the scheduler when "
"providing a destination, a new request body field called ``force`` "
"(defaulted to False) will modify that new behaviour by forcing the evacuate "
"operation to the destination without verifying the scheduler."
msgstr ""
"evacuate actions에서, 요청 바디에 호스트를 제공할 때 기본적인 행동이 변경되었다. 현재, 목적지를 요청할 때 대신에 "
"목적지를 요청하여 모든 필터가 제안된 호스트를 수용하는지 확인하기 위해 스케줄러를 호출할 것이다. 원래 요청을 확인하기 위해. 스케줄러를"
" 호출하지 않으려는 관리자가 있는 경우, 새로운 요청 바디 필드 \"force\" (기본적으로 False로 설정됨)가 존재하여 새로운 "
"행동을 변경할 것이다. 이 새로운 행동은 스케줄러를 확인하지 않고 목적지로 evacuate 연산을 강제할 것이다."

#: ../source/newton.rst:432
msgid ""
"On live-migrate actions, the default behaviour when providing a host in the "
"request body changed. Now, instead of bypassing the scheduler when asking "
"for a destination, it will instead call it with the requested destination to"
" make sure the proposed host is accepted by all the filters and the original"
" request. In case the administrator doesn't want to call the scheduler when "
"providing a destination, a new request body field called ``force`` "
"(defaulted to False) will modify that new behaviour by forcing the live-"
"migrate operation to the destination without verifying the scheduler."
msgstr ""
"live-migrate actions에서, 요청 바디에 호스트를 제공할 때 기본적인 행동이 변경되었다. 현재, 목적지를 요청할 때, "
"목적지를 요청하여 모든 필터가 제안된 호스트를 수용하는지 확인하기 위해 스케줄러를 호출할 것이 아니라, 스케줄러를 호출할 것이다. "
"스케줄러를 호출하지 않기 위해서는, 관리자가 목적지를 제공할 때, 새로운 요청 바디 필드 \"force\" (기본적으로 False)가 "
"추가되어, live-migrate 연산을 목적지로 강제로 진행하고 스케줄러를 확인하지 않는다."

#: ../source/newton.rst:436
msgid ""
"The 2.37 microversion adds support for automatic allocation of network "
"resources for a project when ``networks: auto`` is specified in a server "
"create request. If the project does not have any networks available to it "
"and the ``auto-allocated-topology`` API is available in the Neutron "
"networking service, Nova will call that API to allocate resources for the "
"project. There is some setup required in the deployment for the ``auto-"
"allocated-topology`` API to work in Neutron. See the `Additional features`_ "
"section of the OpenStack Networking Guide for more details for setting up "
"this feature in Neutron."
msgstr ""
"2.37 마이크로 버전이 네트워크 자원에 대한 tự động 할당을 지원합니다. 네트워크: auto를 서버 생성 요청에 지정하면 "
"프로젝트에 대한 tự động 할당이 가능합니다. 프로젝트가 네트워크에 대한 ANY를 가지고 있지 않으며 Neutron 네트워킹 "
"서비스에서 `auto-allocated-topology` API가 उपलबуществ을 있으면 Nova는 프로젝트에 자원을 할당하는 "
"API를 호출합니다. `auto-allocated-topology` API를 Neutron에서 작동하는 데 필요한 설정이 필요합니다. "
"OpenStack 네트워킹 가이드의 `Additional features`_ 섹션에서 이 기능을 Neutron에서 설정하는 방법에 대한 "
"더 많은 정보를 확인하십시오."

#: ../source/newton.rst:446
msgid ""
"The API does not default to 'auto'. However, python-novaclient will default "
"to passing 'auto' for this microversion if no specific network values are "
"provided to the CLI."
msgstr ""
"API는 'auto'로 기본적으로 설정되지 않지만, python-novaclient는 CLI에 특정 네트워크 값이 제공되지 않으면 이 "
"마이크로 버전에서 'auto'를 전달할 것이다."

#: ../source/newton.rst:450
msgid ""
"This feature is not available until all of the compute services in the "
"deployment are running Newton code. This is to avoid sending a server create"
" request to a Mitaka compute that can not understand a network ID of 'auto' "
"or 'none'. If this is the case, the API will treat the request as if "
"``networks`` was not in the server create request body. Once all computes "
"are upgraded to Newton, a restart of the nova-api service will be required "
"to use this new feature."
msgstr ""
"이 기능은 Newton 코드가 모든 컴퓨팅 서비스가 실행되기까지 unavailable입니다. 이 이유는 'auto' 또는 'none'의 "
"네트워크 ID가 이해되지 않는 Mitaka 컴퓨팅 서비스에 서버 생성 요청을 보낼 수 없기 때문입니다. 이 경우, API는 "
"'networks'가 서버 생성 요청 바디에 포함되지 않는 것처럼 요청을 처리합니다. Newton으로 업그레이드된 모든 컴퓨팅 서비스가 "
"완료되면, nova-api 서비스를 재시작해야 새로운 기능을 사용할 수 있습니다."

#: ../source/newton.rst:462
msgid ""
"Nova now defaults to using the glance version 2 protocol for all backend "
"operations for all virt drivers. A ``use_glance_v1`` config option exists to"
" revert to glance version 1 protocol if issues are seen, however that will "
"be removed early in Ocata, and only glance version 2 protocol will be used "
"going forward."
msgstr ""
"nova는 현재 모든 virt 드라이버에 대해 모든 배경 연산을 위해 glance 버전 2 프로토콜을 기본으로 사용합니다. "
"use_glance_v1 config 옵션은 문제가 발생할 경우 glance 버전 1 프로토콜로 돌아가기 위해 존재합니다. 그러나 "
"Ocata의 초기 단계에서 이 옵션은 제거될 예정이며, 나중에만 glance 버전 2 프로토콜을 사용할 것입니다."

#: ../source/newton.rst:466
msgid ""
"Adds a new feature to the ironic virt driver, which allows multiple nova-"
"compute services to be run simultaneously. This uses consistent hashing to "
"divide the ironic nodes between the nova-compute services, with the hash "
"ring being refreshed each time the resource tracker runs."
msgstr ""
"ironic virt 드라이버에 새로운 기능을 추가하여, 여러 nova-compute 서비스가 동시에 실행될 수 있도록 한다. 이 기능은"
" 상관성 해싱을 사용하여 ironic 노드들을 nova-compute 서비스들 사이에 분할한다. 해시 링이 리소스 트래커가 실행될 때마다"
" refresh된다."

#: ../source/newton.rst:472
msgid ""
"Note that instances will still be owned by the same nova-compute service for"
" the entire life of the instance, and so the ironic node that instance is on"
" will also be managed by the same nova-compute service until the node is "
"deleted. This also means that removing a nova-compute service will leave "
"instances managed by that service orphaned, and as such most instance "
"actions will not work until a nova-compute service with the same hostname is"
" brought (back) online."
msgstr ""
"인스턴스에는 인스턴스 전체의 생명주기를 지속해도 nova-compute 서비스에 의해 소유권이 유지되며, 따라서 인스턴스가 있는 "
"ironic 노드도 nova-compute 서비스에 의해 관리되며 노드가 삭제될 때까지. 이 또한 means that nova-"
"compute 서비스를 제거하면 인스턴스 관리를 맡은 서비스가 비어져 있는 것으로 남아 있으므로, most instance "
"actions은 nova-compute 서비스와 동일한 호스트 이름의 nova-compute 서비스가 온라인으로 돌아오기 전까지는 "
"작동하지 않습니다."

#: ../source/newton.rst:480
msgid ""
"When nova-compute services are brought up or down, the ring will eventually "
"re-balance (when the resource tracker runs on each compute). This may result"
" in duplicate compute_node entries for ironic nodes while the nova-compute "
"service pool is re-balancing. However, because any nova-compute service "
"running the ironic virt driver can manage any ironic node, if a build "
"request goes to the compute service not currently managing the node the "
"build request is for, it will still succeed."
msgstr ""
"nova-compute 서비스가 활성화되거나 비활성화되면, 링은 eventually re-balance (resource tracker가"
" 각 컴퓨터에서 실행되는 경우) 할 때까지 재조정됩니다. 이로 인해 ironic 노드에 대해 중복되는 compute_node 엔트리가 "
"발생할 수 있습니다. 그러나 ironic virt driver가-running nova-compute 서비스는 ironic 노드를 관리할"
" 수 있습니다. 따라서 build 요청이 현재 노드를 관리하지 않는 compute 서비스에 보내지면, build 요청은 여전히 성공할 수"
" 있습니다."

#: ../source/newton.rst:488
msgid ""
"There is no configuration to do to enable this feature; it is always "
"enabled.  There are no major changes when only one compute service is "
"running. If more compute services are brought online, the bigger changes "
"come into play."
msgstr ""
"집합이 설정할 필요가 없으며, 이 기능을 활성화할 수 있는 설정이 없으며, 항상 활성화되어 있습니다.  단일 Compute 서비스가 "
"실행되면 주요 변경 사항이 없지만, 더 많은 Compute 서비스를 온라인으로 추가할 경우, 큰 변경 사항이 발생합니다."

#: ../source/newton.rst:493
msgid ""
"Note that this is tested when running with only one nova-compute service, "
"but not more than one. As such, this should be used with caution for "
"multiple compute hosts until it is properly tested in CI."
msgstr ""
"이것은 단일 nova-compute 서비스만을 사용할 때만 테스트되는 것으로 알려져 있으며, 더 이상 1개 이상의 서비스를 사용할 수 "
"없다는 것을 의미한다. 따라서, 여러 컴퓨터 호스트를 사용할 때는 이를 조심스럽게 사용해야 하며, CI에서 적절히 테스트될 때까지."

#: ../source/newton.rst:499
msgid ""
"Multitenant networking for the ironic compute driver is now supported. To "
"enable this feature, ironic nodes must be using the 'neutron' "
"network_interface."
msgstr ""
"ironic 컴퓨터 드라이버에 대한 다중tenant 네트워킹이 현재 지원됩니다. 이 기능을 활성화하려면 ironic 노드는 "
"'neutron' 네트워크 인터페이스를 사용해야 합니다."

#: ../source/newton.rst:503
msgid ""
"The Libvirt driver now uses os-vif plugins for handling plug/unplug actions "
"for the Linux Bridge and OpenVSwitch VIF types. Each os-vif plugin will have"
" its own group in nova.conf for configuration parameters it needs. These "
"plugins will be installed by default as part of the os-vif module "
"installation so no special action is required."
msgstr ""
"os-vif 드라이버는 현재 리브리지와 오픈스위치 VIF 타입을 처리하는 플러그/ unpl러그 액션을 처리하는 데 os-vif 플러그를 "
"사용합니다. 각 os-vif 플러그는 nova.conf에서 own 그룹을 가지고 configuration parameter를 필요로 "
"합니다. 이 플러그는 기본적으로 os-vif 모듈 설치와 함께 설치되어jar가 필요하지 않습니다."

#: ../source/newton.rst:507
msgid "Added hugepage support for POWER architectures."
msgstr "POWER 아키텍처에 큰 페이지 지원을 추가했습니다."

#: ../source/newton.rst:511
msgid ""
"Microversions may now (with microversion 2.27) be requested with the "
"\"OpenStack-API-Version: compute 2.27\" header, in alignment with OpenStack-"
"wide standards. The original format, \"X-OpenStack-Nova-API-Version: 2.27\","
" may still be used."
msgstr ""
"OpenStack-API-Version: compute 2.27\n"
"\n"
"OpenStack-API-Version: 2.27"

#: ../source/newton.rst:515
msgid ""
"Nova has been enabled for mutable config. Certain options may be reloaded by"
" sending SIGHUP to the correct process. Live migration options will apply to"
" live migrations currently in progress. Please refer to the configuration "
"manual."
msgstr ""
"nova가 mutable config에 활성화되어 있습니다. mutable config에 대한 특정 옵션은 SIGHUP를 correct "
"process로 보냈을 때 reload될 수 있습니다. 현재 진행 중인 live migration 옵션은 live migration에 "
"적용됩니다. 구성 manual을 참조하십시오."

#: ../source/newton.rst:520
msgid "DEFAULT.debug"
msgstr "DEFAULT.debug → DEFAULT.debug"

#: ../source/newton.rst:521
msgid "libvirt.live_migration_completion_timeout"
msgstr ""
"libvirt.live_migration_completion_timeout → "
"libvirt.live_migration_completion_timeout"

#: ../source/newton.rst:522
msgid "libvirt.live_migration_progress_timeout"
msgstr ""
"libvirt.live_migration_progress_timeout → "
"libvirt.live_migration_progress_timeout"

#: ../source/newton.rst:527
msgid ""
"The following legacy notifications have been been transformed to a new "
"versioned payload:"
msgstr "다음은 legacy notification이 새로운 버전화된ayload로 변환되었습니다."

#: ../source/newton.rst:530
msgid "instance.delete"
msgstr "인스턴스.delete"

#: ../source/newton.rst:531
msgid "instance.pause"
msgstr "인스턴스.정지"

#: ../source/newton.rst:532
msgid "instance.power_on"
msgstr "인스턴스. power_on"

#: ../source/newton.rst:533
msgid "instance.shelve"
msgstr "instance.shelve → 인스턴스. 셰일"

#: ../source/newton.rst:534
msgid "instance.suspend"
msgstr "인스턴스.정지"

#: ../source/newton.rst:535
msgid "instance.restore"
msgstr "인스턴스.restore"

#: ../source/newton.rst:536
msgid "instance.resize"
msgstr "인스턴스. 리ไซ즈"

#: ../source/newton.rst:537
msgid "instance.update"
msgstr "인스턴스.update"

#: ../source/newton.rst:538
msgid "compute.exception"
msgstr "compute.exception → 계산 예외"

#: ../source/newton.rst:546
msgid ""
"Nova is now configured to work with two oslo.policy CLI scripts that have "
"been added. The first of these can be called like \"oslopolicy-list-"
"redundant --namespace nova\" and will output a list of policy rules in "
"policy.[json|yaml] that match the project defaults. These rules can be "
"removed from the policy file as they have no effect there. The second script"
" can be called like \"oslopolicy-policy-generator --namespace nova --output-"
"file policy-merged.yaml\" and will populate the policy-merged.yaml file with"
" the effective policy. This is the merged results of project defaults and "
"config file overrides."
msgstr ""
"노바는 현재 두 개의 oslo.policy CLI 스크립트와 함께 작동할 수 있습니다. 이 두 개의 스크립트 중 첫 번째는 "
"\"oslopolicy-list-redundant --namespace nova\"와 같은 이름으로 호출할 수 있으며, "
"policy.[json|yaml]에 매치되는 프로젝트 기본 규칙의 목록을 출력합니다. 이 규칙은 정책 파일에서 제거할 수 있으며, 정책 "
"파일에 영향을 미치지 않는다는 점을 고려하여. 두 번째 스크립트는 \"oslopolicy-policy-generator "
"--namespace nova --output-file policy-merged.yaml\"와 같은 이름으로 호출할 수 있으며, 정책-"
"merged.yaml 파일에 효과적인 정책을 채워넣습니다. 이는 프로젝트 기본 규칙과 구성 파일 오버라이드의 결합 결과입니다."

#: ../source/newton.rst:552
msgid ""
"Added microversion v2.33 which adds paging support for hypervisors, the "
"admin is able to perform paginate query by using limit and marker to get a "
"list of hypervisors. The result will be sorted by hypervisor id."
msgstr ""
"microversion v2.33을 추가하여 하이퍼바이저에 대한 페이지 지원을 추가합니다. 관리자는 limit와 marker를 사용하여 "
"paginate query를 수행하여 하이퍼바이저 목록을 얻을 수 있습니다. 결과는 하이퍼바이저 ID에 따라 정렬됩니다."

#: ../source/newton.rst:556
msgid ""
"The nova-compute worker now communicates with the new placement API service."
" Nova determines the placement API service by querying the OpenStack service"
" catalog for the service with a service type of 'placement'. If there is no "
"placement entry in the service catalog, nova-compute will log a warning and "
"no longer try to reconnect to the placement API until the nova-worker "
"process is restarted."
msgstr ""
"nova-compute 노드 현재 새로운 배치 API 서비스와 통신합니다. nova는 '배치' 서비스 타입을 가진 서비스를 "
"OpenStack 서비스 카탈로그에서查색하여 배치 API 서비스를 결정합니다. 배치 API 서비스가 서비스 카탈로그에 존재하지 않는다면 "
"nova-compute는 경고 메시지를 로그하고 배치 API 서비스에 다시 연결하려고 노력하지 않으며 nova-worker 프로세스가 "
"재시작될 때까지 배치 API 서비스에 다시 연결하려고 노력하지 않습니다."

#: ../source/newton.rst:560
msgid ""
"A new [placement] section is added to the nova.conf configuration file for "
"configuration options affecting how Nova interacts with the new placement "
"API service. This contains the usual keystone auth and session options."
msgstr ""
"nova.conf configuration file에 새로운 [placement] 섹션을 추가하여 nova와 새로운 [placement]"
" API 서비스와의 상호작용에 영향을 미치는 설정 옵션을 포함합니다. 이에는 일반적으로 keystone 인증 및 세션 옵션을 포함합니다."

#: ../source/newton.rst:564
msgid ""
"The pointer_model configuration option and hw_pointer_model image property "
"was added to specify different pointer models for input devices. This "
"replaces the now deprecated use_usb_tablet option."
msgstr ""
"pointer_model 구성 옵션과 hw_pointer_model 이미지 프로퍼티가 입력 장치에 다른 포인터 모델을 "
"spécificar하기 위해 추가되었다. 이는 현재 비활성화 된 use_usb_tablet 옵션을 대체한다."

#: ../source/newton.rst:568
msgid ""
"The nova-policy command line is implemented as a tool to experience the "
"under-development feature policy discovery. User can input the credentials "
"information and the instance info, the tool will return a list of API which "
"can be allowed to invoke. There isn't any contract for the interface of the "
"tool due to the feature still under-development."
msgstr ""
"nova-policy 명령 줄은 개발 중인 기능인 정책 발견을 경험하기 위한 도구로 구현된다. 사용자는 자격증명서 정보와 인스턴스 정보를"
" 입력할 수 있으며, 도구는 API를 허용할 수 있는 목록을 반환한다. 도구의 인터페이스는 아직 개발 중이므로 인터페이스에 대한 계약이 "
"없다."

#: ../source/newton.rst:572
msgid ""
"Add a nova-manage command to refresh the quota usages for a project or user."
"  This can be used when the usages in the quota-usages database table are "
"out-of-sync with the actual usages.  For example, if a resource usage is at "
"the limit in the quota_usages table, but the actual usage is less, then nova"
" will not allow VMs to be created for that project or user. The nova-manage "
"command can be used to re-sync the quota_usages table with the actual usage."
msgstr ""
"nova-manage 명령을 추가하여 프로젝트 또는 사용자가 사용하는 자산의 수용량을 재 refreshed 할 수 있습니다.  이 명령은"
" 자산 수용량 데이터 테이블의 사용량이 실제 사용량과 일치하지 않을 때 사용할 수 있습니다.  예를 들어, 자산 사용량이 수용량 테이블의"
" 제한점에 도달했지만 실제 사용량이 적을 때, nova는 프로젝트 또는 사용자가 VM을 생성할 수 없습니다.  nova-manage "
"명령은 사용량 테이블을 실제 사용량과 일치시키는 데 사용할 수 있습니다."

#: ../source/newton.rst:576
msgid ""
"Libvirt driver will attempt to update the time of a suspended and/or a "
"migrated guest in order to keep the guest clock in sync. This operation will"
" require the guest agent to be configured and running in order to be able to"
" run. However, this operation will not be disruptive."
msgstr ""
"libvirt 드라이버는 중단된 및/또한 이주된 게스트의 시간을 업데이트하여 게스트 시계를 동기화하는 것을 목표로 합니다. 이 연산은 "
"게스트 एजент이 구성된 및 실행된 상태에서만 가능합니다. 그러나 이 연산은 중단적이지 않습니다."

#: ../source/newton.rst:580
msgid ""
"This release includes a new implementation of the vendordata metadata "
"system. Please see the blueprint at "
"http://specs.openstack.org/openstack/nova-"
"specs/specs/newton/approved/vendordata-reboot.html for a detailed "
"description. There is also documentation in the Nova source tree in "
"vendordata.rst."
msgstr ""
"이 릴리즈에는 새로운 비즈니스 데이터 메타데이터 시스템의 구현이 포함됩니다. 더 많은 설명은 "
"http://specs.openstack.org/openstack/nova-"
"specs/specs/newton/approved/vendordata-reboot.html 에서 blue print을 확인하십시오. "
"또한, Nova 소스 트리에서 vendordata.rst에 포함된 설명이 있습니다."

#: ../source/newton.rst:584
msgid ""
"The 2.32 microversion adds support for virtual device role tagging. Device "
"role tagging is an answer to the question 'Which device is which, inside the"
" guest?' When booting an instance, an optional arbitrary 'tag' parameter can"
" be set on virtual network interfaces and/or block device mappings. This tag"
" is exposed to the instance through the metadata API and on the config "
"drive. Each tagged virtual network interface is listed along with "
"information about the virtual hardware, such as bus type (ex: PCI), bus "
"address (ex: 0000:00:02.0), and MAC address. For tagged block devices, the "
"exposed hardware metadata includes the bus (ex: SCSI), bus address (ex: "
"1:0:2:0) and serial number."
msgstr ""
"2.32 마이크로 버전은 가상 장치 역 태그를 지원합니다. 가상 장치 역 태그는 '가상 장치가 어떤 것인지'라는 질문에 "
"대한ตอบ답입니다. 인스턴스를 부트로면, 가상 네트워크 인터페이스와/or 블록 장치 매핑에 대한 선택적-arbitrary- '태그' 매개"
" 변수를 설정할 수 있습니다. 이 태그는 인스턴스에 metadata API와 config drive를 통해 노출됩니다. 각 태그된 가상 "
"네트워크 인터페이스는 가상 하드웨어와 같은 bus type (ex: PCI), bus address (ex: 0000:00:02.0), "
"MAC address와 같은 정보와 함께 liệt어집니다. 태그된 블록 장치에 대한 노출된 하드웨어 metadata에는 bus (ex: "
"SCSI), bus address (ex: 1:0:2:0) 및 시리얼 번호가 포함됩니다."

#: ../source/newton.rst:598
msgid ""
"The 2.32 microversion also adds the 2016-06-30 version to the metadata API. "
"Starting with 2016-06-30, the metadata contains a 'devices' sections which "
"lists any devices that are tagged as described in the previous paragraph, "
"along with their hardware metadata."
msgstr ""
"2016-06-30 버전을 metadata API에 추가한다. 2016-06-30부터 시작하여, metadata에는 'devices' "
"섹션이 포함되어 있으며, 이전 문장에서 설명된 것과 같은 장치에 대한 hardware metadata가 포함된다."

#: ../source/newton.rst:612
msgid ""
"If a deployer has updated their deployment to using cellsv2 using either the"
" simple_cell_setup or the map_cell0/map_cell_and_hosts/map_instances combo "
"and they add a new host into the cell it may cause build failures or "
"reschedules until they run the \"nova-manage cell_v2 discover_hosts\" "
"command. This is because the scheduler will quickly become aware of the host"
" but nova-api will not know how to route the request to that host until it "
"has been \"discovered\". In order to avoid that it is advised that new "
"computes are disabled until the discover command has been run."
msgstr ""
"집합을 사용하여 배포를 업데이트하여 cellsv2를 사용하는 배포자가 simple_cell_setup 또는 "
"map_cell0/map_cell_and_hosts/map_instances를 사용하여 배포를 업데이트하고 새로운 호스트를 추가할 경우,"
" build 실패나 재배치가 발생할 수 있습니다. 이 경우, nova-manage cell_v2 discover_hosts 명령을 실행할"
" 때까지 새로운 컴퓨터를 비활성화하는 것이 권장됩니다. 이 이유는 스케줄러가 호스트에 빠르게 인식하지만 nova-api가 호스트에 요청을"
" 라우팅하는 방법을 알기까지 nova-api가 호스트를 \"발견\"해야 할 때문입니다."

#: ../source/newton.rst:620
msgid ""
"When running Nova Compute and Cinder Volume or Backup services on the same "
"host they must use a shared lock directory to avoid rare race conditions "
"that can cause volume operation failures (primarily attach/detach of "
"volumes). This is done by setting the \"lock_path\" to the same directory in"
" the \"oslo_concurrency\" section of nova.conf and cinder.conf. This issue "
"affects all previous releases utilizing os-brick and shared operations on "
"hosts between Nova Compute and Cinder data services."
msgstr ""
"Nova Compute 및 Cinder Volume 또는 백업 서비스를 동일한 호스트에서 실행할 때는 공유.lock 디렉터리를 사용하여 드문 경쟁 조건을 피해야 합니다. 이 경쟁 조건은 볼륨 연산에 대한 실패를 일으킬 수 있으며 (주로 볼륨을 연결/분리하는 경우) . 이 문제는 os-brick 및 호스트 간 Nova Compute 및 Cinder 데이터 서비스의 공유 연산을 사용하는 모든 이전 버전을 영향을 미치게 합니다. \n"
"\n"
"다음은 glossary mappings을 사용하여 원문을 한국어로 번역한 결과입니다.\n"
"\n"
"* Nova Compute 및 Cinder Volume 또는 백업 서비스를 동일한 호스트에서 실행할 때는 공유.lock 디렉터리를 사용하여 드문 경쟁 조건을 피해야 합니다. \n"
"  - \"Nova Compute\" → \"Nova Compute\"\n"
"  - \"Cinder Volume\" → \"Cinder Volume\"\n"
"  - \"Backup\" → \"Backup\"\n"
"  - \"lock_path\" → \"lock_path\"\n"
"  - \"oslo_concurrency\" → \"oslo_concurrency\"\n"
"  - \"nova.conf\" → \"nova.conf\"\n"
"  - \"cinder.conf\" → \"cinder.conf\"\n"
"* 이 경쟁 조건은 볼륨 연산에 대한 실패를 일으킬 수 있으며 (주로 볼륨을 연결/분리하는 경우) \n"
"  - \"volume\" → \"볼륨\"\n"
"  - \"attach\" → \"연결\"\n"
"  - \"detach\" → \"분리\"\n"
"* 이 문제는 os-brick 및 호스트 간 Nova Compute 및 Cinder 데이터 서비스의 공유 연산을 사용하는 모든 이전 버전을 영향을 미치게 합니다. \n"
"  - \"os-brick\" → \"os-brick\"\n"
"  - \"shared operations\" → \"공유 연산\"\n"
"  - \"hosts\" → \"호스트\"\n"
"  - \"Nova Compute\" → \"Nova Compute\"\n"
"  - \"Cinder\" → \"Cinder\"\n"
"  - \"data services\" → \"데이터 서비스\"\n"
"  - \"previous releases\" → \"전체 버전\"\n"
"  - \"utilizing\" → \"사용\"\n"
"  - \"os-brick\" → \"os-brick\"\n"
"  - \"shared operations\" → \"공유 연산\"\n"
"  - \"hosts\" → \"호스트\"\n"
"  - \"Nova Compute\" → \"Nova Compute\"\n"
"  - \"Cinder\" → \"Cinder"

#: ../source/newton.rst:624
msgid ""
"When using virtual device role tagging, the metadata on the config drive "
"lags behind the metadata obtained from the metadata API. For example, if a "
"tagged virtual network interface is detached from the instance, its tag "
"remains in the metadata on the config drive. This is due to the nature of "
"the config drive, which, once written, cannot be easily updated by Nova."
msgstr ""
"가상 장치 역 태그를 사용할 때, config drive의 메타데이터가 메타데이터 API에서 얻은 메타데이터와 일치하지 않는다. 예를 "
"들어, 가상 네트워크 인터페이스를 인스턴스에서 분리하면, 태그가 config drive의 메타데이터에 남아 있다. 이것은 config "
"drive의 성질에 따라, config drive에 쓰여진 후에 Nova가 쉽게 업데이트할 수 없는 nature에 따라이다."

#: ../source/newton.rst:634
msgid ""
"All cloudpipe configuration options have been added to the 'cloudpipe' "
"group. They should no longer be included in the 'DEFAULT' group."
msgstr ""
"모든 클라우드파이프 구성 옵션은 '클라우드파이프' 그룹에 추가되었으며, 더 이상 'DEFAULT' 그룹에 포함되지 않도록 할 수 "
"있습니다."

#: ../source/newton.rst:638
msgid ""
"All crypto configuration options have been added to the 'crypto' group. They"
" should no longer be included in the 'DEFAULT' group."
msgstr ""
"모든 cripto 구성 옵션은 'cripto' 그룹에 추가되었으며, 더 이상 'DEFAULT' 그룹에 포함되지 않도록 할 수 있습니다."

#: ../source/newton.rst:642
msgid ""
"All WSGI configuration options have been added to the 'wsgi' group. They "
"should no longer be included in the 'DEFAULT' group."
msgstr "WSGI 구성 옵션은 'wsgi' 그룹에 추가되었으며, 더 이상 'DEFAULT' 그룹에 포함되지 않도록 해야 합니다."

#: ../source/newton.rst:646
msgid ""
"Aggregates are being moved to the API database for CellsV2. In this release,"
" the online data migrations will move any aggregates you have in your main "
"database to the API database, retaining all attributes. Until this is "
"complete, new attempts to create aggregates will return an HTTP 409 to avoid"
" creating aggregates in one place that may conflict with aggregates you "
"already have and are yet to be migrated."
msgstr ""
"집합은 CellsV2 API 데이터베이스로 이동되고 있습니다. 이 릴리즈에서 온라인 데이터 마이그레이션은 주요 데이터베이스에 있는 모든 "
"집합을 API 데이터베이스로 이동하고 모든 속성을 유지합니다. 이 작업이 완료될 때까지, 새로운 집합을 생성하는 시도는 HTTP 409를"
" 반환하여 집합을 하나의 위치에만 생성하지 않도록 conflict가 있는 집합과 이미 마이그레이션되지 않은 집합을 피하기 위해 "
"시도합니다."

#: ../source/newton.rst:650
msgid ""
"Note that aggregates can no longer be soft-deleted as the API database does "
"not replicate the legacy soft-delete functionality from the main database. "
"As such, deleted aggregates are not migrated and the behavior users will "
"experience will be the same as if a purge of deleted records was performed."
msgstr ""
"집합은 더 이상 소프트 데leted 할 수 없으며 API 데이터베이스는 주요 데이터베이스의 전통적인 소프트 데leted 기능을 복제하지 "
"않기 때문에. 따라서 제거된 집합은 migrate되지 않으며, 사용자가 경험할 행동은 제거된 레코드의 파urge를 수행한 것과 동일하다."

#: ../source/newton.rst:654
msgid ""
"The nova-manage db online_data_migrations command will now migrate server "
"groups to the API database. New server groups will be automatically created "
"in the API database but existing server groups must be manually migrated "
"using the nova-manage command."
msgstr ""
"nova-manage db online_data_migrations 명령은 현재 API 데이터베이스로 서버 그룹을 이동하고, 새로운 서버"
" 그룹은 API 데이터베이스에서 tự động 생성되지만, 현재 존재하는 서버 그룹은 manual로 nova-manage 명령을 사용하여"
" 이동해야 합니다."

#: ../source/newton.rst:658
msgid ""
"The get_metrics API has been replaced by populate_metrics in "
"nova.compute.monitors.base module. This change is introduced to allow each "
"monitor plugin to have the flexibility of setting it's own metric value "
"types. The in-tree metrics plugins are modified as a part of this change. "
"However, the out-of-tree plugins would have to adapt to the new API in order"
" to work with nova."
msgstr ""
"get_metrics API는 nova.compute.monitors.base 모듈의 populate_metrics에 의해 대체되었다. "
"이 변경은 각 모니터 플러그인에 own metric value types를 설정할 수 있는灵活성을 제공하기 위해 도입되었다. 이 변경은 "
"인-트리 메트릭스 플러그인에 일부 변경이 이루어졌지만, 아웃-오브-트리 플러그인은 nova와 함께 작동하기 위해 새로운 API를适应해야 "
"한다."

#: ../source/newton.rst:662
msgid ""
"For the Virtuozzo Storage driver to work with os-brick <1.4.0, you need to "
"allow \"pstorage-mount\" in rootwrap filters for nova-compute."
msgstr ""
"virtuozzo 스토리지 드라이버가 os-brick <1.4.0와 함께 작동하려면 nova-compute에 대해 \"pstorage-"
"mount\"을 rootwrap 필터에 allow करन해야 합니다."

#: ../source/newton.rst:666
msgid ""
"You must update the rootwrap configuration for the compute service if you "
"use ploop images, so that \"ploop grow\" filter is changed to "
"\"prl_disk_tool resize\"."
msgstr ""
"`compute` 서비스를 사용할 때 `ploop` 이미지의 경우, `ploop`-grow` 필터를 `prl_disk_tool "
"resize`로 변경해야 합니다."

#: ../source/newton.rst:674
msgid ""
"'nova-manage db sync' can now sync the cell0 database. The cell0 db is "
"required to store instances that cannot be scheduled to any cell. Before the"
" 'db sync' command is called a cell mapping for cell0 must have been created"
" using 'nova-manage cell_v2 map_cell0'. This command only needs to be called"
" when upgrading to CellsV2."
msgstr ""
"nova-manage db sync는 현재 세ลล0 데이터베이스를.sync할 수 있습니다. 세ลล0 데이터베이스는 어떤 세ลล에 스케줄이"
" 할 수 없을 때 인스턴스를 저장해야 하는 경우에 사용됩니다. 'db sync' 명령을 호출하기 전에 'nova-manage "
"cell_v2 map_cell0' 명령을 호출하여 세ลล0의 mapping이 생성되어야 합니다. 이 명령은 CellsV2로 업그레이드할 "
"때만 호출해야 합니다."

#: ../source/newton.rst:682
msgid ""
"A new nova-manage command has been added which will upgrade a deployment to "
"cells v2. Running the command will setup a single cell containing the "
"existing hosts and instances. No data or instances will be moved during this"
" operation, but new data will be added to the nova_api database. New "
"instances booted after this point will be placed into the cell. Please note "
"that this does not mean that cells v2 is fully functional at this time, but "
"this is a significant part of the effort to get there. The new command is "
"\"nova-manage cell_v2 simple_cell_setup --transport_url <transport_url>\" "
"where transport_url is the connection information for the current message "
"queue used by Nova. Operators must create a new database for cell0 before "
"running ``cell_v2 simple_cell_setup``. The simple cell setup command expects"
" the name of the cell0 database to be ``<main database name>_cell0`` as it "
"will create a cell mapping for cell0 based on the main database connection, "
"sync the cell0 database, and associate existing hosts and instances with the"
" single cell."
msgstr ""
"nova-manage 명령은 nova-manage 명령의 새로운 버전으로, 배포를 세포 v2로 업그레이드합니다. 명령을 실행하면, 현재 "
"호스트와 인스턴스를 포함하는 단일 세포가 설정됩니다. 이 연산에서 데이터나 인스턴스를 이동하지 않지만 nova_api 데이터베이스에 "
"새로운 데이터가 추가됩니다. 이fter 이 지점에서 부팅된 인스턴스는 세포에 배치됩니다. 이 연산이 세포 v2가 완전히 기능적이지는 "
"않지만, 이에 대한 노력의 중요한 부분입니다. nova-manage 명령은 \"nova-manage cell_v2 "
"simple_cell_setup --transport_url <transport_url>\"입니다. transport_url는 현재 "
"메시지 큐를 사용하는 노바의 연결 정보입니다. 노바 운영자들은 cell0를 Before running \"cell_v2 "
"simple_cell_setup\"을 위해 새로운 데이터베이스를 생성해야합니다. simple_cell_setup 명령은 cell0 "
"데이터베이스 이름이 \"main database name_cell0\"로 expectations합니다. 이 명령은 세포0에 대한 "
"mapping을 생성하고, main 데이터베이스와 동기화하고, 현재 호스트와 인스턴스를 단일 세포와 연결합니다."

#: ../source/newton.rst:694
msgid ""
"The deprecated configuration option ``client_log_level`` of the section "
"``[ironic]`` has been deleted. Please use the config options "
"``log_config_append`` or ``default_log_levels`` of the ``[DEFAULT]`` "
"section."
msgstr ""
"ironic]의 [client_log_level] 구성을 비활성화한 옛 옵션은 삭제되었습니다. [DEFAULT]의 "
"[log_config_append] 또는 [default_log_levels] 옵션을 사용하십시오."

#: ../source/newton.rst:698
msgid ""
"A new nova-manage command 'nova-manage cell_v2 map_cell0' is now available. "
"Creates a cell mapping for cell0, which is used for storing instances that "
"cannot be scheduled to any cell. This command only needs to be called when "
"upgrading to CellsV2."
msgstr ""
"nova-manage cell_v2 map_cell0는 현재 nova-manage 명령어의 새로운 명령으로 available입니다. "
"cell0는 스케줄을 할 수 없는 인스턴스를 저장하기 위해 사용되는 세ลล입니다. 이 명령은 CellsV2로 업그레이드할 때만 "
"호출해야합니다."

#: ../source/newton.rst:705
msgid ""
"The default value of the ``pointer_model`` configuration option has been set"
" to 'usbtablet'."
msgstr "``pointer_model`` 설정 옵션의 기본值는 'usbtablet'로 설정되었습니다."

#: ../source/newton.rst:710
msgid ""
"The following policy enforcement points have been removed as part of the "
"restructuring of the Nova API code. The attributes that could have been "
"hidden with these policy points will now always be shown / accepted."
msgstr ""
"다음 정책 enforcement point은 노바 API 코드의 재구성에 따라 제거되었다. 이러한 정책 point가 사용된 속성은 이제 "
"항상 표시 / 수용된다."

#: ../source/newton.rst:715
msgid ""
"``os_compute_api:os-disk-config`` - show / accept ``OS-DCF:diskConfig`` "
"parameter on servers"
msgstr ""
"``os_compute_api:os-disk-config`` - 집합 / 수용 ``OS-DCF:diskConfig`` 속성에 대한 정보를"
" 서버에 표시합니다."

#: ../source/newton.rst:718
msgid ""
"``os-access-ips`` - show / accept ``accessIPv4`` and ``accessIPv6`` "
"parameters on servers"
msgstr ""
"`os-access-ips` - `서버`에 `accessIPv4` 및 `accessIPv6` 매개 변수를 표시하거나 수용합니다."

#: ../source/newton.rst:721
msgid "The following entry points have been removed"
msgstr "다음 API 엔드포인트가 제거되었습니다."

#: ../source/newton.rst:723
msgid ""
"``nova.api.v21.extensions.server.resize`` - allowed accepting additional "
"parameters on server resize requests."
msgstr ""
"nova.api.v21.extensions.server.resize - server resize 요청에 추가 매개 변수를 허용합니다."

#: ../source/newton.rst:726
msgid ""
"``nova.api.v21.extensions.server.update`` - allowed accepting additional "
"parameters on server update requests."
msgstr ""
"nova.api.v21.extensions.server.update - 서버 업데이트 요청에 추가 매개 변수를 수용하는 것을 허용합니다."

#: ../source/newton.rst:729
msgid ""
"``nova.api.v21.extensions.server.rebuild`` - allowed accepting additional "
"parameters on server rebuild requests."
msgstr ""
"nova.api.v21.extensions.server.rebuild - server 재건 요청에 추가 매개 변수를 수용하는 것을 "
"허용합니다."

#: ../source/newton.rst:734
msgid ""
"Flavors are being moved to the API database for CellsV2. In this release, "
"the online data migrations will move any flavors you have in your main "
"database to the API database, retaining all attributes. Until this is "
"complete, new attempts to create flavors will return an HTTP 409 to avoid "
"creating flavors in one place that may conflict with flavors you already "
"have and are yet to be migrated."
msgstr ""
"Flavors는 CellsV2 API 데이터베이스로 이동되고 있습니다. 이 릴리즈에서, 온라인 데이터 이민은 main 데이터베이스에 있는"
" 모든 flavors를 API 데이터베이스로 이동하고, 모든 속성을 유지합니다. 이까지 완료되지 않은 경우, 새로운 flavor를 "
"생성하는 시도는 HTTP 409를 반환하여, 이미 데이터베이스에 있는 flavors와 충돌할 수 있는 flavor를 하나의 위치에서만 "
"생성하지 않도록 방지합니다."

#: ../source/newton.rst:738
msgid ""
"Note that flavors can no longer be soft-deleted as the API database does not"
" replicate the legacy soft-delete functionality from the main database. As "
"such, deleted flavors are not migrated and the behavior users will "
"experience will be the same as if a purge of deleted records was performed."
msgstr ""
"집합 API 데이터베이스는 전통적인 소프트 데리트 기능을 chính 데이터베이스에서 복사하지 않기 때문에 더 이상 플레버를 소프트 "
"데리트할 수 없습니다. 따라서 제거된 플레버는 이식되지 않으며, 사용자들이 경험할 행동은 제거된 레코드의 청소가 수행된 것과 동일합니다."

#: ../source/newton.rst:742
msgid "The 2.37 microversion enforces the following:"
msgstr "2.37 마이크로 버전은 다음을 강제합니다."

#: ../source/newton.rst:744
msgid ""
"``networks`` is required in the server create request body for the API. "
"Specifying ``networks: auto`` is similar to not requesting specific networks"
" when creating a server before 2.37."
msgstr ""
"네트워크는 서버 생성 요청 바디에 필요합니다. API에 ``networks: auto``를 지정하는 것은 이전 2.37 이전에 서버를 "
"생성할 때 특정 네트워크를 요청하지 않는 것과 동일합니다."

#: ../source/newton.rst:747
msgid ""
"The ``uuid`` field in the ``networks`` object of a server create request is "
"now required to be in UUID format, it cannot be a random string. More "
"specifically, the API used to support a nic uuid with a \"br-\" prefix but "
"that is a legacy artifact which is no longer supported."
msgstr ""
"``uuid`` 필드가 서버에 대한 네트워크 객체의 생성 요청에 있는 `uuid` 형식이 now required로 바뀌어야 합니다. 이 "
"필드는 random string이 아니어야 합니다. 더 specificsally, API는 nic uuid에 \"br-\" prefix가"
" 있는 uuid를 지원합니다. 그러나 이는 legacy artifact로 support가 더 이상 되어 있지 않습니다."

#: ../source/newton.rst:754
msgid ""
"It is now required that the glance environment used by Nova exposes the "
"version 2 REST API. This API has been available for many years, but "
"previously Nova only used the version 1 API."
msgstr ""
"다음은 다음과 같습니다.\n"
"\n"
"현재 노바가 사용하는 글랜스 환경은 버전 2 REST API를 노출해야 합니다. 이 API는 많은 년도로 사용이 가능했지만 이전에는 노바가만 버전 1 API를 사용했습니다."

#: ../source/newton.rst:758
msgid ""
"imageRef input to the REST API is now restricted to be UUID or an empty "
"string only. imageRef input while create, rebuild and rescue server etc must"
" be a valid UUID now. Previously, a random image ref url containing image "
"UUID was accepted. But now all the reference of imageRef must be a valid "
"UUID (with below exception) otherwise API will return 400. Exception- In "
"case boot server from volume. Previously empty string was allowed in "
"imageRef and which is ok in case of boot from volume. Nova will keep the "
"same behavior and allow empty string in case of boot from volume only and "
"400 in all other case."
msgstr ""
"imageRef REST API의 입력은 UUID 또는 비어있는 문자열만을 허용합니다. imageRef 입력은 create, "
"rebuild 및 rescue server etc.에서만 유효 UUID이여야 합니다. 이전에 random image ref url에 "
"image UUID가 포함된 URL이 수용되었습니다. 그러나 현재 imageRef의 모든 참조는 유효 UUID이여야 하며 (아래 예외를 "
"제외함) API는 400을 반환합니다. 예외 - 부트 서버에서 볼륨에서. 이전에 비어있는 문자열이 imageRef에 허용되었으며, "
"볼륨에서 부트하는 경우에는 정상적으로 작동했습니다. Nova는 동일한 행동을 유지하고, 볼륨에서 부트하는 경우에는 비어있는 문자열을 "
"허용하고, 다른 경우에는 400을 반환합니다."

#: ../source/newton.rst:763
msgid ""
"Prior to Grizzly release default instance directory names were based on "
"instance.id field, for example directory for instance could be named "
"``instance-00000008``. In Grizzly this mechanism was changed, instance.uuid "
"is used as an instance directory name, e.g. path to instance:"
msgstr ""
"Grizzly 릴리스 이전에 디폴트 인스턴스 디렉토리 이름은 인스턴스.id 필드에 기반을 두어야 했다. 예를 들어 인스턴스는 "
"``instance-00000008``라는 이름으로 이름이 지어졌을 것이다. Grizzly에서는 이 메커니즘을 변경하여, "
"인스턴스.uuid를 인스턴스 디렉토리 이름으로 사용하기 시작했다. 예를 들어 인스턴스의 경로는 다음과 같이 이름이 지어졌을 것이다."

#: ../source/newton.rst:768
msgid ""
"``/opt/stack/data/nova/instances/34198248-5541-4d52-a0b4-a6635a7802dd/``."
msgstr ""
"``/opt/stack/data/nova/instances/34198248-5541-4d52-a0b4-a6635a7802dd/`` \n"
"\n"
"(가용 구역, 호스트 집합, 인스턴스)"

#: ../source/newton.rst:770
msgid ""
"In Newton backward compatibility is dropped. For instances that haven't been"
" restarted since Folsom and earlier maintenance should be scheduled before "
"upgrade(stop, rename directory to instance.uuid, then start) so Nova will "
"start using new paths for instances."
msgstr ""
"Newton에서 후 backward compatibility가 drop되었습니다. Folsom과 이전의 유지 관리 이후에 재시작하지 않은"
" 인스턴스를 위해, 업그레이드(stop, rename directory to instance.uuid, then start) 전에는 "
"scheduling이 필요합니다. Nova는 이로 인해 인스턴스에서 새로운 경로를 사용하기 시작합니다."

#: ../source/newton.rst:777
msgid ""
"The ironic driver now requires python-ironicclient>=1.5.0 (previously "
">=1.1.0), and requires the ironic service to support API version 1.20 or "
"higher. As usual, ironic should be upgraded before nova for a smooth upgrade"
" process."
msgstr ""
"ironic 드라이버는 현재 python-ironicclient>=1.5.0 (이전에는 >=1.1.0)로 필요하며, ironic 서비스는"
" API 버전 1.20 이상을 지원해야 합니다. 일반적으로, ironic은 nova와 함께 업그레이드하는 것을 권장합니다."

#: ../source/newton.rst:781
msgid ""
"The ironic driver now requires python-ironicclient>=1.6.0, and requires the "
"ironic service to support API version 1.21."
msgstr ""
"ironic 드라이버는 현재 python-ironicclient>=1.6.0을 필요로 하고, ironic 서비스는 API 버전 1.21를"
" 지원해야 합니다."

#: ../source/newton.rst:785
msgid ""
"Keypairs have been moved to the API database, using an online data "
"migration. During the first phase of the migration, instances will be given "
"local storage of their key, after which keypairs will be moved to the API "
"database."
msgstr ""
"키ペア가 API 데이터베이스에 이동되었습니다. 온라인 데이터 마이그레이션을 사용하여. 마이그레이션의 첫 번째 단계에서, 인스턴스는 키를 "
"지역적으로 저장할 수 있는 스토리지가 제공되며, 이후 키ペ어가 API 데이터베이스에 이동됩니다."

#: ../source/newton.rst:789
msgid ""
"Default value of live_migration_tunnelled config option in libvirt section "
"has been changed to False. After upgrading nova to Newton all live "
"migrations will be non-tunnelled unless live_migration_tunnelled is "
"explicitly set to True. It means that, by default, the migration traffic "
"will not go through libvirt and therefore will no longer be encrypted."
msgstr ""
"live_migration_tunnelled config 옵션의 libvirt 섹션의 기본값이 False 로 변경되었다. Newton으로"
" nova를 업그레이드 한 후에, live_migration_tunnelled 옵션에 True 로明시로 설정되지 않으면 tất cả "
"live migration은 tunnelled가 아닌 것이 된다. 이는, 기본적으로, migration 트래픽이 libvirt를 통과하지"
" 않기 때문에 더 이상 암호화가 되지 않는다는 의미이다."

#: ../source/newton.rst:793
msgid ""
"With the introduction of os-vif, some networking related configuration "
"options have moved, and users will need to update their ``nova.conf``. For "
"OpenVSwitch users the following options have moved from ``[DEFAULT]`` to "
"``[vif_plug_ovs]`` - network_device_mtu - ovs_vsctl_timeout For Linux Bridge"
" users the following options have moved from ``[DEFAULT]`` to "
"``[vif_plug_linux_bridge]`` - use_ipv6 - iptables_top_regex - "
"iptables_bottom_regex - iptables_drop_action - forward_bridge_interface - "
"vlan_interface - flat_interface - network_device_mtu For backwards "
"compatibility, and ease of upgrade, these options will continue to work from"
" ``[DEFAULT]`` during the Newton release. However they will not in future "
"releases."
msgstr ""
"os-vif를 도입하면 일부 네트워크 관련 구성 옵션들이 이동되었으며, 사용자는 ``nova.conf``을 업데이트해야 할 것입니다. "
"OpenVSwitch 사용자에게는 다음 옵션들이 ``[DEFAULT]``에서 ``[vif_plug_ovs]``로 이동했습니다. - "
"network_device_mtu - ovs_vsctl_timeout Linux Bridge 사용자에게는 다음 옵션들이 "
"``[DEFAULT]``에서 ``[vif_plug_linux_bridge]``로 이동했습니다. - use_ipv6 - "
"iptables_top_regex - iptables_bottom_regex - iptables_drop_action - "
"forward_bridge_interface - vlan_interface - flat_interface - "
"network_device_mtu. 이 옵션들이 Newton 릴리스 동안 backwards compatibility와 업그레이드의 "
"용이성을 위해 ``[DEFAULT]``에서 작동합니다. 그러나 향후 릴리스에서는 작동하지 않습니다."

#: ../source/newton.rst:802
msgid "The minimum required version of libvirt has been increased to 1.2.1"
msgstr "libvirt의 최소한의 필요한 버전이 1.2.1로 증가했다."

#: ../source/newton.rst:806
msgid ""
"The minimum required QEMU version is now checked and has been set to 1.5.3"
msgstr "QEMU 버전은 현재 1.5.3으로 확인되고 설정되었습니다."

#: ../source/newton.rst:810
msgid ""
"The network_api_class option was deprecated in Mitaka and is removed in "
"Newton. The use_neutron option replaces this functionality."
msgstr "네트워크 API 클래스 옵션은 미타카에서 비고화되었으며 뉴턴에서 제거되었다. 사용_neutron 옵션은 이 기능을 대체한다."

#: ../source/newton.rst:814
msgid ""
"The newton release has a lot of online migrations that must be performed "
"before you will be able to upgrade to ocata. Please take extra note of this "
"fact and budget time to run these online migrations before you plan to "
"upgrade to ocata. These migrations can be run without downtime with ``nova-"
"manage db online_data_migrations``."
msgstr ""
"Newton 릴리스는 OCATA로 업그레이드를 할 수 있는지 여부에 따라 많은 온라인 이민이 필요합니다. 이민을 수행해야 할 때는 "
"OCATA로 업그레이드를 할 수 있습니다. 이민을 수행하기 전에 extra 주의를 기울여 budget 시간을 부여하여 온라인 이민을 "
"수행해야 합니다. 온라인 이민은 downtime이 없는 상태에서 ``nova-manage db "
"online_data_migrations``를 사용하여 수행할 수 있습니다."

#: ../source/newton.rst:820
msgid ""
"The ``notify_on_state_change`` configuration option was StrOpt, which would "
"accept any string or None in the previous release.  Starting in the Newton "
"release, it allows only three values: None, ``vm_state``, "
"``vm_and_task_state``. The default value is None."
msgstr ""
"``notify_on_state_change`` 구성 옵션은 StrOpt로, 이전 버전에서는任何 문자열 또는 None을 수용했다. "
"Newton 버전에서 시작하여, 이 옵션은 단지 세 가지 값만을 허용한다: None, ``vm_state``, "
"``vm_and_task_state``. 기본 값은 None이다."

#: ../source/newton.rst:827
msgid ""
"The deprecated auth parameter ``admin_auth_token`` was removed from the "
"[ironic] config option group. The use of ``admin_auth_token`` is insecure "
"compared to the use of a proper username/password."
msgstr ""
"Deprecated auth parameter `admin_auth_token`은 [ironic] config option group에서"
" 제거되었다. `admin_auth_token`을 사용하는 것은 적절한 사용자 이름/비밀번호를 사용하는 것보다 insecure하다."

#: ../source/newton.rst:832
msgid ""
"The previously deprecated config option ``listen`` of the group "
"``serial_console`` has been removed, as it was never used in the code."
msgstr ""
"이전으로 비활성화된 config 옵션인 `listen`은 `serial_console` 그룹의 옵션으로 사용되지 않았기 때문에 "
"제거되었다."

#: ../source/newton.rst:837
msgid ""
"The 'manager' option in [cells] group was deprecated in Mitaka and now it is"
" removed completely in newton. There is no impact."
msgstr ""
"'manager' 옵션은 [cells] 그룹의 '마니저' 옵션은 미타카에서弃용되었으며, 새로운 뉴턴에서는 완전히 제거되었다. 이에 대한 "
"영향을 받지 않는다."

#: ../source/newton.rst:841
msgid ""
"The following deprecated configuration options have been removed from the "
"``cinder`` section of ``nova.conf``:"
msgstr "다음은 `cinder` 섹션의 `nova.conf`에서 제거된 deprecated 구성 옵션입니다."

#: ../source/newton.rst:844 ../source/newton.rst:913
msgid "``ca_certificates_file``"
msgstr "``ca_certificates_file``"

#: ../source/newton.rst:845 ../source/newton.rst:914
msgid "``api_insecure``"
msgstr "``API 비보안``"

#: ../source/newton.rst:846
msgid "``http_timeout``"
msgstr "집합"

#: ../source/newton.rst:850
msgid ""
"The 'destroy_after_evacuate' workaround option has been removed as the "
"workaround is no longer necessary."
msgstr "'destroy_after_evacuate' 옵션은 더 이상 workaround이 필요하지 않기 때문에 제거되었습니다."

#: ../source/newton.rst:854
msgid ""
"The config options 'osapi_compute_ext_list' and 'osapi_compute_extension' "
"were deprecated in mitaka. Hence these options were completely removed in "
"newton, as v2 API is removed and v2.1 API doesn't provide the option of "
"configuring extensions."
msgstr ""
"`osapi_compute_ext_list` 및 `osapi_compute_extension` 옵션은 mitaka에서弃기되었다. 따라서 "
"이러한 옵션은 newton에서 완전히 제거되었다. v2 API가 제거되었으며 v2.1 API는 확장 기능을 구성할 수 있는 옵션을 "
"제공하지 않는다."

#: ../source/newton.rst:858
msgid ""
"The deprecated config option ``remove_unused_kernels`` has been removed from"
" the ``[libvirt]`` config section. No replacement is required, as this "
"behaviour is no longer relevant."
msgstr ""
"deprecated config option `remove_unused_kernels`가 `[libvirt]` config "
"section에서 제거되었다. 이 behaviour는 더 이상 중요하지 않기 때문에 대체가 필요하지 않다."

#: ../source/newton.rst:862
msgid ""
"The extensible resource tracker was deprecated in the 13.0.0 release and has"
" now been removed. Custom resources in the nova.compute.resources namespace "
"selected by the compute_resources configuration parameter will not be "
"loaded."
msgstr ""
"13.0.0 릴리스에서 확장 가능한 리소스 트래커가弃용되었으며 현재 제거되었습니다. nova.compute.resources "
"네임스페이스에서 선택된 compute_resources 구성 매개 변수에 의해 선택된 custom 리소스는 로드되지 않습니다."

#: ../source/newton.rst:866
msgid ""
"The legacy v2 API code was deprecated since Liberty release. The legacy v2 "
"API code was removed in Newton release. We suggest that users should move to"
" v2.1 API which compatible v2 API with more restrict input validation and "
"microversions support. If users are still looking for v2 compatible API "
"before switch to v2.1 API, users can use v2.1 API code as v2 API compatible "
"mode. That compatible mode is closer to v2 API behaviour which is v2 API "
"compatible without restrict input validation and microversions support. So "
"if using openstack_compute_api_legacy_v2 in /etc/nova/api-paste.ini for the "
"API endpoint /v2, users need to switch the endpoint to "
"openstack_compute_api_v21_legacy_v2_compatible instead."
msgstr ""
"legacy v2 API 코드는 리버티 릴리스 이후에 비상대적이었습니다. legacy v2 API 코드는 뉴턴 릴리스에서 제거되었습니다."
" legacy v2 API 코드를 사용하는 사용자는 v2.1 API로 전환하는 것을 권장합니다. v2.1 API는 v2 API와 더 "
"강력한 입력 유효성 검사 및 마이크로 버전 지원을 제공합니다. v2 API와 비슷한 행동을 제공하는 v2.1 API 코드를 사용하는 "
"사용자는 v2 API와 비슷한 행동을 제공하는 v2 API 코드를 사용할 수 있습니다. v2.1 API 코드는 v2 API와 비슷한 "
"행동을 제공하는 v2 API 코드와 마이크로 버전 지원을 제외한 강력한 입력 유효성 검사와 마이크로 버전 지원을 제공합니다. 따라서 "
"openstack_compute_api_legacy_v2를 /v2 API 엔드포인트에 사용하는 사용자는 "
"openstack_compute_api_v21_legacy_v2_compatible 엔드포인트를 사용해야 합니다."

#: ../source/newton.rst:870
msgid ""
"The 'live_migration_flag' and 'block_migration_flag' options in libvirt "
"section that were deprecated in Mitaka have been completely removed in "
"Newton, because nova automatically sets correct migration flags. New config "
"options has been added to retain possibility to turn tunnelling, auto-"
"converge and post-copy on/off, respectively named "
"``live_migration_tunnelled``, ``live_migration_permit_auto_converge`` and "
"``live_migration_permit_post_copy``."
msgstr ""
"live_migration_flag 및 block_migration_flag 옵션은 미타카에서弃기된 libvirt 섹션의 옵션입니다. "
"Newton에서 완전히 제거되었습니다. nova가 correct migration flags를 tự động 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct "
"migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct "
"migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct "
"migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct "
"migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct "
"migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct "
"migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct "
"migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, "
"nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct migration flags를 "
"설정하는 것을 이유로, nova가 correct migration flags를 설정하는 것을 이유로, nova가 correct"

#: ../source/newton.rst:877
msgid ""
"The 'memcached_server' option in DEFAULT section which was deprecated in "
"Mitaka has been completely removed in Newton. This has been replaced by "
"options from oslo cache section."
msgstr ""
"'oslo cache' 섹션의 옵션을 사용하여 'memcached_server' 옵션을 사용할 수 있습니다. 이 옵션은 미타카에서弃용된 "
"'DEFAULT' 섹션의 'memcached_server' 옵션을 완전히 제거했습니다."

#: ../source/newton.rst:881
msgid ""
"The service subcommand of nova-manage was deprecated in 13.0. Now in 14.0 "
"the service subcommand is removed. Use service-* commands from python-"
"novaclient or the os-services REST resource instead."
msgstr ""
"nova-manage 서비스 서브комmando의 service subcommand은 13.0에서 deprecated되었으며, "
"14.0에서 service subcommand이 제거되었다. service-* 명령어를 python-novaclient 또는 os-"
"services REST 리소스에서 사용하도록 하십시오."

#: ../source/newton.rst:885
msgid ""
"The network_device_mtu option in Nova is deprecated for removal in 13.0.0 "
"since network MTU should be specified when creating the network."
msgstr ""
"네트워크 장치 MTU 옵션(Nova)은 13.0.0 버전부터 제거되기 위해弃용되었습니다. 네트워크 MTU를 생성할 때 지정해야 하는데, "
"따라서弃용되었습니다."

#: ../source/newton.rst:889
msgid ""
"Legacy v2 API code is already removed. A set of policy rules in the "
"policy.json, which are only used by legacy v2 API, are removed. Both v2.1 "
"API and v2.1 compatible mode API are using same set of new policy rules "
"which are with prefix ``os_compute_api``."
msgstr ""
"어제는 전통적인 v2 API 코드가 이미 제거되었다. policy.json에만 사용되는 전통적인 v2 API에만 사용되는 정책 규칙이 "
"제거되었다. v2.1 API와 v2.1 호환 모드 API는 모두 새로운 정책 규칙을 사용하고, 그 규칙은 "
"prefix``os_compute_api``로 시작한다."

#: ../source/newton.rst:894
msgid ""
"Removed the ``security_group_api`` configuration option that was deprecated "
"in Mitaka. The correct security_group_api option will be chosen based on the"
" value of ``use_neutron`` which provides a more coherent user experience."
msgstr ""
"``security_group_api`` 설정 옵션을 Mitaka에서弃약한 ``security_group_api`` 설정 옵션을 "
"제거했습니다. correct security_group_api 옵션은 use_neutron의 값에 따라 선택됩니다. "
"use_neutron의 값이 제공하는 사용자 경험은 더 일관적입니다."

#: ../source/newton.rst:898
msgid ""
"The deprecated ``volume_api_class`` config option has been removed. We only "
"have one sensible backend for it, so don't need it anymore."
msgstr ""
"Deprecated `volume_api_class` 구성 옵션은 제거되었습니다. 단일 합리적인 백엔드만 있기 때문에 더 이상 필요하지 "
"않습니다."

#: ../source/newton.rst:902
msgid ""
"The libvirt option 'iscsi_use_multipath' has been renamed to "
"'volume_use_multipath'."
msgstr ""
"libvirt 옵션 'iscsi_use_multipath'는 'volume_use_multipath'로 이름이 변경되었습니다."

#: ../source/newton.rst:906
msgid ""
"The 'wsgi_default_pool_size' and 'wsgi_keep_alive' options have been renamed"
" to 'default_pool_size' and 'keep_alive' respectively."
msgstr ""
"'wsgi_default_pool_size' 및 'wsgi_keep_alive' 옵션은 각각 'default_pool_size' 및 "
"'keep_alive'로 이름이 변경되었습니다."

#: ../source/newton.rst:910
msgid ""
"The following deprecated configuration options have been removed from the "
"``neutron`` section of nova.conf:"
msgstr "다음은 nova.conf의 neutron 섹션에서 제거된 deprecated 구성 옵션입니다."

#: ../source/newton.rst:915
msgid "``url_timeout``"
msgstr "집합"

#: ../source/newton.rst:919
msgid ""
"The ability to load a custom scheduler host manager via the "
"``scheduler_host_manager`` configuration option was deprecated in the 13.0.0"
" Mitaka release and is now removed in the 14.0.0 Newton release."
msgstr ""
"``scheduler_host_manager`` 설정 옵션을 통해 custom scheduler host manager를 로드하는 능력은"
" 13.0.0 미타카 릴리스에서 비활성화되었으며, 14.0.0 뉴턴 릴리스에서 제거되었습니다."

#: ../source/newton.rst:923
msgid ""
"DB2 database support was removed from tree. This is a non open source "
"database that had no 3rd party CI, and a set of constraints that meant we "
"had to keep special casing it in code. It also made the online data "
"migrations needed for cells v2 and placement engine much more difficult. "
"With 0% of OpenStack survey users reporting usage we decided it was time to "
"remove this to focus on features needed by the larger community."
msgstr ""
"DB2 데이터베이스 지원이 트리에서 제거되었습니다. 이 데이터베이스는 3차-party CI가 없고, 3차-party CI가 없기 때문에 "
"특정 casing을 코드에 유지해야 한다는 제약이 있는 비공식 데이터베이스였습니다. 또한 세ลล스 v2 및 배치 엔진에 대한 온라인 "
"데이터 마이그레이션은 더도 더더욱 어려워졌습니다. 0%의 오픈 스텝 सरveys 사용자들이 사용하는 것을 보고, 더 큰 커뮤니티가 "
"필요하다는 것을 알았기 때문에 이를 제거하여 특성에 집중해야 한다는 것을 결정했습니다."

#: ../source/newton.rst:927
msgid ""
"Delete the deprecated ``glance.host``, ``glance.port``, ``glance.protocol`` "
"configuration options. ``glance.api_servers`` must be set to have a working "
"config. There is currently no default for this config option, so a value "
"must be set."
msgstr ""
"Deprecated ``glance.host``, ``glance.port``, ``glance.protocol`` 설정 옵션을 "
"삭제합니다. ``glance.api_servers`` must be set to have a working config. There is"
" currently no default for this config option, so a value must be set."

#: ../source/newton.rst:931
msgid ""
"Only virt drivers in the nova.virt namespace may be loaded. This has been "
"the case according to nova docs for several releases, but a quirk in some "
"library code meant that loading things outside the namespace continued to "
"work unintentionally. That has been fixed, which means \"compute_driver = "
"nova.virt.foo\" is invalid (and now enforced as such), and should be "
"\"compute_driver = foo\" instead."
msgstr ""
"만약 nova.virt namespace에만 virt 드라이버가 로드될 수 있다면만 로드가 가능하다. nova docs에 따르면 여러 "
"릴리즈 동안 이와 같은 경우가 있었지만, 일부 라이브러리 코드의 오류로 인해 namespace outside에 로드하는 경우도 "
"무의식적으로 작동했다. 이 오류가แก져서, \"compute_driver = nova.virt.foo\"는 무效하고 (이제 "
"enforce가 된다는 것을 의미한다) \"compute_driver = foo\"가 대신 사용되어야 한다."

#: ../source/newton.rst:935
msgid ""
"The default policy for updating volume attachments, commonly referred to as "
"swap volume, has been changed from ``rule:admin_or_owner`` to "
"``rule:admin_api``. This is because it is called from the volume service "
"when migrating volumes, which is an admin-only operation by default, and "
"requires calling an admin-only API in the volume service upon completion. So"
" by default it would not work for non-admins."
msgstr ""
"default policy for updating volume attachments, commonly referred to as swap"
" volume, has been changed from ``rule:admin_or_owner`` to "
"``rule:admin_api``. 이것은 volume service에서 볼륨을 mig리팅하는 동안 볼륨을 호출하는 thing이기 "
"때문에, 기본적으로 admin-only operation이기 때문에, 완료 후 volume service에 admin-only API를 "
"호출해야하는 thing이기 때문에, default로 non-admins에 대해 작동하지 않게되었습니다."

#: ../source/newton.rst:939
msgid ""
"The deprecated osapi_v21.enabled config option has been removed. This "
"previously allowed you a way to disable the v2.1 API. That is no longer "
"something we support, v2.1 is mandatory."
msgstr ""
"osapi_v21.enabled 설정 옵션은 더 이상 사용할 수 없습니다. 이 옵션은 이전에 v2.1 API를 비활성화할 수 있는 방법을"
" 제공했습니다. 그러나 현재는 더 이상 지원하지 않습니다. v2.1 API는 필수적입니다."

#: ../source/newton.rst:943
msgid ""
"Now VMwareVCDriver will set disk.EnableUUID=True by default in all guest VM "
"configuration file. To enable udev to generate /dev/disk/by-id"
msgstr ""
"VMware VCDriver에서부터는 모든 게스트 VM의 구성 파일에서 디스크.EnableUUID=True를 기본적으로 true로 "
"설정합니다. 이로 인해 udev가 /dev/disk/by-id를 생성할 수 있습니다."

#: ../source/newton.rst:953
msgid ""
"All barbican config options in Nova are now deprecated and may be removed as"
" early as 15.0.0 release. All of these options are moved to the Castellan "
"library."
msgstr ""
"다음은 Nova의 Barbican config 옵션은 모두 현재 deprecated되어 15.0.0 버전부터 제거될 수 있습니다. 모든 "
"옵션은 Castellan 라이브러리에서 이동되었습니다."

#: ../source/newton.rst:957
msgid ""
"The cells.driver configuration option is now deprecated and will be removed "
"at Ocata cycle."
msgstr "세ลล(driver) 구성 옵션은 현재 오카타(cycle) 시리즈에서弃용되고 제거될 예정입니다."

#: ../source/newton.rst:962
msgid ""
"The feature to download *Glance* images via file transfer instead of HTTP is"
" now deprecated and may be removed as early as the 15.0.0 release. The "
"config options ``filesystems`` in the section ``image_file_url`` are "
"affected as well as the derived sections ``image_file_url:<list entry "
"name>`` and their config options ``id`` and ``mountpoint``."
msgstr ""
"*Glance* 이미지의 파일 전송 대신 HTTP를 사용하여 다운로드하는 기능은 현재 비고화되었습니다. 15.0.0 릴리스부터 제거할 수"
" 있는 경우가 있습니다. *Glance* 이미지의 config 옵션인 *filesystems*은 또한 *image_file_url* "
"섹션에 영향을 미치며, 그에 따라 *image_file_url:<list entry name>* 섹션의 config 옵션인 *id*와 "
"*mountpoint*도 영향을 받습니다."

#: ../source/newton.rst:966
msgid ""
"As mentioned in the release notes of the Mitaka release (version 13.0.0), "
"the EC2API support was fully removed. The *s3* image service related config "
"options were still there but weren't used anywhere in the code since Mitaka."
" These are now deprecated and may be removed as early as the 15.0.0 release."
" This affects ``image_decryption_dir``, ``s3_host``, ``s3_port``, "
"``s3_access_key``, ``s3_secret_key``, ``s3_use_ssl``, ``s3_affix_tenant``."
msgstr ""
"Mitaka 릴리스 (バージョン 13.0.0)의 릴리스 노트에 설명된 것과 같이 EC2API 지원은 완전히 제거되었다. *s3* 이미지 "
"서비스와 관련된 구성 옵션은 여전히 존재했지만 Mitaka 이후에 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 Mitaka 이후로 사용되지 않았기 때문에 "
"Mitaka 이후"

#: ../source/newton.rst:970
msgid ""
"The ``default_flavor`` config option is now deprecated and may be removed as"
" early as the 15.0.0 release. It is an option which was only relevant for "
"the deprecated EC2 API and is not used in the Nova API."
msgstr ""
"``default_flavor`` 설정 옵션은 현재 버전 15.0.0 이상부터弃기되며, 더 sớm에도弃기될 수 있습니다. 이 설정 옵션은"
" EC2 API의弃기된 API만에 relevance가 있었으며, Nova API에서 사용되지 않습니다."

#: ../source/newton.rst:974
msgid ""
"The ``fatal_exception_format_errors`` config option is now deprecated and "
"may be removed as early as the 15.0.0 release. It is an option which was "
"only relevant for Nova internal testing purposes to ensure that errors in "
"formatted exception messages got detected."
msgstr ""
"``fatal_exception_format_errors`` 설정 옵션은 현재 비고정된 상태이며 15.0.0 릴리스부터 제거될 수 "
"있습니다. 이 설정 옵션은 오직 노바 내부 테스트 용도에만 relevance가 있었으며, 포매팅된 예외 메시지에 오류가 감지되는 것을 "
"보장하기 위해만 사용되었습니다."

#: ../source/newton.rst:978
msgid ""
"The ``image_info_filename_pattern``, ``checksum_base_images``, and "
"``checksum_interval_seconds`` options have been deprecated in the "
"``[libvirt]`` config section. They are no longer used. Any value given will "
"be ignored."
msgstr ""
"``image_info_filename_pattern``, ``checksum_base_images``, 및 "
"``checksum_interval_seconds`` 옵션은 ``[libvirt]`` 구성 섹션에서弃용되었습니다. 더 이상 사용되지 "
"않습니다.ใด든지 주어진 값은 무시됩니다."

#: ../source/newton.rst:982
msgid ""
"The following nova-manage commands are deprecated for removal in the Nova "
"15.0.0 Ocata release:"
msgstr "다음 nova-manage 명령은 Nova 15.0.0 Ocata 릴리스에서 제거할 예정입니다."

#: ../source/newton.rst:985
msgid "nova-maange account scrub"
msgstr "nova-maange 계정 관리"

#: ../source/newton.rst:986
msgid "nova-manage fixed *"
msgstr "nova-manage fixed * → nova-manage fixed * (집합 )"

#: ../source/newton.rst:987
msgid "nova-manage floating *"
msgstr "nova-manage floating * → nova-manage floating * (가용 IP 할당)"

#: ../source/newton.rst:988
msgid "nova-manage network *"
msgstr "nova-manage 네트워크"

#: ../source/newton.rst:989
msgid "nova-manage project scrub"
msgstr "nova-manage 프로젝트 집합 청소"

#: ../source/newton.rst:990
msgid "nova-manage vpn *"
msgstr ""
"nova-manage vpn → nova-manage vpn (가lossary mapping이 없기 때문에 원문으로 유지합니다)"

#: ../source/newton.rst:992
msgid ""
"These commands only work with nova-network which is itself deprecated in "
"favor of Neutron."
msgstr "이 명령어는 nova-network만 작동하며, nova-network 자체는 Neutron에 의해 중단되었습니다."

#: ../source/newton.rst:997
msgid ""
"The ``nova-manage vm list`` command is deprecated and will be removed in the"
" 15.0.0 Ocata release. Use the ``nova list`` command from python-novaclient "
"instead."
msgstr ""
"``nova-manage vm list`` 명령은 15.0.0 Ocata 릴리스에서弃용되며 16.0.0 Pike 릴리스에서 제거됩니다."
"  대신에 파이썬 노바 클라이언트에서 ``nova list`` 명령을 사용하십시오."

#: ../source/newton.rst:1001
msgid ""
"The auth parameters ``admin_username``, ``admin_password``, "
"``admin_tenant_name`` and ``admin_url`` of the [ironic] config option group "
"are now deprecated and will be removed in a future release. Using these "
"parameters will log a warning. Please use ``username``, ``password``, "
"``project_id`` (or ``project_name``) and ``auth_url`` instead. If you are "
"using Keystone v3 API, please note that the name uniqueness for project and "
"user only holds inside the same hierarchy level, so you must also specify "
"domain information for user (i.e. ``user_domain_id`` or "
"``user_domain_name``) and for project, if you are using ``project_name`` "
"(i.e. ``project_domain_id`` or ``project_domain_name``)."
msgstr ""
"ironic 구성 옵션 그룹의 [admin_username] , [admin_password] , [admin_tenant_name] ,"
" [admin_url] parameter은 현재 deprecated되어 향후 릴리스에서 제거될 예정입니다. 이 parameter을 "
"사용하면 경고 로그를 생성합니다. 대신 [username] , [password] , [project_id] (또는 "
"[project_name]) 및 [auth_url]를 사용하세요. Keystone v3 API를 사용하는 경우 프로젝트와 사용자 이름의 "
"유일성은 동일한 계층 수준에서만 적용되므로 사용자 (i.e. [user_domain_id] 또는 [user_domain_name]) 및 "
"프로젝트 (i.e. [project_name]을 사용하는 경우 [project_domain_id] 또는 "
"[project_domain_name])에 대한 도메인 정보를 지정해야 합니다."

#: ../source/newton.rst:1010
msgid ""
"The config option ``snapshot_name_template`` in the ``DEFAULT`` group is now"
" deprecated and may be removed as early as the 15.0.0 release. The code "
"which used this option isn't used anymore since late 2012."
msgstr ""
"`snapshot_name_template` 옵션의 `DEFAULT` 그룹에 있는 `snapshot_name_template` 옵션은 "
"현재 비고해져 있으며 15.0.0 릴리스에서 제거될 수 있습니다. 이 옵션을 사용하는 코드는 2012년 후반부터 사용되지 않습니다."

#: ../source/newton.rst:1016
msgid ""
"The ``nova-all`` binary is deprecated. This was an all in one binary for "
"nova services used for testing in the early days of OpenStack, but was never"
" intended for real use."
msgstr ""
"``nova-all`` 바이너리는弃용되었습니다. 이는 OpenStack의 초기 단계에서 Nova 서비스를 테스트하기 위해 사용되는 모든 "
"일에 포함된 바이너스였지만 실제 사용에 never intended for was."

#: ../source/newton.rst:1020
msgid ""
"Nova network is now deprecated. Based on the results of the current "
"OpenStack User Survey less than 10% of our users remain on Nova network. "
"This is the signal that it is time migrate to Neutron. No new features will "
"be added to Nova network, and bugs will only be fixed on a case by case "
"basis."
msgstr ""
"Nova 네트워크는 현재 deprecated입니다. 현재 OpenStack User Survey의 결과에 따르면, 우리의 사용자 중 "
"10% 이하만 Nova 네트워크를 사용하고 있습니다. 이것은 Neutron으로 migrate하는 시간이到了라는 신호입니다. Nova "
"네트워크에 새로운 기능이 추가되지 않으며, 단일 사례에만 bug를修复합니다."

#: ../source/newton.rst:1024
msgid ""
"The ``/os-certificates`` API is deprecated, as well as the ``nova-cert`` "
"service which powers it. The related config option ``cert_topic`` is also "
"now marked for deprecation and may be removed as early as 15.0.0 Ocata "
"release. This is a vestigial part of the Nova API that existed only for EC2 "
"support, which is now maintained out of tree. It does not interact with any "
"of the rest of nova, and should not just be used as a certificates as a "
"service, which is all it is currently good for."
msgstr ""
"``/os-certificates`` API는 또한 ``nova-cert`` 서비스도 비활성화되었습니다. 관련된 구성 옵션 "
"``cert_topic``도 현재 비활성화되었습니다. 15.0.0 Ocata 릴리스부터 제거될 수 있습니다. 이 Nova API의 일부는"
" EC2 지원에만 존재했으며 현재 트리外에서 유지되고 있습니다. 이 API는 nova의 다른 부분과 상호 작용하지 않으며, 현재는 "
"only certificates as a service로 사용할 수 있습니다."

#: ../source/newton.rst:1028
msgid ""
"All the APIs which proxy to other services were deprecated in this API "
"version. Those APIs will return 404 on Microversion 2.36 or higher. The API "
"user should use native API as instead of using those pure proxy for other "
"REST APIs. The quotas and limits related to network resources 'fixed_ips', "
"'floating ips', 'security_groups', 'security_group_rules', 'networks' are "
"filtered out of os-quotas and limit APIs respectively and those quotas "
"should be managed through OpenStack network service. For using nova-network,"
" you only can use API and manage quotas under Microversion '2.36'. The 'os-"
"fping' API was deprecated also, this API is only related to nova-network and"
" depend on the deployment. The deprecated APIs are as below:"
msgstr ""
"다음 API는 다른 서비스에 대한 프록시를 위해 사용되는 모든 API가 이 API 버전에서弃기되었다. 이 API 버전에서 2.36 이상의"
" Microversion에서 404를 반환하는 API가 있다. API 사용자는 native API를 사용하여 다른 REST API에 "
"대한纯 프록시 대신 native API를 사용해야 한다. 네트워크 자원 'fixed_ips', 'floating ips', "
"'security_groups', 'security_group_rules', 'networks'와 관련된 자원 수 제한은 os-"
"quotas와 limit API에서 필터링되어, OpenStack 네트워크 서비스를 통해 관리해야 한다. nova-network를 "
"사용하려면, 2.36의 Microversion에서만 API를 사용하고, 자원 수 제한을 관리할 수 있다. 또한, os-fping "
"API도弃기되었다. 이 API는 nova-network에만 관련되어, 배포에 따라 사용된다.弃기된 API는 아래와 같다."

#: ../source/newton.rst:1040
msgid "/images"
msgstr "집합"

#: ../source/newton.rst:1041
msgid "/os-networks"
msgstr "집합"

#: ../source/newton.rst:1042
msgid "/os-fixed-ips"
msgstr "/os-정상 IP"

#: ../source/newton.rst:1043
msgid "/os-floating-ips"
msgstr "/os-float-ips"

#: ../source/newton.rst:1044
msgid "/os-floating-ips-bulk"
msgstr "/os-float-ips-bulk"

#: ../source/newton.rst:1045
msgid "/os-floating-ip-pools"
msgstr "/os-floating-ip-pools"

#: ../source/newton.rst:1046
msgid "/os-floating-ip-dns"
msgstr "/os-floating-ip-dns → /os-float-ing-ip-dns"

#: ../source/newton.rst:1047
msgid "/os-security-groups"
msgstr "/os-보안 그룹"

#: ../source/newton.rst:1048
msgid "/os-security-group-rules"
msgstr "/os-보안 그룹 규칙"

#: ../source/newton.rst:1049
msgid "/os-security-group-default-rules"
msgstr "/os-시큐리티 그룹 - 디폴트 규칙"

#: ../source/newton.rst:1050
msgid "/os-volumes"
msgstr "/os-volumes"

#: ../source/newton.rst:1051
msgid "/os-snapshots"
msgstr "집합"

#: ../source/newton.rst:1052
msgid "/os-baremetal-nodes"
msgstr "/o-baremetal-nodes/ → /o Bare Metal 노드/"

#: ../source/newton.rst:1053
msgid "/os-fping"
msgstr "/os-fping"

#: ../source/newton.rst:1057
msgid ""
"Nova option 'use_usb_tablet' will be deprecated in favor of the global "
"'pointer_model'."
msgstr ""
"nova 옵션 'use_usb_tablet'는 global 'pointer_model'를 사용하기 위해 deprecated됩니다."

#: ../source/newton.rst:1061
msgid ""
"The quota_driver configuration option is now deprecated and will be removed "
"in a subsequent release."
msgstr "quotadriver 구성 옵션은 현재 비고정화되었으며 다음 릴리스에서 제거될 예정입니다."

#: ../source/newton.rst:1081
msgid ""
"Corrected response for the case where an invalid status value is passed as a"
" filter to the list servers API call. As there are sufficient statuses "
"defined already, any invalid status should not be accepted. As of "
"microversion 2.38, the API will return 400 HTTPBadRequest if an invalid "
"status is passed to list servers API for both admin as well as non admin "
"user."
msgstr ""
"잘못된 상태 값이 list servers API 호출에 필터로 전달되는 경우에 대한 수정된 응답입니다. 이미 충분한 상태 값이 정의되어 "
"있기 때문에 잘못된 상태 값은 수용되지 shouldn't be accepted. 2.38 microversion부터 API는 400 "
"HTTPBadRequest를 반환합니다. admin 및 non admin 사용자에게 list servers API에 잘못된 상태 값이 "
"전달되면."

#: ../source/newton.rst:1092
msgid ""
"When instantiating an instance based on an image with the metadata "
"hw_vif_multiqueue_enabled=true, if flavor.vcpus is less than the limit of "
"the number of queues on a tap interface in the kernel, nova uses "
"flavor.vcpus as the number of queues. if not, nova uses the limit. The "
"limits are as follows:"
msgstr ""
"hw_vif_multiqueue_enabled=true의 metadata를 사용하여 이미지에 기반을 둔 인스턴스를 인스턴스화할 때, "
"flavor.vcpus가 tap 인터페이스의 큐의 한계 수보다 적을 때, nova는 flavor.vcpus를 큐의 수로 사용합니다. "
"flavor.vcpus가 한계보다 많을 때, nova는 한계를 사용합니다. 한계는 다음과 같습니다."

#: ../source/newton.rst:1098
msgid "kernels prior to 3.0: 1"
msgstr "3.0 이전의 커널: 1"

#: ../source/newton.rst:1099
msgid "kernels 3.x: 8"
msgstr "kernels 3.x: 8"

#: ../source/newton.rst:1100
msgid "kernels 4.x: 256"
msgstr "kernels 4.x: 256"

#: ../source/newton.rst:1118
msgid ""
"The API policy defaults are now defined in code like configuration options. "
"Because of this, the sample policy.json file that is shipped with Nova is "
"empty and should only be necessary if you want to override the API policy "
"from the defaults in the code. To generate the policy file you can run::"
msgstr ""
"API 정책 기본값은 jetzt like configuration options과 같은 코드에서 정의됩니다. 이로 인해 Nova와 함께 "
"배포되는 policy.json 파일은 비어있고, 기본값을 코드에서 override하고 싶다면만 필요합니다. policy 파일을 생성할 "
"수는 다음과 같습니다."

#: ../source/newton.rst:1127
msgid ""
"network_allocate_retries config param now allows only positive integer "
"values or 0."
msgstr ""
"네트워크 할당 시 재시도 수를 구성 매개 변수에 대해 현재는 only positive integer values or 0을 허용합니다."

#: ../source/newton.rst:1132
msgid ""
"The ``api_rate_limit`` configuration option has been removed. The option was"
" disabled by default back in the Havana release since it's effectively "
"broken for more than one API worker. It has been removed because the legacy "
"v2 API code that was using it has also been removed."
msgstr ""
"``api_rate_limit`` 설정 옵션은 제거되었습니다. 옵션은 기본적으로 Havana 릴리스에서부터 disabled되었습니다. "
"이는 하나 이상의 API 워커에 대해 효과적으로 깨져 있습니다. legacy v2 API 코드가 사용하는 옵션을 제거한 것도 이유입니다."

#: ../source/newton.rst:1136
msgid ""
"The default flavors that nova has previously had are no longer created as "
"part of the first database migration. New deployments will need to create "
"appropriate flavors before first use."
msgstr ""
"기본적인 플레버가 nova가 이전에 가지고 existed 하였던 것은 더 이상 첫 번째 데이터베이스 이민화에 포함되지 않아 now. "
"새로운 배포는 첫 번째 사용 전에 적절한 플레버를 생성해야합니다."

#: ../source/newton.rst:1140
msgid ""
"The network configuration option 'fake_call' has been removed. It hasn't "
"been used for several cycles, and has no effect on any code, so there should"
" be no impact."
msgstr ""
"네트워크 구성 옵션 'fake_call'은 제거되었다. 몇 번의 사이클 동안 사용되지 않았으며 어떤 코드에도 영향을 미치지 않기 때문에 "
"영향을 받지 않는다."

#: ../source/newton.rst:1144
msgid ""
"The XenServer configuration option 'iqn_prefix' has been removed. It was not"
" used anywhere and has no effect on any code, so there should be no impact."
msgstr ""
"XenServer 구성 옵션 'iqn_prefix'가 제거되었다. 그것은 nowhere에 사용되지 않았으며 어떤 코드에도 영향을 미치지 "
"않기 때문에 영향을 받지 않는다."

#: ../source/newton.rst:1148
msgid ""
"Virt drivers are no longer loaded with the import_object_ns function, which "
"means that only virt drivers in the nova.virt namespace can be loaded."
msgstr ""
"virt 드라이버는 더 이상 import_object_ns 함수와 함께 로드되지 않으며, nova.virt 네임스페이스에서만 virt "
"드라이버가 로드될 수 있습니다."

#: ../source/newton.rst:1152
msgid ""
"New configuration option sync_power_state_pool_size has been added to set "
"the number of greenthreads available for use to sync power states. Default "
"value (1000) matches the previous implicit default value provided by "
"Greenpool. This option can be used to reduce the number of concurrent "
"requests made to the hypervisor or system with real instance power states "
"for performance reasons."
msgstr ""
"새한 구성 옵션인 sync_power_state_pool_size는 사용자에게 sync power states를 위해 사용할 수 있는 "
"greenthread의 수를 설정할 수 있는 옵션을 추가했습니다. 기본값은 이전에 Greenpool이 제공한 비어있는 기본값과 "
"일치합니다. (1000) 이 옵션은 성능 이유로 인하여 실제 인스턴스 파워 상태를 사용하는 하이퍼바이저 또는 시스템에 동시적으로 요청을 "
"줄이기 위해 사용할 수 있습니다."

#: ../source/ocata.rst:3
msgid "Ocata Series Release Notes"
msgstr "Ocata 시리즈 릴리스 노트"

#: ../source/pike.rst:3
msgid "Pike Series Release Notes"
msgstr "피크 시리즈 릴리스 노트"

#: ../source/queens.rst:3
msgid "Queens Series Release Notes"
msgstr "Queen의 시리즈 릴리스 노트"

#: ../source/rocky.rst:3
msgid "Rocky Series Release Notes"
msgstr ""
"**Rocky Series Release Notes**\n"
"\n"
"집합: **Rocky Series Release Notes**"

#: ../source/stein.rst:3
msgid "Stein Series Release Notes"
msgstr "스틴 시리즈 릴리스 노트"

#: ../source/train.rst:3
msgid "Train Series Release Notes"
msgstr "트레인 시리즈 릴리스 노트"

#: ../source/unreleased.rst:3
msgid "Current Series Release Notes"
msgstr "현재 시리즈 릴리스 노트"

#: ../source/ussuri.rst:3
msgid "Ussuri Series Release Notes"
msgstr "Ussuri Series Release Notes"

#: ../source/victoria.rst:3
msgid "Victoria Series Release Notes"
msgstr "비クト리아 시리즈 릴리스 노트"

#: ../source/wallaby.rst:3
msgid "Wallaby Series Release Notes"
msgstr "Wallaby Series Release Notes"

#: ../source/xena.rst:3
msgid "Xena Series Release Notes"
msgstr "Xena 시리즈 릴리스 노트"

#: ../source/yoga.rst:3
msgid "Yoga Series Release Notes"
msgstr "Yoga Series Release Notes"

#: ../source/zed.rst:3
msgid "Zed Series Release Notes"
msgstr "Zed Series Release Notes"
